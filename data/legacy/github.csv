project_id,urls,contributors,language_breakdown,readme_raw,readme_summarized
0,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_01/,,,"# wiki-capstone
Public repository for DSC 180B senior capstone project exploring racial bias in Oscars and Golden Globes Award Shows.

How to Run:
  This project consists of two parts: data collection and creating visuals. Both ""test-project"" and ""full-project"" targets
  will run both parts. 

  Targets:
  1. ""clean"" : Removes any test data created and removes any visualizations created. Does not remove full data because that is needed for the visualizations.
  2. ""test-project"": Runs the data collection process for only years 1934-1935. It also creates visuals using data from years 1934-2008.
  3. ""full-project"": Runs the data collection process for all years 1934- 2008. It also creates visuals using this data. (Delete ""data/"" folder before running this target)
  
  
","# wiki-capstone
Public repository for DSC 180B senior capstone project exploring racial bias in Oscars and Golden Globes Award Shows.

How to Run:
  This project consists of two parts: data collection and creating visuals. Both ""test-project"" and ""full-project"" targets
  will run both parts. 

  Targets:
  1. ""clean"" : Removes any test data created and removes any visualizations created. Does not remove full data because that is needed for the visualizations.
  2. ""test-project"": Runs the data collection process for only years 1934-1935. It also creates visuals using data from years 1934-2008.
  3. ""full-project"": Runs the data collection process for all years 1934- 2008. It also creates visuals using this data. (Delete ""data/"" folder before running this target)
  
  
"
1,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_02/,,,"# DSC180B Wikipedia Engagement

This project is focused on helping people understand engagement on a wikipedia page through analysis of supply and demand on the page.

## Usage
```
launch-scipy-ml.sh
git clone https://github.com/Jwlin17/DSC180B.git
cd DSC180B
python run.py test-project
```

```
launch-scipy-ml.sh -i jwlin/wiki-engagement
```

## Files

**./config/test-params.json** - holds input values to run test-project command

**./config/env.json** - docker config data

**./config/data-params.json** - directory where data should be output to

**./notebooks/WikiEngagementEDA.ipynb** - EDA analysis for our data set

**./src/engagement_score.py** - contains relevant functions for creating the engagement score

**./src/wikiparser.py** - contains relevant functions for parsing wikipedia dump files

**./website_data/article_titles.txt** - contains article titles to populate search bar

**./website_data/content_score.png** - content score formula

**./website_data/editor_score.png** - editor score formula

**./website_data/engagement_scores.json** - engagement scores to graph on website

**./index.html** - website html

**./requirements.txt** - required packages

**./run.py** - call run.py to run data analysis

## Output Files

**./data/raw/zips** - holds all the zipped files from wikidump / lightdump

**./data/raw/extracted** - holds all the extracted zip files

**./data/temp** - holds the created lightdump data parsed from en-wiki dump

**./data/out** - contains the .png charts of mscore over time and csv of article title / mscore

## Sources

Link to my writeup report: https://docs.google.com/document/d/17lQ96NeAWvI133FgFkBsB234vdQLecg9g2wfLc_Y79o/edit?usp=sharing

Wiki-Media Dumps I Used: https://dumps.wikimedia.org/enwiki/20200101/

Wiki-Monitor Lightdump information: http://wwm.phy.bme.hu/

Paper that I replicated: https://arxiv.org/pdf/1107.3689.pdf

Alternative methods of studying controversy: https://www.opensym.org/ws2012/p18wikisym2012.pdf
","# DSC180B Wikipedia Engagement

This project is focused on helping people understand engagement on a wikipedia page through analysis of supply and demand on the page.

## Usage
```
launch-scipy-ml.sh
git clone https://github.com/Jwlin17/DSC180B.git
cd DSC180B
python run.py test-project
```

```
launch-scipy-ml.sh -i jwlin/wiki-engagement
```

## Files

**./config/test-params.json** - holds input values to run test-project command

**./config/env.json** - docker config data

**./config/data-params.json** - directory where data should be output to

**./notebooks/WikiEngagementEDA.ipynb** - EDA analysis for our data set

**./src/engagement_score.py** - contains relevant functions for creating the engagement score

**./src/wikiparser.py** - contains relevant functions for parsing wikipedia dump files

**./website_data/article_titles.txt** - contains article titles to populate search bar

**./website_data/content_score.png** - content score formula

**./website_data/editor_score.png** - editor score formula

**./website_data/engagement_scores.json** - engagement scores to graph on website

**./index.html** - website html

**./requirements.txt** - required packages

**./run.py** - call run.py to run data analysis

## Output Files

**./data/raw/zips** - holds all the zipped files from wikidump / lightdump

**./data/raw/extracted** - holds all the extracted zip files

**./data/temp** - holds the created lightdump data parsed from en-wiki dump

**./data/out** - contains the .png charts of mscore over time and csv of article title / mscore

## Sources

Link to my writeup report: https://docs.google.com/document/d/17lQ96NeAWvI133FgFkBsB234vdQLecg9g2wfLc_Y79o/edit?usp=sharing

Wiki-Media Dumps I Used: https://dumps.wikimedia.org/enwiki/20200101/

Wiki-Monitor Lightdump information: http://wwm.phy.bme.hu/

Paper that I replicated: https://arxiv.org/pdf/1107.3689.pdf

Alternative methods of studying controversy: https://www.opensym.org/ws2012/p18wikisym2012.pdf
"
2,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_03/,,,"# DSC180B Final Project: Investigating the Biaseness of Wikipedia and the Media in the Scope of COVID-19
Which is the most unbiased source of COVID-19 information: A analysis of 20 News agency and Wikipedia

## Overview

COVID-19 has gained increasing attention since its breakout in China in the early 2020. At this stage where full understanding of the virus is still developing, information related to COVID-19 in the media coverage may sometimes be misleading. There is diverging, even self-conflicting, news coverage from newspapers. It is becoming increasingly difficult to find out to figure out which statements being thrown at us are facts that we should be listening to or simple just rumors we should ignore. Wikipedia viewing and editing data is a record of people’s information-seeking and engagement behavior that could be used as an unbiased indicator of public opinion. It might reveal what people actually learn from the media coverage and how people react to the COVID-19. On the other hand, it could be seen as a platform where “random people” get to contribute and therefore making it a biased and untrustworthy source. We defined the trustworthiness of a source through a topic distribution matrix. Since we can divide the broad topic of COVID-19 into sub topics such as origins of the virus, symptoms, economic implications, how it spreads etc, we will measure how trustworthy a source is by how many of these subtopics it covers. Specifically, we wanted to answer the question, “How biased are certain popular news outlets in how they cover the effects and the spread of COVID-19?""

## Usage

Our project was implemented with a combination of python and R. We use python for data ingesting, cleaning, and transformation. We use R for running the STM model since the package is R-exclusive. And finally, Python for final measurements and visualization.

The project pipeline could be run by:
```
launch-scipy-ml.sh
git clone git@github.com:SchootHuang/DSC180B-Coronavirus-Wikipedia.git
cd DSC180B
python run.py #USER-SPECIFIED-TARGET#
```

For demoing purpuse:
1. Please refer to EDA_news.ipynb for the EDA process of news dataset

2. please refer to 2020-COVID.R for topic modeling in R.

3. Finally, refer to the visualization.ipynb notebook for visualization, and Biaseness_measurement.ipynb for the biaseness evaluation with Wasserstain distance and Forbenius Norm distance.


## Resources

Link to our Project Website: https://schoothuang.github.io/DSC180B-Coronavirus-Wikipedia/ 

Wiki-Media Dumps dataset Used: https://dumps.wikimedia.org/enwiki/20200101/

Wiki-Monitor Lightdump information: http://wwm.phy.bme.hu/

All the News 2.0 dataset: https://components.one/datasets/all-the-news-2-news-articles-dataset/ 
","# DSC180B Final Project: Investigating the Biaseness of Wikipedia and the Media in the Scope of COVID-19
Which is the most unbiased source of COVID-19 information: A analysis of 20 News agency and Wikipedia

## Overview

COVID-19 has gained increasing attention since its breakout in China in the early 2020. At this stage where full understanding of the virus is still developing, information related to COVID-19 in the media coverage may sometimes be misleading. There is diverging, even self-conflicting, news coverage from newspapers. It is becoming increasingly difficult to find out to figure out which statements being thrown at us are facts that we should be listening to or simple just rumors we should ignore. Wikipedia viewing and editing data is a record of people’s information-seeking and engagement behavior that could be used as an unbiased indicator of public opinion. It might reveal what people actually learn from the media coverage and how people react to the COVID-19. On the other hand, it could be seen as a platform where “random people” get to contribute and therefore making it a biased and untrustworthy source. We defined the trustworthiness of a source through a topic distribution matrix. Since we can divide the broad topic of COVID-19 into sub topics such as origins of the virus, symptoms, economic implications, how it spreads etc, we will measure how trustworthy a source is by how many of these subtopics it covers. Specifically, we wanted to answer the question, “How biased are certain popular news outlets in how they cover the effects and the spread of COVID-19?""

## Usage

Our project was implemented with a combination of python and R. We use python for data ingesting, cleaning, and transformation. We use R for running the STM model since the package is R-exclusive. And finally, Python for final measurements and visualization.

The project pipeline could be run by:
```
launch-scipy-ml.sh
git clone git@github.com:SchootHuang/DSC180B-Coronavirus-Wikipedia.git
cd DSC180B
python run.py #USER-SPECIFIED-TARGET#
```

For demoing purpuse:
1. Please refer to EDA_news.ipynb for the EDA process of news dataset

2. please refer to 2020-COVID.R for topic modeling in R.

3. Finally, refer to the visualization.ipynb notebook for visualization, and Biaseness_measurement.ipynb for the biaseness evaluation with Wasserstain distance and Forbenius Norm distance.


## Resources

Link to our Project Website: https://schoothuang.github.io/DSC180B-Coronavirus-Wikipedia/ 

Wiki-Media Dumps dataset Used: https://dumps.wikimedia.org/enwiki/20200101/

Wiki-Monitor Lightdump information: http://wwm.phy.bme.hu/

All the News 2.0 dataset: https://components.one/datasets/all-the-news-2-news-articles-dataset/ 
"
3,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_04/,,,"# Name That Raga: Classification and Analysis of Indian Classical Music

## Background
Indian Classical music contains two primary divisions - North Indian Classical (_Hindustani_), and South Indian Classical (_Carnatic_). While both styles have their fundamental differences, the underlying structure of styles can be captured in a _raga/raag/ragam_. A raga is defined as “a pattern of notes having characteristic intervals, rhythms, and embellishments, used as a basis for improvisation.” A raga can be compared to a type of scale in Western classical music. Though Western classical music does not have a direct equivalent to this concept, a raga is somewhat comparable to certain scales, such as a natural harmonic minor or a major scale. Every song has a raga that it is set to. For example, the raga _Kirvani_ is equivalent to the natural harmonic scale in Western music.

Our goal with this project is to quantify ragas in a way that we can then build a raga identification tool. This tool would be able to “listen” to an audio clip and be able to identify the raga that the song is set to. It would mimic what seasoned listeners of Indian classical music do already: try to identify a raga while listening to music. By quantifying the features of a ragam, we will attempt to build a raga identification classifier. 

To manage the scope of this project with the time we have, we will build this tool to be functional for 10 prominent Hindustani/Carnatic ragas. These 10 prominent ragas are known as _thaats_. In Hindustani music, these are known as: _Asavari, Bilawal, Bhairav, Bhairavi, Kafi, Kalyan, Khamaj, Marva, Poorvi,_ and _Todi_. The corresponding ragas in Carnatic are known as _Natabhairavi, Dheerashankarabharanam, Mayamalavagowlai, Hanumatodi, Karaharapriya, Kalyani, Harikhamboji, Gamanashama, Kamarvardhani_, and _Shubhapantuvarali_. 


## How To Run: Example

Our repository includes a small amount of test data that you can run this pipeline on and obtain results. 

1. Clone this repository to have a local copy of these files.
2. On the command line, navigate to this repository locally 
3. The command *python run.py test-project* runs the pipeline with the test-project target. This will load, clean, extract features, and build a model with the small amount of data included in the *testdata_raw* folder. 
4. You should now have a csv for loaded data, cleaned data, and the model that was built. 

## How To Run: Classification of Your Own Audio Files

1. Clone this repository. 
2. On the command line, navigate to this repository locally. 
3. Add your own data to *data/raw*
4. On the command line, the command *python run.py full-project* runs the pipeline with the full-project target. This will load, clean, extract features, and build a model with the data that you have included in the *data/raw* folder.
5. You should now have a csv for loaded data, cleaned data, and the model that was built.
","# Name That Raga: Classification and Analysis of Indian Classical Music

## Background
Indian Classical music contains two primary divisions - North Indian Classical (_Hindustani_), and South Indian Classical (_Carnatic_). While both styles have their fundamental differences, the underlying structure of styles can be captured in a _raga/raag/ragam_. A raga is defined as “a pattern of notes having characteristic intervals, rhythms, and embellishments, used as a basis for improvisation.” A raga can be compared to a type of scale in Western classical music. Though Western classical music does not have a direct equivalent to this concept, a raga is somewhat comparable to certain scales, such as a natural harmonic minor or a major scale. Every song has a raga that it is set to. For example, the raga _Kirvani_ is equivalent to the natural harmonic scale in Western music.

Our goal with this project is to quantify ragas in a way that we can then build a raga identification tool. This tool would be able to “listen” to an audio clip and be able to identify the raga that the song is set to. It would mimic what seasoned listeners of Indian classical music do already: try to identify a raga while listening to music. By quantifying the features of a ragam, we will attempt to build a raga identification classifier. 

To manage the scope of this project with the time we have, we will build this tool to be functional for 10 prominent Hindustani/Carnatic ragas. These 10 prominent ragas are known as _thaats_. In Hindustani music, these are known as: _Asavari, Bilawal, Bhairav, Bhairavi, Kafi, Kalyan, Khamaj, Marva, Poorvi,_ and _Todi_. The corresponding ragas in Carnatic are known as _Natabhairavi, Dheerashankarabharanam, Mayamalavagowlai, Hanumatodi, Karaharapriya, Kalyani, Harikhamboji, Gamanashama, Kamarvardhani_, and _Shubhapantuvarali_. 


## How To Run: Example

Our repository includes a small amount of test data that you can run this pipeline on and obtain results. 

1. Clone this repository to have a local copy of these files.
2. On the command line, navigate to this repository locally 
3. The command *python run.py test-project* runs the pipeline with the test-project target. This will load, clean, extract features, and build a model with the small amount of data included in the *testdata_raw* folder. 
4. You should now have a csv for loaded data, cleaned data, and the model that was built. 

## How To Run: Classification of Your Own Audio Files

1. Clone this repository. 
2. On the command line, navigate to this repository locally. 
3. Add your own data to *data/raw*
4. On the command line, the command *python run.py full-project* runs the pipeline with the full-project target. This will load, clean, extract features, and build a model with the data that you have included in the *data/raw* folder.
5. You should now have a csv for loaded data, cleaned data, and the model that was built.
"
4,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_05/,,,"<p align=""center"">
    <!-- <b style=""font-size: 45px;"">Red Means Go</b><br> -->
    <img width=""614"" height=""345"" src=""https://raw.githubusercontent.com/codencoding/Red-Means-Go/gh-pages/images/site-logo.png"">
</p>

## Abstract
YouTube has become a significant source of income for many content creators, and they are always looking for the best way to grow their channel. When a YouTube video gets more views, the Content Creator makes more money. The purpose of our project is to analyze the significance of the various features of a YouTube preview thumbnail that can contribute to a video’s success. We believe that there are features in YouTube thumbnails that can be extracted and used to identify what makes a video more appealing to potential viewers. 

## Introduction
Our research question is what YouTube thumbnail features, if any, have an effect on the amount of views that the video gets. Our hypothesis is that the more popular videos will likely have more provocative thumbnails that grab that attention of viewers. We believe that popular thumbnails will have more contrast, brightness, engaging subject matter, or just general attractiveness to potential viewers. 
We used YouTube’s Data API (v3) to create a data set that’s sourced from a YouTube search for “Fortnite”. We scrape the first 200 query results, and then scrape the first 100 videos from each unique channel, resulting in around 10,000 videos. Then we create a gamut of statistics from the metadata to best assess how well the video is doing according to YouTube. Because our data is from YouTube, which is an ever changing ecosystem, our results are only indicative of the data we scraped on April 16th, 2020. The thumbnails were created by YouTube content creators and/ or their thumbnail designers, while video metadata was provided by YouTube.
We think our analysis is an interesting investigation because YouTube is a very prominent cultural influence, and so being able to better attract a larger audience would be of general interest to anyone pursuing a YouTube-based career. Because of the recent monetization of YouTube on a large scale, more and more people are trying to make a living off of YouTube. We hope that our results will help give any potential YouTuber insight into what features are most relevant in a thumbnail. In addition, as avid YouTube viewers, we have an intuition that thumbnails play a factor in YouTube’s recommendation algorithm. We hoped to learn more about this rather vague recommendation system and figure out what factors help in a video’s success, if any.

## Methods
The features we will use to address our question are a combination of metadata and image features. For the metadata features, we use the view count of the YouTube video as well as a z-scored view count, calculated by taking fortnite videos from the same channel within the same month, that way we can compare channel to channel through the z-scores. For our image features, we are using image brightness, saturation, hue, unique_rgb_ratio (number of unique rgb values divided by the number of pixels), and the number of faces present (using DeepFace). For our metadata features, we used the YouTube Data API (v3) to get our metadata such as views. For the image features, we used the skimage library to extract image brightness, saturation, hue, and rgb values. For face recognition, we used DeepFace from DLib. We  computed our own extracted features such as unique_rgb_ratio and z-score views. 
The analytical techniques we are using are as follows. For initial eda, we computed the correlation between each image feature column and the z_views column. This way we can see if there is any correlation between a specific image feature and the amount of views the video got (relative to similar videos from the same channel). For a deeper analysis, we wanted to combine images features to see if a certain combination of image features would attract more viewers. To do this, we first tried feeding all image feature columns into a random forest regressor and gradient boosted regressor. Then we plotted the predicted values vs. the real values to see any patterns in the predictions. We also multiplied sets of two/three/four features together to make higher level features, then trained a linear regression model with these features. We also looked at the predictions for this model to see any patterns in the predictions. We did not use a neural net to generate features for regression as the generated high level features are not clear enough to make a distinction between which image features have impact on the video views.

## Results
The results of our deep dive into how image features of thumbnails relate to video views are that there is no strong correlation between our thumbnail image features and video views. To show this, we’ve selected a couple visualizations to help see these results.

<p align=""center"">
    <img width=""614"" height=""345"" src=""https://raw.githubusercontent.com/codencoding/Red-Means-Go/gh-pages/images/fig1.png"">
</p>

<p align=""center"">
    <small> Figure (1): Random Forest Regression on the z-score for video views </small>
</p>

<p align=""center"">
    <img width=""614"" height=""345"" src=""https://raw.githubusercontent.com/codencoding/Red-Means-Go/gh-pages/images/fig2.png"">
</p>

<p align=""center"">
    <small> Figure (2): Gradient Boosted Regression on the z-score for video views </small>
</p>

The first two charts we chose to include are scatterplots of the predicted z_views vs the actual z_views. These predictions were obtained by training a random forest regressor and a gradient boosted regressor on the numerical image features gathered from our image processing ('unique_rgb_ratio','mean_hue', 'mean_saturation','mean_brightness', 'contrast', 'edge_score', numFaces’). The purpose of this graph is to show the lack of correlation between the predictions and the real values, which is shown by the score (coefficient of determination R<sup>2</sup>) for each being close to 0. A score of 0 is achieved by always predicting the mean value of z_views, making these regressors worse than the most naive approach. This shows the lack of relationship between the image features we used and the value of z_views that the video gets.

<p align=""center"">
    <img width=""640"" height=""360"" src=""https://raw.githubusercontent.com/codencoding/Red-Means-Go/gh-pages/images/fig3.png"">
</p>

<p align=""center"">
    <small> Figure (3): Thumbnail image statistics </small>
</p>

The third visualization we chose to include is a combination of  bar charts comparing the good performance videos (z_views > 1) and the poor performance videos (z_views < -1). We plot the values of the standard descriptive statistics to give a summary of values for the selected numerical image features. The adjacent bars allow for easy comparison for the different subsets of videos. This represents a conditional subset approach we used. By splitting up the data and looking at videos that did “well” and videos that did “poorly” , we are able to see some promising differences in thumbnail image features. Looking at the image features “unique_rgb_ratio” and “mean_hue”, we see consistent higher values for good videos than bad videos. This sheds light on the theory that more colorful thumbnails see greater success. We also see consistent differences in “contrast” and “edge_score”, this time the poorly performing videos having higher values. This alludes to “busier” thumbnails seeing less success. However, these differences are slight and not statistically significant, but we have hope that these features will direct some future analysis. 

## Discussion
For all models, the predictions scored worse than predicting the mean for all values, indicating no such patterns exists, alluding to a lack of correlation between our image features and video views. The score for each model is either negative or very close to 0. According to SKLearn’s documentation, a score of 0 would be achieved by predicting the mean of the target column for all values. Because our models scored around 0 while trained on thumbnail features, we cannot say that there is any significant correlation between our thumbnail features and video views. We think this result is likely due to the relative importance of the thumbnail to the content of the video. We additionally looked at the correlation of the individual image features and video views, and found no significant correlation.

Since there are so many aspects that go into whether someone watches the video such as title, thumbnail, video duration, and most importantly, the contents of the video, it makes sense that there would not be a strong correlation between thumbnail image features and video views. It is also worth noting that these results are specifically for Fortnite Gaming videos uploaded in March/April 2020, and our image features were relatively basic. If the scope of this project was larger, we could look at more genres and more videos per genre, along with more advanced image features. We still think that video thumbnails affect video views, but we lack the quantifiable results to say so. 

With the growth of social media platforms such as YouTube, the job title of ‘content creator’ has become a more common and financially viable occupation. As such, our work on YouTube thumbnails will help creators put numbers to the trends they inherently sense in the ever changing YouTube thumbnail meta, allowing them to make changes to their thumbnails with less guesswork and backed with more relevant data. Besides the impact on those creating thumbnails, our work also explores how audiences of specific genres of videos react to different types of thumbnails because thumbnails are the front cover of a video and holds the potential to attract millions of users who aren’t already followers.

Our approach takes advantage of the digital format as we were able to scrape and process mass amounts of data which wouldn’t have been easy to do manually which enabled us to acquire and work with a substantially larger dataset. Since we parameterized our work, it is very flexible and can be configured to analyze different genres of Youtube videos which could be useful for a Youtuber as it would provide them with a snapshot of the current YouTube thumbnail meta and act as a soft guide when they are making their own.
We could expand our project scope in the future by looking at other video games in the YouTube Gaming category or even other YouTube categories (such as make-up videos or VLOGS). Another direction is creating a live thumbnail meta analysis. We suspect that certain features in thumbnails rise and fall in popularity similar to fashion, so having a live trend analysis of thumbnails could prove useful. For instance, if faces in the thumbnail start becoming less popular, YouTubers might want to stay away from putting their face on a thumbnail. However, they might use the thumbnail meta analysis as a way to go against the meta which would make their thumbnails stick out. 

## Getting Started / Extending this Project

### Running the Pipeline

To run the main pipeline, run the command “python run.py” in the repository’s root directory. To run this command you also need a youtube API key. The key can be obtained by following this guide: (https://developers.google.com/youtube/v3/getting-started). Once obtained, you need to create a file named “api_key.json” in the root directory of the repo. It needs to have one key called “api_keys”, which is a list of API keys as strings. You can include as many keys as you’d like to gather more datasets, as the youtube API has a daily limit. Additionally, the command “python run.py test-project” can be run, which runs the analysis on a curated test dataset. This is good for a quick option that does not require fiddling with configuration parameters or creating an API key. 

### Output Files

When running this pipeline locally, 5 output files will be generated in the “data/local/*selected-game*/video_data/” directory. The first file is the “scrape_MM_DD_YY.json” file, which represents a dataset of video_ids returned by the search result, and for each video_id, includes the specified number of video_ids from that channel that also match the *selected-game*. The second file is “*selected-game*_full_features_MM_DD_YY.csv”, which represents the dataset of metadata features, basic image features, and advanced image features for all videos in the dataset created on the date in the filename. The third file is “*selected-game*_full_metadata_MM_DD_YY.csv”, which contains only the metadata features for the dataset gathered on the specified date. The fourth file is “*selected-game*_requests.json”, which contains the massive amount of data returned by the youtube api for each video_id. This is constantly updated for any new videos that are scraped, which is why it does not have a date in the filename. The last file is “*selected-game*_summary_metadata_MM_DD_YY.csv”, this contains just the metadata for only the videos that appear in the search results, and not the videos within the channels that are contained in “*selected-game*_full_features_MM_DD_YY.csv”.


### Further Analysis

Once the pipeline has been run, further analysis of the data gathered can be found in the notebook “notebooks/eda/cwynne_combined_analysis.ipynb”. In the third cell of the notebook, specify which dataset that the analysis should be done on, usually selecting one of the generated output files mentioned above. The variable “data_path” should be set to the desired dataset. The target columns can also be changed in this cell, although it is recommended that the defaults are used. 


### Configuration Parameters
The run.py script will access the configuration file stored in “config/config-scraping.json”. This json file contains different parameters that allow you to change what data is scraped / analyzed. Below are the parameters in further detail.


- selected-game: this  key represents what keyword will be used to search for videos. This does not have to be a game, but can be any keyword of your choosing. 
- thumbnail-qual: This key holds a dictionary which can be thought of as a switch, put a 1 in any quality that you want a dataset of thumbnails for. 
- test-videos-dir: This is the filepath of the test video folder, which contains files such as the video ids and metadata of the test subset.
- test-thumbs-dir: This is the filepath of the directory of thumbnails for the test dataset.
- api-service-name: This generally should not change, if the youtube api changes this argument, then this should be changed. 
- api-version: This is the version of the API, currently the most updated version is “v3”, but this could be changed to use later or earlier versions, however the code has not been tested on different api versions.
- videos-dir: This is the location of the currently scraped video data directory. Files in this folder are scraped based on the keyword and include the csv of video ids in the dataset and metadata information. This is only a local directory and is used for anything that is not the test dataset
- full-metadata-csv-write-path: This is the filepath to write the metadata for all videos that were scraped. This includes the search result videos and the specified number of videos per channel by the key “videos-per-channel”).
- summary-metadata-csv-write-path: This is the filepath to write only the initial search result videos and not the extra channel videos. 
- requests-dic-read-path: This is the file path for a local json of request data that could exist from a previous run of this pipeline. If nothing is provided it will create a new file using “requests-dic-write-path” that can be used for reruns of the pipeline. This is to prevent re-requesting the same info for videos and making unnecessary API calls.
- requests-dic-write-path: This is the path where the requests data json file is written for quicker reruns or saving of new requests data. 
- num-recent-videos: This is the number of search results that will be gathered based on the “selected-game” keyword being searched with the youtube api. 
- videos-per-channel: This is the videos per channel that are scraped, based on the different channels that uploaded videos in the search result for our keyword.
- scrape-write-dir: This is the directory that all of our scraped video data gets written to. This includes the dataset of video ids, and the metadata dataset for these videos. This changes based on the “selected-game” keyword. 
- full-features-write-name: This is the name of the file that contains metadata, basic image features, and advanced image features. It is stored in the directory specified by “videos-dir”
- overwrite: if set to true, it will overwrite any previously scraped data that was scraped on the same calendar date. If set to false, it will simply add a number to the end of the new requests data file. 


## References
Louise Myers 2019, accessed April 6, 2020, 

[https://louisem.com/198803/how-to-youtube-thumbnails](https://louisem.com/198803/how-to-youtube-thumbnails)

EmpLemon 2020, accessed May 4, 2020,

[https://www.youtube.com/watch?v=-6-i75wDIBE](https://www.youtube.com/watch?v=-6-i75wDIBE)
","<p align=""center"">
    <!-- <b style=""font-size: 45px;"">Red Means Go</b><br> -->
    <img width=""614"" height=""345"" src=""https://raw.githubusercontent.com/codencoding/Red-Means-Go/gh-pages/images/site-logo.png"">
</p>

## Abstract
YouTube has become a significant source of income for many content creators, and they are always looking for the best way to grow their channel. When a YouTube video gets more views, the Content Creator makes more money. The purpose of our project is to analyze the significance of the various features of a YouTube preview thumbnail that can contribute to a video’s success. We believe that there are features in YouTube thumbnails that can be extracted and used to identify what makes a video more appealing to potential viewers. 

## Introduction
Our research question is what YouTube thumbnail features, if any, have an effect on the amount of views that the video gets. Our hypothesis is that the more popular videos will likely have more provocative thumbnails that grab that attention of viewers. We believe that popular thumbnails will have more contrast, brightness, engaging subject matter, or just general attractiveness to potential viewers. 
We used YouTube’s Data API (v3) to create a data set that’s sourced from a YouTube search for “Fortnite”. We scrape the first 200 query results, and then scrape the first 100 videos from each unique channel, resulting in around 10,000 videos. Then we create a gamut of statistics from the metadata to best assess how well the video is doing according to YouTube. Because our data is from YouTube, which is an ever changing ecosystem, our results are only indicative of the data we scraped on April 16th, 2020. The thumbnails were created by YouTube content creators and/ or their thumbnail designers, while video metadata was provided by YouTube.
We think our analysis is an interesting investigation because YouTube is a very prominent cultural influence, and so being able to better attract a larger audience would be of general interest to anyone pursuing a YouTube-based career. Because of the recent monetization of YouTube on a large scale, more and more people are trying to make a living off of YouTube. We hope that our results will help give any potential YouTuber insight into what features are most relevant in a thumbnail. In addition, as avid YouTube viewers, we have an intuition that thumbnails play a factor in YouTube’s recommendation algorithm. We hoped to learn more about this rather vague recommendation system and figure out what factors help in a video’s success, if any.

## Methods
The features we will use to address our question are a combination of metadata and image features. For the metadata features, we use the view count of the YouTube video as well as a z-scored view count, calculated by taking fortnite videos from the same channel within the same month, that way we can compare channel to channel through the z-scores. For our image features, we are using image brightness, saturation, hue, unique_rgb_ratio (number of unique rgb values divided by the number of pixels), and the number of faces present (using DeepFace). For our metadata features, we used the YouTube Data API (v3) to get our metadata such as views. For the image features, we used the skimage library to extract image brightness, saturation, hue, and rgb values. For face recognition, we used DeepFace from DLib. We  computed our own extracted features such as unique_rgb_ratio and z-score views. 
The analytical techniques we are using are as follows. For initial eda, we computed the correlation between each image feature column and the z_views column. This way we can see if there is any correlation between a specific image feature and the amount of views the video got (relative to similar videos from the same channel). For a deeper analysis, we wanted to combine images features to see if a certain combination of image features would attract more viewers. To do this, we first tried feeding all image feature columns into a random forest regressor and gradient boosted regressor. Then we plotted the predicted values vs. the real values to see any patterns in the predictions. We also multiplied sets of two/three/four features together to make higher level features, then trained a linear regression model with these features. We also looked at the predictions for this model to see any patterns in the predictions. We did not use a neural net to generate features for regression as the generated high level features are not clear enough to make a distinction between which image features have impact on the video views.

## Results
The results of our deep dive into how image features of thumbnails relate to video views are that there is no strong correlation between our thumbnail image features and video views. To show this, we’ve selected a couple visualizations to help see these results.

<p align=""center"">
    <img width=""614"" height=""345"" src=""https://raw.githubusercontent.com/codencoding/Red-Means-Go/gh-pages/images/fig1.png"">
</p>

<p align=""center"">
    <small> Figure (1): Random Forest Regression on the z-score for video views </small>
</p>

<p align=""center"">
    <img width=""614"" height=""345"" src=""https://raw.githubusercontent.com/codencoding/Red-Means-Go/gh-pages/images/fig2.png"">
</p>

<p align=""center"">
    <small> Figure (2): Gradient Boosted Regression on the z-score for video views </small>
</p>

The first two charts we chose to include are scatterplots of the predicted z_views vs the actual z_views. These predictions were obtained by training a random forest regressor and a gradient boosted regressor on the numerical image features gathered from our image processing ('unique_rgb_ratio','mean_hue', 'mean_saturation','mean_brightness', 'contrast', 'edge_score', numFaces’). The purpose of this graph is to show the lack of correlation between the predictions and the real values, which is shown by the score (coefficient of determination R<sup>2</sup>) for each being close to 0. A score of 0 is achieved by always predicting the mean value of z_views, making these regressors worse than the most naive approach. This shows the lack of relationship between the image features we used and the value of z_views that the video gets.

<p align=""center"">
    <img width=""640"" height=""360"" src=""https://raw.githubusercontent.com/codencoding/Red-Means-Go/gh-pages/images/fig3.png"">
</p>

<p align=""center"">
    <small> Figure (3): Thumbnail image statistics </small>
</p>

The third visualization we chose to include is a combination of  bar charts comparing the good performance videos (z_views > 1) and the poor performance videos (z_views < -1). We plot the values of the standard descriptive statistics to give a summary of values for the selected numerical image features. The adjacent bars allow for easy comparison for the different subsets of videos. This represents a conditional subset approach we used. By splitting up the data and looking at videos that did “well” and videos that did “poorly” , we are able to see some promising differences in thumbnail image features. Looking at the image features “unique_rgb_ratio” and “mean_hue”, we see consistent higher values for good videos than bad videos. This sheds light on the theory that more colorful thumbnails see greater success. We also see consistent differences in “contrast” and “edge_score”, this time the poorly performing videos having higher values. This alludes to “busier” thumbnails seeing less success. However, these differences are slight and not statistically significant, but we have hope that these features will direct some future analysis. 

## Discussion
For all models, the predictions scored worse than predicting the mean for all values, indicating no such patterns exists, alluding to a lack of correlation between our image features and video views. The score for each model is either negative or very close to 0. According to SKLearn’s documentation, a score of 0 would be achieved by predicting the mean of the target column for all values. Because our models scored around 0 while trained on thumbnail features, we cannot say that there is any significant correlation between our thumbnail features and video views. We think this result is likely due to the relative importance of the thumbnail to the content of the video. We additionally looked at the correlation of the individual image features and video views, and found no significant correlation.

Since there are so many aspects that go into whether someone watches the video such as title, thumbnail, video duration, and most importantly, the contents of the video, it makes sense that there would not be a strong correlation between thumbnail image features and video views. It is also worth noting that these results are specifically for Fortnite Gaming videos uploaded in March/April 2020, and our image features were relatively basic. If the scope of this project was larger, we could look at more genres and more videos per genre, along with more advanced image features. We still think that video thumbnails affect video views, but we lack the quantifiable results to say so. 

With the growth of social media platforms such as YouTube, the job title of ‘content creator’ has become a more common and financially viable occupation. As such, our work on YouTube thumbnails will help creators put numbers to the trends they inherently sense in the ever changing YouTube thumbnail meta, allowing them to make changes to their thumbnails with less guesswork and backed with more relevant data. Besides the impact on those creating thumbnails, our work also explores how audiences of specific genres of videos react to different types of thumbnails because thumbnails are the front cover of a video and holds the potential to attract millions of users who aren’t already followers.

Our approach takes advantage of the digital format as we were able to scrape and process mass amounts of data which wouldn’t have been easy to do manually which enabled us to acquire and work with a substantially larger dataset. Since we parameterized our work, it is very flexible and can be configured to analyze different genres of Youtube videos which could be useful for a Youtuber as it would provide them with a snapshot of the current YouTube thumbnail meta and act as a soft guide when they are making their own.
We could expand our project scope in the future by looking at other video games in the YouTube Gaming category or even other YouTube categories (such as make-up videos or VLOGS). Another direction is creating a live thumbnail meta analysis. We suspect that certain features in thumbnails rise and fall in popularity similar to fashion, so having a live trend analysis of thumbnails could prove useful. For instance, if faces in the thumbnail start becoming less popular, YouTubers might want to stay away from putting their face on a thumbnail. However, they might use the thumbnail meta analysis as a way to go against the meta which would make their thumbnails stick out. 

## Getting Started / Extending this Project

### Running the Pipeline

To run the main pipeline, run the command “python run.py” in the repository’s root directory. To run this command you also need a youtube API key. The key can be obtained by following this guide: (https://developers.google.com/youtube/v3/getting-started). Once obtained, you need to create a file named “api_key.json” in the root directory of the repo. It needs to have one key called “api_keys”, which is a list of API keys as strings. You can include as many keys as you’d like to gather more datasets, as the youtube API has a daily limit. Additionally, the command “python run.py test-project” can be run, which runs the analysis on a curated test dataset. This is good for a quick option that does not require fiddling with configuration parameters or creating an API key. 

### Output Files

When running this pipeline locally, 5 output files will be generated in the “data/local/*selected-game*/video_data/” directory. The first file is the “scrape_MM_DD_YY.json” file, which represents a dataset of video_ids returned by the search result, and for each video_id, includes the specified number of video_ids from that channel that also match the *selected-game*. The second file is “*selected-game*_full_features_MM_DD_YY.csv”, which represents the dataset of metadata features, basic image features, and advanced image features for all videos in the dataset created on the date in the filename. The third file is “*selected-game*_full_metadata_MM_DD_YY.csv”, which contains only the metadata features for the dataset gathered on the specified date. The fourth file is “*selected-game*_requests.json”, which contains the massive amount of data returned by the youtube api for each video_id. This is constantly updated for any new videos that are scraped, which is why it does not have a date in the filename. The last file is “*selected-game*_summary_metadata_MM_DD_YY.csv”, this contains just the metadata for only the videos that appear in the search results, and not the videos within the channels that are contained in “*selected-game*_full_features_MM_DD_YY.csv”.


### Further Analysis

Once the pipeline has been run, further analysis of the data gathered can be found in the notebook “notebooks/eda/cwynne_combined_analysis.ipynb”. In the third cell of the notebook, specify which dataset that the analysis should be done on, usually selecting one of the generated output files mentioned above. The variable “data_path” should be set to the desired dataset. The target columns can also be changed in this cell, although it is recommended that the defaults are used. 


### Configuration Parameters
The run.py script will access the configuration file stored in “config/config-scraping.json”. This json file contains different parameters that allow you to change what data is scraped / analyzed. Below are the parameters in further detail.


- selected-game: this  key represents what keyword will be used to search for videos. This does not have to be a game, but can be any keyword of your choosing. 
- thumbnail-qual: This key holds a dictionary which can be thought of as a switch, put a 1 in any quality that you want a dataset of thumbnails for. 
- test-videos-dir: This is the filepath of the test video folder, which contains files such as the video ids and metadata of the test subset.
- test-thumbs-dir: This is the filepath of the directory of thumbnails for the test dataset.
- api-service-name: This generally should not change, if the youtube api changes this argument, then this should be changed. 
- api-version: This is the version of the API, currently the most updated version is “v3”, but this could be changed to use later or earlier versions, however the code has not been tested on different api versions.
- videos-dir: This is the location of the currently scraped video data directory. Files in this folder are scraped based on the keyword and include the csv of video ids in the dataset and metadata information. This is only a local directory and is used for anything that is not the test dataset
- full-metadata-csv-write-path: This is the filepath to write the metadata for all videos that were scraped. This includes the search result videos and the specified number of videos per channel by the key “videos-per-channel”).
- summary-metadata-csv-write-path: This is the filepath to write only the initial search result videos and not the extra channel videos. 
- requests-dic-read-path: This is the file path for a local json of request data that could exist from a previous run of this pipeline. If nothing is provided it will create a new file using “requests-dic-write-path” that can be used for reruns of the pipeline. This is to prevent re-requesting the same info for videos and making unnecessary API calls.
- requests-dic-write-path: This is the path where the requests data json file is written for quicker reruns or saving of new requests data. 
- num-recent-videos: This is the number of search results that will be gathered based on the “selected-game” keyword being searched with the youtube api. 
- videos-per-channel: This is the videos per channel that are scraped, based on the different channels that uploaded videos in the search result for our keyword.
- scrape-write-dir: This is the directory that all of our scraped video data gets written to. This includes the dataset of video ids, and the metadata dataset for these videos. This changes based on the “selected-game” keyword. 
- full-features-write-name: This is the name of the file that contains metadata, basic image features, and advanced image features. It is stored in the directory specified by “videos-dir”
- overwrite: if set to true, it will overwrite any previously scraped data that was scraped on the same calendar date. If set to false, it will simply add a number to the end of the new requests data file. 


## References
Louise Myers 2019, accessed April 6, 2020, 

[https://louisem.com/198803/how-to-youtube-thumbnails](https://louisem.com/198803/how-to-youtube-thumbnails)

EmpLemon 2020, accessed May 4, 2020,

[https://www.youtube.com/watch?v=-6-i75wDIBE](https://www.youtube.com/watch?v=-6-i75wDIBE)
"
5,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_06/,,,"# DSC180B Project - Quantifying Style
# RestoreNet: Quantifying the Restoration of WWII Documents

Welcome to the my current version of the project.

### Prerequisites

What things you need to use the program (TBD)

```
BeautifulSoup - pip install beautifulsoup4
REGEX - pip install regex
Requests - pip3 install requests

TensorFlow - pip3 install tensorflow
PyTorch - pip3 install torch torchvision
```

### Getting Started

First, go ahead and clone this repository to your local directory
```
git clone https://github.com/Emmanuel-Diaz/DSC180B-Project.git
```

then install all necessary packages

```
pip install -r requirements.txt
```


### Usage


**Collecting World War II Images**

Enter the following command to collect from [ww2db](http://ww2db.com/photo.php)

```
python run.py scrape [NUM_IMAGES] [TIME_PERIOD]
```

```NUM_IMAGES``` Number of World War II images to scrape<br>
```TIME_PERIOD``` Time period of images [Pre-War, Mid-War, Late-War]<br>


**Computing features**
Enter the following command to compute features on the data collected

```
python run.py features
```

**Restoring a test example**

Enter the following command to run the network on the 'Spoils of War' image, just like in the report output.
```
python run.py test-project
```

### Training

**Performing a trained restoration**
In order to do a trained restoration, you will either need to
1. Download Pre-trained weights found [here]() and place them into your ```data/out``` directory.
2. Train your own weights using your collected data. (TBD)

**Running with custom image and mask**

To use your own custom image and mask for restoration, please perform the following steps.
1. Place your image to be restored ```img``` in the ```data/input``` directory. (```img``` can be ```.png```,```.jpeg```,```.jpg```)
2. Place your custom mask ```mask``` in the ```data/input``` directory. (```mask``` must be ```.png```)
3. Run the following command with your file names.

```
python run.py -t [True/False] -i <img> -m <mask.png>
```

```-t``` Use a trained model (e.g weights are located in ```data/out```)<br>
```-i``` Degraded image file name<br>
```-m``` Mask image file name<br>


## Built With

* [Python](https://www.python.org/) - Language used
* [PyTorch](https://www.pytorch.org) - Net Framework


## Version

1.2 Added selective mask inpainting

## Author

* [Emmanuel Diaz](https://github.com/Emmanuel-Diaz)

* *Deep Generative Prior* by Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, Ping Luo 
	* [Report](https://arxiv.org/abs/2003.13659)
	* [GitHub](https://github.com/XingangPan/deep-generative-prior)
","# DSC180B Project - Quantifying Style
# RestoreNet: Quantifying the Restoration of WWII Documents

Welcome to the my current version of the project.

### Prerequisites

What things you need to use the program (TBD)

```
BeautifulSoup - pip install beautifulsoup4
REGEX - pip install regex
Requests - pip3 install requests

TensorFlow - pip3 install tensorflow
PyTorch - pip3 install torch torchvision
```

### Getting Started

First, go ahead and clone this repository to your local directory
```
git clone https://github.com/Emmanuel-Diaz/DSC180B-Project.git
```

then install all necessary packages

```
pip install -r requirements.txt
```


### Usage


**Collecting World War II Images**

Enter the following command to collect from [ww2db](http://ww2db.com/photo.php)

```
python run.py scrape [NUM_IMAGES] [TIME_PERIOD]
```

```NUM_IMAGES``` Number of World War II images to scrape<br>
```TIME_PERIOD``` Time period of images [Pre-War, Mid-War, Late-War]<br>


**Computing features**
Enter the following command to compute features on the data collected

```
python run.py features
```

**Restoring a test example**

Enter the following command to run the network on the 'Spoils of War' image, just like in the report output.
```
python run.py test-project
```

### Training

**Performing a trained restoration**
In order to do a trained restoration, you will either need to
1. Download Pre-trained weights found [here]() and place them into your ```data/out``` directory.
2. Train your own weights using your collected data. (TBD)

**Running with custom image and mask**

To use your own custom image and mask for restoration, please perform the following steps.
1. Place your image to be restored ```img``` in the ```data/input``` directory. (```img``` can be ```.png```,```.jpeg```,```.jpg```)
2. Place your custom mask ```mask``` in the ```data/input``` directory. (```mask``` must be ```.png```)
3. Run the following command with your file names.

```
python run.py -t [True/False] -i <img> -m <mask.png>
```

```-t``` Use a trained model (e.g weights are located in ```data/out```)<br>
```-i``` Degraded image file name<br>
```-m``` Mask image file name<br>


## Built With

* [Python](https://www.python.org/) - Language used
* [PyTorch](https://www.pytorch.org) - Net Framework


## Version

1.2 Added selective mask inpainting

## Author

* [Emmanuel Diaz](https://github.com/Emmanuel-Diaz)

* *Deep Generative Prior* by Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, Ping Luo 
	* [Report](https://arxiv.org/abs/2003.13659)
	* [GitHub](https://github.com/XingangPan/deep-generative-prior)
"
6,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_07/,,,,
7,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_08/,,,"# SuperBowlCapstone

Collects superbowl and non-superbowl commercials from ispot and adforum respectively.
Generates features from their audio and video content
Builds multiple models based off of the generated features.
Creates multiple visualizations.

## Usage Instructions

Potential run.py arguments:
* data: does the webscraping
* fxt: feature extraction, requires data to be run
* analyze: model and visual generation, requires fxt to be run
* test-project: using given test commercials, tests project.

## Project Contents

```
ROOT FOLDER
├── .gitignore
├── README.md
├── config
│   ├── data-params.json
│   ├── test-params.json
│   └── env.json
├── chosen data folder
│   ├── chosen superbowl commercial folder
│   │   └── chosen audio folder
│   └── chosen non-superbowl commercial folder
│       └── chosen audio folder
├── notebooks
│   └── .gitkeep
├── run.py
└── src
    └── etl.py
```

### `src`

* `ad_scraper.py`: Library code that collects non-superbowl commercials from adforum based off of collected superbowl commercials.
* `ispot_scrape.py`: Library code that collects superbowl commercials from ispot.
* `video_processing.py`: Library code that generates visual features.
* `extract_audio_features.py`: Library code that generates audio features.
* `vis_video.py`: Library code that generates visualizations for generated features.
* `predictor.py`: Library code that generates predictions from Logistic Regression and Random Forest models.

### `config`

* `data-params.json`: Common parameters for getting data, serving as
  inputs to library code.
  
* `test-params.json`: parameters for running small process on small
  test data.

### `notebooks`

* Jupyter notebooks for analysis, mainly audio-orientated.
","# SuperBowlCapstone

Collects superbowl and non-superbowl commercials from ispot and adforum respectively.
Generates features from their audio and video content
Builds multiple models based off of the generated features.
Creates multiple visualizations.

## Usage Instructions

Potential run.py arguments:
* data: does the webscraping
* fxt: feature extraction, requires data to be run
* analyze: model and visual generation, requires fxt to be run
* test-project: using given test commercials, tests project.

## Project Contents

```
ROOT FOLDER
├── .gitignore
├── README.md
├── config
│   ├── data-params.json
│   ├── test-params.json
│   └── env.json
├── chosen data folder
│   ├── chosen superbowl commercial folder
│   │   └── chosen audio folder
│   └── chosen non-superbowl commercial folder
│       └── chosen audio folder
├── notebooks
│   └── .gitkeep
├── run.py
└── src
    └── etl.py
```

### `src`

* `ad_scraper.py`: Library code that collects non-superbowl commercials from adforum based off of collected superbowl commercials.
* `ispot_scrape.py`: Library code that collects superbowl commercials from ispot.
* `video_processing.py`: Library code that generates visual features.
* `extract_audio_features.py`: Library code that generates audio features.
* `vis_video.py`: Library code that generates visualizations for generated features.
* `predictor.py`: Library code that generates predictions from Logistic Regression and Random Forest models.

### `config`

* `data-params.json`: Common parameters for getting data, serving as
  inputs to library code.
  
* `test-params.json`: parameters for running small process on small
  test data.

### `notebooks`

* Jupyter notebooks for analysis, mainly audio-orientated.
"
8,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_09/,,,"# IlliterateBot


### To test our code use the command:

python run.py test


## Limitations of Code 

### CNN Feature Vectors
Parts of the code that we have used we are unable to include into our pipeline. 
This includes creating the VGG16 CNN feature vectors, when transforming our book cover images
to feature vectors. When running our code that transforms our book cover images to feature vectors on our code, 
our code would constantly time out, and thus we were only able to run it on our jupyter notebooks where we
would need to split the data accordingly, and save the feature vectors immediately before the kernel crashes, 
and we would have to run the code again, and save the feature vectors separately before it crashes again. Because of
this, it is difficult to include it into our pipeline without changing the code each time we run. 



### Tesseract OCR
In our report we discussed our experiences using Tesseract OCR. The code used to generate the visualizations found in our report is in our Tesseract Jupyter Notebook. 
We were not able to add this code to our main pipeline since PyTesseract, a Python wrapper for Tesseract, requires a local installation of the Tesseract engine and requires placement directly
in the local AppData folder.


","# IlliterateBot


### To test our code use the command:

python run.py test


## Limitations of Code 

### CNN Feature Vectors
Parts of the code that we have used we are unable to include into our pipeline. 
This includes creating the VGG16 CNN feature vectors, when transforming our book cover images
to feature vectors. When running our code that transforms our book cover images to feature vectors on our code, 
our code would constantly time out, and thus we were only able to run it on our jupyter notebooks where we
would need to split the data accordingly, and save the feature vectors immediately before the kernel crashes, 
and we would have to run the code again, and save the feature vectors separately before it crashes again. Because of
this, it is difficult to include it into our pipeline without changing the code each time we run. 



### Tesseract OCR
In our report we discussed our experiences using Tesseract OCR. The code used to generate the visualizations found in our report is in our Tesseract Jupyter Notebook. 
We were not able to add this code to our main pipeline since PyTesseract, a Python wrapper for Tesseract, requires a local installation of the Tesseract engine and requires placement directly
in the local AppData folder.


"
9,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_10/,,,# Fair-Policing-Capstone,# Fair-Policing-Capstone
10,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_11/,,,"# DSC180B Capstone Project

## Description of Contents

The project consists of these portions:
```
PROJECT
├── .gitignore
├── README.md
├── config
│   ├── data-params.json
│   ├── env.json
│   ├── model.json
│   ├── process-params.json
│   ├── test-data-params.json
│   ├── test-model.json
│   └── test-process-params.json
├── imgs
├── notebooks
│   ├── prop_score.ipynb
│   └── sandbox.ipynb
├── references
│   └── .gitkeep
├── requirements.txt
├── run.py
├── index.html
├── index.md
└── src
│   ├── etl.py
│   ├── model_vod.py
│   ├── model.py
│   └── viz.py
```

### `src`

* `etl.py`: Script to perform Extract Transform Load.
* `model_vod.py`: Script to perform Veil of Darkness analysis.
* `model.py`: Script to perform propensity score analysis.
* `viz.py`: Script to visualize findings from both sets of analysis.

### `config`

* `data-params.json`: Common parameters for getting data, serving as inputs to library code.
* `env.json`: Environment paramters for GitHub repository and Docker image.
* `model.json`: Model parameters for for performing propensity score analysis.
* `process-params.json`: Common parameters for cleaning and processing data.
* `test-data-params.json`: Common parameters for getting test data, serving as inputs to library code.
* `test-model.json`: Model parameters for for performing propensity score analysis on test data.
* `test-process-params.json`: Common parameters for cleaning and processing test data.

### `notebooks`

* `prop_score.ipynb`: Imports code from `src` for the purpose of running the propensity score analysis. 
* `sandbox.ipynb`: Imports code from `src` for the purpose of analysis. 

### Description of Targets

* `!python run.py data`: Collects traffic stops data from Stanford Open Policing Portal and cleans it.
* `!python run.py model`: Performs propensity score and veil of darkness analysis on traffic stops.
* `!python run.py test-project`: Ingests, cleans, and runs model on a subset of the traffic stops data for the purpose of testing.","# DSC180B Capstone Project

## Description of Contents

The project consists of these portions:
```
PROJECT
├── .gitignore
├── README.md
├── config
│   ├── data-params.json
│   ├── env.json
│   ├── model.json
│   ├── process-params.json
│   ├── test-data-params.json
│   ├── test-model.json
│   └── test-process-params.json
├── imgs
├── notebooks
│   ├── prop_score.ipynb
│   └── sandbox.ipynb
├── references
│   └── .gitkeep
├── requirements.txt
├── run.py
├── index.html
├── index.md
└── src
│   ├── etl.py
│   ├── model_vod.py
│   ├── model.py
│   └── viz.py
```

### `src`

* `etl.py`: Script to perform Extract Transform Load.
* `model_vod.py`: Script to perform Veil of Darkness analysis.
* `model.py`: Script to perform propensity score analysis.
* `viz.py`: Script to visualize findings from both sets of analysis.

### `config`

* `data-params.json`: Common parameters for getting data, serving as inputs to library code.
* `env.json`: Environment paramters for GitHub repository and Docker image.
* `model.json`: Model parameters for for performing propensity score analysis.
* `process-params.json`: Common parameters for cleaning and processing data.
* `test-data-params.json`: Common parameters for getting test data, serving as inputs to library code.
* `test-model.json`: Model parameters for for performing propensity score analysis on test data.
* `test-process-params.json`: Common parameters for cleaning and processing test data.

### `notebooks`

* `prop_score.ipynb`: Imports code from `src` for the purpose of running the propensity score analysis. 
* `sandbox.ipynb`: Imports code from `src` for the purpose of analysis. 

### Description of Targets

* `!python run.py data`: Collects traffic stops data from Stanford Open Policing Portal and cleans it.
* `!python run.py model`: Performs propensity score and veil of darkness analysis on traffic stops.
* `!python run.py test-project`: Ingests, cleans, and runs model on a subset of the traffic stops data for the purpose of testing."
11,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_12/,,,"# DSC180B
Exploring Predictive Policing in San Diego for DSC180B Capstone Project

[Here](https://chuanyuanyeh.github.io/predpol_study/) is the link to the website.

Link to the GIS map can be found at https://arcg.is/1CmX0r

## Usage Instructions

To replicate the entire (or subsets of the) project, copy and paste `python run.py` in the command line while in the root directory followed by the arguments below:
* `data`: Ingests raw data from online sources.
* `process`: Runs the pipeline for cleaning and formatting raw datasets.
* `eda`: Performs exploratory data analysis and outputs visualizations.
* `analyze`: Performs statistical tests on differences in observed proportions between PredPol and non-PredPol instances.
* `test-project`: Runs the entire pipeline from start to end on a smaller, versioned test data.

For example, running the code below would reproduce the entire project:

`python run.py data process eda analyze`

## Description of Contents

The project consists of these portions:
```
PROJECT
├── config
    ├── data-params.json
    ├── process-params.json
    ├── eda-params.json
    ├── analyze-params.json
    ├── test-data-params.json
    ├── test-process-params.json
    ├── test-eda-params.json
    ├── test-analyze-params.json
    └── env.json
├── data
    ├── raw
    └── cleaned
├── notebooks
    └── .gitkeep
├── references
    ├── arrest_charges.json
    ├── arrest_types.json
    ├── crime_charges.json
    ├── crime_types.json
    ├── divisions_mapper.json
    ├── nhgis0005_ds172_2010_block_codebook.txt
    └── races.json
└── src
    ├── etl.py
    ├── eda.py
    ├── analyze.py
    └── geospatial.py
├── test_data
    ├── raw
    └── cleaned
├── viz
    ├── EDA
        ├── Arrests
        ├── Crime
        └── Stops
    └── Analysis
        ├── Arrests
        ├── Crime
        └── Stops
├── .gitignore
├── Dockerfile
├── README.md
├── requirements.txt
├── run.py
```

### `config/`

* `data-params.json`: Common parameters for getting data, serving as
  inputs to library code.
* `process-params.json`: Parameters for processing data.
* `eda-params.json`: Parameters for exploratory analysis on each dataset.
* `analyze-params.json`: Parameters for statistical testings and analyses.
* `env.json`: Parameters for loading virtual environment.
* Also contains similar configurations for test data.
  
### `data/`

* `raw/`: Raw datasets from original source.
* `cleaned/`: Cleaned datasets.

### `notebooks/`

* Jupyter notebooks for *analyses* and *code development*
  - notebooks will be removed after migration to library code.

### `references/`

* Data Dictionaries, references to external sources.

### `src/`

* `etl.py`: Library code that executes tasks useful for getting data.

### `test_data/`

* Versioned test data.

### `viz/`

* Visual outputs from EDA and analyses pipelines.

### `Dockerfile`

* Docker image to replicate the environment the project was developed in. 

### `requirements.txt`

* Python libraries/modules used as well as their corresponding versions.

### `run.py`

* Main driver for project replication
","# DSC180B
Exploring Predictive Policing in San Diego for DSC180B Capstone Project

[Here](https://chuanyuanyeh.github.io/predpol_study/) is the link to the website.

Link to the GIS map can be found at https://arcg.is/1CmX0r

## Usage Instructions

To replicate the entire (or subsets of the) project, copy and paste `python run.py` in the command line while in the root directory followed by the arguments below:
* `data`: Ingests raw data from online sources.
* `process`: Runs the pipeline for cleaning and formatting raw datasets.
* `eda`: Performs exploratory data analysis and outputs visualizations.
* `analyze`: Performs statistical tests on differences in observed proportions between PredPol and non-PredPol instances.
* `test-project`: Runs the entire pipeline from start to end on a smaller, versioned test data.

For example, running the code below would reproduce the entire project:

`python run.py data process eda analyze`

## Description of Contents

The project consists of these portions:
```
PROJECT
├── config
    ├── data-params.json
    ├── process-params.json
    ├── eda-params.json
    ├── analyze-params.json
    ├── test-data-params.json
    ├── test-process-params.json
    ├── test-eda-params.json
    ├── test-analyze-params.json
    └── env.json
├── data
    ├── raw
    └── cleaned
├── notebooks
    └── .gitkeep
├── references
    ├── arrest_charges.json
    ├── arrest_types.json
    ├── crime_charges.json
    ├── crime_types.json
    ├── divisions_mapper.json
    ├── nhgis0005_ds172_2010_block_codebook.txt
    └── races.json
└── src
    ├── etl.py
    ├── eda.py
    ├── analyze.py
    └── geospatial.py
├── test_data
    ├── raw
    └── cleaned
├── viz
    ├── EDA
        ├── Arrests
        ├── Crime
        └── Stops
    └── Analysis
        ├── Arrests
        ├── Crime
        └── Stops
├── .gitignore
├── Dockerfile
├── README.md
├── requirements.txt
├── run.py
```

### `config/`

* `data-params.json`: Common parameters for getting data, serving as
  inputs to library code.
* `process-params.json`: Parameters for processing data.
* `eda-params.json`: Parameters for exploratory analysis on each dataset.
* `analyze-params.json`: Parameters for statistical testings and analyses.
* `env.json`: Parameters for loading virtual environment.
* Also contains similar configurations for test data.
  
### `data/`

* `raw/`: Raw datasets from original source.
* `cleaned/`: Cleaned datasets.

### `notebooks/`

* Jupyter notebooks for *analyses* and *code development*
  - notebooks will be removed after migration to library code.

### `references/`

* Data Dictionaries, references to external sources.

### `src/`

* `etl.py`: Library code that executes tasks useful for getting data.

### `test_data/`

* Versioned test data.

### `viz/`

* Visual outputs from EDA and analyses pipelines.

### `Dockerfile`

* Docker image to replicate the environment the project was developed in. 

### `requirements.txt`

* Python libraries/modules used as well as their corresponding versions.

### `run.py`

* Main driver for project replication
"
12,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_13/,,,"# RML vs Traffic Collisions
How recreational marijuana legalization(RML) affects traffic-related scenes in California using Difference in Differences model.
","# RML vs Traffic Collisions
How recreational marijuana legalization(RML) affects traffic-related scenes in California using Difference in Differences model.
"
13,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_14/,,,"# DSC180B Visualization

https://siqihuang47.github.io/dsc180b_visualization/
","# DSC180B Visualization

https://siqihuang47.github.io/dsc180b_visualization/
"
14,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_15/,,,"# DSC180BPipeline
Our pipleine allows you to download data from the GWAS Catalog, Clean the data, Merge the data, and then create plots such as Q-Q plots, P-value Histograms, and Manhattan Plots. These plots allow you to analyze the GWAS data and to determine which SNPs are significant. Our pipeline runs on a small data set but the images it produces is not very significant because the data is to small. When we run the data on the actual data we are using the images are much better and meaningful to analyze. 
To run the pipepline on the sample data in the direction run this command in the terminal:
  python run.py test-project
This command should output a histogram plot, a q-q plot, and a Manhattan plot.

To run the data downloading portion run this command:
  python run.py test
This command should output a directory of the data you are trying to download.

To run the cleaning and plot making portion run this command:
  python run.py clean
 This command should clean the data and output the charts.
 
You can modify the code to work for you specific trait you are analyzing from the GWAS Catalog in the data collection portion and the whole pipeline should work
","# DSC180BPipeline
Our pipleine allows you to download data from the GWAS Catalog, Clean the data, Merge the data, and then create plots such as Q-Q plots, P-value Histograms, and Manhattan Plots. These plots allow you to analyze the GWAS data and to determine which SNPs are significant. Our pipeline runs on a small data set but the images it produces is not very significant because the data is to small. When we run the data on the actual data we are using the images are much better and meaningful to analyze. 
To run the pipepline on the sample data in the direction run this command in the terminal:
  python run.py test-project
This command should output a histogram plot, a q-q plot, and a Manhattan plot.

To run the data downloading portion run this command:
  python run.py test
This command should output a directory of the data you are trying to download.

To run the cleaning and plot making portion run this command:
  python run.py clean
 This command should clean the data and output the charts.
 
You can modify the code to work for you specific trait you are analyzing from the GWAS Catalog in the data collection portion and the whole pipeline should work
"
15,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_16/,,,"# Predicting Disease Risk Through Machine Learning

Traditional epidemiology techniques, most notably polygenic risk scoring, have been used by researchers and well-known companies, such as Takeda, MiCom Labs, and 23andMe, to calculate the disease risk of patients and consumers. However, recent research has shown limitations in polygenic risk scoring due to its inability to model high dimensional data with complex interactions (Wai, 2019). As humans, millions of potentially disease-contributing genetic variants exist in the genome, so the inability to leverage such information limits the power of polygenic risk scoring to accurately determine the disease risk of individuals. In this project, the viability of machine learning in disease risk prediction for Coronary Artery Disease, Alzheimer’s, and Diabetes Mellitus is explored. It is shown how machine learning models, including Support Vector Machines (SVMs), Logistic Regression, K Nearest Neighbors, Decision Trees, Random Forest, and Gaussian Naive Bayes, compare in their ability to effectively predict disease risk and how they may offer alternate and possibly better methods over traditional techniques. 

## Usage Instructions

In order to use the different components of this project, please run `python run.py` along with a target of your choice:

* `clean`: Cleans the data directory
* `data`: Downloads the data from GWAS Catalog according to data-params.json
* `simulate-one`: Simulates a SNP population for the training GWAS
* `simulate-both`: Simulates a SNP population for both the training GWAS and the test GWAS
* `model`: 
   * If there is no simulated data for the test GWAS: 
          Splits the training GWAS simulated data into a train and test subset. Model is trained on the training subset, filtered to only contain SNPs also present in the test GWAS, in order to simulate sampling. The model is then tested on the test subset and results are reported (and saved).
   * If there is simulated data for the test GWAS (run via `simulate-both` target):
          Model is trained on simulated data (filtered to contain SNPs present in both GWAS's) from the training GWAS. Model is tested on simulated data from the test GWAS and results are reported (and saved).
* `test-project`: Tests project using test data
* `run-project`: Runs entire project according to config files

## Description of Contents

The project consists of these portions:
```
PROJECT
├── config
│   ├── data-params.json
│   ├── env.json
│   ├── model-params.json
│   └── test-params.json
├── notebooks
│   ├── Build_Model.ipynb
│   └── Simulate_Data.ipynb
├── src
│   ├── etl.py
│   ├── model.py
│   └── visualize_data.py
├── testdata
│   ├── alzheimer's
│   ├── coronary_artery
│   └── diabetes_type1_melittus
├── .gitignore
├── README.md
├── requirements.txt
└── run.py
```

### `root`

* `run.py`: Python script to run main command.

### `src`

* `etl.py`: Library code that executes tasks useful for getting data and transforming it into a machine-learning-ready format.

* `model.py`: Library code that builds and tests multiple models, and reports the results.

* `visualize_data.py`: Library code that generates a variety of visualization that are useful for analysis.

### `config`

* `data-params.json`: Parameters for downloading data from the GWAS Catalog and preparing for model building

* `env.json`: Environment information

* `model-params.json`: Contains the (sklearn) models that are tested, and the parameters to use for each model.

* `test-params.json`: Parameters for preparing test data for model building

### `testdata`

This directory contains two summary data files from the GWAS catalog for different diseases, one is used for building a training set and the other is use for a test set. Which is which can be found in the `data-params.json` configuration file (and changed).

* `alzheimer's`: Contains two summary statistics CSV's from GWAS studies on Alzheimer's Disease.

* `coronary_artery`: Contains two summary statistics CSV's from GWAS studies on Coronary Artery Disease.

* `diabetes_type1_melittus`: Contains two summary statistics CSV's from GWAS studies on Diabetes Type I.

### `notebooks`

* `Build_Model.ipynb`: Notebook walking through the model building/validation process

* `Simulate_Data.ipynb`: Notebook walking through the population simulation process
","# Predicting Disease Risk Through Machine Learning

Traditional epidemiology techniques, most notably polygenic risk scoring, have been used by researchers and well-known companies, such as Takeda, MiCom Labs, and 23andMe, to calculate the disease risk of patients and consumers. However, recent research has shown limitations in polygenic risk scoring due to its inability to model high dimensional data with complex interactions (Wai, 2019). As humans, millions of potentially disease-contributing genetic variants exist in the genome, so the inability to leverage such information limits the power of polygenic risk scoring to accurately determine the disease risk of individuals. In this project, the viability of machine learning in disease risk prediction for Coronary Artery Disease, Alzheimer’s, and Diabetes Mellitus is explored. It is shown how machine learning models, including Support Vector Machines (SVMs), Logistic Regression, K Nearest Neighbors, Decision Trees, Random Forest, and Gaussian Naive Bayes, compare in their ability to effectively predict disease risk and how they may offer alternate and possibly better methods over traditional techniques. 

## Usage Instructions

In order to use the different components of this project, please run `python run.py` along with a target of your choice:

* `clean`: Cleans the data directory
* `data`: Downloads the data from GWAS Catalog according to data-params.json
* `simulate-one`: Simulates a SNP population for the training GWAS
* `simulate-both`: Simulates a SNP population for both the training GWAS and the test GWAS
* `model`: 
   * If there is no simulated data for the test GWAS: 
          Splits the training GWAS simulated data into a train and test subset. Model is trained on the training subset, filtered to only contain SNPs also present in the test GWAS, in order to simulate sampling. The model is then tested on the test subset and results are reported (and saved).
   * If there is simulated data for the test GWAS (run via `simulate-both` target):
          Model is trained on simulated data (filtered to contain SNPs present in both GWAS's) from the training GWAS. Model is tested on simulated data from the test GWAS and results are reported (and saved).
* `test-project`: Tests project using test data
* `run-project`: Runs entire project according to config files

## Description of Contents

The project consists of these portions:
```
PROJECT
├── config
│   ├── data-params.json
│   ├── env.json
│   ├── model-params.json
│   └── test-params.json
├── notebooks
│   ├── Build_Model.ipynb
│   └── Simulate_Data.ipynb
├── src
│   ├── etl.py
│   ├── model.py
│   └── visualize_data.py
├── testdata
│   ├── alzheimer's
│   ├── coronary_artery
│   └── diabetes_type1_melittus
├── .gitignore
├── README.md
├── requirements.txt
└── run.py
```

### `root`

* `run.py`: Python script to run main command.

### `src`

* `etl.py`: Library code that executes tasks useful for getting data and transforming it into a machine-learning-ready format.

* `model.py`: Library code that builds and tests multiple models, and reports the results.

* `visualize_data.py`: Library code that generates a variety of visualization that are useful for analysis.

### `config`

* `data-params.json`: Parameters for downloading data from the GWAS Catalog and preparing for model building

* `env.json`: Environment information

* `model-params.json`: Contains the (sklearn) models that are tested, and the parameters to use for each model.

* `test-params.json`: Parameters for preparing test data for model building

### `testdata`

This directory contains two summary data files from the GWAS catalog for different diseases, one is used for building a training set and the other is use for a test set. Which is which can be found in the `data-params.json` configuration file (and changed).

* `alzheimer's`: Contains two summary statistics CSV's from GWAS studies on Alzheimer's Disease.

* `coronary_artery`: Contains two summary statistics CSV's from GWAS studies on Coronary Artery Disease.

* `diabetes_type1_melittus`: Contains two summary statistics CSV's from GWAS studies on Diabetes Type I.

### `notebooks`

* `Build_Model.ipynb`: Notebook walking through the model building/validation process

* `Simulate_Data.ipynb`: Notebook walking through the population simulation process
"
16,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_17/,,,"# Clustering-Germ-Layers

## __General__

Data scraping from https://portal.gdc.cancer.gov/ through selenium for analysis across the different germ layers and cancers. 

<br>
This is done through the use of Google's chromedriver. The chromedriver.exe included is for Windows with Chrome 83. If this does not work for your computer, go to this site https://chromedriver.chromium.org/ and download the matching driver and change chrome_driver_location to that of the correct one. MacOS may need to rename the driver to include the .exe extension.

<br><br>

## __File Usage__

### [Param_config.json](config/param_config.json)

- createDict 
    - Required keys: 
        - chrome_driver_location
        - data_dict (if not specified in command line)
            - if done through command line, remember to change it in param_config to file you want to use
    - Optional:
        - headless
        - time_wait, implicit_wait, after_sort_wait
- queryData
    - Required:
        - chrome_driver_location
        - data_dict (made from createDict)
        - samples
    - Optional:
        - headless
        - time_wait, implicit_wait, after_sort_wait
        - sort_using, sort_direction
        - file_names
- downloadData
    - Required:
        - chrome_driver_location
        - keep_tar_files, tar_dir, maf_dir
        - manual_csv_files (if not specified by pattern on the command line)
        - download_inds
    - Optional:
        - headless
        - time_wait, implicit_wait, after_sort_wait, download_wait


Example:
```
{
    ""chrome_driver_location"" : ""chromedriver.exe"",
    ""headless"" : true,
    ""data_dict"": ""references/data_dictionary.csv"",
    ""file_names"" : [""Q1.csv"", ""Q2.csv""],
    ""sort_using"" : [""Size"", ""Project""],
    ""sort_direction"": [""up"", ""up""],
    ""samples"": [133, 272],
    ""time_wait"" : 2,
    ""implicit_wait"" : 3,
    ""after_sort_wait"" : 5,
    ""download_wait"" : 60,
    ""manual_csv_files"" : [""Q1.csv""],
    ""download_inds"" : [""1-2,7-12""],
    ""keep_tar"" : false,
    ""tar_dir"" : ""testdata/tars"",
    ""maf_dir"": ""testdata/mafs""
}
```
<br>

### [Query_config.json](config/query_config.json)

- queryData
    - All categories are up to user to pick and choose. A list of categories, types, and descriptions are in data dictionary file made using createDict.
        - Number after is the number of queries to be made using it so make sure that the numbers used match. The queries are made in order.
            - Must match with files_names and samples from param_config.json
        - Ex.  
        > { ""files.data_format"" : [""maf"", 2] } -> [""files.data_format in [""maf""]"", ""files.data_format in [""maf""]""] 

        > { \
            ""data_format"" : [""maf"", 1, ""vcf"", 1], \
            ""access : [""open"", 2] \
          } -------> \
          [""files.data_format in [""maf""] and files.access in [""open""], ""files.data_format in [""vcf""] and files.access in [""open""]]

    - Incomplete category names may be autofilled to include the starting class name by the dictionary file, if they match to existing entries. 
        - Ex.
        > { ""data_format"" : [""maf"", 2] } -> [""files.data_format in [""maf""]"", ""files.data_format in [""maf""]""]

        > { ""data_for"" : [""open"", 2] } -> Error


Example:
```
{
    ""data_category"" : [""Simple Nucleotide Variation"", 2],
    ""data_format"" : [""maf"", 1, ""vcf"", 1],
    ""cases.primary_site"" : [""bronchus and lung"", 2],
    ""file_size_min"": [""> 1000"", 2],
    ""file_size_max"": [""< 10MB"", 2],
    ""access"" : [""open"", 2]
}
```
<br><br>

## __Command Line used by [run.py](run.py)__

<> means optional argument
```
(base) py run.py createDict config/param_config.json <Data_dict.csv>
```
- createDict creates a dictionary file in location specified in param_config.file. Overidden by CSV path on command line. 
    - if want to use command line CSV in queryData, then change the data_dict path in the param_config.file

<br>

```
(base) py run.py queryData config/param_config.json config/query_config.json
```
- queryData creates CSV files that match parameters in both the parameter and query config files with the URLs that link to the matching files.

<br>

```
(base) py run.py downloadData config/param_config.json <CSV matching pattern> <pattern 2> <...>
```
- downloadData uses indicies from param_config file and either the entered CSV files in param_config or the ones specified in the command line to find the files to download. The created annotations file is the same name as the created maf.gz file for ease of matching together.

<br><br>

## Appendix

Parameter file keys are the only ones included here as all information on the query keys can be found from the data file created by createDict. Grouped if highly related.

Used by more than one command

- chrome_driver_location \
    Path location of the chromedriver.exe file 

- data_dict \
    Data dictionary file made from createDict and can be used in queryData. In createDict, this value can be overridden by the CSV named in the commandline and only the latter CSV file is created.

    - It is scraped from TCGA using a vague query that holds all the different combinations for the keys. However, the possible values for those keys must be found from https://portal.gdc.cancer.gov/query and searching those terms in the query bar. This gives a list for all the possible values for that key. 
    - This file is only to locally see all the keys, types, and descriptions, as well as to help fix some user input errors in queryData.

- headless \
    Set to False if want to see movement across the site. \
    **WARNING**: Webpages load faster when True so if code breaks, then either change back or increase wait times.
    
- time_wait, implicit_wait, after_sort_wait, download_wait \
    Timing variables to change. There are default values if they are not specified; however, slower connections must be fixed by increasing these times so page is loaded.

    - Time_wait is the time the code waits after each step.
    - Implicit_wait is the time the code waits after loading a new page.
    - After_sort_wait is the time waiting for the data to be sorted which usually is kept higher than the others as it takes much longer to do.
        - However, if you are not sorting the data then setting the time much longer works.
    - Download_wait is the time that the program waits to download each file so larger files can be given more time to download.

queryData

- file_names \
    The list of CSV names that the queryData results will be saved to. If not specified, uses default names for queries.

- samples \
    The number of top results from a query search that will be saved to that query's CSV file. The max samples per CSV file is 1000 results.

- sort_using, sort_direction \
    Before the data is scraped, it is sorted by user specifications and then the top results are saved to the CSV file in queryData. If not specified, then uses the default sorting that TCGA stores files as. 
    - Sorts are done in order given so sorting by [""Size"",""Project""] is different than [""Project"", ""Size""]
    - Sort_using values are limited to:
        - ""Access"", ""Data Category"", ""Data Format"", ""File Name"", ""Project"", ""Size""
    - Sort_direction gives direction of sorting and is limited to:
        - ""Up"", ""Down""


downloadData

- download_inds \
    Indexes of the files in manual_csv_files that want to be downloaded to make downloading specific samples easier.
    - Ex. 
    > array_conv([""1-3,5"", ""1,3-4""]) => [[1,2,3,5], [1,3,4]]
    
    This downloads the files at these positions from the specified CSV files. \
    **WARNING** : As the first row of the CSV file is the column names, row 2 is the first data value and python starts at a 0 index so [1] points to the 3rd row in the CSV file. 

- keep_tar_files, tar_dir, maf_dir    
    - keep_tar_files is a boolean, that when True, says to keep the downloaded tar files along with the extracted maf.gz files. Otherwise, only the maf.gz files.
    - tar_dir is the directory where the chromedriver will save the downloaded tar files into.
    - maf_dir is the directory that the extracted maf.gz and corresponding annotation files are placed into.

    **WARNING**:  If there is an outer directory used by tar_dir and maf_dir and it is not created before running downloadData, the code will break.
    - In the param_config.json example, the testdata directory must be made in advance. However, the individual directories of tar_dir and maf_dir will be made through the code if they do not already exist.

- manual_csv_files \
    CSV file names that hold the location of the desired files to be downloaded. If new pattern is given in command line, this is overridden by CSVs found using that pattern. However, these CSVs found using the pattern are still subject to matching the download_inds.
","# Clustering-Germ-Layers

## __General__

Data scraping from https://portal.gdc.cancer.gov/ through selenium for analysis across the different germ layers and cancers. 

<br>
This is done through the use of Google's chromedriver. The chromedriver.exe included is for Windows with Chrome 83. If this does not work for your computer, go to this site https://chromedriver.chromium.org/ and download the matching driver and change chrome_driver_location to that of the correct one. MacOS may need to rename the driver to include the .exe extension.

<br><br>

## __File Usage__

### [Param_config.json](config/param_config.json)

- createDict 
    - Required keys: 
        - chrome_driver_location
        - data_dict (if not specified in command line)
            - if done through command line, remember to change it in param_config to file you want to use
    - Optional:
        - headless
        - time_wait, implicit_wait, after_sort_wait
- queryData
    - Required:
        - chrome_driver_location
        - data_dict (made from createDict)
        - samples
    - Optional:
        - headless
        - time_wait, implicit_wait, after_sort_wait
        - sort_using, sort_direction
        - file_names
- downloadData
    - Required:
        - chrome_driver_location
        - keep_tar_files, tar_dir, maf_dir
        - manual_csv_files (if not specified by pattern on the command line)
        - download_inds
    - Optional:
        - headless
        - time_wait, implicit_wait, after_sort_wait, download_wait


Example:
```
{
    ""chrome_driver_location"" : ""chromedriver.exe"",
    ""headless"" : true,
    ""data_dict"": ""references/data_dictionary.csv"",
    ""file_names"" : [""Q1.csv"", ""Q2.csv""],
    ""sort_using"" : [""Size"", ""Project""],
    ""sort_direction"": [""up"", ""up""],
    ""samples"": [133, 272],
    ""time_wait"" : 2,
    ""implicit_wait"" : 3,
    ""after_sort_wait"" : 5,
    ""download_wait"" : 60,
    ""manual_csv_files"" : [""Q1.csv""],
    ""download_inds"" : [""1-2,7-12""],
    ""keep_tar"" : false,
    ""tar_dir"" : ""testdata/tars"",
    ""maf_dir"": ""testdata/mafs""
}
```
<br>

### [Query_config.json](config/query_config.json)

- queryData
    - All categories are up to user to pick and choose. A list of categories, types, and descriptions are in data dictionary file made using createDict.
        - Number after is the number of queries to be made using it so make sure that the numbers used match. The queries are made in order.
            - Must match with files_names and samples from param_config.json
        - Ex.  
        > { ""files.data_format"" : [""maf"", 2] } -> [""files.data_format in [""maf""]"", ""files.data_format in [""maf""]""] 

        > { \
            ""data_format"" : [""maf"", 1, ""vcf"", 1], \
            ""access : [""open"", 2] \
          } -------> \
          [""files.data_format in [""maf""] and files.access in [""open""], ""files.data_format in [""vcf""] and files.access in [""open""]]

    - Incomplete category names may be autofilled to include the starting class name by the dictionary file, if they match to existing entries. 
        - Ex.
        > { ""data_format"" : [""maf"", 2] } -> [""files.data_format in [""maf""]"", ""files.data_format in [""maf""]""]

        > { ""data_for"" : [""open"", 2] } -> Error


Example:
```
{
    ""data_category"" : [""Simple Nucleotide Variation"", 2],
    ""data_format"" : [""maf"", 1, ""vcf"", 1],
    ""cases.primary_site"" : [""bronchus and lung"", 2],
    ""file_size_min"": [""> 1000"", 2],
    ""file_size_max"": [""< 10MB"", 2],
    ""access"" : [""open"", 2]
}
```
<br><br>

## __Command Line used by [run.py](run.py)__

<> means optional argument
```
(base) py run.py createDict config/param_config.json <Data_dict.csv>
```
- createDict creates a dictionary file in location specified in param_config.file. Overidden by CSV path on command line. 
    - if want to use command line CSV in queryData, then change the data_dict path in the param_config.file

<br>

```
(base) py run.py queryData config/param_config.json config/query_config.json
```
- queryData creates CSV files that match parameters in both the parameter and query config files with the URLs that link to the matching files.

<br>

```
(base) py run.py downloadData config/param_config.json <CSV matching pattern> <pattern 2> <...>
```
- downloadData uses indicies from param_config file and either the entered CSV files in param_config or the ones specified in the command line to find the files to download. The created annotations file is the same name as the created maf.gz file for ease of matching together.

<br><br>

## Appendix

Parameter file keys are the only ones included here as all information on the query keys can be found from the data file created by createDict. Grouped if highly related.

Used by more than one command

- chrome_driver_location \
    Path location of the chromedriver.exe file 

- data_dict \
    Data dictionary file made from createDict and can be used in queryData. In createDict, this value can be overridden by the CSV named in the commandline and only the latter CSV file is created.

    - It is scraped from TCGA using a vague query that holds all the different combinations for the keys. However, the possible values for those keys must be found from https://portal.gdc.cancer.gov/query and searching those terms in the query bar. This gives a list for all the possible values for that key. 
    - This file is only to locally see all the keys, types, and descriptions, as well as to help fix some user input errors in queryData.

- headless \
    Set to False if want to see movement across the site. \
    **WARNING**: Webpages load faster when True so if code breaks, then either change back or increase wait times.
    
- time_wait, implicit_wait, after_sort_wait, download_wait \
    Timing variables to change. There are default values if they are not specified; however, slower connections must be fixed by increasing these times so page is loaded.

    - Time_wait is the time the code waits after each step.
    - Implicit_wait is the time the code waits after loading a new page.
    - After_sort_wait is the time waiting for the data to be sorted which usually is kept higher than the others as it takes much longer to do.
        - However, if you are not sorting the data then setting the time much longer works.
    - Download_wait is the time that the program waits to download each file so larger files can be given more time to download.

queryData

- file_names \
    The list of CSV names that the queryData results will be saved to. If not specified, uses default names for queries.

- samples \
    The number of top results from a query search that will be saved to that query's CSV file. The max samples per CSV file is 1000 results.

- sort_using, sort_direction \
    Before the data is scraped, it is sorted by user specifications and then the top results are saved to the CSV file in queryData. If not specified, then uses the default sorting that TCGA stores files as. 
    - Sorts are done in order given so sorting by [""Size"",""Project""] is different than [""Project"", ""Size""]
    - Sort_using values are limited to:
        - ""Access"", ""Data Category"", ""Data Format"", ""File Name"", ""Project"", ""Size""
    - Sort_direction gives direction of sorting and is limited to:
        - ""Up"", ""Down""


downloadData

- download_inds \
    Indexes of the files in manual_csv_files that want to be downloaded to make downloading specific samples easier.
    - Ex. 
    > array_conv([""1-3,5"", ""1,3-4""]) => [[1,2,3,5], [1,3,4]]
    
    This downloads the files at these positions from the specified CSV files. \
    **WARNING** : As the first row of the CSV file is the column names, row 2 is the first data value and python starts at a 0 index so [1] points to the 3rd row in the CSV file. 

- keep_tar_files, tar_dir, maf_dir    
    - keep_tar_files is a boolean, that when True, says to keep the downloaded tar files along with the extracted maf.gz files. Otherwise, only the maf.gz files.
    - tar_dir is the directory where the chromedriver will save the downloaded tar files into.
    - maf_dir is the directory that the extracted maf.gz and corresponding annotation files are placed into.

    **WARNING**:  If there is an outer directory used by tar_dir and maf_dir and it is not created before running downloadData, the code will break.
    - In the param_config.json example, the testdata directory must be made in advance. However, the individual directories of tar_dir and maf_dir will be made through the code if they do not already exist.

- manual_csv_files \
    CSV file names that hold the location of the desired files to be downloaded. If new pattern is given in command line, this is overridden by CSVs found using that pattern. However, these CSVs found using the pattern are still subject to matching the download_inds.
"
17,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_18/,,,"
# Understanding miRNA in pre-Type 1 Diabetes

By Pete Sheurpukdi & Derrick Liu

This repository contains the code used our the paper [Understanding miRNA in pre-Type 1 Diabetes](https://github.com/Derrick56007/miRNA_preT1Diabetes/raw/master/report.pdf)

Requirements
------------

- Docker: https://docs.docker.com/get-docker/

Install
--------------

```
docker pull derrick56007/mirna_pre_t1d:latest
```

Usage
------------

```
docker run -ti derrick56007/mirna_pre_t1d:latest python run.py test-project
```
","
# Understanding miRNA in pre-Type 1 Diabetes

By Pete Sheurpukdi & Derrick Liu

This repository contains the code used our the paper [Understanding miRNA in pre-Type 1 Diabetes](https://github.com/Derrick56007/miRNA_preT1Diabetes/raw/master/report.pdf)

Requirements
------------

- Docker: https://docs.docker.com/get-docker/

Install
--------------

```
docker pull derrick56007/mirna_pre_t1d:latest
```

Usage
------------

```
docker run -ti derrick56007/mirna_pre_t1d:latest python run.py test-project
```
"
18,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_19/,,,"# DSC180B_Genome_Project -- GWAS on Alzheimer’s Disease

### Tony Zhang, Zhuoyuan Ren, Haoshu Qin

Visualization Website: https://tonyzhanghm.github.io/adgwas_website/

## Introduction

In this study, we conducted a Genome-Wise Analysis Study on Late-Onset Alzheimer's Disease (LOAD). For experiment details, please refer to the [paper](https://github.com/TonyZhanghm/DSC180B_Genome_01/blob/master/GWAS_on_Alzheimer_s_Disease_report.pdf). 

## Usage

### Environment (Docker)
Docker images: https://hub.docker.com/repository/docker/tonyzhanghm/genetics

### Commands
Clone the repo: `git clone https://github.com/TonyZhanghm/DSC180B_Genome_01.git`

To run the whole experiment: `python run.py test-project`

To run the project step by step: `python run.py` with following flags:  
`get_data`: download the raw data  and the tools needed.   
`filter`: filter the dataset with [PLINK 1.9](https://www.cog-genomics.org/plink/). The specific parameter choices could be found in the paper.    
`pca`: run principal component analysis with [PLINK 1.9](https://www.cog-genomics.org/plink/).  
`plot_pca`: plot pariplots for the first 5 principal components with [seaborn](https://seaborn.pydata.org/).  
`plot_eigenval`: plot the scree plot.   
`logistic`: run the association test with logistic regression.   
`manhattan`: plot the manhattan plot with [bioinfokit](https://reneshbedre.github.io//blog/howtoinstall.html).  
`regional`: plot regional plots for the nine genes of interests.   
`qqplot`: plot a qqplot on the test results.  
`meta`: run metal analysis with [METAL](https://genome.sph.umich.edu/wiki/METAL_Documentation).  

The data will be stored in `data/` and the experiment results will be store in `data/output/`. 

## Development Updates

### Checkpoint-1 (04/12/2020)
- Request data from source: UK Biobank and NIAGADS
- Understand the analysis methods: meta analysis, Manhattan plot, regional association plot. 
- Write a survey of the data you are using, the relationship and appropriateness of the data to the problem under examination, and the context in which the data was created.
- Summarize relevant details of the data generating process, describing the population that the data represents, whether that population is relevant to the question at hand, while addressing possible questions of data reliability.
- Understand how to use population stratification on our data so that it can apply to other races besides European descent.
- no new code added

### Checkpoint-2 (04/26/2020)
- Describe the source of the backup dataset, the population that the data represents, whether that population is relevant to the question at hand, while addressing possible questions of data reliability. (Scott)
- Perform preprocessing quality controls using Plink commands (Jared, Tony)
- Statistically assess the quality of the data (Tony)
- EDA (Barplot, PCA, Scatter matrix plot, Scree Plot) (All)
- Perform multi-covariate association analysis with logistic regression (Tony)

","# DSC180B_Genome_Project -- GWAS on Alzheimer’s Disease

### Tony Zhang, Zhuoyuan Ren, Haoshu Qin

Visualization Website: https://tonyzhanghm.github.io/adgwas_website/

## Introduction

In this study, we conducted a Genome-Wise Analysis Study on Late-Onset Alzheimer's Disease (LOAD). For experiment details, please refer to the [paper](https://github.com/TonyZhanghm/DSC180B_Genome_01/blob/master/GWAS_on_Alzheimer_s_Disease_report.pdf). 

## Usage

### Environment (Docker)
Docker images: https://hub.docker.com/repository/docker/tonyzhanghm/genetics

### Commands
Clone the repo: `git clone https://github.com/TonyZhanghm/DSC180B_Genome_01.git`

To run the whole experiment: `python run.py test-project`

To run the project step by step: `python run.py` with following flags:  
`get_data`: download the raw data  and the tools needed.   
`filter`: filter the dataset with [PLINK 1.9](https://www.cog-genomics.org/plink/). The specific parameter choices could be found in the paper.    
`pca`: run principal component analysis with [PLINK 1.9](https://www.cog-genomics.org/plink/).  
`plot_pca`: plot pariplots for the first 5 principal components with [seaborn](https://seaborn.pydata.org/).  
`plot_eigenval`: plot the scree plot.   
`logistic`: run the association test with logistic regression.   
`manhattan`: plot the manhattan plot with [bioinfokit](https://reneshbedre.github.io//blog/howtoinstall.html).  
`regional`: plot regional plots for the nine genes of interests.   
`qqplot`: plot a qqplot on the test results.  
`meta`: run metal analysis with [METAL](https://genome.sph.umich.edu/wiki/METAL_Documentation).  

The data will be stored in `data/` and the experiment results will be store in `data/output/`. 

## Development Updates

### Checkpoint-1 (04/12/2020)
- Request data from source: UK Biobank and NIAGADS
- Understand the analysis methods: meta analysis, Manhattan plot, regional association plot. 
- Write a survey of the data you are using, the relationship and appropriateness of the data to the problem under examination, and the context in which the data was created.
- Summarize relevant details of the data generating process, describing the population that the data represents, whether that population is relevant to the question at hand, while addressing possible questions of data reliability.
- Understand how to use population stratification on our data so that it can apply to other races besides European descent.
- no new code added

### Checkpoint-2 (04/26/2020)
- Describe the source of the backup dataset, the population that the data represents, whether that population is relevant to the question at hand, while addressing possible questions of data reliability. (Scott)
- Perform preprocessing quality controls using Plink commands (Jared, Tony)
- Statistically assess the quality of the data (Tony)
- EDA (Barplot, PCA, Scatter matrix plot, Scree Plot) (All)
- Perform multi-covariate association analysis with logistic regression (Tony)

"
19,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_20/,,,"# HinDroid-with-Embeddings

![Docker Cloud Build Status](https://img.shields.io/docker/cloud/build/davidzz/hindroid-xl)

## Overview

Malware detection for android applications is an expanding field with the introduction of the [Hindroid](https://www.cse.ust.hk/~yqsong/papers/2017-KDD-HINDROID.pdf) model (DOI:[10.1145/3097983.3098026](https://doi.org/10.1145/3097983.3098026)). It proposes a method that transforms the semantic relationships between Android applications and their decompiled source code to a Heterogeneous Information Network (HIN) and uses similarities from various meta-paths between apps to construct a model for malware classification. To further explore the field, we aim to extend the HinDroid model to improve the accuracy in specific subsets of the AMD dataset. Our effort will be focused on finding better representations for both apps as well as APIs and discovering methods to incorporate them as additional features in a new model. In the meantime, we plan to evaluate how the proposed model captures the features that are relevant to the classification task and compare to that of the HinDroid baseline. Our contributions can be utilized in systems where the analysis of malware and interpretable features are more important than mere detection.

## Usage
Docker image on Docker Hub:
[davidzyx/hindroid-xl](https://hub.docker.com/repository/docker/davidzz/hindroid)

On Datahub, create a pod using a custom image with 4 CPU and 32 GB of memory. If you use another configuration, use at least 6GB memory per CPU.
```bash
launch.sh -i davidzz/hindroid-xl -c 4 -m 32
```

Modify the config file located in `config/data-params.json`. If you want to run a test drive, use `config/test-params.json`. Put either `data` or `test` as the first argument.

The HinDroid baseline uses the driver file `run.py` with 3 targets: `ingest`, `process`, and `model`. Put each target space-separated as arguments in the call. To run the whole pipeline, use
```bash
python run.py data ingest process model
```

`process` target will save `.npz` files in `data/processed/` for generating various embeddings.

## System Prerequisites and Definitions

- APK - Executable file for Android
- Smali code - Human readable code decompiled from Dalvik bytecode contained the APK
- Heterogeneous Information Network (HIN) - A graph where its nodes may not be of the same type
- API Extraction
  - Use regex to match specific patterns
  - API calls and method blocks
![api](https://i.imgur.com/nx3rKKv.png)

## HinDroid Efficiency Improvements

In HinDroid, the amount of APIs that are used in the final gram matrix calculations can have dimension of several millions. Even in sparse format, these matrices take up huge computational resources for calculation. We are able to reduce the number of API used in HinDroid to 1000 APIs while retaining the same level of accuracy. Both the time and space complexity for training and inference can be improved by a few orders of magnitude. We achieve this by selecting the top important (larger absolute coefficients) APIs (words) from fitting a logistic regression on each app (document) after applying BM25 extraction on the counts of each API for each app.

| Method            | # of Apps | # of APIs used | RAM used | train+test time |
|-------------------|-----------|----------------|----------|-----------------|
| HinDroid-original | 1670      | 2024313        | 68GB     | 3h29m41s        |
| HinDroid-reduced  | 1670      | 1000           | 0.3GB    | 20s             |

## Embedding Techniques Explored

As we are using NLP approaches such as word2vec which cannot be directly applied to application source code and matrices from HinDroid, our data ingestion pipeline generates text corpus by traversing the graphs following an user defined metapaths and the length of a random walk. Using a metapath `ABPBA` with random walk length 5000, the text corpus may look like 

`app_3 -> api_500 -> api_321 -> api_234 -> api_578 -> app_321 -> api_123…`

where each token represents an application or api node and the neighbouring tokens are
connected by edges in the graph.

### Word2Vec

In a graph where there are two types of nodes: application and api, Word2Vec is the first approach that we attempt to capture the relationship beyond application and apis that have a direct connection in the graph. This traditional and powerful NLP embeddings techniques helps us to learn the similarity between applications not just limited to the shared api, and also the ability to identify the clusters connection between application and api that do not always have a direct connection. Using gensim’s word2vec model, we are able to generate vector embeddings for each application and api found in the text corpus. We successfully converted decompiled Android source code into a vector of numbers for each application and this information can be easily used in a machine learning model.

To evaluate the effectiveness of the generated embeddings, we visualize the embedding clusters by applying dimensionality reduction into two dimensional vectors. The embedding visualization for metapath APA is shown below:

![APA](https://i.imgur.com/TnPyamV.png)

As word2vec does not generate embeddings for unseen words, test applications in our case, we trained a decision tree regressor using the true embeddings for training application as the labels, and the average of all the embeddings of each application’s associated api as the training data. Using this regressor we are able to generate embeddings for test applications using its associated api appear in the training corpus.

### Node2Vec

In node2vec, the entire Heterogeneous Information Network is regarded as an large homogeneous graph and the only theoretical difference to the word2vec approach is the random walk procedure. The graph traversal method is based on a graph where all different types of edges are merged together to be one. This change is adapting to the inability of node2vec to traverse according to a metapath but instead a truly random walk with no specific rules restricting where the next node would be. We choose a return parameter of 2 and a in-out parameter of 1 empirically to perform walks beginning on each app node for 100 times. This results in a corpus similar to the word2vec approach, so we could use the same methodology to match and predict different distributions of app and API embeddings.

As node2vec also does not generate embeddings for test appication, we use the similar approach to generate the embeddings for test application using associated API embeddings.

![node2vec](https://i.imgur.com/auK5rqj.png)

### Metapath2Vec

![metapath2vec equation](https://i.imgur.com/AINz4lr.png) (1)

Metapath2Vec is used as a technique of sampling our next node. We sample our next node using equation (1). Let's use an example to illustrate the process.

Imagine that we have these matrices set up, and our defined metapath is **ABA**. Our metapath-chosen sentence will look like ""app_A API_Y API_Z app_B"". An sample matrix looks something like the following:  
![Matrices](https://i.imgur.com/XIYFrc3.png)

Simplified steps:

1. Pick an app. This will replace app_A.
2. Go to the matrix corresponding to the metapath. For example, the first path in **ABA** is A, so we will look at the A matrix.
3. Go to the row corresponding to the app or API that was chosen.
4. Pick an API. Within a row, the APIs that have a value of 1 is picked using a uniform probability.
5. Repeat 2, 3, and 4 until you are ready to pick an app (app_B). With the API that was chosen (API_Z), look at the column and pick an app that has value 1 with uniform probaility.

![ABA](https://i.imgur.com/8N8IeYi.png)
![ABPBPBBPA](https://i.imgur.com/Wi5C3KW.png)
![ABABBABBBABBBBABBBBBA](https://i.imgur.com/etgIVjM.png)


## Results

Let's take a look at the different accuracies for the original HinDroid approach and the HinDroid approach with additional embedding techniques.

**HinDroid:**

| Metapath | train_acc | test_acc | F1     | TP    | FP   | TN    | FN   |
|----------|-----------|----------|--------|-------|------|-------|------|
| AA       | 1.0000    | 0.9561   | 0.9562 | 158   | 10   | 147   | 4    |
| APA      | 1.0000    | 0.9373   | 0.9412 | 155   | 14   | 145   | 6    |
| ABA      | 0.9149    | 0.8558   | 0.8671 | 147   | 27   | 130   | 19   |
| APBPA    | 0.9040    | 0.8339   | 0.8408 | 140   | 32   | 126   | 22   |

**Reduced:**

| Metapath | train_acc | test_acc | F1     | TP    | FP   | TN    | FN   |
|----------|-----------|----------|--------|-------|------|-------|------|
| AA       | 1.0000    | 0.9561   | 0.9562 | 158   | 10   | 147   | 4    |
| APA      | 1.0000    | 0.9373   | 0.9412 | 155   | 14   | 145   | 6    |
| ABA      | 0.9149    | 0.8558   | 0.8671 | 147   | 27   | 130   | 19   |
| APBPA    | 0.9040    | 0.8339   | 0.8408 | 140   | 32   | 126   | 22   |

**Word2Vec**

| Metapath  | Accuracy | F1     | TN  | FP | FN | TP  |
|-----------|----------|--------|-----|----|----|-----|
| ABA       | 95.06%   | 95.02% | 639 | 32 | 34 | 630 |
| ABPBA     | 94.61%   | 94.59% | 634 | 37 | 35 | 629 |
| APA       | 95.73%   | 95.68% | 647 | 24 | 33 | 631 |
| APBPA     | 94.61%   | 94.55% | 638 | 33 | 39 | 625 |

**Node2Vec**

| Metapath  | Accuracy | F1     | TN  | FP | FN | TP  |
|-----------|----------|--------|-----|----|----|-----|
| N/A       | 97.75%   | 97.77% | 648 | 18 | 12 | 657 |

**Metapath2Vec:**

| Metapath              | train_acc | test_acc | F1     | TP    | FP   | TN    | FN   |
|-----------------------|-----------|----------|--------|-------|------|-------|------|
| AA                    | 0.9736    | 0.9476   | 0.9466 | 621   | 27   | 644   | 43   |
| APA                   | 0.9955    | 0.9296   | 0.9277 | 603   | 33   | 638   | 61   |
| ABA                   | 0.9864    | 0.9633   | 0.9626 | 630   | 15   | 656   | 34   |
| APBPA                 | 0.9900    | 0.9438   | 0.9419 | 608   | 19   | 652   | 56   |
| ABPBA                 | 0.9982    | 0.9524   | 0.9524 | 614   | 10   | 661   | 50   |
| ABPBPBBPA             | 0.9973    | 0.9476   | 0.9455 | 607   | 13   | 658   | 57   |
| ABABBABBBABBBBABBBBBA | 0.9545    | 0.9026   | 0.9027 | 603   | 69   | 602   | 61   |

## Conclusion

From our initial testing, all of our proposed graph embedding techniques are able to achieve similar accuracy and metrics scores. Although it does not seem that the different graph embeddings obtained a higher accuracy score, we believe that using these other graph embedding techniques not only matches with the results of HinDroid, but are also more robust in the sense that hackers are not able to easily rearrange APIs to avoid detection.  

There is definitely further research to do. One being where dummy nodes are added. This will provide us with more evidence of the robustness of the different graph techniques used. The clusters that are present in the visualizations are suggestive of further analysis. Also, we should test out several more different meta-paths. We can test which meta-paths work the best and investigate why it works. We could also do concatenation of embeddings from different meta-paths to form a longer representation and test its effectiveness. 

## References

[1] Passi, Harpreet. Introduction to Malware: Definition, Attacks, Types and Analysis. GreyCampus  
[2] Hou, Shifu and Ye, Yanfang and Song, Yangqiu and Abdulhayoglu, Melih. 2017. HinDroid: An Intelligent Android Malware Detection System Based on Structured Heterogeneous Information Network.  
[3] Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey. 2013. Efficient Estimation of Word Representations in Vector Space.  
[4] Grover, Aditya and Leskovec, Jure. 2016. node2vec: Scalable Feature Learning for Networks.  
[5] Dong, Yuxiao and Chawla, Nitesh and Swami, Ananthram. 2017. metapath2vec: Scalable Representation Learning for Heterogeneous Networks  
","# HinDroid-with-Embeddings

![Docker Cloud Build Status](https://img.shields.io/docker/cloud/build/davidzz/hindroid-xl)

## Overview

Malware detection for android applications is an expanding field with the introduction of the [Hindroid](https://www.cse.ust.hk/~yqsong/papers/2017-KDD-HINDROID.pdf) model (DOI:[10.1145/3097983.3098026](https://doi.org/10.1145/3097983.3098026)). It proposes a method that transforms the semantic relationships between Android applications and their decompiled source code to a Heterogeneous Information Network (HIN) and uses similarities from various meta-paths between apps to construct a model for malware classification. To further explore the field, we aim to extend the HinDroid model to improve the accuracy in specific subsets of the AMD dataset. Our effort will be focused on finding better representations for both apps as well as APIs and discovering methods to incorporate them as additional features in a new model. In the meantime, we plan to evaluate how the proposed model captures the features that are relevant to the classification task and compare to that of the HinDroid baseline. Our contributions can be utilized in systems where the analysis of malware and interpretable features are more important than mere detection.

## Usage
Docker image on Docker Hub:
[davidzyx/hindroid-xl](https://hub.docker.com/repository/docker/davidzz/hindroid)

On Datahub, create a pod using a custom image with 4 CPU and 32 GB of memory. If you use another configuration, use at least 6GB memory per CPU.
```bash
launch.sh -i davidzz/hindroid-xl -c 4 -m 32
```

Modify the config file located in `config/data-params.json`. If you want to run a test drive, use `config/test-params.json`. Put either `data` or `test` as the first argument.

The HinDroid baseline uses the driver file `run.py` with 3 targets: `ingest`, `process`, and `model`. Put each target space-separated as arguments in the call. To run the whole pipeline, use
```bash
python run.py data ingest process model
```

`process` target will save `.npz` files in `data/processed/` for generating various embeddings.

## System Prerequisites and Definitions

- APK - Executable file for Android
- Smali code - Human readable code decompiled from Dalvik bytecode contained the APK
- Heterogeneous Information Network (HIN) - A graph where its nodes may not be of the same type
- API Extraction
  - Use regex to match specific patterns
  - API calls and method blocks
![api](https://i.imgur.com/nx3rKKv.png)

## HinDroid Efficiency Improvements

In HinDroid, the amount of APIs that are used in the final gram matrix calculations can have dimension of several millions. Even in sparse format, these matrices take up huge computational resources for calculation. We are able to reduce the number of API used in HinDroid to 1000 APIs while retaining the same level of accuracy. Both the time and space complexity for training and inference can be improved by a few orders of magnitude. We achieve this by selecting the top important (larger absolute coefficients) APIs (words) from fitting a logistic regression on each app (document) after applying BM25 extraction on the counts of each API for each app.

| Method            | # of Apps | # of APIs used | RAM used | train+test time |
|-------------------|-----------|----------------|----------|-----------------|
| HinDroid-original | 1670      | 2024313        | 68GB     | 3h29m41s        |
| HinDroid-reduced  | 1670      | 1000           | 0.3GB    | 20s             |

## Embedding Techniques Explored

As we are using NLP approaches such as word2vec which cannot be directly applied to application source code and matrices from HinDroid, our data ingestion pipeline generates text corpus by traversing the graphs following an user defined metapaths and the length of a random walk. Using a metapath `ABPBA` with random walk length 5000, the text corpus may look like 

`app_3 -> api_500 -> api_321 -> api_234 -> api_578 -> app_321 -> api_123…`

where each token represents an application or api node and the neighbouring tokens are
connected by edges in the graph.

### Word2Vec

In a graph where there are two types of nodes: application and api, Word2Vec is the first approach that we attempt to capture the relationship beyond application and apis that have a direct connection in the graph. This traditional and powerful NLP embeddings techniques helps us to learn the similarity between applications not just limited to the shared api, and also the ability to identify the clusters connection between application and api that do not always have a direct connection. Using gensim’s word2vec model, we are able to generate vector embeddings for each application and api found in the text corpus. We successfully converted decompiled Android source code into a vector of numbers for each application and this information can be easily used in a machine learning model.

To evaluate the effectiveness of the generated embeddings, we visualize the embedding clusters by applying dimensionality reduction into two dimensional vectors. The embedding visualization for metapath APA is shown below:

![APA](https://i.imgur.com/TnPyamV.png)

As word2vec does not generate embeddings for unseen words, test applications in our case, we trained a decision tree regressor using the true embeddings for training application as the labels, and the average of all the embeddings of each application’s associated api as the training data. Using this regressor we are able to generate embeddings for test applications using its associated api appear in the training corpus.

### Node2Vec

In node2vec, the entire Heterogeneous Information Network is regarded as an large homogeneous graph and the only theoretical difference to the word2vec approach is the random walk procedure. The graph traversal method is based on a graph where all different types of edges are merged together to be one. This change is adapting to the inability of node2vec to traverse according to a metapath but instead a truly random walk with no specific rules restricting where the next node would be. We choose a return parameter of 2 and a in-out parameter of 1 empirically to perform walks beginning on each app node for 100 times. This results in a corpus similar to the word2vec approach, so we could use the same methodology to match and predict different distributions of app and API embeddings.

As node2vec also does not generate embeddings for test appication, we use the similar approach to generate the embeddings for test application using associated API embeddings.

![node2vec](https://i.imgur.com/auK5rqj.png)

### Metapath2Vec

![metapath2vec equation](https://i.imgur.com/AINz4lr.png) (1)

Metapath2Vec is used as a technique of sampling our next node. We sample our next node using equation (1). Let's use an example to illustrate the process.

Imagine that we have these matrices set up, and our defined metapath is **ABA**. Our metapath-chosen sentence will look like ""app_A API_Y API_Z app_B"". An sample matrix looks something like the following:  
![Matrices](https://i.imgur.com/XIYFrc3.png)

Simplified steps:

1. Pick an app. This will replace app_A.
2. Go to the matrix corresponding to the metapath. For example, the first path in **ABA** is A, so we will look at the A matrix.
3. Go to the row corresponding to the app or API that was chosen.
4. Pick an API. Within a row, the APIs that have a value of 1 is picked using a uniform probability.
5. Repeat 2, 3, and 4 until you are ready to pick an app (app_B). With the API that was chosen (API_Z), look at the column and pick an app that has value 1 with uniform probaility.

![ABA](https://i.imgur.com/8N8IeYi.png)
![ABPBPBBPA](https://i.imgur.com/Wi5C3KW.png)
![ABABBABBBABBBBABBBBBA](https://i.imgur.com/etgIVjM.png)


## Results

Let's take a look at the different accuracies for the original HinDroid approach and the HinDroid approach with additional embedding techniques.

**HinDroid:**

| Metapath | train_acc | test_acc | F1     | TP    | FP   | TN    | FN   |
|----------|-----------|----------|--------|-------|------|-------|------|
| AA       | 1.0000    | 0.9561   | 0.9562 | 158   | 10   | 147   | 4    |
| APA      | 1.0000    | 0.9373   | 0.9412 | 155   | 14   | 145   | 6    |
| ABA      | 0.9149    | 0.8558   | 0.8671 | 147   | 27   | 130   | 19   |
| APBPA    | 0.9040    | 0.8339   | 0.8408 | 140   | 32   | 126   | 22   |

**Reduced:**

| Metapath | train_acc | test_acc | F1     | TP    | FP   | TN    | FN   |
|----------|-----------|----------|--------|-------|------|-------|------|
| AA       | 1.0000    | 0.9561   | 0.9562 | 158   | 10   | 147   | 4    |
| APA      | 1.0000    | 0.9373   | 0.9412 | 155   | 14   | 145   | 6    |
| ABA      | 0.9149    | 0.8558   | 0.8671 | 147   | 27   | 130   | 19   |
| APBPA    | 0.9040    | 0.8339   | 0.8408 | 140   | 32   | 126   | 22   |

**Word2Vec**

| Metapath  | Accuracy | F1     | TN  | FP | FN | TP  |
|-----------|----------|--------|-----|----|----|-----|
| ABA       | 95.06%   | 95.02% | 639 | 32 | 34 | 630 |
| ABPBA     | 94.61%   | 94.59% | 634 | 37 | 35 | 629 |
| APA       | 95.73%   | 95.68% | 647 | 24 | 33 | 631 |
| APBPA     | 94.61%   | 94.55% | 638 | 33 | 39 | 625 |

**Node2Vec**

| Metapath  | Accuracy | F1     | TN  | FP | FN | TP  |
|-----------|----------|--------|-----|----|----|-----|
| N/A       | 97.75%   | 97.77% | 648 | 18 | 12 | 657 |

**Metapath2Vec:**

| Metapath              | train_acc | test_acc | F1     | TP    | FP   | TN    | FN   |
|-----------------------|-----------|----------|--------|-------|------|-------|------|
| AA                    | 0.9736    | 0.9476   | 0.9466 | 621   | 27   | 644   | 43   |
| APA                   | 0.9955    | 0.9296   | 0.9277 | 603   | 33   | 638   | 61   |
| ABA                   | 0.9864    | 0.9633   | 0.9626 | 630   | 15   | 656   | 34   |
| APBPA                 | 0.9900    | 0.9438   | 0.9419 | 608   | 19   | 652   | 56   |
| ABPBA                 | 0.9982    | 0.9524   | 0.9524 | 614   | 10   | 661   | 50   |
| ABPBPBBPA             | 0.9973    | 0.9476   | 0.9455 | 607   | 13   | 658   | 57   |
| ABABBABBBABBBBABBBBBA | 0.9545    | 0.9026   | 0.9027 | 603   | 69   | 602   | 61   |

## Conclusion

From our initial testing, all of our proposed graph embedding techniques are able to achieve similar accuracy and metrics scores. Although it does not seem that the different graph embeddings obtained a higher accuracy score, we believe that using these other graph embedding techniques not only matches with the results of HinDroid, but are also more robust in the sense that hackers are not able to easily rearrange APIs to avoid detection.  

There is definitely further research to do. One being where dummy nodes are added. This will provide us with more evidence of the robustness of the different graph techniques used. The clusters that are present in the visualizations are suggestive of further analysis. Also, we should test out several more different meta-paths. We can test which meta-paths work the best and investigate why it works. We could also do concatenation of embeddings from different meta-paths to form a longer representation and test its effectiveness. 

## References

[1] Passi, Harpreet. Introduction to Malware: Definition, Attacks, Types and Analysis. GreyCampus  
[2] Hou, Shifu and Ye, Yanfang and Song, Yangqiu and Abdulhayoglu, Melih. 2017. HinDroid: An Intelligent Android Malware Detection System Based on Structured Heterogeneous Information Network.  
[3] Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey. 2013. Efficient Estimation of Word Representations in Vector Space.  
[4] Grover, Aditya and Leskovec, Jure. 2016. node2vec: Scalable Feature Learning for Networks.  
[5] Dong, Yuxiao and Chawla, Nitesh and Swami, Ananthram. 2017. metapath2vec: Scalable Representation Learning for Heterogeneous Networks  
"
20,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_21/,,,"# Hinreddit

[project website](https://syeehyn.github.io/hinreddit/)

![Docker Cloud Build Status](https://img.shields.io/docker/cloud/build/syeehyn/hinreddit)

As social platforms become accessible nowadays, more and more people get used to posting opinions on various topics online. The existence of nagetive online behaviors such as hateful comments is also unavoidable. These platforms thus become prolific sources for hate detection, which motivates many people to apply various techniques in order to detect hateful users or hateful speeches.

This project investigates contents from Reddit. its goal is to classify hateful posts from the normal ones. This not only enables platforms to improve user experiences, but also helps to maintain a positive online environment.

- [Hinreddit](#hinreddit)
  - [Getting Started](#getting-started)
    - [Prerequisite](#prerequisite)
      - [Use Dockerfile](#use-dockerfile)
    - [Usage](#usage)
      - [etl](#etl)
      - [graph](#graph)
      - [embedding](#embedding)
      - [pipeline](#pipeline)
  - [For Developers](#for-developers)
  - [Contribution](#contribution)
    - [Authors](#authors)
    - [Advisors](#advisors)
  - [References](#references)
  - [License](#license)

----

## Getting Started

### Prerequisite

The project is mainly built upon following packages:

- Data Preprocessing & Feature Extraction

  - [Pandas](https://pandas.pydata.org/)
  
- Labeling & ML Deployment

  - [Pytorch = 1.5](https://pytorch.org/)
  
  - [PyTorch Geometric = 1.5](https://github.com/rusty1s/pytorch_geometric)

#### Use Dockerfile

  You can build a docker image out of the provided [DockerFile](Dockerfile)

  ```bash
  docker build . # This will build using the same env as in a)
  ```

  Run a container, replacing the ID with the output of the previous command

  ```bash
  docker run -it -p 8888:8888 -p 8787:8787 <container_id_or_tag>
  ```

  The above command will give an URL (Like http://(container_id or 127.0.0.1):8888/?token=<sometoken>) which can be used to access the notebook from browser. You may need to replace the given hostname with ""localhost"" or ""127.0.0.1"".

### Usage

#### etl

 - Modify the config file located in `config/data-params.json`. For testing, use `config/test-params.json`, you may define an output root `[data-path]` under `config/data-params.json`.

 - **The HinReddit's etl process uses the python script file `run.py` with target `data[-test]`.**

 - You may change the `nlp_model.zip` file with custom nlp labeling rules.

 - The etl process result will be under ""\<data-path>/raw"" and ""\<data-path>/interim/label"" directories.

#### graph

- **The HinReddit's graph process uses the python script file `run.py` with target `graph[-test]`.**

- The graph process result will be under ""\<data-path>/interim/graph/*.mat""

#### embedding

- Modify the config files located in `config/embedding/graph_<1/2>/[test-]<informax/metapath2vec/node2vec>.json` for corresponding parameters of the embedding models.

- **The HinReddit's embedding process uses the python script file `run.py` with following targets:**

  - `node2vec[-test]`: for node2vec embedding.
  - `metapath2vec[-test]`: for metapath2vec embedding.
  - `infomax[-test]`: for deep graph infomax (DGI) embedding.

#### pipeline

- run `$ python run.py data[-test] graph[-test] node2vec[-test] metapath2vec[-test] infomax[-test]`

- You can find a detailed explaination of configuration arguments [here](./writeups/PARAMS.md)

----

## For Developers

[Development Guide](./writeups/DEVGUIDE.md) is provided and under `./writeups/DEVGUIDE.md`

----

## Contribution

### Authors

- [Chengyu Chen](https://github.com/anniechen0127)
- [Yu-chun Chen](https://github.com/yuc330)
- [Yanyu Tao](https://github.com/lilytaoyy)
- [Shuibenyang Yuan](https://github.com/shy166)

### Advisors

- [Aaron Fraenkel](https://afraenkel.github.io/)
- [Shivam Lakhotia](https://github.com/shivamlakhotia)

<sup>Authors contributed equally to this project</sup>

----

## References

``` 
@paper{Hou/Ye/2017,
  title={HinDroid: {An Intelligent Android Malware Detection System Based on Structured Heterogeneous Information Network}},
  author={Hou, Ye, Song, Abdulhayoglu}
  year={2017}
}
@inproceedings{Fey/Lenssen/2019,
  title={Fast Graph Representation Learning with {PyTorch Geometric}},
  author={Fey, Matthias and Lenssen, Jan E.},
  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},
  year={2019},
}
@article{turc2019,
  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1908.08962v2 },
  year={2019}
}
@course{Koutra/2018,
  title={Mining Large-scale Graph Data},
  author={Danai Koutra},
  link={http://web.eecs.umich.edu/~dkoutra/courses/W18_598/},
  year={2018}
}
@collection{src-d/2019,
  title={Awesome Machine Learning On Source Code},
  author={src-d},
  link={https://github.com/src-d/awesome-machine-learning-on-source-code},
  year={2019}
}
```

----

## License

[Apache License 2.0](LICENSE)
","# Hinreddit

[project website](https://syeehyn.github.io/hinreddit/)

![Docker Cloud Build Status](https://img.shields.io/docker/cloud/build/syeehyn/hinreddit)

As social platforms become accessible nowadays, more and more people get used to posting opinions on various topics online. The existence of nagetive online behaviors such as hateful comments is also unavoidable. These platforms thus become prolific sources for hate detection, which motivates many people to apply various techniques in order to detect hateful users or hateful speeches.

This project investigates contents from Reddit. its goal is to classify hateful posts from the normal ones. This not only enables platforms to improve user experiences, but also helps to maintain a positive online environment.

- [Hinreddit](#hinreddit)
  - [Getting Started](#getting-started)
    - [Prerequisite](#prerequisite)
      - [Use Dockerfile](#use-dockerfile)
    - [Usage](#usage)
      - [etl](#etl)
      - [graph](#graph)
      - [embedding](#embedding)
      - [pipeline](#pipeline)
  - [For Developers](#for-developers)
  - [Contribution](#contribution)
    - [Authors](#authors)
    - [Advisors](#advisors)
  - [References](#references)
  - [License](#license)

----

## Getting Started

### Prerequisite

The project is mainly built upon following packages:

- Data Preprocessing & Feature Extraction

  - [Pandas](https://pandas.pydata.org/)
  
- Labeling & ML Deployment

  - [Pytorch = 1.5](https://pytorch.org/)
  
  - [PyTorch Geometric = 1.5](https://github.com/rusty1s/pytorch_geometric)

#### Use Dockerfile

  You can build a docker image out of the provided [DockerFile](Dockerfile)

  ```bash
  docker build . # This will build using the same env as in a)
  ```

  Run a container, replacing the ID with the output of the previous command

  ```bash
  docker run -it -p 8888:8888 -p 8787:8787 <container_id_or_tag>
  ```

  The above command will give an URL (Like http://(container_id or 127.0.0.1):8888/?token=<sometoken>) which can be used to access the notebook from browser. You may need to replace the given hostname with ""localhost"" or ""127.0.0.1"".

### Usage

#### etl

 - Modify the config file located in `config/data-params.json`. For testing, use `config/test-params.json`, you may define an output root `[data-path]` under `config/data-params.json`.

 - **The HinReddit's etl process uses the python script file `run.py` with target `data[-test]`.**

 - You may change the `nlp_model.zip` file with custom nlp labeling rules.

 - The etl process result will be under ""\<data-path>/raw"" and ""\<data-path>/interim/label"" directories.

#### graph

- **The HinReddit's graph process uses the python script file `run.py` with target `graph[-test]`.**

- The graph process result will be under ""\<data-path>/interim/graph/*.mat""

#### embedding

- Modify the config files located in `config/embedding/graph_<1/2>/[test-]<informax/metapath2vec/node2vec>.json` for corresponding parameters of the embedding models.

- **The HinReddit's embedding process uses the python script file `run.py` with following targets:**

  - `node2vec[-test]`: for node2vec embedding.
  - `metapath2vec[-test]`: for metapath2vec embedding.
  - `infomax[-test]`: for deep graph infomax (DGI) embedding.

#### pipeline

- run `$ python run.py data[-test] graph[-test] node2vec[-test] metapath2vec[-test] infomax[-test]`

- You can find a detailed explaination of configuration arguments [here](./writeups/PARAMS.md)

----

## For Developers

[Development Guide](./writeups/DEVGUIDE.md) is provided and under `./writeups/DEVGUIDE.md`

----

## Contribution

### Authors

- [Chengyu Chen](https://github.com/anniechen0127)
- [Yu-chun Chen](https://github.com/yuc330)
- [Yanyu Tao](https://github.com/lilytaoyy)
- [Shuibenyang Yuan](https://github.com/shy166)

### Advisors

- [Aaron Fraenkel](https://afraenkel.github.io/)
- [Shivam Lakhotia](https://github.com/shivamlakhotia)

<sup>Authors contributed equally to this project</sup>

----

## References

``` 
@paper{Hou/Ye/2017,
  title={HinDroid: {An Intelligent Android Malware Detection System Based on Structured Heterogeneous Information Network}},
  author={Hou, Ye, Song, Abdulhayoglu}
  year={2017}
}
@inproceedings{Fey/Lenssen/2019,
  title={Fast Graph Representation Learning with {PyTorch Geometric}},
  author={Fey, Matthias and Lenssen, Jan E.},
  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},
  year={2019},
}
@article{turc2019,
  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1908.08962v2 },
  year={2019}
}
@course{Koutra/2018,
  title={Mining Large-scale Graph Data},
  author={Danai Koutra},
  link={http://web.eecs.umich.edu/~dkoutra/courses/W18_598/},
  year={2018}
}
@collection{src-d/2019,
  title={Awesome Machine Learning On Source Code},
  author={src-d},
  link={https://github.com/src-d/awesome-machine-learning-on-source-code},
  year={2019}
}
```

----

## License

[Apache License 2.0](LICENSE)
"
21,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_22/,,,"# recordLinkage
DSC 180B Project: Probabilistic Record Linkage

---
![Machine Learning Pipeline](./reports/images/pipeline.png)

The machine learning pipeline can be broken down in 3 steps:
1. graph construction
2. node2vec embedding
3. classifier.

---
To run the program, refer to the config folder for the parameters that can be changed for this program, and use the *python run.py* command to train, test, and evualate the model.Refer to the run.py file for the appropriate command line inputs.

For visualizations and testing can be found in the notebooks directory.

The paper associated with this paper can be found [here](./reports/final_report.pdf)

---
## Example Code

In order to generate the artificial dataset, you can use the command below:
```
python3 gen-data
```

In order to perform the graph construction, use the command:

```
python3 create-small-graphs
```

Lastly, in order to test any changes to the pipeline on example test, use the command:
```
python3 test-project
```

---
## Configuration

1. refer to ./config/datasetGenConfig.json to change the hyperparameters associated with the artificial dataset generation.
2. refer to ./config/test_config.json and ./config/train_config.json to change the hyperparamters associated with generating the graphs and training the classifier.

(add information about the configurations)","# recordLinkage
DSC 180B Project: Probabilistic Record Linkage

---
![Machine Learning Pipeline](./reports/images/pipeline.png)

The machine learning pipeline can be broken down in 3 steps:
1. graph construction
2. node2vec embedding
3. classifier.

---
To run the program, refer to the config folder for the parameters that can be changed for this program, and use the *python run.py* command to train, test, and evualate the model.Refer to the run.py file for the appropriate command line inputs.

For visualizations and testing can be found in the notebooks directory.

The paper associated with this paper can be found [here](./reports/final_report.pdf)

---
## Example Code

In order to generate the artificial dataset, you can use the command below:
```
python3 gen-data
```

In order to perform the graph construction, use the command:

```
python3 create-small-graphs
```

Lastly, in order to test any changes to the pipeline on example test, use the command:
```
python3 test-project
```

---
## Configuration

1. refer to ./config/datasetGenConfig.json to change the hyperparameters associated with the artificial dataset generation.
2. refer to ./config/test_config.json and ./config/train_config.json to change the hyperparamters associated with generating the graphs and training the classifier.

(add information about the configurations)"
22,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_23/,,,"# Malware-detection

## Abstract

Recent work introduced a model using a Heterogeneous Information Network (HIN) representation of Android applications utilizing a meta-path approach to link applications through the API calls contained within them. It was found with multi-kernel learning, the model was able to identify malicious applications with high accuracy. This recent work was the first approach of this kind to be published; therefore, a replication process would allow for deeper understanding of this approach. In this paper, we introduce a framework for improving upon the model through scalability and testable measures with the purpose of maintaining or increasing accuracy while creating an easily executable pipeline. In particular, we employ dimensionality reduction and stochastic techniques to achieve reasonably replicable results. Additionally, we attempt to understand, through model explainability practices, the inner mechanisms of the complex model to better understand possible inaccuracies which may arise in creating a scaled version of a HIN approach.


## Usage Instructions

In a terminal or command window, navigate to the top-level project directory `malware-detection/` and run

`python3 run.py test`

or

`python3 run.py`


* Instructions

`python3 run.py test` - Runs code on test set of 3 benign and 3 malware apps


`python3 run.py` - Runs project on 1000 benign and 1000 malware apps including data collection pipeline

## Output 


`test-data/processes/app_to_api.json`: structure to create A matrix

`test-data/processes/code_block.json`: structure to create B matrix

`test-data/processes/library_dic.json`: structure to create P matrix

`test-data/processes/test_app_api.json`: structure to create A matrix for test set

`test-data/processes/unique_api.text`: txt file with all unique API's

`test-data/matrix/a_matrix.npz`: Sparse format of A matrix 

`test-data/matrix/b_matrix.npz`: Sparse format of B matrix 

`test-data/matrix/p_matrix.npz`: Sparse format of P matrix 

`results/scores.csv`: Performace metrics of model on diffrent kernels

`charts`: Conatins all ouput charts 


## Description of Contents

The project consists of these portions:
```
PROJECT

├── README.md
├── config
│   ├── data-params.json
|   └── test-params.json
|   └── env.json
├── test-data
│   ├── smalli
│   └── processes
│       ├── app_to_api.json
|       ├── code_block.json
|       └── libray_dic.json 
├── notebooks
│   ├── eda.ipynb
|   ├── coefficient_explaining.ipynb
|   ├── feature_extraction.ipynb
|   ├── model.ipynb
|   └── malware_type.ipynb
├── reports
|   └── malware detection.pdf    
├── src
|    ├── __init__.py
|    ├── build_features.py
|    ├── make_dataset.py
|    ├── elt.py
|    ├─ multi-kernel.py
|    ├─ model.py
|    └──coefficient_analysis.py
├── requirements.txt
├── run.py

``` 

### `src`

* `etl.py`: Library code that executes tasks useful for getting data.
* `make_dataset.PY`: Library code that excutes task useful to cleaning and building dataset
* `build_features.py`: Library code that excutes task to extract features from dataset
* `model.py`: Library code  that excutes task to create and test model.
* `Multi-kernel.py`: Library code pertaining the creation of multi kernel
* `coefficient_analysis.py` : Library code that conatins analysis of model outputs 

### `config`
* `env.json`: conatains dokcer conatiner id and outpaths of all files created after running run.py

* `params.json`: Common parameters for getting data, serving as
  inputs to library code.
  
* `test-set.json`: parameters for running small process on small
  test data.


### `notebooks`

* Jupyter notebooks for *analyses*
  - Contains data cleaning, eda, building features and building the model.




","# Malware-detection

## Abstract

Recent work introduced a model using a Heterogeneous Information Network (HIN) representation of Android applications utilizing a meta-path approach to link applications through the API calls contained within them. It was found with multi-kernel learning, the model was able to identify malicious applications with high accuracy. This recent work was the first approach of this kind to be published; therefore, a replication process would allow for deeper understanding of this approach. In this paper, we introduce a framework for improving upon the model through scalability and testable measures with the purpose of maintaining or increasing accuracy while creating an easily executable pipeline. In particular, we employ dimensionality reduction and stochastic techniques to achieve reasonably replicable results. Additionally, we attempt to understand, through model explainability practices, the inner mechanisms of the complex model to better understand possible inaccuracies which may arise in creating a scaled version of a HIN approach.


## Usage Instructions

In a terminal or command window, navigate to the top-level project directory `malware-detection/` and run

`python3 run.py test`

or

`python3 run.py`


* Instructions

`python3 run.py test` - Runs code on test set of 3 benign and 3 malware apps


`python3 run.py` - Runs project on 1000 benign and 1000 malware apps including data collection pipeline

## Output 


`test-data/processes/app_to_api.json`: structure to create A matrix

`test-data/processes/code_block.json`: structure to create B matrix

`test-data/processes/library_dic.json`: structure to create P matrix

`test-data/processes/test_app_api.json`: structure to create A matrix for test set

`test-data/processes/unique_api.text`: txt file with all unique API's

`test-data/matrix/a_matrix.npz`: Sparse format of A matrix 

`test-data/matrix/b_matrix.npz`: Sparse format of B matrix 

`test-data/matrix/p_matrix.npz`: Sparse format of P matrix 

`results/scores.csv`: Performace metrics of model on diffrent kernels

`charts`: Conatins all ouput charts 


## Description of Contents

The project consists of these portions:
```
PROJECT

├── README.md
├── config
│   ├── data-params.json
|   └── test-params.json
|   └── env.json
├── test-data
│   ├── smalli
│   └── processes
│       ├── app_to_api.json
|       ├── code_block.json
|       └── libray_dic.json 
├── notebooks
│   ├── eda.ipynb
|   ├── coefficient_explaining.ipynb
|   ├── feature_extraction.ipynb
|   ├── model.ipynb
|   └── malware_type.ipynb
├── reports
|   └── malware detection.pdf    
├── src
|    ├── __init__.py
|    ├── build_features.py
|    ├── make_dataset.py
|    ├── elt.py
|    ├─ multi-kernel.py
|    ├─ model.py
|    └──coefficient_analysis.py
├── requirements.txt
├── run.py

``` 

### `src`

* `etl.py`: Library code that executes tasks useful for getting data.
* `make_dataset.PY`: Library code that excutes task useful to cleaning and building dataset
* `build_features.py`: Library code that excutes task to extract features from dataset
* `model.py`: Library code  that excutes task to create and test model.
* `Multi-kernel.py`: Library code pertaining the creation of multi kernel
* `coefficient_analysis.py` : Library code that conatins analysis of model outputs 

### `config`
* `env.json`: conatains dokcer conatiner id and outpaths of all files created after running run.py

* `params.json`: Common parameters for getting data, serving as
  inputs to library code.
  
* `test-set.json`: parameters for running small process on small
  test data.


### `notebooks`

* Jupyter notebooks for *analyses*
  - Contains data cleaning, eda, building features and building the model.




"
23,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_24/,,,"# The Food Chain - A Personalized Restaurant Recommender System

Leveraging Yelp data to develop a user-customizable, similarity-based restaurant recommendation system.

- About this project: http://team05.pythonanywhere.com/about  

- Try it for yourself:  http://team05.pythonanywhere.com/  

## Getting Started 

1.  `pip install -r requirements.txt`
2.  `python -m nltk.downloader all`
3.  `python -m textblob.download_corpora lite`
4.  `python main.py`


## Test Project Locally

1.  Install dependencies (see **Getting Started** through step 3).
2.  `python run.py test-project`

---
Questions? Please reach out to lpdoyle@ucsd.edu or dmarcusthierry@gmail.com :) 
","# The Food Chain - A Personalized Restaurant Recommender System

Leveraging Yelp data to develop a user-customizable, similarity-based restaurant recommendation system.

- About this project: http://team05.pythonanywhere.com/about  

- Try it for yourself:  http://team05.pythonanywhere.com/  

## Getting Started 

1.  `pip install -r requirements.txt`
2.  `python -m nltk.downloader all`
3.  `python -m textblob.download_corpora lite`
4.  `python main.py`


## Test Project Locally

1.  Install dependencies (see **Getting Started** through step 3).
2.  `python run.py test-project`

---
Questions? Please reach out to lpdoyle@ucsd.edu or dmarcusthierry@gmail.com :) 
"
24,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_25/,,,"# CodeHonestly
### Utilize AST graphs to detect code plagiarism

![](logo.png)

`python run.py test-project` to run the project

Note 1: Please make sure there are at least two Python files in data folder, not any subfolder. It is functional out of box.

Note 2: We didn't wrap all the codes into library codes as possible because we might use the code in actual product that we don't want to release a version that 's easy for future development by others.

Website: https://www.codehonestly.com/

This project was founded in DSC 180B, a capstone project for data science undergraduate students at UCSD.
","# CodeHonestly
### Utilize AST graphs to detect code plagiarism

![](logo.png)

`python run.py test-project` to run the project

Note 1: Please make sure there are at least two Python files in data folder, not any subfolder. It is functional out of box.

Note 2: We didn't wrap all the codes into library codes as possible because we might use the code in actual product that we don't want to release a version that 's easy for future development by others.

Website: https://www.codehonestly.com/

This project was founded in DSC 180B, a capstone project for data science undergraduate students at UCSD.
"
25,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_26/,,,"Presentation: https://drive.google.com/open?id=1ick7HAF_w0bjRSVxWURykakYpwKsDwDY


Website: https://nancyvuong.github.io/dsc180b_website/

To run pipeline on test data run python3 run.py test-project.
 
 
The project consists of these portions:
```
PROJECT
├── .gitignore
├── README.md
├── config
│   ├── data-params.json
│   ├── test-params.json
│   └── env.json
├── Notebooks
│   ├── API_Relationships (Model Explainability).ipynb
│   ├── Co_Occurence_EDA.ipynb
│   ├── Create_matrix.ipynb
│   ├── EDA.ipynb
│   ├── Lime.ipynb
│   ├── SVM_Explained.ipynb
│   ├── Scrape-APK.ipynb
│   └── Statistical test.ipynb
├── references
│   └── Hindroid.pdf
├── run.py
├── test-data
│   ├── APK/dating
│   ├── apk_data
│   └── xml_files/dating
└── src
    ├── Correlation_Coef.py
    ├── Filter_Coef.py
    ├── Malware_Types.py
    ├── Percent_API.py
    ├── baseline.py
    ├── create_matricies.py
    ├── model.py
    └── scrape_apk.py
    
```

### `src`

* `Correlation_Coef.py`: Calculated correlation coef for each API in each kernel

* `Filter_Coef.py`: Filters API by corelation coef values

* `Malware_Types.py`: Obtains type and category of malware from amd.arguslab.org 

* `Percent_API.py`: Ranking Algorithim for APIs

* `baseline.py`: Baseline Hindorid model

* `create_matricies.py`: Create A,B and P matricies/kernels for the Hindroid Model

* `model.py`: Final model used for multi-class malware detection

* `scrape_apk.py`: Obtains the Benign apps from https://apkpure.com/sitemap.xml



### 'Reference'

  * http://yes-lab.org/files/HinDroid_KDD2017_Slides_Ye.pdf
  
","Presentation: https://drive.google.com/open?id=1ick7HAF_w0bjRSVxWURykakYpwKsDwDY


Website: https://nancyvuong.github.io/dsc180b_website/

To run pipeline on test data run python3 run.py test-project.
 
 
The project consists of these portions:
```
PROJECT
├── .gitignore
├── README.md
├── config
│   ├── data-params.json
│   ├── test-params.json
│   └── env.json
├── Notebooks
│   ├── API_Relationships (Model Explainability).ipynb
│   ├── Co_Occurence_EDA.ipynb
│   ├── Create_matrix.ipynb
│   ├── EDA.ipynb
│   ├── Lime.ipynb
│   ├── SVM_Explained.ipynb
│   ├── Scrape-APK.ipynb
│   └── Statistical test.ipynb
├── references
│   └── Hindroid.pdf
├── run.py
├── test-data
│   ├── APK/dating
│   ├── apk_data
│   └── xml_files/dating
└── src
    ├── Correlation_Coef.py
    ├── Filter_Coef.py
    ├── Malware_Types.py
    ├── Percent_API.py
    ├── baseline.py
    ├── create_matricies.py
    ├── model.py
    └── scrape_apk.py
    
```

### `src`

* `Correlation_Coef.py`: Calculated correlation coef for each API in each kernel

* `Filter_Coef.py`: Filters API by corelation coef values

* `Malware_Types.py`: Obtains type and category of malware from amd.arguslab.org 

* `Percent_API.py`: Ranking Algorithim for APIs

* `baseline.py`: Baseline Hindorid model

* `create_matricies.py`: Create A,B and P matricies/kernels for the Hindroid Model

* `model.py`: Final model used for multi-class malware detection

* `scrape_apk.py`: Obtains the Benign apps from https://apkpure.com/sitemap.xml



### 'Reference'

  * http://yes-lab.org/files/HinDroid_KDD2017_Slides_Ye.pdf
  
"
26,https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_27/,,,"# Graph-Based-Malware-Prediction
This project analyzes multiple graph embeddings for malware predictions based on their smali codes. Project results and interactive demonstrations can be found on our [project website](https://maishuliang.github.io/malware-detection-viz/)

## Overview

Nowadays, Android is dominating the smartphone market as an open source and customizable operating system. Many hackers targeted Android applications by disseminating malwares, posing serious threats to users. Historically, mobile security products such as Norton and Lookout, are heavily relied upon as major defense against such threats. Recently, many machine learning based methods have been invented for malware detection. A successful one of them is creating features from a Heterogeneous Information Network ([HinDroid](https://www.cse.ust.hk/~yqsong/papers/2017-KDD-HINDROID.pdf)). However, it is confined in such a way that it ignores more comprehensive information which can be extracted from graph representation. In this project, we will explore different meta-paths and incorporate various graph embedding methods in the task of malware prediction. We propose to build upon our previous work in HinDroid replication, more specifically we will attempt to use deep learning graph embedding techniques including Node2vec and Metapath2vec.

## Usage
Docker image on Docker Hub:
[b4zhang/malware_detection_with_graph_embedding](https://hub.docker.com/r/b4zhang/malware_detection_with_graph_embedding)

On Datahub, create a pod using the custom image.

To test-run the project, use or modify the default `config/test-params.json` and run
```bash
python run.py test
```

To run the project, modify the config file `config/data-params.json` and run
```bash
python run.py data process
```
or run `data` and `process` separately.

`data` target will save decompiled apk files under the folder as specified by the argument `apk_out_path` in `config/data-params.json`.

`process` target will save the Word2Vec model and Neural Network Model under the folder as specified by the argument `model_out_path` in `config/data-params.json`.
","# Graph-Based-Malware-Prediction
This project analyzes multiple graph embeddings for malware predictions based on their smali codes. Project results and interactive demonstrations can be found on our [project website](https://maishuliang.github.io/malware-detection-viz/)

## Overview

Nowadays, Android is dominating the smartphone market as an open source and customizable operating system. Many hackers targeted Android applications by disseminating malwares, posing serious threats to users. Historically, mobile security products such as Norton and Lookout, are heavily relied upon as major defense against such threats. Recently, many machine learning based methods have been invented for malware detection. A successful one of them is creating features from a Heterogeneous Information Network ([HinDroid](https://www.cse.ust.hk/~yqsong/papers/2017-KDD-HINDROID.pdf)). However, it is confined in such a way that it ignores more comprehensive information which can be extracted from graph representation. In this project, we will explore different meta-paths and incorporate various graph embedding methods in the task of malware prediction. We propose to build upon our previous work in HinDroid replication, more specifically we will attempt to use deep learning graph embedding techniques including Node2vec and Metapath2vec.

## Usage
Docker image on Docker Hub:
[b4zhang/malware_detection_with_graph_embedding](https://hub.docker.com/r/b4zhang/malware_detection_with_graph_embedding)

On Datahub, create a pod using the custom image.

To test-run the project, use or modify the default `config/test-params.json` and run
```bash
python run.py test
```

To run the project, modify the config file `config/data-params.json` and run
```bash
python run.py data process
```
or run `data` and `process` separately.

`data` target will save decompiled apk files under the folder as specified by the argument `apk_out_path` in `config/data-params.json`.

`process` target will save the Word2Vec model and Neural Network Model under the folder as specified by the argument `model_out_path` in `config/data-params.json`.
"
27,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_53,,,"# FaceMaskDetection
In an attempt to bring more transparency in artificial intelligence in a high stakes situation such as the Coronavirus pandemic, our aim was to create a model that would be able to determine if an individual was wearing a mask correctly, incorrectly, or not at all. Utilizing a subsection of the dataset MaskedFace-Net, we were able to train a model with the Inception Resnet V1 model. Moreover, as this dataset further breaks down incorrect mask usage into why, such as uncovered chin, mouth, or nose area, we aimed to apply GradCAM in order to build transparency and trust, and ultimately ensure that our model was coming to the conclusion for the right reasons.


## Project Stucture

#### Config files

These files contain important links and file paths to images and our dataset that are used by our run files 

#### Presentation

These contain brief snippets of our notebook to give you an idea of how our model was built and the underlying code and output for different features of our project. Our notebook GradCam EDA looks at implementing an algorithm that can identify what our neural network would look at to identify whether a mask would be work correctly and this is displayed in the gradcam presentation

#### run.py

This is our main file run file that calls in methods given in gradcam.py and etl.py. To run on a given image type the command  ```python run.py test``` and in order to edit the file_path for another image, run the command  ```python run.py run_grad``` and give the file path in the data_input.py file to test it out. 

### src
The src folder contains information on the functions used to train the model and our config folder contains parameter information that simplifies the working of run.py. 

## Usage of GradCam

In order to run out code on a given input path, type the command ```python run.py test``` from the main directory. 

This calls the etl.py function which presents a list of stats for our images in our dataset as well as invokes the gradcam class defined in **gradcam.py** 

Based on a predefinied path, Gradcam will be applied to the image rendering a heatmap of what the netowrk looked at to make our prediction. As seen below here are examples of what GradCam looked at to make a prediction regarding the correct wearing of FaceMaks. This increases one trust in the Neural Netowrk as it bceomes more Explainable to the Human Eye. 

![image](https://drive.google.com/uc?export=view&id=10EIantVsmZLYXwfyJI6VtpXCQ1fwNJhS)
![image](https://drive.google.com/uc?export=view&id=1kqw8QJYPR7vOBCco7p4XcVZ7xQKexdIR)

Looking at these images, neural networks make a lot more sense intuitively as we know why our network made that prediction.


# Results and Discussion

The result of our model was an accuracy of 96% in being able to classify between the three classes: improper face mask usage, no mask, and proper face mask usage. In terms of Grad-CAM, the implementation was successful in building trust and transparency within our model: the model was looking at the correct areas to determine the face mask usage.

# Website and Frontend links
Checkout our website and demo: https://elizabethmkim.github.io/FaceMaskDetection/

Checkout our frontend repo:  https://github.com/elizabethmkim/FaceMaskDetection 
","# FaceMaskDetection
In an attempt to bring more transparency in artificial intelligence in a high stakes situation such as the Coronavirus pandemic, our aim was to create a model that would be able to determine if an individual was wearing a mask correctly, incorrectly, or not at all. Utilizing a subsection of the dataset MaskedFace-Net, we were able to train a model with the Inception Resnet V1 model. Moreover, as this dataset further breaks down incorrect mask usage into why, such as uncovered chin, mouth, or nose area, we aimed to apply GradCAM in order to build transparency and trust, and ultimately ensure that our model was coming to the conclusion for the right reasons.


## Project Stucture

#### Config files

These files contain important links and file paths to images and our dataset that are used by our run files 

#### Presentation

These contain brief snippets of our notebook to give you an idea of how our model was built and the underlying code and output for different features of our project. Our notebook GradCam EDA looks at implementing an algorithm that can identify what our neural network would look at to identify whether a mask would be work correctly and this is displayed in the gradcam presentation

#### run.py

This is our main file run file that calls in methods given in gradcam.py and etl.py. To run on a given image type the command  ```python run.py test``` and in order to edit the file_path for another image, run the command  ```python run.py run_grad``` and give the file path in the data_input.py file to test it out. 

### src
The src folder contains information on the functions used to train the model and our config folder contains parameter information that simplifies the working of run.py. 

## Usage of GradCam

In order to run out code on a given input path, type the command ```python run.py test``` from the main directory. 

This calls the etl.py function which presents a list of stats for our images in our dataset as well as invokes the gradcam class defined in **gradcam.py** 

Based on a predefinied path, Gradcam will be applied to the image rendering a heatmap of what the netowrk looked at to make our prediction. As seen below here are examples of what GradCam looked at to make a prediction regarding the correct wearing of FaceMaks. This increases one trust in the Neural Netowrk as it bceomes more Explainable to the Human Eye. 

![image](https://drive.google.com/uc?export=view&id=10EIantVsmZLYXwfyJI6VtpXCQ1fwNJhS)
![image](https://drive.google.com/uc?export=view&id=1kqw8QJYPR7vOBCco7p4XcVZ7xQKexdIR)

Looking at these images, neural networks make a lot more sense intuitively as we know why our network made that prediction.


# Results and Discussion

The result of our model was an accuracy of 96% in being able to classify between the three classes: improper face mask usage, no mask, and proper face mask usage. In terms of Grad-CAM, the implementation was successful in building trust and transparency within our model: the model was looking at the correct areas to determine the face mask usage.

# Website and Frontend links
Checkout our website and demo: https://elizabethmkim.github.io/FaceMaskDetection/

Checkout our frontend repo:  https://github.com/elizabethmkim/FaceMaskDetection 
"
28,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_52,,,"# DSC180B Face Mask Detection

This repository focuses on the creation of a Face Mask Detection report.
Link to website: https://athena112233.github.io/DSC180B_Project_Webpage/

-----------------------------------------------------------------------------------------------------------------

### Introduction
* This repo is about training an Convolutional Neunral Network(CNN) image classification model on MaskedFace-Net. MaskedFace-Net is a dataset that contains more than 60,000 images of person either wearing a mask not. For images that contain a person wearing a mask, the dataset is further splited into either a person is wearing a mask properly or not. In this repo, we've trained a model on this dataset and also implemented a Grad-CAM algorithm on the model.

##### config 
* This folder contains the parameters for running each target. (Make sure the paths to the model and image are correct!)

##### model
* This folder contains a trained model parameters(model.pt)

##### my_image
* This folder contains all the custom images that you want the model to test on. This folder will only be created when a custom image path is provided in /config

##### notebook
* This folder contains the exploratory data analysis(EDA) of the MaskedFace-Net.

##### result
* This folder contains the images that display the result of model prediction, Grad-CAM algorithm, and Integrated Gradient.

##### src
* This folder contains the .py files for model architecture, training procedure, testing procedure, Integrated Gradient, and Grad-CAM algorithm.

##### run.py
* This `run.py` file will the specified target.

##### submission.json
* `submission.json` contains the general structure of this repo.

### How to run this repo with explanation:
*  Please visit the `EDA.ipynb` inside the `notebook folder` to understand the MaskedFace-Net. Once you understand the daatset, then you can process to run the repo

To run this repo on GPU (highly recommended), run the following lines in a terminal

```
launch-scipy-ml-gpu.sh -i j0e2r1r0/face-mask-detection -c 4 -m 8
git clone https://github.com/gatran/DSC180B-Face-Mask-Detection
cd DSC180B-Face-Mask-Detection
```

OR

To run this repo on CPU, run the following lines in a terminal

```
launch.sh -i j0e2r1r0/face-mask-detection -c 4 -m 8
git clone https://github.com/gatran/DSC180B-Face-Mask-Detection
cd DSC180B-Face-Mask-Detection
```

Then you can start to run the various targets we've provided in ```src/``` folder

To train a model on your own, run the following line in a terminal

```
python run.py training
```

To test the model, run the following line in a terminal

```
python run.py testing
```

To implement the Grad-Cam algorithm on the model, run the following in a terminal

```
python run.py gradcam
```

To implement the Integrated Gradient algorithm on the model, run the following in a terminal

```
python run.py ig
```

##### Contributions
* Gavin Tran: train the model and generate output
* Che-Wei Lin: implement gradcam and generate output
* Athena Liu: create a report based on those outputs
","# DSC180B Face Mask Detection

This repository focuses on the creation of a Face Mask Detection report.
Link to website: https://athena112233.github.io/DSC180B_Project_Webpage/

-----------------------------------------------------------------------------------------------------------------

### Introduction
* This repo is about training an Convolutional Neunral Network(CNN) image classification model on MaskedFace-Net. MaskedFace-Net is a dataset that contains more than 60,000 images of person either wearing a mask not. For images that contain a person wearing a mask, the dataset is further splited into either a person is wearing a mask properly or not. In this repo, we've trained a model on this dataset and also implemented a Grad-CAM algorithm on the model.

##### config 
* This folder contains the parameters for running each target. (Make sure the paths to the model and image are correct!)

##### model
* This folder contains a trained model parameters(model.pt)

##### my_image
* This folder contains all the custom images that you want the model to test on. This folder will only be created when a custom image path is provided in /config

##### notebook
* This folder contains the exploratory data analysis(EDA) of the MaskedFace-Net.

##### result
* This folder contains the images that display the result of model prediction, Grad-CAM algorithm, and Integrated Gradient.

##### src
* This folder contains the .py files for model architecture, training procedure, testing procedure, Integrated Gradient, and Grad-CAM algorithm.

##### run.py
* This `run.py` file will the specified target.

##### submission.json
* `submission.json` contains the general structure of this repo.

### How to run this repo with explanation:
*  Please visit the `EDA.ipynb` inside the `notebook folder` to understand the MaskedFace-Net. Once you understand the daatset, then you can process to run the repo

To run this repo on GPU (highly recommended), run the following lines in a terminal

```
launch-scipy-ml-gpu.sh -i j0e2r1r0/face-mask-detection -c 4 -m 8
git clone https://github.com/gatran/DSC180B-Face-Mask-Detection
cd DSC180B-Face-Mask-Detection
```

OR

To run this repo on CPU, run the following lines in a terminal

```
launch.sh -i j0e2r1r0/face-mask-detection -c 4 -m 8
git clone https://github.com/gatran/DSC180B-Face-Mask-Detection
cd DSC180B-Face-Mask-Detection
```

Then you can start to run the various targets we've provided in ```src/``` folder

To train a model on your own, run the following line in a terminal

```
python run.py training
```

To test the model, run the following line in a terminal

```
python run.py testing
```

To implement the Grad-Cam algorithm on the model, run the following in a terminal

```
python run.py gradcam
```

To implement the Integrated Gradient algorithm on the model, run the following in a terminal

```
python run.py ig
```

##### Contributions
* Gavin Tran: train the model and generate output
* Che-Wei Lin: implement gradcam and generate output
* Athena Liu: create a report based on those outputs
"
29,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_51,,,"The purpose of this code is generating captions from an image and creating attention maps to help explain the model’s reasoning for the captions generated.
In order to test the robustness of the model we also use counterfactual images to see how the model's prediction changes when certain object are removed. 
Using this infomation we can also generate an object importance map to show which object in an image are most important to the caption generation process.

This project has six targets: data, train, evaluate_model, generate_viz, counterfactual_production, and explain_model. 
  - **data**: This target loads in the COCO dataset and prepares it for our image captioning model. 
  - **train**: This target builds the encoder and decoder in our image captioning model and trains it with the COCO dataset. 
  - **evaluate_model**: This target evaluates the trained model using beam search caption generation and BLEU score. 
  - **generate_viz**: This target generates a visualization of the attention maps at each stage of the caption generation process.
  - **counterfactual_production**: This target creates all of the files necessary to generate the counterfactuals (such as masks) and 
                                   then produces the counterfactual images.
  - **explain_model**: This target takes all of the counterfactual images and generates caption based on the new counterfactuals. 
                       Then compares the caption change from the original caption to generate a visualization to explain object 
                       importance using BERT similarity score.

To run the four targets, clone our repo to the dsmlp server and execute the command ‘python run.py all’ to run all the targets in sequence or 
'python run.py <target>' to run a single target. To run on a small set of test data execute: ‘python run.py test’. The output images will be saved 
 to the same directory as run.py.


Docker Repo: https://hub.docker.com/layers/140345085/afosado/capstone_project/final_docker/images/sha256-198c698d15e7a67d1bba8180a30c21dbf00dfd5e839189a94c06e6ffe96f9fac?context=explore

Demo Website: https://afosado.github.io/180b_capstone_xai/index.html
","The purpose of this code is generating captions from an image and creating attention maps to help explain the model’s reasoning for the captions generated.
In order to test the robustness of the model we also use counterfactual images to see how the model's prediction changes when certain object are removed. 
Using this infomation we can also generate an object importance map to show which object in an image are most important to the caption generation process.

This project has six targets: data, train, evaluate_model, generate_viz, counterfactual_production, and explain_model. 
  - **data**: This target loads in the COCO dataset and prepares it for our image captioning model. 
  - **train**: This target builds the encoder and decoder in our image captioning model and trains it with the COCO dataset. 
  - **evaluate_model**: This target evaluates the trained model using beam search caption generation and BLEU score. 
  - **generate_viz**: This target generates a visualization of the attention maps at each stage of the caption generation process.
  - **counterfactual_production**: This target creates all of the files necessary to generate the counterfactuals (such as masks) and 
                                   then produces the counterfactual images.
  - **explain_model**: This target takes all of the counterfactual images and generates caption based on the new counterfactuals. 
                       Then compares the caption change from the original caption to generate a visualization to explain object 
                       importance using BERT similarity score.

To run the four targets, clone our repo to the dsmlp server and execute the command ‘python run.py all’ to run all the targets in sequence or 
'python run.py <target>' to run a single target. To run on a small set of test data execute: ‘python run.py test’. The output images will be saved 
 to the same directory as run.py.


Docker Repo: https://hub.docker.com/layers/140345085/afosado/capstone_project/final_docker/images/sha256-198c698d15e7a67d1bba8180a30c21dbf00dfd5e839189a94c06e6ffe96f9fac?context=explore

Demo Website: https://afosado.github.io/180b_capstone_xai/index.html
"
30,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_50,,,"# StockMarket_explainableAI
Contributing Members: 
- Sohyun Lee
- Shin Ehara
- Jou-Ying Lee

## Abstract
Deep learning architectures are now publicly recognized and repeatedly proven to be powerful in a wide range of high-level prediction tasks. While these algorithms’ modeling generally have beyond satisfactory performances with apposite tuning, the long-troubling issue of this specific learning lies in the un-explainability of model learning and predicting. This interpretability of “how” machines learn is often times even more important than ensuring machines outputting “correct” predictions. Especially in the field of finance, users’ ability to dissect how and why an algorithm reached a conclusion from a business standpoint is integral for later applications of i.e., to be incorporated for business decision making, etc. This project studies similar prior work done on image recognition in the financial market and takes a step further on explaining predictions outputted by the Convolutional Neural Network by applying the Grad-CAM algorithm. 

Project Website at: https://connielee99.github.io/Explainable-AI-in-Finance/

## Instructions on Runing Project
* This project aims to apply the Grad-CAM technique to a CNN model trained on images that represent closing prices during the first hour of market exchange. 
* **To engineer data and create a CNN model**, you would need to run each notebook in `notebooks` folder in the following order:
	* **1. Run every cell in `Data Processing.ipynb`**
		* This notebooke is preprocessing the raw data by extracting closing prices during first hour after market open and labeling depends on prices increasing or decreasing
		* **Input:** `raw_NIFTY100.csv`
		* **output:** `first_combined.csv` contains closing prices during the first hour of market exchange
	* **2. Run every cell in `Image Conversion.ipynb`**
		* This notebook is for an image conversion with `first_combined.csv` data. We will converse data into image with Gramian Angular Algorithm.
		*  **Input:**`first_combined.csv`
		*  **output** `.png` images in `imgs` folder
	* **3. Run every cell in `CNN.ipynb`**
		* This notebook uses FastAI, a PyTorch-based deep learning library, to build the neural network, which is able to figure out the relationship between input features and find hidden relationship with them. The input data is an image dataset with labels, which is converted from time series with Gramian Angular Field algorithm as described in the previous sections.

* **To run Grad-CAM**: 
	- Clone the Grad-CAM submodule we have included in repo homepage.
	- Navigate to <i>StockMarket_explainableAI/test</i> and put <i>test_imgs</i> folder inside this cloned submodule folder.
	- Set your directory to be in this submodule, and run the following command (feel free to modify the last part in the code for specific images):
		* python3 main.py demo1 -a resnet34 -t layer4 -i test_imgs/2017-01-03.png -k 1

## Directory Structure
* **config**</br>
	This folder contains json files for main and testing parameters
	* `data_params.json`</br>contains parameters for running main on all data
	* `test_params.json`</br>contains parameters for running main on test data
* **data**</br>
	This folder contains all stock data from time series to image representation</br>
	**imgs**</br>
	* This folder contains all images converted from time series. ex) 2017-01-02.png
	
	**raw data**</br>
	* `raw_NIFTY100.csv`</br>contains raw stoack market data; time series data

	**processed data**</br>
	* `first_combined.csv`</br>contains closing prices during the first hour of market exchange
	* `gramian_df.csv`</br>contains data after implementing gramian angular algorithm
	* `label_dir_2.csv`</br>contains data with label Whether the price goes up or down that day
* **gradcam_submodule @ fd10ff7**</br>
	This folder is the submodule for gradcam
	
* **notebooks**</br>
	This folder is the notebook directory
	
	* `CNN + Grad-CAM.ipynb`</br>is the development notebook for CNN and GradCam implementation
	* `Data Processing.ipynb`</br>is the notebook that wraps together data cleaning to feature engineering
	* `EDA.ipynb`</br>is the notebook with eda work demonstration
	* `Image Conversion.ipynb`</br>is the notebook with image conversion work done
* **references**</br>
	This folder contains additional information/references in regards to our project
	
	**report_img**</br>
	* This folder contains images extracted from coded notebooks and included in the written report

* **src**</br>
	This folder contains library codes extracted from notebooks
	
	**features**</br>
	* `build_features.py`</br>scripts to build features from merged data
	* `build_labels.py`</br>scripts to create labels for image classification
	* `build_images.py`</br>scripts to convert and save time series data to images
	
	**model**</br>
	* `gradcam.py`</br>scripts to implement gradcam

* **test**</br>
      This folder contains test results and test images
      		
* **`Dockerfile`**</br>
	This is the dockerfile necessary to build the environment for this project development
* **`run.py`**</br>
	This is the main python file to execute our program
","# StockMarket_explainableAI
Contributing Members: 
- Sohyun Lee
- Shin Ehara
- Jou-Ying Lee

## Abstract
Deep learning architectures are now publicly recognized and repeatedly proven to be powerful in a wide range of high-level prediction tasks. While these algorithms’ modeling generally have beyond satisfactory performances with apposite tuning, the long-troubling issue of this specific learning lies in the un-explainability of model learning and predicting. This interpretability of “how” machines learn is often times even more important than ensuring machines outputting “correct” predictions. Especially in the field of finance, users’ ability to dissect how and why an algorithm reached a conclusion from a business standpoint is integral for later applications of i.e., to be incorporated for business decision making, etc. This project studies similar prior work done on image recognition in the financial market and takes a step further on explaining predictions outputted by the Convolutional Neural Network by applying the Grad-CAM algorithm. 

Project Website at: https://connielee99.github.io/Explainable-AI-in-Finance/

## Instructions on Runing Project
* This project aims to apply the Grad-CAM technique to a CNN model trained on images that represent closing prices during the first hour of market exchange. 
* **To engineer data and create a CNN model**, you would need to run each notebook in `notebooks` folder in the following order:
	* **1. Run every cell in `Data Processing.ipynb`**
		* This notebooke is preprocessing the raw data by extracting closing prices during first hour after market open and labeling depends on prices increasing or decreasing
		* **Input:** `raw_NIFTY100.csv`
		* **output:** `first_combined.csv` contains closing prices during the first hour of market exchange
	* **2. Run every cell in `Image Conversion.ipynb`**
		* This notebook is for an image conversion with `first_combined.csv` data. We will converse data into image with Gramian Angular Algorithm.
		*  **Input:**`first_combined.csv`
		*  **output** `.png` images in `imgs` folder
	* **3. Run every cell in `CNN.ipynb`**
		* This notebook uses FastAI, a PyTorch-based deep learning library, to build the neural network, which is able to figure out the relationship between input features and find hidden relationship with them. The input data is an image dataset with labels, which is converted from time series with Gramian Angular Field algorithm as described in the previous sections.

* **To run Grad-CAM**: 
	- Clone the Grad-CAM submodule we have included in repo homepage.
	- Navigate to <i>StockMarket_explainableAI/test</i> and put <i>test_imgs</i> folder inside this cloned submodule folder.
	- Set your directory to be in this submodule, and run the following command (feel free to modify the last part in the code for specific images):
		* python3 main.py demo1 -a resnet34 -t layer4 -i test_imgs/2017-01-03.png -k 1

## Directory Structure
* **config**</br>
	This folder contains json files for main and testing parameters
	* `data_params.json`</br>contains parameters for running main on all data
	* `test_params.json`</br>contains parameters for running main on test data
* **data**</br>
	This folder contains all stock data from time series to image representation</br>
	**imgs**</br>
	* This folder contains all images converted from time series. ex) 2017-01-02.png
	
	**raw data**</br>
	* `raw_NIFTY100.csv`</br>contains raw stoack market data; time series data

	**processed data**</br>
	* `first_combined.csv`</br>contains closing prices during the first hour of market exchange
	* `gramian_df.csv`</br>contains data after implementing gramian angular algorithm
	* `label_dir_2.csv`</br>contains data with label Whether the price goes up or down that day
* **gradcam_submodule @ fd10ff7**</br>
	This folder is the submodule for gradcam
	
* **notebooks**</br>
	This folder is the notebook directory
	
	* `CNN + Grad-CAM.ipynb`</br>is the development notebook for CNN and GradCam implementation
	* `Data Processing.ipynb`</br>is the notebook that wraps together data cleaning to feature engineering
	* `EDA.ipynb`</br>is the notebook with eda work demonstration
	* `Image Conversion.ipynb`</br>is the notebook with image conversion work done
* **references**</br>
	This folder contains additional information/references in regards to our project
	
	**report_img**</br>
	* This folder contains images extracted from coded notebooks and included in the written report

* **src**</br>
	This folder contains library codes extracted from notebooks
	
	**features**</br>
	* `build_features.py`</br>scripts to build features from merged data
	* `build_labels.py`</br>scripts to create labels for image classification
	* `build_images.py`</br>scripts to convert and save time series data to images
	
	**model**</br>
	* `gradcam.py`</br>scripts to implement gradcam

* **test**</br>
      This folder contains test results and test images
      		
* **`Dockerfile`**</br>
	This is the dockerfile necessary to build the environment for this project development
* **`run.py`**</br>
	This is the main python file to execute our program
"
31,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_49,,,"# Racial_Classification_XAI_Model

keyword: Deep Learning, Convolutional Neural Network, Integrated-Gradient, Grad-CAM, Web Application

Website: https://michael4706.github.io/XAI_Website/

Static Web app: this [demo](https://nicole9925.github.io/facial-analysis-frontend/) (once you clicked the demo, just press submit to run it) is our web application that runs sample image. Make sure you visit the [Web Application](https://michael4706.github.io/XAI_Website/webapp/) (or just click this link) to play with it. If you want to run the web application with your own image, please visit the Web Application section below and follow the intrusctions.


![sample result](sample_result.png)

### Introduction
This project is about visualizing Convolutional Neural Network (CNN) with XAI techniques: Grad-cam and Integrated-Gradient. We used the FairFace dataset to train our models. This dataset contains about 80000+ training images and 10000+ validation images. The dataset contains three different categories(labels): age range(9 classes), race(7 classes), and gender(2 classes). We implemented a model that combined the first 14 layers from resnet50 as pre-trained layers with our self-defined layers. We trained three models on each of the different categories using the same model structure except changing the number of outputs from the final layer to match each category's number of classes. Then, we applied XAI to visualize models' decision-making with heatmaps. We want to examine what features or regions the models focus on given an image. Also, we are interested in comparing the heatmaps generated by the biased and unbiased models. The FairFace Dataset has an equal distribution of race. Therefore, we created a dataset with an unequal distribution of race and trained a biased model with this dataset.

##### config
* The parameters to run the scripts. Make sure to visit this file before running the code.

##### run.py
* script to train the model, run integrated-gradients, and calculate the statistics for the model.

##### src
* folder that contains the source code.

##### models
* Contained dlib_mod that helps to preprocess the images. You also recommend you to save your trained model here.

##### test_data
* Contains sample data from FairFace Dataset.

### How to run the code
1. please the my docker image: `michael459165/capstone2:new8` and run the code inside this container.
2. please go to the config file to change the parameters. This file has 5 sections, each corresponds to a set of parameters to execute a particular task.
3. Type `python run.py train_model` to train your model.
4. Type `python run.py generate_stats` to generate statistics and plots.
5. Type `python run.py run_test` to generate just ONE heatmap for both Integrated Gradient and Grad-CAM on the test sample. This will generate the heatmaps of the class with the HIGHEST predictive probability.
6. Type `python run.py run_custom_img` to generate just ONE heatmap for both Integrated Gradient and Grad-CAM on YOUR own image. This will generate the heatmaps of the class with the HIGHEST predictive probability.

Note: All the functions in util.py are well documented. Please feel free to explore and modify the code!

### Web Application
We also made a Web App to showcase our work. Please clone [this repository](https://github.com/nicole9925/facial-analysis-webapp) and follow the instruction to run it locally. If you want to deploy the Web App online, please visit [frontend](https://github.com/nicole9925/facial-analysis-frontend) and [backend](https://github.com/nicole9925/facial-analysis-backend) repositories for further instruction. 

### More stuff you can do
* If you don't want to train the model (because it takes a long time), then please visit the ""temp"" branch from this repository. There are trained models under the models folder and sample integrated-gradient results and statistics under the visualization folder. Please just download the files you need and run the code from this main branch. 

### Reference
[1]Selvaraju, Ramprasaath R., et al. ""Grad-cam: Visual explanations from deep networks via gradient-based localization."" Proceedings of the IEEE international conference on computer vision. 2017.

[2]Grad-CAM implementation in Keras[Source code]. https://github.com/jacobgil/keras-grad-cam.

[3]Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. ""Axiomatic attribution for deep networks."" International Conference on Machine Learning. PMLR, 2017.

[4]Integrated Gradients[Source code]. https://github.com/hiranumn/IntegratedGradients.

[5]@inproceedings{karkkainenfairface,
      title={FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation},
      author={Karkkainen, Kimmo and Joo, Jungseock},
      booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
      year={2021},
      pages={1548--1558}
    }

[6] FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age[Source code].https://github.com/dchen236/FairFace.

[7] Draelos, Rachel. “Grad-CAM: Visual Explanations from Deep Networks.” Glass Box, 29 May 2020. https://glassboxmedicine.com/2020/05/29/grad-cam-visual-explanations-from-deep-networks/#:~:text=Grad%2DCAM%20can%20be%20used%20for%20understanding%20a%20model's%20predictions,choice%20than%20Guided%20Grad%2DCAM.
","# Racial_Classification_XAI_Model

keyword: Deep Learning, Convolutional Neural Network, Integrated-Gradient, Grad-CAM, Web Application

Website: https://michael4706.github.io/XAI_Website/

Static Web app: this [demo](https://nicole9925.github.io/facial-analysis-frontend/) (once you clicked the demo, just press submit to run it) is our web application that runs sample image. Make sure you visit the [Web Application](https://michael4706.github.io/XAI_Website/webapp/) (or just click this link) to play with it. If you want to run the web application with your own image, please visit the Web Application section below and follow the intrusctions.


![sample result](sample_result.png)

### Introduction
This project is about visualizing Convolutional Neural Network (CNN) with XAI techniques: Grad-cam and Integrated-Gradient. We used the FairFace dataset to train our models. This dataset contains about 80000+ training images and 10000+ validation images. The dataset contains three different categories(labels): age range(9 classes), race(7 classes), and gender(2 classes). We implemented a model that combined the first 14 layers from resnet50 as pre-trained layers with our self-defined layers. We trained three models on each of the different categories using the same model structure except changing the number of outputs from the final layer to match each category's number of classes. Then, we applied XAI to visualize models' decision-making with heatmaps. We want to examine what features or regions the models focus on given an image. Also, we are interested in comparing the heatmaps generated by the biased and unbiased models. The FairFace Dataset has an equal distribution of race. Therefore, we created a dataset with an unequal distribution of race and trained a biased model with this dataset.

##### config
* The parameters to run the scripts. Make sure to visit this file before running the code.

##### run.py
* script to train the model, run integrated-gradients, and calculate the statistics for the model.

##### src
* folder that contains the source code.

##### models
* Contained dlib_mod that helps to preprocess the images. You also recommend you to save your trained model here.

##### test_data
* Contains sample data from FairFace Dataset.

### How to run the code
1. please the my docker image: `michael459165/capstone2:new8` and run the code inside this container.
2. please go to the config file to change the parameters. This file has 5 sections, each corresponds to a set of parameters to execute a particular task.
3. Type `python run.py train_model` to train your model.
4. Type `python run.py generate_stats` to generate statistics and plots.
5. Type `python run.py run_test` to generate just ONE heatmap for both Integrated Gradient and Grad-CAM on the test sample. This will generate the heatmaps of the class with the HIGHEST predictive probability.
6. Type `python run.py run_custom_img` to generate just ONE heatmap for both Integrated Gradient and Grad-CAM on YOUR own image. This will generate the heatmaps of the class with the HIGHEST predictive probability.

Note: All the functions in util.py are well documented. Please feel free to explore and modify the code!

### Web Application
We also made a Web App to showcase our work. Please clone [this repository](https://github.com/nicole9925/facial-analysis-webapp) and follow the instruction to run it locally. If you want to deploy the Web App online, please visit [frontend](https://github.com/nicole9925/facial-analysis-frontend) and [backend](https://github.com/nicole9925/facial-analysis-backend) repositories for further instruction. 

### More stuff you can do
* If you don't want to train the model (because it takes a long time), then please visit the ""temp"" branch from this repository. There are trained models under the models folder and sample integrated-gradient results and statistics under the visualization folder. Please just download the files you need and run the code from this main branch. 

### Reference
[1]Selvaraju, Ramprasaath R., et al. ""Grad-cam: Visual explanations from deep networks via gradient-based localization."" Proceedings of the IEEE international conference on computer vision. 2017.

[2]Grad-CAM implementation in Keras[Source code]. https://github.com/jacobgil/keras-grad-cam.

[3]Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. ""Axiomatic attribution for deep networks."" International Conference on Machine Learning. PMLR, 2017.

[4]Integrated Gradients[Source code]. https://github.com/hiranumn/IntegratedGradients.

[5]@inproceedings{karkkainenfairface,
      title={FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation},
      author={Karkkainen, Kimmo and Joo, Jungseock},
      booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
      year={2021},
      pages={1548--1558}
    }

[6] FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age[Source code].https://github.com/dchen236/FairFace.

[7] Draelos, Rachel. “Grad-CAM: Visual Explanations from Deep Networks.” Glass Box, 29 May 2020. https://glassboxmedicine.com/2020/05/29/grad-cam-visual-explanations-from-deep-networks/#:~:text=Grad%2DCAM%20can%20be%20used%20for%20understanding%20a%20model's%20predictions,choice%20than%20Guided%20Grad%2DCAM.
"
32,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_54,,,"# Snake NeuralBackedDecisionTrees
DSC180B Group 6 Snake Classification using Neural Backed Decision Trees

Team Members:

Nikolas Racelis-Russell - A15193225

Weihua (Cedric) Zhao - A14684029 

Rui Zheng - A15046475

## Abstract

Our project focuses on building explanable image classification models on snake images from https://www.aicrowd.com/challenges/snakeclef2021-snake-species-identification-challenge/dataset_files. Our plan is to apply gradcam to images of different snake species, construct a Densenet, and transform them into decision trees to visualize the classification process. 

### Demo

Link to our website:  https://nikolettuce.github.io/DSC180B_06_NeuralBackedDecisionTrees/

#### GradCAM

Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses the class-specific gradient information flowing into the final convolutional layer of a CNN to produce a coarse localization map of the important regions in the image. [1]

#### Neural Backed Decision Tree

Nowadays, machine learning has been applied in multifaceted areas of our life. While its prominence grows, its interpretabilty leaves people insecure because of the fact that people hardly see through the classfication decision process. Many attempts to solve this problem either ends up with the cost of interpretability or the cost of accuracy. Intended to avoid this dilemma in our snake classification process, we applied Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging.[2] 

##### Induced Hierarchy

When creating the hierarchy tree, one thing to note is that ""[it] requires pre-trained model weights"". We took row vectors wk : k ∈ [1, K], each representing a class, from the fully- connected layer weights W; then, we ran hierarchical agglomerative clustering on the normalized class representatives wk/kwkk2. Last but not least, we built the leaf nodes based on the weights. [2]

##### Loss Conversion

### Practical Use

Our project has valid applications in real life. Wild snakes are prevalent on mountains, and hikers have high possibilities to encounter them. A genuine snake classifier would provide useful information to hikers about whether the snake is venomous or not; thus, they can avoid the snake if a certain dangerous species emerge.

### Methods
Our classiciation starts with a baseline Densenet model with 5 epochs. In terms of performance, it reached a F-score of 0.495 on validation data and accuracy of 0.66 on validation data. We then managed to improve the model later with higher accuracy.

Then we applied Grad-CAM to five different snake pictures with different features. The first category includes pictures where snakes blend in with the background. The second category includes pictures where snakes differ from the background. The third category includes pictures where snakes appear with other objects, like hand. Grad-CAM performs well on localizing the target object. [3]

Last, but not least, we are currently working on transforming the CNN models to decision trees. For now, we have a general hierarchy tree where you can see each decision. Though the decision is not clear for now, we will manage to elucidate them in the upcoming weeks.

When creating the hierarchy tree, one thing to note is that ""[it] requires pre-trained model weights"". We took row vectors wk : k ∈ [1, K], each representing a class, from the fully- connected layer weights W; then, we ran hierarchical agglomerative clustering on the normalized class representatives wk/kwkk2. Last but not least, we built the leaf nodes based on the weights. [4]
### Results

#### Heatmaps

<img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/0a00cdd2b8.jpg"" width=""200""/> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam%201.jpg"" width=""200""/> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam_gb%201.jpg"" width=""200""> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/gb%201.jpg"" width=""200""> 


<img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/0a7eded849.jpg"" width=""200""/> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam%204.jpg"" width=""200""/> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam_gb%204.jpg"" width=""200""> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/gb%204.jpg"" width=""200""> 

<img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/0a54501d6d.jpg"" width=""200""/> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam%207.jpg"" width=""200""/> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam_gb%207.jpg"" width=""200""> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/gb%207.jpg"" width=""200""> 

#### Hierarchy Trees

<img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/Screen%20Shot%202021-02-07%20at%205.32.34%20PM.png"">

### Conclusions

## Installation

To use this project, please run build.sh and allocate at least 30 GB of hard drive space to install the data.

Then run python run.py data to first process the data before using the test target

To run the CNN and test on the snake dataset, run python run.py test

## Resources

1. Grad-CAM Paper https://arxiv.org/pdf/1610.02391v1.pdf
2. @misc{wan2021nbdt, title={NBDT: Neural-Backed Decision Trees}, author={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Henry Jin and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez}, year={2021}, eprint={2004.00221}, archivePrefix= {arXiv}, primaryClass={cs.CV} }

3. Grad-CAM code  https://github.com/jacobgil/pytorch-grad-cam

4. NBDT: https://github.com/alvinwan/neural-backed-decision-trees


","# Snake NeuralBackedDecisionTrees
DSC180B Group 6 Snake Classification using Neural Backed Decision Trees

Team Members:

Nikolas Racelis-Russell - A15193225

Weihua (Cedric) Zhao - A14684029 

Rui Zheng - A15046475

## Abstract

Our project focuses on building explanable image classification models on snake images from https://www.aicrowd.com/challenges/snakeclef2021-snake-species-identification-challenge/dataset_files. Our plan is to apply gradcam to images of different snake species, construct a Densenet, and transform them into decision trees to visualize the classification process. 

### Demo

Link to our website:  https://nikolettuce.github.io/DSC180B_06_NeuralBackedDecisionTrees/

#### GradCAM

Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses the class-specific gradient information flowing into the final convolutional layer of a CNN to produce a coarse localization map of the important regions in the image. [1]

#### Neural Backed Decision Tree

Nowadays, machine learning has been applied in multifaceted areas of our life. While its prominence grows, its interpretabilty leaves people insecure because of the fact that people hardly see through the classfication decision process. Many attempts to solve this problem either ends up with the cost of interpretability or the cost of accuracy. Intended to avoid this dilemma in our snake classification process, we applied Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging.[2] 

##### Induced Hierarchy

When creating the hierarchy tree, one thing to note is that ""[it] requires pre-trained model weights"". We took row vectors wk : k ∈ [1, K], each representing a class, from the fully- connected layer weights W; then, we ran hierarchical agglomerative clustering on the normalized class representatives wk/kwkk2. Last but not least, we built the leaf nodes based on the weights. [2]

##### Loss Conversion

### Practical Use

Our project has valid applications in real life. Wild snakes are prevalent on mountains, and hikers have high possibilities to encounter them. A genuine snake classifier would provide useful information to hikers about whether the snake is venomous or not; thus, they can avoid the snake if a certain dangerous species emerge.

### Methods
Our classiciation starts with a baseline Densenet model with 5 epochs. In terms of performance, it reached a F-score of 0.495 on validation data and accuracy of 0.66 on validation data. We then managed to improve the model later with higher accuracy.

Then we applied Grad-CAM to five different snake pictures with different features. The first category includes pictures where snakes blend in with the background. The second category includes pictures where snakes differ from the background. The third category includes pictures where snakes appear with other objects, like hand. Grad-CAM performs well on localizing the target object. [3]

Last, but not least, we are currently working on transforming the CNN models to decision trees. For now, we have a general hierarchy tree where you can see each decision. Though the decision is not clear for now, we will manage to elucidate them in the upcoming weeks.

When creating the hierarchy tree, one thing to note is that ""[it] requires pre-trained model weights"". We took row vectors wk : k ∈ [1, K], each representing a class, from the fully- connected layer weights W; then, we ran hierarchical agglomerative clustering on the normalized class representatives wk/kwkk2. Last but not least, we built the leaf nodes based on the weights. [4]
### Results

#### Heatmaps

<img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/0a00cdd2b8.jpg"" width=""200""/> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam%201.jpg"" width=""200""/> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam_gb%201.jpg"" width=""200""> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/gb%201.jpg"" width=""200""> 


<img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/0a7eded849.jpg"" width=""200""/> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam%204.jpg"" width=""200""/> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam_gb%204.jpg"" width=""200""> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/gb%204.jpg"" width=""200""> 

<img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/0a54501d6d.jpg"" width=""200""/> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam%207.jpg"" width=""200""/> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam_gb%207.jpg"" width=""200""> <img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/gb%207.jpg"" width=""200""> 

#### Hierarchy Trees

<img src=""https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/Screen%20Shot%202021-02-07%20at%205.32.34%20PM.png"">

### Conclusions

## Installation

To use this project, please run build.sh and allocate at least 30 GB of hard drive space to install the data.

Then run python run.py data to first process the data before using the test target

To run the CNN and test on the snake dataset, run python run.py test

## Resources

1. Grad-CAM Paper https://arxiv.org/pdf/1610.02391v1.pdf
2. @misc{wan2021nbdt, title={NBDT: Neural-Backed Decision Trees}, author={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Henry Jin and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez}, year={2021}, eprint={2004.00221}, archivePrefix= {arXiv}, primaryClass={cs.CV} }

3. Grad-CAM code  https://github.com/jacobgil/pytorch-grad-cam

4. NBDT: https://github.com/alvinwan/neural-backed-decision-trees


"
33,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_16,,,"# DSC-180B-Team6

We were able to replicate the ThunderHill race track using the Unity 3D game engine and integrated Unity with the track and robot into the LGSVL simulator. Once the integration was complete we were able to see our robot with the Thunderhill Track as our map in the simulator. We were then able to virtualize the functions of the IMU, odometry and lidar sensors and RGB-D cameras to better visualize what our robot perceives in the simulation. Finally we were able to fully visualize what our robot sees with the virtual sensors using Autoware Rviz which displays the location and point cloud map of the vehicle and its surroundings.
","# DSC-180B-Team6

We were able to replicate the ThunderHill race track using the Unity 3D game engine and integrated Unity with the track and robot into the LGSVL simulator. Once the integration was complete we were able to see our robot with the Thunderhill Track as our map in the simulator. We were then able to virtualize the functions of the IMU, odometry and lidar sensors and RGB-D cameras to better visualize what our robot perceives in the simulation. Finally we were able to fully visualize what our robot sees with the virtual sensors using Autoware Rviz which displays the location and point cloud map of the vehicle and its surroundings.
"
35,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_14,,,"DSC 180B Autonomous Vehicle Team 1 [Organization
Repository](https://github.com/UCSDAutonomousVehicles2021Team1) to
serve as a collection of all the repositories built for this project.

* [autonomous_navigation_image_segmentation](autonomous_navigation_image_segmentation)
* [autonomous_navigation_light_sensitivity](autonomous_navigation_light_sensitivity)
* [autonomous_nav_mapping_docker](autonomous_nav_mapping_docker)
* [camera_mapping_navigation_website](camera_mapping_navigation_website)
* [f1tenth_racecar_dl_custom](f1tenth_racecar_dl_custom)
* [rtabmap_mapping_tuning](rtabmap_mapping_tuning)


","DSC 180B Autonomous Vehicle Team 1 [Organization
Repository](https://github.com/UCSDAutonomousVehicles2021Team1) to
serve as a collection of all the repositories built for this project.

* [autonomous_navigation_image_segmentation](autonomous_navigation_image_segmentation)
* [autonomous_navigation_light_sensitivity](autonomous_navigation_light_sensitivity)
* [autonomous_nav_mapping_docker](autonomous_nav_mapping_docker)
* [camera_mapping_navigation_website](camera_mapping_navigation_website)
* [f1tenth_racecar_dl_custom](f1tenth_racecar_dl_custom)
* [rtabmap_mapping_tuning](rtabmap_mapping_tuning)


"
36,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_13,,,"# Data Visualizations and Interface For Autonomous Robots

This project aims to create data visualizations and an interactive interface for autonomous robots. The intent and design of visualizations created for this project were catered towards optimizing racing performance on the [Thunderhill track](https://www.thunderhill.com/). Visualizations include birdseye view of optimal path on mapped track, live camera feed, lidar readings, IMU data (position and orientation) visualized, battery status display, and various other visualizations to show the health and status of the vehicle. The interface that displays all of these various tools and visualizations are meant to be interactive and communicate with the Autonomous Robot via Rosbridge, which not only allow users to control the Gazebo Robot action by simple interface interaction like clicking button or inputing text, but also monitor the realtime situation of the robot navigating in map. The visualizations will be primarily illustrated through Python, ROS, Gazebo, RViz, and other robotics software. After analyzing the performance of A* algorithm versus the performance of RRT* algorithm, it is determined that RRT* performs better. Ultimately, RRT* algorithms is the primary navigation algorithm used and illustrated in this project.

## Running the project
First clone the repository:
```
$ git clone https://github.com/dannyluo12/Autonomous_robot_data_visualization_and_interface.git
```
Launch docker container using image:
```
$ launch.sh -i dannyluo12/visualization_and_interface:latest -c 4 -m 8 -P Always
```
* This command launches a [dockerhub](https://hub.docker.com/repository/docker/dannyluo12/visualization_and_interface) container with the necessary OS libraries, tools, and dependencies to successfully run the project. Certain dependencies will be vital for creating the visualizations and genearting the interface.

## Building the project using `run.py`
* Use the command `python run.py data` to create data folder. Will contain directories to properly store image and sensor data that is outputted.
* Use the command `python run.py clean` to ensure that data is scaled properly to optimize runtime. Includes imaging data for running navigation algorithms as well as executing interface.
* Use the command `python run.py analyze` to compare the performance of A* algorithm to RRT* for navigation on the same map.
* Use the command `python run.py test` to run the visualization of RRT algorithm in test data, output images can be found in the testdata/step_out and testdata/test_out directories.
* Use the command `python run.py all` to run the visualization of RRT algorithm on cleaned data/map, output images can be found in the data/step_out and data/test_out directories.

### Contributions:
<b>Yuxi Luo</b> <br />
Contributed to developing visualizations for RRT* and A* algorithms. Tested performance of each navigation algorithm to benchmark each and determine better performer. Collected and cleaned data from alternative groups to enable visualization and interface development. Tested different ROSBAGS for data type compatibility. Investigated various forms of visualization from different ROS topics (diff sensors, camera, lidar, etc.). Helped in managing and updating Github repo, report, and project website.

<b>Seokmin Hong</b> <br />
Contributed by implementing the UCSD simulated track inside the Gazebo simulator, as well as implementing the RRT* and A* algorithms that can be used for G-Mapping SLAM. Also wrote Rviz scripts and interactive interface scripts to allow autonomous navigation with a simple pressing of a button. Helped teammates by creating and writing the report, as well as creating the demonstration videos of the interactive interface and Gazebo simulations.

<b>Jia Shi</b> <br />
Contributed to the research of visualization and interface. Developed an interactive interface with roslibjs and webridge to connect ROS with web page. Worked with teammate to integrate interface with Gazebo robot to allow controlling. Also created visualization demo with ROS bag data from other teams. Helped teammates with the coding and helped with the setup and completion of github repo.
","# Data Visualizations and Interface For Autonomous Robots

This project aims to create data visualizations and an interactive interface for autonomous robots. The intent and design of visualizations created for this project were catered towards optimizing racing performance on the [Thunderhill track](https://www.thunderhill.com/). Visualizations include birdseye view of optimal path on mapped track, live camera feed, lidar readings, IMU data (position and orientation) visualized, battery status display, and various other visualizations to show the health and status of the vehicle. The interface that displays all of these various tools and visualizations are meant to be interactive and communicate with the Autonomous Robot via Rosbridge, which not only allow users to control the Gazebo Robot action by simple interface interaction like clicking button or inputing text, but also monitor the realtime situation of the robot navigating in map. The visualizations will be primarily illustrated through Python, ROS, Gazebo, RViz, and other robotics software. After analyzing the performance of A* algorithm versus the performance of RRT* algorithm, it is determined that RRT* performs better. Ultimately, RRT* algorithms is the primary navigation algorithm used and illustrated in this project.

## Running the project
First clone the repository:
```
$ git clone https://github.com/dannyluo12/Autonomous_robot_data_visualization_and_interface.git
```
Launch docker container using image:
```
$ launch.sh -i dannyluo12/visualization_and_interface:latest -c 4 -m 8 -P Always
```
* This command launches a [dockerhub](https://hub.docker.com/repository/docker/dannyluo12/visualization_and_interface) container with the necessary OS libraries, tools, and dependencies to successfully run the project. Certain dependencies will be vital for creating the visualizations and genearting the interface.

## Building the project using `run.py`
* Use the command `python run.py data` to create data folder. Will contain directories to properly store image and sensor data that is outputted.
* Use the command `python run.py clean` to ensure that data is scaled properly to optimize runtime. Includes imaging data for running navigation algorithms as well as executing interface.
* Use the command `python run.py analyze` to compare the performance of A* algorithm to RRT* for navigation on the same map.
* Use the command `python run.py test` to run the visualization of RRT algorithm in test data, output images can be found in the testdata/step_out and testdata/test_out directories.
* Use the command `python run.py all` to run the visualization of RRT algorithm on cleaned data/map, output images can be found in the data/step_out and data/test_out directories.

### Contributions:
<b>Yuxi Luo</b> <br />
Contributed to developing visualizations for RRT* and A* algorithms. Tested performance of each navigation algorithm to benchmark each and determine better performer. Collected and cleaned data from alternative groups to enable visualization and interface development. Tested different ROSBAGS for data type compatibility. Investigated various forms of visualization from different ROS topics (diff sensors, camera, lidar, etc.). Helped in managing and updating Github repo, report, and project website.

<b>Seokmin Hong</b> <br />
Contributed by implementing the UCSD simulated track inside the Gazebo simulator, as well as implementing the RRT* and A* algorithms that can be used for G-Mapping SLAM. Also wrote Rviz scripts and interactive interface scripts to allow autonomous navigation with a simple pressing of a button. Helped teammates by creating and writing the report, as well as creating the demonstration videos of the interactive interface and Gazebo simulations.

<b>Jia Shi</b> <br />
Contributed to the research of visualization and interface. Developed an interactive interface with roslibjs and webridge to connect ROS with web page. Worked with teammate to integrate interface with Gazebo robot to allow controlling. Also created visualization demo with ROS bag data from other teams. Helped teammates with the coding and helped with the setup and completion of github repo.
"
37,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_12,,,"# DSC 180 Autonomous Systems

Neghena Faizyar, Garrett Gibo, Shiyin Liang

## Data
To get sample data: 
[Link to Data](https://drive.google.com/drive/folders/1wh7EtgtrS8Wi8xBIe1VwzFDBnp751XHv?usp=sharing)

Download this data to put into the data/raw folder.

## Usage 

### ROS

A large portion of this project is a series of ROS packages that can be launched
directly or integrated in other ROS packages. This repo has been made in such a
way that it serves as a full ROS workspace, thus to run the packages that are
contained here, simply run:

``` sh
# build project
catkin_make

# source ROS packages
source devel/setup.bash

# launch main node
roslaunch simulation main.launch
```

This will launch a gazebo simulation containing a test vehicle with sensors that
were used for this project. Because the simulation requires both ROS and gazebo
which have large graphical portions, these packages must be run a system that
has ROS setup already and also has some type of grapical interface, for example
X on linux based systems.

### Analysis

The second portion of this project is analysis that is done on the data that
is gathered from both simulation and real sensors. 

To run any of the following targets, the command is:

```sh
python run.py <target>
```

Information on the targets is found below.

#### Targets

* `cep`: Calculates the Circular Error Probable (CEP), and 
2D Root Mean Square (2DRMS), and then plots and creates a graph of the CEP 
and 2DRMS circles with the datapoints. 

* `clean_data`: Extract, transform, and clean the raw GPS data so
that it can be used for anaylsis.

* `get_path`: Takes in CSV of GPS coordinates and cleans/filters points to create
a usable path.

* `ground_truth`: Plot ground truth coordinates against estimated coordinates 
for reported GPS values.

* `robot`: Creates an instance of the
[dronekit-sitl](https://dronekit-python.readthedocs.io/en/latest/develop/sitl_setup.html),
which can be used to generate realistic sensor data that can be used
as a template for the following targets.

* `robot_client` Provides the interface to connect to a specified robot.
The client connects over tcp or udp and uses the
[MAVLink](https://mavlink.io/en/messages/common.html), standard for
the messages.

* `test`: Runs our projects test code by extracting, transforming, and then 
cleaning the raw GPS test data such that it could be used. 

* `visualize`: Create visualizations for all of our data using bokeh. It will 
plot the line the GPS reports it has traveled and uploads it into the vis 
folder of our repository. 
","# DSC 180 Autonomous Systems

Neghena Faizyar, Garrett Gibo, Shiyin Liang

## Data
To get sample data: 
[Link to Data](https://drive.google.com/drive/folders/1wh7EtgtrS8Wi8xBIe1VwzFDBnp751XHv?usp=sharing)

Download this data to put into the data/raw folder.

## Usage 

### ROS

A large portion of this project is a series of ROS packages that can be launched
directly or integrated in other ROS packages. This repo has been made in such a
way that it serves as a full ROS workspace, thus to run the packages that are
contained here, simply run:

``` sh
# build project
catkin_make

# source ROS packages
source devel/setup.bash

# launch main node
roslaunch simulation main.launch
```

This will launch a gazebo simulation containing a test vehicle with sensors that
were used for this project. Because the simulation requires both ROS and gazebo
which have large graphical portions, these packages must be run a system that
has ROS setup already and also has some type of grapical interface, for example
X on linux based systems.

### Analysis

The second portion of this project is analysis that is done on the data that
is gathered from both simulation and real sensors. 

To run any of the following targets, the command is:

```sh
python run.py <target>
```

Information on the targets is found below.

#### Targets

* `cep`: Calculates the Circular Error Probable (CEP), and 
2D Root Mean Square (2DRMS), and then plots and creates a graph of the CEP 
and 2DRMS circles with the datapoints. 

* `clean_data`: Extract, transform, and clean the raw GPS data so
that it can be used for anaylsis.

* `get_path`: Takes in CSV of GPS coordinates and cleans/filters points to create
a usable path.

* `ground_truth`: Plot ground truth coordinates against estimated coordinates 
for reported GPS values.

* `robot`: Creates an instance of the
[dronekit-sitl](https://dronekit-python.readthedocs.io/en/latest/develop/sitl_setup.html),
which can be used to generate realistic sensor data that can be used
as a template for the following targets.

* `robot_client` Provides the interface to connect to a specified robot.
The client connects over tcp or udp and uses the
[MAVLink](https://mavlink.io/en/messages/common.html), standard for
the messages.

* `test`: Runs our projects test code by extracting, transforming, and then 
cleaning the raw GPS test data such that it could be used. 

* `visualize`: Create visualizations for all of our data using bokeh. It will 
plot the line the GPS reports it has traveled and uploads it into the vis 
folder of our repository. 
"
38,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_11,,,"# Autonomous Vehicles Capstone: Odometry and IMU 


We are Team 4 in the Autonomous Vehicles Data Science Capstone Project. Our project revolves around the IMU, Odometry efforts while we collectively work to build a 1/5 scale racing autonomous vehicle. 

For a vehicle to successfully navigate istelf and even race autonomously, it is essential for the vehicle to be able localize itself within its environment. This is where Odometry and IMU data can greatly support the robot’s navigational ability. Wheel Odometry provides useful measurements to estimate the position of the car through the use of wheel’s circumference and rotations per second. IMU, which stands for Interial Measurement Unit, is 9 axis sensor that can sense linear acceleration, angular velocity, and magnetic fields. Together, these data sources can provide us crucial information in deriving a Position Estimate (how far our robot has traveled) and a Compass Heading (orientation of the robot/where it’s headed).

Our aim is to calibrate, tune, and analyze Odometry and IMU data to provide most accurate Position Estimate, Heading, and data readings to achieve high performant autonomous navigation and racing ability.

*Developed by: Pranav Deshmane and Sally Poon*

### Usage

```
python run.py <target>
```
The Targets are: 
 
* `conversion` - This will extract the data from the raw ROS bags, clean them, and convert them to csv files to be analyzed
 
* `viz_analysis` - This will run the visualizations used in our analysis for IMU and Odometry calibration, tuning, and testing

* `test` - This will test the conversion and visualization process with sample data chosen from our raw data

### Resources
In the resources folder:

* `Openlog_Artemis_IMU_Guide` - Guide we developed for SparkFun Openlog Artemis IMU to improve future experience and aid in the installation, setup, and integration with Jetson NX and ROS.

* `Calibration_OLA_Artemis` - Calibration guide we developed for SparkFun Openlog Artemis IMU to aid in calibration process, analysis for future students/users

* `Setup for Odometry_IMU` - Guide we developed for the Odometry to aid in tuning process, analysis, and setup of Odometry and VESC interaction to improve experience for future students/users.

* `Apollo3`, `ICM-20948`, `Artemis_Hardware` - Sparkfun Openlog Artemis IMU Hardware Specifications, used to cross reference PIN headers that were needed to be configured correctly during integration process. 




### Additional ROS package 
* `ros_imu_yaw_pkg` 
ROS package we developed to aid in the integration of the OLA Artemis IMU to ROS. It allows the orientation quaternion readings derived from the IMU to be easily converted into Euler angles and Yaw heading. This is to improve the debugging process within ROS and helps to easily visualize the Yaw heading. This package can be run in parallel as a complement to the main ROS package used to interface with the OLA Artemis IMU and can easily integrate with the rest of your current ROS system in place as a separate node. Overall, this is to aid in the development process within ROS when deriving Yaw Heading from the OLA Artemis IMU. 

","# Autonomous Vehicles Capstone: Odometry and IMU 


We are Team 4 in the Autonomous Vehicles Data Science Capstone Project. Our project revolves around the IMU, Odometry efforts while we collectively work to build a 1/5 scale racing autonomous vehicle. 

For a vehicle to successfully navigate istelf and even race autonomously, it is essential for the vehicle to be able localize itself within its environment. This is where Odometry and IMU data can greatly support the robot’s navigational ability. Wheel Odometry provides useful measurements to estimate the position of the car through the use of wheel’s circumference and rotations per second. IMU, which stands for Interial Measurement Unit, is 9 axis sensor that can sense linear acceleration, angular velocity, and magnetic fields. Together, these data sources can provide us crucial information in deriving a Position Estimate (how far our robot has traveled) and a Compass Heading (orientation of the robot/where it’s headed).

Our aim is to calibrate, tune, and analyze Odometry and IMU data to provide most accurate Position Estimate, Heading, and data readings to achieve high performant autonomous navigation and racing ability.

*Developed by: Pranav Deshmane and Sally Poon*

### Usage

```
python run.py <target>
```
The Targets are: 
 
* `conversion` - This will extract the data from the raw ROS bags, clean them, and convert them to csv files to be analyzed
 
* `viz_analysis` - This will run the visualizations used in our analysis for IMU and Odometry calibration, tuning, and testing

* `test` - This will test the conversion and visualization process with sample data chosen from our raw data

### Resources
In the resources folder:

* `Openlog_Artemis_IMU_Guide` - Guide we developed for SparkFun Openlog Artemis IMU to improve future experience and aid in the installation, setup, and integration with Jetson NX and ROS.

* `Calibration_OLA_Artemis` - Calibration guide we developed for SparkFun Openlog Artemis IMU to aid in calibration process, analysis for future students/users

* `Setup for Odometry_IMU` - Guide we developed for the Odometry to aid in tuning process, analysis, and setup of Odometry and VESC interaction to improve experience for future students/users.

* `Apollo3`, `ICM-20948`, `Artemis_Hardware` - Sparkfun Openlog Artemis IMU Hardware Specifications, used to cross reference PIN headers that were needed to be configured correctly during integration process. 




### Additional ROS package 
* `ros_imu_yaw_pkg` 
ROS package we developed to aid in the integration of the OLA Artemis IMU to ROS. It allows the orientation quaternion readings derived from the IMU to be easily converted into Euler angles and Yaw heading. This is to improve the debugging process within ROS and helps to easily visualize the Yaw heading. This package can be run in parallel as a complement to the main ROS package used to interface with the OLA Artemis IMU and can easily integrate with the rest of your current ROS system in place as a separate node. Overall, this is to aid in the development process within ROS when deriving Yaw Heading from the OLA Artemis IMU. 

"
39,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_55,,,"# Webpage Link
[https://yikaihao.github.io/DSC180_Webpage/](https://yikaihao.github.io/DSC180_Webpage/)
","# Webpage Link
[https://yikaihao.github.io/DSC180_Webpage/](https://yikaihao.github.io/DSC180_Webpage/)
"
40,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_58,,,"# DSC180B-Project
The data we have are grabbed from the /teams directory: malware and popular-apps.

The purpose is to perform search on each software folder to find its smali files
and perform method-call analysis to build markov chain and get holistic
information of specific software and finally build a improved MAMADroid to
classify specific ware to be benign or malware.

It consists process_smali() to parse smali file and generate call-analysis.

To run it, execute python run.py <targets>.
Targets including 'feature', 'model', 'analysis', 'test'

### Responsibilities

* Jian Jiao developed code which parses content, generates features,
  builds model, perform analysis, improve model, generate results.
* Zihan Qin developed report and help partner to test code and debug.

### Project Webpage
https://kamui-jiao.github.io/DSC180B-Page/
","# DSC180B-Project
The data we have are grabbed from the /teams directory: malware and popular-apps.

The purpose is to perform search on each software folder to find its smali files
and perform method-call analysis to build markov chain and get holistic
information of specific software and finally build a improved MAMADroid to
classify specific ware to be benign or malware.

It consists process_smali() to parse smali file and generate call-analysis.

To run it, execute python run.py <targets>.
Targets including 'feature', 'model', 'analysis', 'test'

### Responsibilities

* Jian Jiao developed code which parses content, generates features,
  builds model, perform analysis, improve model, generate results.
* Zihan Qin developed report and help partner to test code and debug.

### Project Webpage
https://kamui-jiao.github.io/DSC180B-Page/
"
41,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_57,,,"# hindroid_replication
## HinDroid: An Intelligent Android Malware Detection System. Based on Structured Heterogeneous Information Network.
# Malware Detection Using API Relationships + Hindroid
# HOW TO RUN 
`python run.py test`
+ `requirements.txt`

By July 2020, Android OS is still a leading mobile operating system that holds 74.6% of market share worldwide, attracting numerous crazy cyber-criminals who are targeting at the largest crowd.¹ Also, due to its open-resource feature and flexible system updating policy, it is 50 times more likely to get infected compared to ios systems.² Thus, developing a strong malware detection system becomes the number one priority.

The current state of malware(malicious software) detection for the growing android OS application market involves looking solely at the API(Application Programming Interface) calls. API is a set of programming instructions that allow outside parties or individuals to access the platform or the application.³ A good and daily example will be the login options displaying on the app interface like “Login with Twitter”.4 Malwares can collect personal information easily from APIs, so analyzing APIs is a critical part of identifying malwares.

Hindroid formulates a meta-path based approach to highlight relationships across API calls to aggregate similarities and better detect malware.Individual APIs appearing in the ransomware could be harmless, but the combination of them could indicate “this ransomware intends to write malicious code into system kernel.”5 You wouldn’t want to see a group of “write”, “printStackTrace”, and “load” APIs appearing in your app’s smali file.5


# Data Generation Process
The data generation process and its relationship to the problem (i.e. for domain problems)
The data for identifying malware is primarily the android play store, although in order to obtain the respective APK’s for these apps the data is directly downloaded from `https://apkpure.com/`.

This data is then unpackaged using the apktools library that allows us to view the subsequent smali code and app binaries.

The smali code and app binaries contain a lot of the information derived from the Java source code that allows us to map the number of API calls and the relationships between them. 

# Observed Data 
Overall, from each android app, what’s most relevant to classifying Malware vs Benign - are the API calls, code blocks(methods) and packages these API calls occur in, classified as matrices 
A, P, B, I. This form of organizing data explains the relationship between these API calls. It provides a story, more in depth, than just the sheer number of API calls per app. 

The Hindroid model observes the same relationship of data to better classify malware or not, through the relationship of the above defined matrices. Reducing the ability of apps to just add a larger number of API calls to get classified as benign. 

# Conclusion
The process of identifying the relationship of API calls, is taking the idea of the subsequent network it creates - thus to not just look at the information queried by the call but also the way it interacts with other API’s in different levels of the codebase. The applicability of this lies beyond that of malware detection in android apps, but probably in the roots of graph theory and how relationships with API calls can be better identified and mapped out to provide more insight











## Responsibilities:
## Report:
Neel Shah: Malware Detection Using API Relationships + Hindroid, Data Generation Process, Observed Data, Conclusion
Mandy Ma: Malware Detection Using API Relationships + Hindroid, Citation
## Code:
	Neel Shah: Refining codes, create repository, transfer code format to be able to run from terminal
	Mandy Ma: Code algorithms,Debug code, Refining code


## Citation
O'Dea, Published by S., and Aug 17. “Mobile OS Market Share 2019.” Statista, 17 Aug. 2020, www.statista.com/statistics/272698/global-market-share-held-by-mobile-operating-systems-since-2009/. 
Panda Security Panda Security specializes in the development of endpoint security products and is part of the WatchGuard portfolio of IT security solutions. Initially focused on the development of antivirus software. “Android Devices 50 Times More Infected Compared to IOS - Panda Security.” Panda Security Mediacenter, 14 Jan. 2019, www.pandasecurity.com/en/mediacenter/mobile-security/android-more-infected-than-ios/
App-press.com. 2020. What Is An API And SDK? - App Press. [online] Available at: <https://www.app-press.com/blog/what-is-an-api-and-sdk#:~:text=API%20%3D%20Application%20Programming%20Interface,usually%20packaged%20in%20an%20SDK.> [Accessed 31 October 2020].
“5 Examples of APIs We Use in Our Everyday Lives: Nordic APIs |.” Nordic APIs, 10 Dec. 2019, nordicapis.com/5-examples-of-apis-we-use-in-our-everyday-lives/. 
Shifu Hou, Yanfang Ye ∗ , Yangqiu Song, and Melih Abdulhayoglu. 2017. HinDroid: An Intelligent Android Malware Detection System Based on Structured Heterogeneous Information Network. In Proceedings of KDD’17, August 13-17, 2017, Halifax, NS, Canada, , 9 pages. DOI: 10.1145/3097983.3098026
","# hindroid_replication
## HinDroid: An Intelligent Android Malware Detection System. Based on Structured Heterogeneous Information Network.
# Malware Detection Using API Relationships + Hindroid
# HOW TO RUN 
`python run.py test`
+ `requirements.txt`

By July 2020, Android OS is still a leading mobile operating system that holds 74.6% of market share worldwide, attracting numerous crazy cyber-criminals who are targeting at the largest crowd.¹ Also, due to its open-resource feature and flexible system updating policy, it is 50 times more likely to get infected compared to ios systems.² Thus, developing a strong malware detection system becomes the number one priority.

The current state of malware(malicious software) detection for the growing android OS application market involves looking solely at the API(Application Programming Interface) calls. API is a set of programming instructions that allow outside parties or individuals to access the platform or the application.³ A good and daily example will be the login options displaying on the app interface like “Login with Twitter”.4 Malwares can collect personal information easily from APIs, so analyzing APIs is a critical part of identifying malwares.

Hindroid formulates a meta-path based approach to highlight relationships across API calls to aggregate similarities and better detect malware.Individual APIs appearing in the ransomware could be harmless, but the combination of them could indicate “this ransomware intends to write malicious code into system kernel.”5 You wouldn’t want to see a group of “write”, “printStackTrace”, and “load” APIs appearing in your app’s smali file.5


# Data Generation Process
The data generation process and its relationship to the problem (i.e. for domain problems)
The data for identifying malware is primarily the android play store, although in order to obtain the respective APK’s for these apps the data is directly downloaded from `https://apkpure.com/`.

This data is then unpackaged using the apktools library that allows us to view the subsequent smali code and app binaries.

The smali code and app binaries contain a lot of the information derived from the Java source code that allows us to map the number of API calls and the relationships between them. 

# Observed Data 
Overall, from each android app, what’s most relevant to classifying Malware vs Benign - are the API calls, code blocks(methods) and packages these API calls occur in, classified as matrices 
A, P, B, I. This form of organizing data explains the relationship between these API calls. It provides a story, more in depth, than just the sheer number of API calls per app. 

The Hindroid model observes the same relationship of data to better classify malware or not, through the relationship of the above defined matrices. Reducing the ability of apps to just add a larger number of API calls to get classified as benign. 

# Conclusion
The process of identifying the relationship of API calls, is taking the idea of the subsequent network it creates - thus to not just look at the information queried by the call but also the way it interacts with other API’s in different levels of the codebase. The applicability of this lies beyond that of malware detection in android apps, but probably in the roots of graph theory and how relationships with API calls can be better identified and mapped out to provide more insight











## Responsibilities:
## Report:
Neel Shah: Malware Detection Using API Relationships + Hindroid, Data Generation Process, Observed Data, Conclusion
Mandy Ma: Malware Detection Using API Relationships + Hindroid, Citation
## Code:
	Neel Shah: Refining codes, create repository, transfer code format to be able to run from terminal
	Mandy Ma: Code algorithms,Debug code, Refining code


## Citation
O'Dea, Published by S., and Aug 17. “Mobile OS Market Share 2019.” Statista, 17 Aug. 2020, www.statista.com/statistics/272698/global-market-share-held-by-mobile-operating-systems-since-2009/. 
Panda Security Panda Security specializes in the development of endpoint security products and is part of the WatchGuard portfolio of IT security solutions. Initially focused on the development of antivirus software. “Android Devices 50 Times More Infected Compared to IOS - Panda Security.” Panda Security Mediacenter, 14 Jan. 2019, www.pandasecurity.com/en/mediacenter/mobile-security/android-more-infected-than-ios/
App-press.com. 2020. What Is An API And SDK? - App Press. [online] Available at: <https://www.app-press.com/blog/what-is-an-api-and-sdk#:~:text=API%20%3D%20Application%20Programming%20Interface,usually%20packaged%20in%20an%20SDK.> [Accessed 31 October 2020].
“5 Examples of APIs We Use in Our Everyday Lives: Nordic APIs |.” Nordic APIs, 10 Dec. 2019, nordicapis.com/5-examples-of-apis-we-use-in-our-everyday-lives/. 
Shifu Hou, Yanfang Ye ∗ , Yangqiu Song, and Melih Abdulhayoglu. 2017. HinDroid: An Intelligent Android Malware Detection System Based on Structured Heterogeneous Information Network. In Proceedings of KDD’17, August 13-17, 2017, Halifax, NS, Canada, , 9 pages. DOI: 10.1145/3097983.3098026
"
42,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_56,,,"# Malware Detecting using Control Flow Graphs
By [Edwin Huang](https://www.linkedin.com/in/edwin-huang-671a77100/), [Sabrina Ho](https://www.linkedin.com/in/sabrinaho7/)

[Link to Webpage](https://iamsabhoho.github.io/dsc180b-malware/)

## Introduction
There are many malware detection tools available in the market, including pattern-based, behavior-based methods, etc, with the prompt development of artificial intelligence, many modern data analysis methods are applied to detecting malware in recent years. We are interested in investigating the effectiveness of different data analysis methods for detecting certain types of malware.

As the number of malicious software (malware) increases throughout the past few decades, malware detection has become a challenge for app developers, companies hosting the apps, and people using the apps. There are many pieces of research conducted on malware detection since it first appeared in the early 1970s. Just like the paper we studied in our first quarter, it uses the HIN (Heterogeneous Information Network) structure to classify the Android applications. It also compared its own method against other popular methods such as Naive Bayes and Decision Tree, and other known commercial mobile security products, to test its performance. The result showed that their method performs better than the other methods with an accuracy of 98% while other others only achieve an average of 90\%. After studying the paper, we are more curious about the detecting effectiveness of an analysis method when applied to a certain type of malware.

![Project Pipeline](/img/flow.png)

Not everyone has access to tools that can detect whether or not the app they just downloaded is malicious or not. Our motivation to conduct this research is to hope to produce a recommending tool that can be easily accessed by the general public for detecting malware. Optimistically, we want to reduce the chance of people downloading malicious apps and potentially prevent their devices from being hacked. To achieve that, we will be classifying applications using Control Flow Graphs and different similarity-based methods including k-nearest neighbors (kNN) as well as Random Forest classifier to see if different methods can detect certain types of malware or any specific features.

We are interested in analyzing whether one classifier has better performance in detecting certain types of malware or specific features, and designing a framework for recommending a method with a specific set of parameters for a certain type of malware and provide users a more friendly interface. With the similarity-based approach, we believe that it will detect malware with much higher accuracy and will be more flexible for applications that evolved over time as they become more complicated.


## Related Work

### Mamadroid
MamaDroid is a system that detects Android malware by the apps' behaviors. This method extract call graphs from APKs, which are represented using nodes and edges in a graph object. From each graph, sequences of probabilities are extracted, representing one feature vector per APK. These probabilistic feature vectors are used for malware classification. MamaDroid also abstracts each API call to the family and package level, which inspired us to abstract to the class level. This is discussed further later.  

### Hindroid
Hindroid is a system that parses SMALI code extracted from APKs and uses them to create four different graphs, which are represented by large matrices. Within these matrices, each value in a matrix corresponds to an edge. A combination of these matrices are used to classify malicious software and benign apps.  

### Metapath2Vec
Metapath2Vec is a node representation learning model that uses predefined paths based on the node types. These paths define where the the program can traverse the graph. Following is an example of a metapath. In this case below, the metapath is Type 1 → Type 2 → Type 1 → Type 3 → Type 1.

![Metapath2vec Example](/img/m2v.png) 

With predefined meta-paths, we can traverse a graph according to these node types to generate a large corpus, which is then fed into Node2Vec to obtain representations of words. This method will obtain one vector for one node within the graph.

### Word2Vec
Word2vec is a model that turns text into numerical representations. It is trained on a large corpus, and outputs a representation for each word in the corpus. Below is a famous example of Word2Vec: King and Queen and Men and Women.

![Word2vec Example](/img/w2v.png) 

Since Word2Vec measures the similarity between words using Cosine similarity, we can see from the above vector space that the word King is similar to Queen, and Men is similar to Women.

### Doc2Vec
Similar to Word2Vec, Doc2Vec turns a whole document/paragraph into numerical representations instead of word representations. If we can obtain one corpus from each of the apps by applying metapath2vec, then we can treat each corpus as its own document, and then feed it into the Doc2Vec model to learn representations for each of the documents. These vector representations can then be used in the classification process.


## Data
The data we will be using is randomly downloaded from APK Pure and AMD malware dataset. It consists of labeled malware and other popular and unpopular (random) applications. Among our random apps downloaded from APK Pure, there might be one app out of five that might be a malware since they are apps that have little or no reviews. Rather than using .SMALI files, we will be working with APK files directly. From the APK files, we will be extracting a new form of representation called Control Flow Graphs. With APK files, we can easily generate control flow graphs through Androguard, which is a powerful tool to disassemble and decompile Android applications.

### Control Flow Graphs
A Control Flow Graph (CFG) is a representation using graph notation of all paths that might be traversed through a program during execution. Firstly, a CFG consists of nodes and edges. Control Flow is the order in which individual statements, instructions, or function calls of an imperative program are executed or evaluated. Imperative meaning statements that change a program’s state. Each node in the CFG represents a basic block, or a straight-line piece of code without any jumps or jump targets. In our case, a node in the CFG is an API or method call. A jump statement is a statement that changes the program’s flow into another place of the source code. For example, from line 4 to line 60, or from file 1 to file 6. The following figures are two simple control flow graphs.

![A Simple Control Flow Graph](/img/cfg.png)

A node in our CFG can call another API (node). A node can be visualized as one of the circles in Figure 4, and the ""call"" action can be visualized by the arrow(edge). Each node has attributes. There are 7 Boolean attributes for each node, and 3 different edge types.

| Node Attributes | |
| --- | --- |
| External | If a node is an external method |
| Entrypoint | If a node is not called by anything |
| Native | If a node is native |
| Public | If the node is a public method |
| Static | If the node is a static method |
| Node | If none of the above are True |
| APK Node | If the node is an APK |

We can pick from the 6 Boolean attributes and create node types based, such as: “external, public Node”, “external, static Node” and “entrypoint, native Node”. There can be more than 20 different node types.  

| Edge Types | |
| --- | --- |
| Calls | API to API |
| Contains | APK to API |
| In | API to APK |

Together, nodes and edges can build paths like: “external, public Node - calls -> external, static Node” or “APK - contains -> external, public Node”. The following is a control flow graph example with code block to explain how nodes are called:

```
Class: Lclass0/package0/exampleclass; ## let’s call this A
# direct methods
.method public constructor <init>()V
    if ....:
    	# api, call this B:
    	Lclass1/package1/example;->doSomething(Ljava/lang/String;)V
	else:
    	# api, call this C:
    	Lclass2/package2/example;->perform(L1/2/3)F
```

![Control Flow Graph With Code Block Example](/img/cfg1.png)

The method **constructor** calls API A, which calls API B: **doSomething** and calls API C: **perform**. Since API B and API C will jump to other places within the source code, the flow of the program is broken, and this jump is recorded in the control flow graph. 

The Control Flow Graph from an app records tens of thousands of these calls, and represents them as edges, where each edge contains two nodes.


### Common Graph
Since we obtained a large number of CFGs for a large number of APKs, we need to figure out a way to connect all of these graphs so the representations for each API will be the same. We want the API representations to be the same so when we are classifying we know that all feature vectors are built the same way. This is to avoid us creating random feature vectors, and will result in the model classifying randomly. To make sure we are building features correctly, we must create a common graph that links every CFG together. Also, during the testing phase, we can use these node embeddings to build a feature vector for an unseen app.

The common graph we built contains a total of 1,950,729 nodes, and 215,604,110 edges. Our common graph is simply a union of all the control flow graphs that we obtained from separate apps. This is not only to make sure that each distinct API node are consistent throughout our training and testing process, but to make sure that all our CFGs are on the same space. First, all the edges of each separate graphs are extracted, along with their weights and node types of each edge. Then, these information are loaded altogether to become a common graph. 

![Common Graph Example](/img/common_graph2.png)

The figure above demonstrates what a common graph looks like by combining two control flow graphs from two different apps. On the left we have red and blue applications, which both have five nodes consisting of A, B, and C nodes connected together and other nodes of its own. When combining them, we generate the graph on the right, which merge the shared nodes with each other. Duplicate nodes are joined to be one, while the edges are still preserved. As you can see, the similar A, B, C sequence is preserved as well. This is important when we are building feature vectors for an app. The common graph ensures the same node representations for two graphs. This means that when we are building feature vectors for apps, the same representations are used for two similar apps. Conversely, if two apps are not similar and do not share the same sequences, then their representations will be very different. Like in the common graph in Figure 6, this will make sure that the similarity and differences between the apps are preserved.

### Data Generating Process/ETL
The raw data we are investigating is code written by app developers. In order to turn something into a malware, you have to alter the source code, which will allow hackers to plant certain types of malicious code. If a developer were to hijack a device, then the app would need special Root permissions. Often targeting API calls that represent System Calls is one of the ways to alter the source code. For that reason, source code is an essential part in determining whether an application is malicious or not. 

As mentioned above, we will be using control flow graphs converted directly from the APK files. We are looking at the sequence of which these system calls are made and define them as meta paths. We were able to obtain one CFG for each APK. We extracted this by using Androguard’s AnalysisAPK method in its misc module which returns an analysis object. Afterwards, we called .get\_call\_graph() on the analysis object to obtain the CFG. At this stage, we also perform some feature extraction specifically on the nodes of the graph. We extract the string representation of these nodes as well as node type. The string representations of nodes is used to build the corpus, and the node type is used to build meta paths. We then exported this graph as a compressed gml file to save on disk. We hypothesize that our method will perform better than our baseline model, since metapath2vec can capture the relations and context within the graphs, giving the feature vector much more information. Also, our metapaths are traversed in the beginning of our process to learn all possible metapaths. Using this method, we ensure that the model learns the different sequences that a malware could have, and use this information in future classification. 

### EDA
We have a total of 8435 malicious software and a total of 802 benign applications, which is a combination of popular apks and random apps. While generating control flow graph objects from the APK files, there was an error of “*Missing AndroidManifest.xml*,” so we were not able to generate those graphs and will be working with fewer benign apps \footnote{We looked into why there might be missing Android Manifest files error, interestingly, we found that some of the apps having this issue contain the manifest while some do not. However, the apps that have this issue do not decompile correctly, and do not create a graph correctly as well.}. To counteract the imbalance between malware and benign apps, we calculated class weights and used it in the classification process to ensure we are penalizing the model in a balanced way.

![Benign vs. Malicious](/img/bm_counts.png)

To further understand our benign and malicious data, we perform analysis on these graph objects by comparing the node types, as well as the counts of both nodes and edges.

![Benign vs. Malicious: Node Type Counts](/img/bv_node_types.png)

The figure above shows the comparisons of node types between benign applications and malicious code. From the two distributions, we can see that malware contains a lot more types of nodes compared to benign apps. Specifically, most of the malware contains five types of node. If we limit the range of that bar, we can see the figure below for a more clear distribution of benign apps. The left distribution also indicates that majority of the benign apps have five types of nodes.

![Benign vs. Malicious: Node Type Counts](/img/bv_node_types1.png)

But because of how imbalanced our data is, we plot the number of node types based on the percentage, as shown in the below figure. From this figure we can conclude that over half of both benign and malicious apps have more than 5 types of nodes. 

![Benign vs. Malicious: Node Type Counts (%)](/img/bv_node_types_p.png)

To further look into what are these node types, we analyze the top node types from both benign and malware separately. The following figures are node types distributions. On the left shows the benign node types spread, which over 50\% of the nodes are Public Node, followed by Node, External Node, Public Static Node, Static Node, and the other types. Similarly, for malware, Public Node is the top most node type found in the graphs. Followed by External Node, Node, Public Static Node, Static Node, and the others. Both benign and malware have similar top nodes.

![Benign: Top Node Types and The Counts](/img/counts_pie_b.png)
![Malicious: Top Node Types and The Counts](/img/counts_pie_m.png)

The figure below is a scatter plot of number of edges and number of nodes for both benign (red) and malware (blue). Although it seems that benign has a lot more apps in this plot, malware are just all packed together. We can also see that benign apps have larger number of edges and nodes compared to malware, this is because benign apps are larger in terms of APK sizes and that they might be more complicated.

![Benign vs. Malicious: Number of Edges and Number of Nodes](/img/bm_edges_nodes.png)

Since the figure above has outliers for benign apps and we want to focus on the malware, we limit the range so that it looks like the figure below. From this figure we see that malware is indeed packed together and that they have a lot less edge and nodes compared to the benign apps.

![Benign vs. Malicious: Number of Edges and Number of Nodes (Malware Focused)](/img/bm_edges_nodes1.png)


## Methods

### Feature Extraction
For our baseline, we extract probabilistic sequences from all possible edges of the APK, which serves as the feature vector for classification. For the Metapath2Vec model, we first create a common graph, then traverse it using Metapath2Vec to learn representations of nodes, which is used to build feature vectors. For our Doc2Vec method, we treat each APK as one document, and Doc2Vec produces one feature vector for one document.

### Baseline: Mamadroid
We build Mamadroid as our baseline model. As introduced earlier, it extract call graphs that are represented using nodes and edges. With the graphs, it extracts all the possible edges based on the family or package level. It then extract sequences of probabilities of the edges occurring. This probabilistic feature vector is used for classification. We abstract API calls to both Family and Package level.

For example, 
```
Example API call = ""LFamily/Package""
Family Level = ""LFamily""
Package Level = ""LFamily/Package""
```

In Family level, there are seven possible families and 100 total possible edges. However, in Package level, there are 226 possible packages and a total of 51,239 possible edges. The number of possible families and packages are found on Android's Developers page. Those families and packages that are not found on that webpage is abstracted to ""self-defined"". Specifically, in family level, we will obtain a feature vector of 100 elements for one app. In package level, we obtain a feature vector with 51,239 elements for one app. These feature vectors are then used for classification. After obtaining the vector embeddings, we classify using Random Forest model, 1-Nearest Neighbors, and 3-Nearest Neighbors.

### Metapath2Vec Model Using Common Graph
As mentioned earlier, we also abstracted our API calls to the class level. For example, an API call looks like this: ""Lfamily/package/class; → doSomething()V"" at the class level, it is: ""Lfamily/package/class;"". The reason for this is there could be user-defined classes, which is not picked up in MamaDroid. We hope that we can obtain more information by abstracting to the class level, but not get too much information at the API level which might result in performance issues. We do not abstract anything to be ""self-defined"" as MamaDroid has.

1. Run a Depth First Search to explore all the node types that could be in an APK, and create metapaths.
2. Build a common graph by combining all the separate control flow graphs representing different apps.
3. Perform an uniform metapath walk on the common graph to obtain a huge corpus.
4. Perform node2vec to learn node representations of the huge corpus.
5. Build feature vectors for each app, using the node embeddings learned from step 3, by combining embeddings of unique nodes of each app.
6. Classification using built feature vectors.

Explanations: 
We run a depth first search to explore all node types and metapaths since we do not know how convoluted an app's CFG may be and we need flexible metapaths for each app. Also, there is the possibility of the malware being intentionally obfuscated. Therefore, we need flexible metapaths for each app, which we will later use as the predefined metapaths in our metapath2vec step. The reason our feature vector is a component wise combination of node embeddings is because when two vectors are added together, a new vector is obtained. As visualized below:  

![Vector Addition And Subtraction](/img/vec.png)

This will provide more information about the APKs that we will classify. The model can more easily learn the distinction between similar and different vectors, by the direction and magnitude to where they point. Of course, the component wise combination can also be other aggregations, such as taking an average, percentiles, and dot products. When encountering an unseen app, unique nodes of that app is extracted. Representations of each of the unique nodes are then found from the trained word2vec model, and some component wise combination is performed to obtain a feature vector for classification. 

### Doc2Vec Model
1. For each app, extract all possible metapaths using Depth First Search, as well as perform metapath2vec on that app to obtain a corpus.
2. Take each corpus from each app, append them, and turn them into a list of Tagged Documents.
3. Run the Tagged Documents into Doc2Vec to obtain a vector representation for each app.
4. Take the vector representations for each tagged document, and use them as feature vectors for classification.

The Doc2Vec model is very straight forward, taking in documents and returning representations for those documents. When there is an unseen app, a corpus is extracted from that app using metapath and is treated as a document. This document is then fed into the Doc2Vec model, and a vector representation is ""inferred"" using the .infer_vector() method.  


## Results and Analysis
### Baseline Results
The following tables are results from our baseline model, MamaDroid, corresponding to Family and Package mode. Surprisingly, our MamaDroid using control flow graphs performs better than its original model. Let's first take a look at the Family mode. The table below is the confusion matrix, we calculated precision and recall scores based on it.

| | Random Forest | 1-NN | 3-NN |
| --- | --- | --- | --- |
| True Negative | 68 | 61 | 59 |
| False Negative | 4 | 9 | 8 |
| False Positive  | 11 | 18 | 20 |
| True Positive | 1269 | 1264 | 1265 |

We compare the results to the original MamaDroid model. In the table below, we list the original MamaDroid results on it as well to better compare it. We see that our version of MamaDroid has better performance in all F1-Score, precision and recall scores, where we obtain an F measure of 0.994 and the original model only has 0.880. Similarly to precision and recall scores, we obtain 0.991 and 0.997, where the original model has 0.840 and 0.920 as their results. In addition to Random Forest, our 1-NN and 3-NN models also outperform the original MamaDroid model. But all our three models have similar results.

<table>
    <tbody>
        <tr>
            <td rowspan=2>PCA = 10 Component</td>
            <td colspan=2>Random Forest</td>
            <td rowspan=2>1-NN</td>
            <td rowspan=2>3-NN</td>
        </tr>
        <tr>
            <td>Original</td>
            <td>Ours</td>
        </tr>
        <tr>
            <td>F1-Score</td>
            <td>0.880</td>
            <td>0.994</td>
            <td>0.980</td>
            <td>0.994</td>
        </tr>
        <tr>
            <td>Precision</td>
            <td>0.840</td>
            <td>0.991</td>
            <td>0.985</td>
            <td>0.994</td>
        </tr>
        <tr>
            <td>Recall</td>
            <td>0.920</td>
            <td>0.997</td>
            <td>0.994</td>
            <td>0.994</td>
        </tr>
    </tbody>
</table>

Next, we have results for our MamaDroid Package mode. The following table is the confusion matrix. The numbers are close to what we obtain for Family level. However, the true negatives for all three similarity-based models are slightly larger.

| | Random Forest | 1-NN | 3-NN |
| --- | --- | --- | --- |
| True Negative | 70 | 70 | 70 |
| False Negative | 1 | 6 | 1 |
| False Positive  | 15 | 15 | 15 |
| True Positive | 1266 | 1261 | 1266 |

We also compared the results of Package mode to the original MamaDroid results. In the table shown below, we also list out the results of original MamaDroid on the left to compare it with the ones we obtain. As a result, our model was able to achieve an F measure of 0.993, precision score of 0.988, and recall score of 0.999, whereas the original model only has performance of 0.940, 0.940, and 0.950. Both 1-NN and 3-NN models also have very similar numbers as Random Forest model.

<table>
    <tbody>
        <tr>
            <td rowspan=2>PCA = 10 Component</td>
            <td colspan=2>Random Forest</td>
            <td rowspan=2>1-NN</td>
            <td rowspan=2>3-NN</td>
        </tr>
        <tr>
            <td>Original</td>
            <td>Ours</td>
        </tr>
        <tr>
            <td>F1-Score</td>
            <td>0.940</td>
            <td>0.993</td>
            <td>0.992</td>
            <td>0.994</td>
        </tr>
        <tr>
            <td>Precision</td>
            <td>0.940</td>
            <td>0.988</td>
            <td>0.988</td>
            <td>0.988</td>
        </tr>
        <tr>
            <td>Recall</td>
            <td>0.950</td>
            <td>0.999</td>
            <td>0.995</td>
            <td>0.999</td>
        </tr>
    </tbody>
</table>
  
### Metapath2Vec/Common Graph (Partial)
For our Metapath2Vec model, we unfortunately do not have the complete results due to large computational time it takes to build the common graph with all the data we have. The complete common graph consists of 1,950,729 nodes, and 215,604,110 edges. However, we did obtain results working with a smaller subset of the Common Graph, consisting of: 87,539 nodes and 15,617,223 edges.

| | Random Forest | 1-NN | 3-NN |
| --- | --- | --- | --- |
| True Negative | 94 | 89 | 73 |
| False Negative | 16 | 31 | 36 |
| False Positive  | 20 | 25 | 41 |
| True Positive | 1679 | 1664 | 1659 |

We tested on the entire test set, and surprisingly the performance was okay. Initially, we thought that there might be an error, however, upon inspecting our code, we were traversing the smaller common graph correctly. We believe we can obtain this result because of the large amount of edges in the common graph as well as our walk length of 500. We set the walk length to 500 to compensate for the smaller subset of graphs that we are using, and therefore can capture more information per walk. Because of this, the Word2Vec model can learn more about those nodes and provide a better representation. 

| | Random Forest | 1-NN | 3-NN |
| --- | --- | --- | --- |
| TF1-Score | **0.989** | 0.983 | 0.977 |
| Precision | 0.992 | 0.966 | 0.982 |
| Recall | 0.642 | 0.959 | 0.959 |

Even though the F1-Scores were high, our True Negative and False Positives are higher than our baseline and Doc2vec model. This is again due to the smaller subset that we are using for this experiment. Because of the smaller subset, we do not have a lot of representations for nodes. There could have been some nodes that the word2vec model has never seen before, and therefore cannot infer a good representation for it. 

### Doc2Vec
The following tables are the results for Doc2Vec with our similarity-based models: Random Forest, 1-NN, and 3-NN. Table below is the confusion matrix, which is used to compute the precision and recall scores. The Doc2Vec performed worse than the baseline model on the Random Forest model, and it seems like it was struggling with classifying benign apps.

| | Random Forest | 1-NN | 3-NN |
| --- | --- | --- | --- |
| True Negative | 109 | 56 | 43 |
| False Negative | 606 | 68 | 71 |
| False Positive  | 5 | 58 | 31 |
| True Positive | 1089 | 1627 | 1664 |

In the table below, we have our F measure, precision, and recall scores. We notice that our Random Forest model only has an F measure of 0.781, which is a lot lower than our baseline. On the other hand, both our k Nearest Neighbors perform much better than Random Forest. The Random Forest classifier has an emphasis on certain features when training, and focuses on some feature more than others. However, the 1-NN and 3-NN models both look at an unseen vector's closest neighbors, therefore utilizing all the features in the vector. We believe this is why the 1-NN and 3-NN models performed better in this experiment.

| | Random Forest | 1-NN | 3-NN |
| --- | --- | --- | --- |
| TF1-Score | 0.781 | 0.963 | **0.970** |
| Precision | 0.992 | 0.966 | 0.982 |
| Recall | 0.642 | 0.959 | 0.959 |


## Conclusion
In conclusion, our baseline model is able to achieve a better performance than the original work that we have studied. Although our Doc2Vec did not perform better than the baseline Random Forest model, our k Nearest Neighbors models performed almost as good as our baseline. From this, we can see that Control Flow Graphs might be a good choice when it comes to choosing representations for source code. Again, control flow graphs show the jumps in code. From our EDA: Figure 12, even though malware has a small number of nodes, they have a large amount of edges. This means that there could be lots of instances where the program is jumping around in the source code. All this is recorded in the CFG representation and could provide much more information about an APK.

 Although we successfully created a complete common graph, we were unable to obtain all the node embeddings from it due to time and memory constraints. Therefore we built a smaller common graph to see how it performs. If time and resources allowed, we hope to finish the metapath traversal of the complete common graph. Judging from the results using the smaller common graph, if we were to scale up the model might out-perform our baseline.

For our future work, we plan on investigating other vector embeddings technique and perhaps instead of using only similarity-based models, we could also implement graph neural networks (GNN). In addition to neural networks, we are also interested in graph classification specifically. Since our data format is already in the form of multiple apps, it can be easy to normalize and transform data for a GNN model. Of so many researches we have seen on malware detection, not a lot of them uses control flow graphs as their input data. Since our experiments confirmed that using control flow graphs is not any worse than using other forms of data, we are curious to know if control flow graphs can outperform in other models.

## Acknowledgement
We would like to express our gratitude to our mentors for our Capstone project: Professor Aaron Fraenkel, who provided us with lots of resources and ideas throughout the entire process of our research, and Shivam Lakhotia, who guided us through the project and assisted us every week. 

## References
[1] MamaDroid, https://arxiv.org/pdf/1612.04433.pdf

[2] Hindroid, https://www.cse.ust.hk/~yqsong/papers/2017-KDD-HINDROID.pdf

[3] Metapath2Vec, https://ericdongyx.github.io/papers/KDD17-dong-chawla-swami-metapath2vec.pdf

[4] Word2Vec, https://radimrehurek.com/gensim/models/word2vec.html

[5] Doc2Vec, https://radimrehurek.com/gensim/models/doc2vec.html

[6] Androguard, https://androguard.blogspot.com/2011/02/android-apps-visualization.html

[7] StellarGraph, https://github.com/stellargraph/stellargraph

[8] SDK, https://developer.android.com/studio/releases/platforms

[9] x2vec, https://iopscience.iop.org/article/10.1088/2632-072X/aba83d/pdf

[10] Learning Embeddings of Directed Networks with Text-Associated Nodes—with Application in Software Package Dependency Networks, https://arxiv.org/pdf/1809.02270.pdf

","# Malware Detecting using Control Flow Graphs
By [Edwin Huang](https://www.linkedin.com/in/edwin-huang-671a77100/), [Sabrina Ho](https://www.linkedin.com/in/sabrinaho7/)

[Link to Webpage](https://iamsabhoho.github.io/dsc180b-malware/)

## Introduction
There are many malware detection tools available in the market, including pattern-based, behavior-based methods, etc, with the prompt development of artificial intelligence, many modern data analysis methods are applied to detecting malware in recent years. We are interested in investigating the effectiveness of different data analysis methods for detecting certain types of malware.

As the number of malicious software (malware) increases throughout the past few decades, malware detection has become a challenge for app developers, companies hosting the apps, and people using the apps. There are many pieces of research conducted on malware detection since it first appeared in the early 1970s. Just like the paper we studied in our first quarter, it uses the HIN (Heterogeneous Information Network) structure to classify the Android applications. It also compared its own method against other popular methods such as Naive Bayes and Decision Tree, and other known commercial mobile security products, to test its performance. The result showed that their method performs better than the other methods with an accuracy of 98% while other others only achieve an average of 90\%. After studying the paper, we are more curious about the detecting effectiveness of an analysis method when applied to a certain type of malware.

![Project Pipeline](/img/flow.png)

Not everyone has access to tools that can detect whether or not the app they just downloaded is malicious or not. Our motivation to conduct this research is to hope to produce a recommending tool that can be easily accessed by the general public for detecting malware. Optimistically, we want to reduce the chance of people downloading malicious apps and potentially prevent their devices from being hacked. To achieve that, we will be classifying applications using Control Flow Graphs and different similarity-based methods including k-nearest neighbors (kNN) as well as Random Forest classifier to see if different methods can detect certain types of malware or any specific features.

We are interested in analyzing whether one classifier has better performance in detecting certain types of malware or specific features, and designing a framework for recommending a method with a specific set of parameters for a certain type of malware and provide users a more friendly interface. With the similarity-based approach, we believe that it will detect malware with much higher accuracy and will be more flexible for applications that evolved over time as they become more complicated.


## Related Work

### Mamadroid
MamaDroid is a system that detects Android malware by the apps' behaviors. This method extract call graphs from APKs, which are represented using nodes and edges in a graph object. From each graph, sequences of probabilities are extracted, representing one feature vector per APK. These probabilistic feature vectors are used for malware classification. MamaDroid also abstracts each API call to the family and package level, which inspired us to abstract to the class level. This is discussed further later.  

### Hindroid
Hindroid is a system that parses SMALI code extracted from APKs and uses them to create four different graphs, which are represented by large matrices. Within these matrices, each value in a matrix corresponds to an edge. A combination of these matrices are used to classify malicious software and benign apps.  

### Metapath2Vec
Metapath2Vec is a node representation learning model that uses predefined paths based on the node types. These paths define where the the program can traverse the graph. Following is an example of a metapath. In this case below, the metapath is Type 1 → Type 2 → Type 1 → Type 3 → Type 1.

![Metapath2vec Example](/img/m2v.png) 

With predefined meta-paths, we can traverse a graph according to these node types to generate a large corpus, which is then fed into Node2Vec to obtain representations of words. This method will obtain one vector for one node within the graph.

### Word2Vec
Word2vec is a model that turns text into numerical representations. It is trained on a large corpus, and outputs a representation for each word in the corpus. Below is a famous example of Word2Vec: King and Queen and Men and Women.

![Word2vec Example](/img/w2v.png) 

Since Word2Vec measures the similarity between words using Cosine similarity, we can see from the above vector space that the word King is similar to Queen, and Men is similar to Women.

### Doc2Vec
Similar to Word2Vec, Doc2Vec turns a whole document/paragraph into numerical representations instead of word representations. If we can obtain one corpus from each of the apps by applying metapath2vec, then we can treat each corpus as its own document, and then feed it into the Doc2Vec model to learn representations for each of the documents. These vector representations can then be used in the classification process.


## Data
The data we will be using is randomly downloaded from APK Pure and AMD malware dataset. It consists of labeled malware and other popular and unpopular (random) applications. Among our random apps downloaded from APK Pure, there might be one app out of five that might be a malware since they are apps that have little or no reviews. Rather than using .SMALI files, we will be working with APK files directly. From the APK files, we will be extracting a new form of representation called Control Flow Graphs. With APK files, we can easily generate control flow graphs through Androguard, which is a powerful tool to disassemble and decompile Android applications.

### Control Flow Graphs
A Control Flow Graph (CFG) is a representation using graph notation of all paths that might be traversed through a program during execution. Firstly, a CFG consists of nodes and edges. Control Flow is the order in which individual statements, instructions, or function calls of an imperative program are executed or evaluated. Imperative meaning statements that change a program’s state. Each node in the CFG represents a basic block, or a straight-line piece of code without any jumps or jump targets. In our case, a node in the CFG is an API or method call. A jump statement is a statement that changes the program’s flow into another place of the source code. For example, from line 4 to line 60, or from file 1 to file 6. The following figures are two simple control flow graphs.

![A Simple Control Flow Graph](/img/cfg.png)

A node in our CFG can call another API (node). A node can be visualized as one of the circles in Figure 4, and the ""call"" action can be visualized by the arrow(edge). Each node has attributes. There are 7 Boolean attributes for each node, and 3 different edge types.

| Node Attributes | |
| --- | --- |
| External | If a node is an external method |
| Entrypoint | If a node is not called by anything |
| Native | If a node is native |
| Public | If the node is a public method |
| Static | If the node is a static method |
| Node | If none of the above are True |
| APK Node | If the node is an APK |

We can pick from the 6 Boolean attributes and create node types based, such as: “external, public Node”, “external, static Node” and “entrypoint, native Node”. There can be more than 20 different node types.  

| Edge Types | |
| --- | --- |
| Calls | API to API |
| Contains | APK to API |
| In | API to APK |

Together, nodes and edges can build paths like: “external, public Node - calls -> external, static Node” or “APK - contains -> external, public Node”. The following is a control flow graph example with code block to explain how nodes are called:

```
Class: Lclass0/package0/exampleclass; ## let’s call this A
# direct methods
.method public constructor <init>()V
    if ....:
    	# api, call this B:
    	Lclass1/package1/example;->doSomething(Ljava/lang/String;)V
	else:
    	# api, call this C:
    	Lclass2/package2/example;->perform(L1/2/3)F
```

![Control Flow Graph With Code Block Example](/img/cfg1.png)

The method **constructor** calls API A, which calls API B: **doSomething** and calls API C: **perform**. Since API B and API C will jump to other places within the source code, the flow of the program is broken, and this jump is recorded in the control flow graph. 

The Control Flow Graph from an app records tens of thousands of these calls, and represents them as edges, where each edge contains two nodes.


### Common Graph
Since we obtained a large number of CFGs for a large number of APKs, we need to figure out a way to connect all of these graphs so the representations for each API will be the same. We want the API representations to be the same so when we are classifying we know that all feature vectors are built the same way. This is to avoid us creating random feature vectors, and will result in the model classifying randomly. To make sure we are building features correctly, we must create a common graph that links every CFG together. Also, during the testing phase, we can use these node embeddings to build a feature vector for an unseen app.

The common graph we built contains a total of 1,950,729 nodes, and 215,604,110 edges. Our common graph is simply a union of all the control flow graphs that we obtained from separate apps. This is not only to make sure that each distinct API node are consistent throughout our training and testing process, but to make sure that all our CFGs are on the same space. First, all the edges of each separate graphs are extracted, along with their weights and node types of each edge. Then, these information are loaded altogether to become a common graph. 

![Common Graph Example](/img/common_graph2.png)

The figure above demonstrates what a common graph looks like by combining two control flow graphs from two different apps. On the left we have red and blue applications, which both have five nodes consisting of A, B, and C nodes connected together and other nodes of its own. When combining them, we generate the graph on the right, which merge the shared nodes with each other. Duplicate nodes are joined to be one, while the edges are still preserved. As you can see, the similar A, B, C sequence is preserved as well. This is important when we are building feature vectors for an app. The common graph ensures the same node representations for two graphs. This means that when we are building feature vectors for apps, the same representations are used for two similar apps. Conversely, if two apps are not similar and do not share the same sequences, then their representations will be very different. Like in the common graph in Figure 6, this will make sure that the similarity and differences between the apps are preserved.

### Data Generating Process/ETL
The raw data we are investigating is code written by app developers. In order to turn something into a malware, you have to alter the source code, which will allow hackers to plant certain types of malicious code. If a developer were to hijack a device, then the app would need special Root permissions. Often targeting API calls that represent System Calls is one of the ways to alter the source code. For that reason, source code is an essential part in determining whether an application is malicious or not. 

As mentioned above, we will be using control flow graphs converted directly from the APK files. We are looking at the sequence of which these system calls are made and define them as meta paths. We were able to obtain one CFG for each APK. We extracted this by using Androguard’s AnalysisAPK method in its misc module which returns an analysis object. Afterwards, we called .get\_call\_graph() on the analysis object to obtain the CFG. At this stage, we also perform some feature extraction specifically on the nodes of the graph. We extract the string representation of these nodes as well as node type. The string representations of nodes is used to build the corpus, and the node type is used to build meta paths. We then exported this graph as a compressed gml file to save on disk. We hypothesize that our method will perform better than our baseline model, since metapath2vec can capture the relations and context within the graphs, giving the feature vector much more information. Also, our metapaths are traversed in the beginning of our process to learn all possible metapaths. Using this method, we ensure that the model learns the different sequences that a malware could have, and use this information in future classification. 

### EDA
We have a total of 8435 malicious software and a total of 802 benign applications, which is a combination of popular apks and random apps. While generating control flow graph objects from the APK files, there was an error of “*Missing AndroidManifest.xml*,” so we were not able to generate those graphs and will be working with fewer benign apps \footnote{We looked into why there might be missing Android Manifest files error, interestingly, we found that some of the apps having this issue contain the manifest while some do not. However, the apps that have this issue do not decompile correctly, and do not create a graph correctly as well.}. To counteract the imbalance between malware and benign apps, we calculated class weights and used it in the classification process to ensure we are penalizing the model in a balanced way.

![Benign vs. Malicious](/img/bm_counts.png)

To further understand our benign and malicious data, we perform analysis on these graph objects by comparing the node types, as well as the counts of both nodes and edges.

![Benign vs. Malicious: Node Type Counts](/img/bv_node_types.png)

The figure above shows the comparisons of node types between benign applications and malicious code. From the two distributions, we can see that malware contains a lot more types of nodes compared to benign apps. Specifically, most of the malware contains five types of node. If we limit the range of that bar, we can see the figure below for a more clear distribution of benign apps. The left distribution also indicates that majority of the benign apps have five types of nodes.

![Benign vs. Malicious: Node Type Counts](/img/bv_node_types1.png)

But because of how imbalanced our data is, we plot the number of node types based on the percentage, as shown in the below figure. From this figure we can conclude that over half of both benign and malicious apps have more than 5 types of nodes. 

![Benign vs. Malicious: Node Type Counts (%)](/img/bv_node_types_p.png)

To further look into what are these node types, we analyze the top node types from both benign and malware separately. The following figures are node types distributions. On the left shows the benign node types spread, which over 50\% of the nodes are Public Node, followed by Node, External Node, Public Static Node, Static Node, and the other types. Similarly, for malware, Public Node is the top most node type found in the graphs. Followed by External Node, Node, Public Static Node, Static Node, and the others. Both benign and malware have similar top nodes.

![Benign: Top Node Types and The Counts](/img/counts_pie_b.png)
![Malicious: Top Node Types and The Counts](/img/counts_pie_m.png)

The figure below is a scatter plot of number of edges and number of nodes for both benign (red) and malware (blue). Although it seems that benign has a lot more apps in this plot, malware are just all packed together. We can also see that benign apps have larger number of edges and nodes compared to malware, this is because benign apps are larger in terms of APK sizes and that they might be more complicated.

![Benign vs. Malicious: Number of Edges and Number of Nodes](/img/bm_edges_nodes.png)

Since the figure above has outliers for benign apps and we want to focus on the malware, we limit the range so that it looks like the figure below. From this figure we see that malware is indeed packed together and that they have a lot less edge and nodes compared to the benign apps.

![Benign vs. Malicious: Number of Edges and Number of Nodes (Malware Focused)](/img/bm_edges_nodes1.png)


## Methods

### Feature Extraction
For our baseline, we extract probabilistic sequences from all possible edges of the APK, which serves as the feature vector for classification. For the Metapath2Vec model, we first create a common graph, then traverse it using Metapath2Vec to learn representations of nodes, which is used to build feature vectors. For our Doc2Vec method, we treat each APK as one document, and Doc2Vec produces one feature vector for one document.

### Baseline: Mamadroid
We build Mamadroid as our baseline model. As introduced earlier, it extract call graphs that are represented using nodes and edges. With the graphs, it extracts all the possible edges based on the family or package level. It then extract sequences of probabilities of the edges occurring. This probabilistic feature vector is used for classification. We abstract API calls to both Family and Package level.

For example, 
```
Example API call = ""LFamily/Package""
Family Level = ""LFamily""
Package Level = ""LFamily/Package""
```

In Family level, there are seven possible families and 100 total possible edges. However, in Package level, there are 226 possible packages and a total of 51,239 possible edges. The number of possible families and packages are found on Android's Developers page. Those families and packages that are not found on that webpage is abstracted to ""self-defined"". Specifically, in family level, we will obtain a feature vector of 100 elements for one app. In package level, we obtain a feature vector with 51,239 elements for one app. These feature vectors are then used for classification. After obtaining the vector embeddings, we classify using Random Forest model, 1-Nearest Neighbors, and 3-Nearest Neighbors.

### Metapath2Vec Model Using Common Graph
As mentioned earlier, we also abstracted our API calls to the class level. For example, an API call looks like this: ""Lfamily/package/class; → doSomething()V"" at the class level, it is: ""Lfamily/package/class;"". The reason for this is there could be user-defined classes, which is not picked up in MamaDroid. We hope that we can obtain more information by abstracting to the class level, but not get too much information at the API level which might result in performance issues. We do not abstract anything to be ""self-defined"" as MamaDroid has.

1. Run a Depth First Search to explore all the node types that could be in an APK, and create metapaths.
2. Build a common graph by combining all the separate control flow graphs representing different apps.
3. Perform an uniform metapath walk on the common graph to obtain a huge corpus.
4. Perform node2vec to learn node representations of the huge corpus.
5. Build feature vectors for each app, using the node embeddings learned from step 3, by combining embeddings of unique nodes of each app.
6. Classification using built feature vectors.

Explanations: 
We run a depth first search to explore all node types and metapaths since we do not know how convoluted an app's CFG may be and we need flexible metapaths for each app. Also, there is the possibility of the malware being intentionally obfuscated. Therefore, we need flexible metapaths for each app, which we will later use as the predefined metapaths in our metapath2vec step. The reason our feature vector is a component wise combination of node embeddings is because when two vectors are added together, a new vector is obtained. As visualized below:  

![Vector Addition And Subtraction](/img/vec.png)

This will provide more information about the APKs that we will classify. The model can more easily learn the distinction between similar and different vectors, by the direction and magnitude to where they point. Of course, the component wise combination can also be other aggregations, such as taking an average, percentiles, and dot products. When encountering an unseen app, unique nodes of that app is extracted. Representations of each of the unique nodes are then found from the trained word2vec model, and some component wise combination is performed to obtain a feature vector for classification. 

### Doc2Vec Model
1. For each app, extract all possible metapaths using Depth First Search, as well as perform metapath2vec on that app to obtain a corpus.
2. Take each corpus from each app, append them, and turn them into a list of Tagged Documents.
3. Run the Tagged Documents into Doc2Vec to obtain a vector representation for each app.
4. Take the vector representations for each tagged document, and use them as feature vectors for classification.

The Doc2Vec model is very straight forward, taking in documents and returning representations for those documents. When there is an unseen app, a corpus is extracted from that app using metapath and is treated as a document. This document is then fed into the Doc2Vec model, and a vector representation is ""inferred"" using the .infer_vector() method.  


## Results and Analysis
### Baseline Results
The following tables are results from our baseline model, MamaDroid, corresponding to Family and Package mode. Surprisingly, our MamaDroid using control flow graphs performs better than its original model. Let's first take a look at the Family mode. The table below is the confusion matrix, we calculated precision and recall scores based on it.

| | Random Forest | 1-NN | 3-NN |
| --- | --- | --- | --- |
| True Negative | 68 | 61 | 59 |
| False Negative | 4 | 9 | 8 |
| False Positive  | 11 | 18 | 20 |
| True Positive | 1269 | 1264 | 1265 |

We compare the results to the original MamaDroid model. In the table below, we list the original MamaDroid results on it as well to better compare it. We see that our version of MamaDroid has better performance in all F1-Score, precision and recall scores, where we obtain an F measure of 0.994 and the original model only has 0.880. Similarly to precision and recall scores, we obtain 0.991 and 0.997, where the original model has 0.840 and 0.920 as their results. In addition to Random Forest, our 1-NN and 3-NN models also outperform the original MamaDroid model. But all our three models have similar results.

<table>
    <tbody>
        <tr>
            <td rowspan=2>PCA = 10 Component</td>
            <td colspan=2>Random Forest</td>
            <td rowspan=2>1-NN</td>
            <td rowspan=2>3-NN</td>
        </tr>
        <tr>
            <td>Original</td>
            <td>Ours</td>
        </tr>
        <tr>
            <td>F1-Score</td>
            <td>0.880</td>
            <td>0.994</td>
            <td>0.980</td>
            <td>0.994</td>
        </tr>
        <tr>
            <td>Precision</td>
            <td>0.840</td>
            <td>0.991</td>
            <td>0.985</td>
            <td>0.994</td>
        </tr>
        <tr>
            <td>Recall</td>
            <td>0.920</td>
            <td>0.997</td>
            <td>0.994</td>
            <td>0.994</td>
        </tr>
    </tbody>
</table>

Next, we have results for our MamaDroid Package mode. The following table is the confusion matrix. The numbers are close to what we obtain for Family level. However, the true negatives for all three similarity-based models are slightly larger.

| | Random Forest | 1-NN | 3-NN |
| --- | --- | --- | --- |
| True Negative | 70 | 70 | 70 |
| False Negative | 1 | 6 | 1 |
| False Positive  | 15 | 15 | 15 |
| True Positive | 1266 | 1261 | 1266 |

We also compared the results of Package mode to the original MamaDroid results. In the table shown below, we also list out the results of original MamaDroid on the left to compare it with the ones we obtain. As a result, our model was able to achieve an F measure of 0.993, precision score of 0.988, and recall score of 0.999, whereas the original model only has performance of 0.940, 0.940, and 0.950. Both 1-NN and 3-NN models also have very similar numbers as Random Forest model.

<table>
    <tbody>
        <tr>
            <td rowspan=2>PCA = 10 Component</td>
            <td colspan=2>Random Forest</td>
            <td rowspan=2>1-NN</td>
            <td rowspan=2>3-NN</td>
        </tr>
        <tr>
            <td>Original</td>
            <td>Ours</td>
        </tr>
        <tr>
            <td>F1-Score</td>
            <td>0.940</td>
            <td>0.993</td>
            <td>0.992</td>
            <td>0.994</td>
        </tr>
        <tr>
            <td>Precision</td>
            <td>0.940</td>
            <td>0.988</td>
            <td>0.988</td>
            <td>0.988</td>
        </tr>
        <tr>
            <td>Recall</td>
            <td>0.950</td>
            <td>0.999</td>
            <td>0.995</td>
            <td>0.999</td>
        </tr>
    </tbody>
</table>
  
### Metapath2Vec/Common Graph (Partial)
For our Metapath2Vec model, we unfortunately do not have the complete results due to large computational time it takes to build the common graph with all the data we have. The complete common graph consists of 1,950,729 nodes, and 215,604,110 edges. However, we did obtain results working with a smaller subset of the Common Graph, consisting of: 87,539 nodes and 15,617,223 edges.

| | Random Forest | 1-NN | 3-NN |
| --- | --- | --- | --- |
| True Negative | 94 | 89 | 73 |
| False Negative | 16 | 31 | 36 |
| False Positive  | 20 | 25 | 41 |
| True Positive | 1679 | 1664 | 1659 |

We tested on the entire test set, and surprisingly the performance was okay. Initially, we thought that there might be an error, however, upon inspecting our code, we were traversing the smaller common graph correctly. We believe we can obtain this result because of the large amount of edges in the common graph as well as our walk length of 500. We set the walk length to 500 to compensate for the smaller subset of graphs that we are using, and therefore can capture more information per walk. Because of this, the Word2Vec model can learn more about those nodes and provide a better representation. 

| | Random Forest | 1-NN | 3-NN |
| --- | --- | --- | --- |
| TF1-Score | **0.989** | 0.983 | 0.977 |
| Precision | 0.992 | 0.966 | 0.982 |
| Recall | 0.642 | 0.959 | 0.959 |

Even though the F1-Scores were high, our True Negative and False Positives are higher than our baseline and Doc2vec model. This is again due to the smaller subset that we are using for this experiment. Because of the smaller subset, we do not have a lot of representations for nodes. There could have been some nodes that the word2vec model has never seen before, and therefore cannot infer a good representation for it. 

### Doc2Vec
The following tables are the results for Doc2Vec with our similarity-based models: Random Forest, 1-NN, and 3-NN. Table below is the confusion matrix, which is used to compute the precision and recall scores. The Doc2Vec performed worse than the baseline model on the Random Forest model, and it seems like it was struggling with classifying benign apps.

| | Random Forest | 1-NN | 3-NN |
| --- | --- | --- | --- |
| True Negative | 109 | 56 | 43 |
| False Negative | 606 | 68 | 71 |
| False Positive  | 5 | 58 | 31 |
| True Positive | 1089 | 1627 | 1664 |

In the table below, we have our F measure, precision, and recall scores. We notice that our Random Forest model only has an F measure of 0.781, which is a lot lower than our baseline. On the other hand, both our k Nearest Neighbors perform much better than Random Forest. The Random Forest classifier has an emphasis on certain features when training, and focuses on some feature more than others. However, the 1-NN and 3-NN models both look at an unseen vector's closest neighbors, therefore utilizing all the features in the vector. We believe this is why the 1-NN and 3-NN models performed better in this experiment.

| | Random Forest | 1-NN | 3-NN |
| --- | --- | --- | --- |
| TF1-Score | 0.781 | 0.963 | **0.970** |
| Precision | 0.992 | 0.966 | 0.982 |
| Recall | 0.642 | 0.959 | 0.959 |


## Conclusion
In conclusion, our baseline model is able to achieve a better performance than the original work that we have studied. Although our Doc2Vec did not perform better than the baseline Random Forest model, our k Nearest Neighbors models performed almost as good as our baseline. From this, we can see that Control Flow Graphs might be a good choice when it comes to choosing representations for source code. Again, control flow graphs show the jumps in code. From our EDA: Figure 12, even though malware has a small number of nodes, they have a large amount of edges. This means that there could be lots of instances where the program is jumping around in the source code. All this is recorded in the CFG representation and could provide much more information about an APK.

 Although we successfully created a complete common graph, we were unable to obtain all the node embeddings from it due to time and memory constraints. Therefore we built a smaller common graph to see how it performs. If time and resources allowed, we hope to finish the metapath traversal of the complete common graph. Judging from the results using the smaller common graph, if we were to scale up the model might out-perform our baseline.

For our future work, we plan on investigating other vector embeddings technique and perhaps instead of using only similarity-based models, we could also implement graph neural networks (GNN). In addition to neural networks, we are also interested in graph classification specifically. Since our data format is already in the form of multiple apps, it can be easy to normalize and transform data for a GNN model. Of so many researches we have seen on malware detection, not a lot of them uses control flow graphs as their input data. Since our experiments confirmed that using control flow graphs is not any worse than using other forms of data, we are curious to know if control flow graphs can outperform in other models.

## Acknowledgement
We would like to express our gratitude to our mentors for our Capstone project: Professor Aaron Fraenkel, who provided us with lots of resources and ideas throughout the entire process of our research, and Shivam Lakhotia, who guided us through the project and assisted us every week. 

## References
[1] MamaDroid, https://arxiv.org/pdf/1612.04433.pdf

[2] Hindroid, https://www.cse.ust.hk/~yqsong/papers/2017-KDD-HINDROID.pdf

[3] Metapath2Vec, https://ericdongyx.github.io/papers/KDD17-dong-chawla-swami-metapath2vec.pdf

[4] Word2Vec, https://radimrehurek.com/gensim/models/word2vec.html

[5] Doc2Vec, https://radimrehurek.com/gensim/models/doc2vec.html

[6] Androguard, https://androguard.blogspot.com/2011/02/android-apps-visualization.html

[7] StellarGraph, https://github.com/stellargraph/stellargraph

[8] SDK, https://developer.android.com/studio/releases/platforms

[9] x2vec, https://iopscience.iop.org/article/10.1088/2632-072X/aba83d/pdf

[10] Learning Embeddings of Directed Networks with Text-Associated Nodes—with Application in Software Package Dependency Networks, https://arxiv.org/pdf/1809.02270.pdf

"
43,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_59,,,"![Docker Cloud Build Status](https://img.shields.io/docker/cloud/build/rcgonzal/m2v-adversarial-hindroid)

# m2vDroid: Perturbation-resilient metapath-based Android Malware Detection
An extension of the [HinDroid malware detection system](https://www.cse.ust.hk/~yqsong/papers/2017-KDD-HINDROID.pdf), but using [metapath2vec](https://ericdongyx.github.io/metapath2vec/m2v.html) to encode apps in the Heterogeneous Information Network. We then hope to make the model resilient to adversarial ML like Android HIV. See our [blog post](https://rcgonzalez9061.github.io/m2v-adversarial-hindroid/) for more details on our methods.

## Setup and Usage
To recreate our results, please use our Docker image: `rcgonzal/m2v-adversarial-hindroid` and have access to a directory of Android apps decompiled into smali. (This can be done with the [APKtool](https://ibotpeaches.github.io/Apktool/) and [Smali](https://github.com/JesusFreke/smali) -- included with our Docker image)

Our project can be run using `python run.py [data] [analysis] [model]` with each tag corresponding to different workflows and being executed in the order shown.

The `data` flag will trigger our ETL workflow. It parses apps, constructs the HIN, and then generates `features.csv` using metapath2vec. It will read additional parameters from `config/etl-params/etl-params.json`. Note that including `data_source` will search for apps in that directory and subdirectories, creating `app_list.csv`. Otherwise, an `app_list.csv` can be specified by simply placing it within `outfolder`:

```json
{
    ""outfolder"": ""Path where graph data will be saved"",
    ""parse_params"": {
        ""data_source"": ""Path to folder of decompliled apps, optional."",
        ""nprocs"": ""Number of threads to use when parsing"",
        ""recompute"": ""Boolean, whether or not to reparse apps. Default, skips apps that exist in app data heap""   
    },
    ""feature_params"": {
        ""walk_args"": ""Arguments for stellargraph.data.UniformRandomMetaPathWalk"",
        ""w2v_args"": ""Arguments for gensim.models.Word2Vec, excl. walks""
    }
}
```

The `analysis` flag will generate analysis on our data and any necessary plots, reading in additional parameters from `config/analysis-params/analysis-params.json`. `jobs` is a dictionary of jobs to be performed along with their parmeters. Currently the only available job is `plots`:

```json
{
    ""data_path"": ""path to folder of data to load (akin to the outfolder in etl-params)"",
    ""jobs"": {
        ""plots"": {
            ""update_figs"": ""Boolean, whether or not to update figures in report and blog post"",
            ""no_labels"": ""Boolean, whether or not to include class labels on plots""
        }
    }
}
```

","![Docker Cloud Build Status](https://img.shields.io/docker/cloud/build/rcgonzal/m2v-adversarial-hindroid)

# m2vDroid: Perturbation-resilient metapath-based Android Malware Detection
An extension of the [HinDroid malware detection system](https://www.cse.ust.hk/~yqsong/papers/2017-KDD-HINDROID.pdf), but using [metapath2vec](https://ericdongyx.github.io/metapath2vec/m2v.html) to encode apps in the Heterogeneous Information Network. We then hope to make the model resilient to adversarial ML like Android HIV. See our [blog post](https://rcgonzalez9061.github.io/m2v-adversarial-hindroid/) for more details on our methods.

## Setup and Usage
To recreate our results, please use our Docker image: `rcgonzal/m2v-adversarial-hindroid` and have access to a directory of Android apps decompiled into smali. (This can be done with the [APKtool](https://ibotpeaches.github.io/Apktool/) and [Smali](https://github.com/JesusFreke/smali) -- included with our Docker image)

Our project can be run using `python run.py [data] [analysis] [model]` with each tag corresponding to different workflows and being executed in the order shown.

The `data` flag will trigger our ETL workflow. It parses apps, constructs the HIN, and then generates `features.csv` using metapath2vec. It will read additional parameters from `config/etl-params/etl-params.json`. Note that including `data_source` will search for apps in that directory and subdirectories, creating `app_list.csv`. Otherwise, an `app_list.csv` can be specified by simply placing it within `outfolder`:

```json
{
    ""outfolder"": ""Path where graph data will be saved"",
    ""parse_params"": {
        ""data_source"": ""Path to folder of decompliled apps, optional."",
        ""nprocs"": ""Number of threads to use when parsing"",
        ""recompute"": ""Boolean, whether or not to reparse apps. Default, skips apps that exist in app data heap""   
    },
    ""feature_params"": {
        ""walk_args"": ""Arguments for stellargraph.data.UniformRandomMetaPathWalk"",
        ""w2v_args"": ""Arguments for gensim.models.Word2Vec, excl. walks""
    }
}
```

The `analysis` flag will generate analysis on our data and any necessary plots, reading in additional parameters from `config/analysis-params/analysis-params.json`. `jobs` is a dictionary of jobs to be performed along with their parmeters. Currently the only available job is `plots`:

```json
{
    ""data_path"": ""path to folder of data to load (akin to the outfolder in etl-params)"",
    ""jobs"": {
        ""plots"": {
            ""update_figs"": ""Boolean, whether or not to update figures in report and blog post"",
            ""no_labels"": ""Boolean, whether or not to include class labels on plots""
        }
    }
}
```

"
44,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_38,,,"### Background

The purpose of phrase mining is to extract high-quality phrases from a large amount of text corpus. It identifies the phrases instead of an unigram word, which provides a much more understanding of the text.  In this study, we apply AutoPhrase method into two different datasets and compare the decreasing quality ranked list of phrase ranked list in multi-words and single word. Our datasets are from the abstract of Scientific papers in English with the English knowledge base from Wikipedia. Through this project, we will be able to understand the advantages of the AutoPhrase method and how to implement Autophrase in two datasets by identifying different outcomes it produces. 


### Requirements
##### If you run in the local:
Linux or MacOS with g++, Java and gensim installed.


##### You can also use our docker images to run the code. No need any install. It stores in submission.json file.


### Purpose of the Code

For Final Replication, our code would do the data ingestion proportion first, to pull data as the input corpus for future use from the cloud. Then to perform some basic EDA on it. We would run the autophrase algorithm along with phrasal segmentation, analyzing the results. At the end, we manually label the high-quality phrases and select 3 phrases and put those into the phrase embedding model to return five most similar phrases as the result.

### Code Content
Some Python Scripts, involved in etl.py, eda.py, auto.py,visual.py, and example.py to download, process data, analyze, visualize data and find the most similar phrase by building the model.

	
### How to Run the Code

##### To get the data:     -      run python run.py etl


This downloads the data from Illinois University in the directory specified in config/etl-params.json and do data cleaning process. You can find the result in the data/raw folder.




##### To do the EDA for the data     -       run python run.py eda


This performs exploratory data analysis and saves the figures in the location specified in config/eda-params.json. You can find the graphs in the data/eda folder.



##### To run autophrase algorithm and get the segementation result       -        run python run.py auto


This performs autophrase algorithm and phrasal segmentation saves the results in the location specified in config/auto-params.json. You can find the result in the data/output folder.



##### To analyze the output of autophrase           -         run python run.py visual


This performs analysis on the results and saves the figures in the location specified in config/visual-params.json. You can find the two distributions in the data/output folder.


##### To find the most 5 similar phrases           -         run python run.py example


This ask the users to manually label the high-quality phrase. It builds the word2vec model on the phrasal segmentation results to obtain phrase embedding based on random sampleing. It also report the top-5 similar phrases based on the 3 high-quality phrases from your previous annotations.However, if the users want to try their own sampling, they can manually label the high-quality phrases in sample.txt, which stored the output file by changing the configuration. You can find the result in the data/output/example folder.


##### To run whole project       -          run python run.py all

It will complete the whole process with results. The defualt is the dataset DBLP.txt, if you want to try other dataset, edit the configuation file to your own dataset. All of the input and result can find the in the data folder.


##### To make a test run.          -        run python run.py test

It will implement dataset DBLP.5k.txt, which is a test data to check the whole process is working. DBLP.5k.txt is sampled from the original dataset DBLP.txt. This compares the result between tf-idf scores, autophrase quality scores, and their multiplication.


### Notebook Contents
In the notebook, it will help the users visulized all the results from the run.py with some brief explanations.



### Work Cited

Professor Jingbo Shang’s Github: https://github.com/shangjingbo1226/AutoPhrase


Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, Jiawei Han, ""Automated Phrase Mining from Massive Text Corpora"", accepted by IEEE Transactions on Knowledge and Data Engineering, Feb. 2018.




### Responsibilities
We discussed the general idea of the replication project and outlined the steps of the process together.


Tiange Wan: some of code portion and report portion, and revised the report portion.


Yicen Ma: some of code portion and report portion, and revised the code portion.


Anant Gandhi: participated in another repo





","### Background

The purpose of phrase mining is to extract high-quality phrases from a large amount of text corpus. It identifies the phrases instead of an unigram word, which provides a much more understanding of the text.  In this study, we apply AutoPhrase method into two different datasets and compare the decreasing quality ranked list of phrase ranked list in multi-words and single word. Our datasets are from the abstract of Scientific papers in English with the English knowledge base from Wikipedia. Through this project, we will be able to understand the advantages of the AutoPhrase method and how to implement Autophrase in two datasets by identifying different outcomes it produces. 


### Requirements
##### If you run in the local:
Linux or MacOS with g++, Java and gensim installed.


##### You can also use our docker images to run the code. No need any install. It stores in submission.json file.


### Purpose of the Code

For Final Replication, our code would do the data ingestion proportion first, to pull data as the input corpus for future use from the cloud. Then to perform some basic EDA on it. We would run the autophrase algorithm along with phrasal segmentation, analyzing the results. At the end, we manually label the high-quality phrases and select 3 phrases and put those into the phrase embedding model to return five most similar phrases as the result.

### Code Content
Some Python Scripts, involved in etl.py, eda.py, auto.py,visual.py, and example.py to download, process data, analyze, visualize data and find the most similar phrase by building the model.

	
### How to Run the Code

##### To get the data:     -      run python run.py etl


This downloads the data from Illinois University in the directory specified in config/etl-params.json and do data cleaning process. You can find the result in the data/raw folder.




##### To do the EDA for the data     -       run python run.py eda


This performs exploratory data analysis and saves the figures in the location specified in config/eda-params.json. You can find the graphs in the data/eda folder.



##### To run autophrase algorithm and get the segementation result       -        run python run.py auto


This performs autophrase algorithm and phrasal segmentation saves the results in the location specified in config/auto-params.json. You can find the result in the data/output folder.



##### To analyze the output of autophrase           -         run python run.py visual


This performs analysis on the results and saves the figures in the location specified in config/visual-params.json. You can find the two distributions in the data/output folder.


##### To find the most 5 similar phrases           -         run python run.py example


This ask the users to manually label the high-quality phrase. It builds the word2vec model on the phrasal segmentation results to obtain phrase embedding based on random sampleing. It also report the top-5 similar phrases based on the 3 high-quality phrases from your previous annotations.However, if the users want to try their own sampling, they can manually label the high-quality phrases in sample.txt, which stored the output file by changing the configuration. You can find the result in the data/output/example folder.


##### To run whole project       -          run python run.py all

It will complete the whole process with results. The defualt is the dataset DBLP.txt, if you want to try other dataset, edit the configuation file to your own dataset. All of the input and result can find the in the data folder.


##### To make a test run.          -        run python run.py test

It will implement dataset DBLP.5k.txt, which is a test data to check the whole process is working. DBLP.5k.txt is sampled from the original dataset DBLP.txt. This compares the result between tf-idf scores, autophrase quality scores, and their multiplication.


### Notebook Contents
In the notebook, it will help the users visulized all the results from the run.py with some brief explanations.



### Work Cited

Professor Jingbo Shang’s Github: https://github.com/shangjingbo1226/AutoPhrase


Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, Jiawei Han, ""Automated Phrase Mining from Massive Text Corpora"", accepted by IEEE Transactions on Knowledge and Data Engineering, Feb. 2018.




### Responsibilities
We discussed the general idea of the replication project and outlined the steps of the process together.


Tiange Wan: some of code portion and report portion, and revised the report portion.


Yicen Ma: some of code portion and report portion, and revised the code portion.


Anant Gandhi: participated in another repo





"
45,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_42,,,"# Analyzing Movies Using Phrase Mining

https://a04-capstone-group-02.github.io/movie-analysis-webpage/

## Setup

### Clone the repository

```
git clone --recursive https://github.com/A04-Capstone-Group-02/movie-analysis.git
```

### Download dataset

Download the [CMU Movie Summary Corpus dataset](http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz) and move its files to `data/raw/`, or run the `download` target.

Note that to run this repository on the UCSD DSMLP server, the dataset must be manually uploaded, since the DSMLP server cannot connect to the data source link.

### Docker

Build a docker container with the `Dockerfile` or the remote image `991231/movie-analysis` in the docker hub.

### Note

To run the `clustering` target, we highly recommend enabling GPU to ensure reasonable running time, since this target heavily interacts with a transformer model. Running other targets without GPU will not be an issue.

## Run

Execute the running script with the following command:

```
python run.py [all] [test] [download] [data] [eda] [classification] [clustering]
```

### `all` target

Run `data`, `eda`, `classification` and `clustering` targets in this exact order.

### `test` target

Runs the same 4 targets in the same order as the `all` target, but using the test data in `test/data/raw` and the test configurations.

### `download` target

Download the CMU Movie Summary Corpus dataset and set up the `data` directory.

### `data` target

Run the ETL pipeline to process the raw data. This target will run AutoPhrase to extract quality phrases, clean categories, combine the processed data into a dataframe, and generate a profile report of the dataset.

The configuration file for this target is `etl.json` (or `etl_test.json` for `test` target), which contains the following items:

- `data_in`: the path to the input data (relative to the root)
- `false_positive_phrases`: phrases to remove from the quality phrase list
- `false_positive_substrings`: substrings to remove from the quality phrase list

The configuration file for the AutoPhrase submodule is `autophrase.json`, which contains the following items:

- `MIN_SUP`: the minimum count of a phrase to include in the training process
- `MODEL`: the path to the output model (relative to the root)
- `RAW_TRAIN`: the path to the raw corpus for training (relative to the root)
- `TEXT_TO_SEG`: the path to the raw corpus for segmentation (relative to the root)
- `THREAD`: the number of threads to use

### `eda` target

Run the EDA pipeline. This target will find the temporal change of quality phrase distributions and generate visualizations to show the findings.

The configuration file for this target is `eda.json` (or `eda_test.json` for `test` target), which contains the following items:

- `data_in`: the path to the input data (relative to the root)
- `data_out`: the path to the output directory (relative to the root)
- `example_movie`: example movie to profile
- `year_start`: the earliest year to analyze
- `year_end`: the latest year to analyze
- `decade_start`: the earliest decade to analyze
- `decade_end`: the latest decade to analyze
- `phrase_count_threshold`: the minimum count of a quality phrase to be included in the analysis
- `stop_words`: the stop words to ignore in the analysis
- `compact`: whether to output a full or compact visualization
- `n_bars`: number of bars to display in the bar plots
- `movie_name_overflow`: number of characters in visualization until ellipses
- `dpi`: subplot dpi (dot per inches)
- `fps`: fps (frame per second) of the bar chart race animation
- `seconds_per_period`: the time each subplot will take in the bar chart race animation

### `classification` target

Run the classification pipeline. This target will transform the data into a TF-IDF matrix, fit a one-vs-rest logistic regression as the classifier and tune the parameters if specified.

The configuration file for this target is `classification.json`, which contains the following items:

- `data`: the path to the input data (relative to the root)
- `baseline`: a boolean indicator to specify running baseline (true-like) or parameter tuning (false-like)
- `top_genre`: a number to specify the number of genres in the final output plot, default is 10 
- `top_phrase`: a number of specify the number of words/phrases in the final output plot, default is 10

### `clustering` target

Run the clustering pipeline. This target will pick representative sentences based on average sublinear TF-IDF score on the quality phrases, calculate document embeddings by average the Sentence-BERT embeddings of the representative sentences, and visualize the clusters.

The configuration file for this target is `clustering.json`, which contains the following items:

- `clu_num_workers`: the number of workers to use
- `clu_rep_sentences_path`: the path to the checkpoint representative sentences file (relative to the root), or an empty string `""""` to disable the checkpoint
- `clu_doc_embeddings_path`: the path to the checkpoint document embeddings file (relative to the root), or an empty string `""""` to disable the checkpoint
- `clu_dim_reduction`: the dimensionality reduction method to apply on the document embeddings for visualization, choose one from `{""PCA"", ""TSNE""}`
- `clu_sbert_base`: the sentence transformer model to use, can be either a pretrained model or a path to the saved model
- `clu_sbert_finetune`: enable finetuning or not
- `clu_sbert_finetune_config`: configurations for finetuning, will only be used if finetuning is enabled
  - `train_size`: total number of training pairs to sample
  - `sample_per_pair`: number of training pairs to sample per sampled document pair
  - `train_batch_size`: batch size for training
  - `epochs`: number of epochs to train
- `clu_num_clusters`: number of clusters to generate
- `clu_num_rep_features`: number of top representative features to store
- `clu_rep_features_min_support`: the minimum support of a feature to be analyzed with summarizing the clusters

## Contributors

- Daniel Lee
- Huilai Miao
- Yuxuan Fan
","# Analyzing Movies Using Phrase Mining

https://a04-capstone-group-02.github.io/movie-analysis-webpage/

## Setup

### Clone the repository

```
git clone --recursive https://github.com/A04-Capstone-Group-02/movie-analysis.git
```

### Download dataset

Download the [CMU Movie Summary Corpus dataset](http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz) and move its files to `data/raw/`, or run the `download` target.

Note that to run this repository on the UCSD DSMLP server, the dataset must be manually uploaded, since the DSMLP server cannot connect to the data source link.

### Docker

Build a docker container with the `Dockerfile` or the remote image `991231/movie-analysis` in the docker hub.

### Note

To run the `clustering` target, we highly recommend enabling GPU to ensure reasonable running time, since this target heavily interacts with a transformer model. Running other targets without GPU will not be an issue.

## Run

Execute the running script with the following command:

```
python run.py [all] [test] [download] [data] [eda] [classification] [clustering]
```

### `all` target

Run `data`, `eda`, `classification` and `clustering` targets in this exact order.

### `test` target

Runs the same 4 targets in the same order as the `all` target, but using the test data in `test/data/raw` and the test configurations.

### `download` target

Download the CMU Movie Summary Corpus dataset and set up the `data` directory.

### `data` target

Run the ETL pipeline to process the raw data. This target will run AutoPhrase to extract quality phrases, clean categories, combine the processed data into a dataframe, and generate a profile report of the dataset.

The configuration file for this target is `etl.json` (or `etl_test.json` for `test` target), which contains the following items:

- `data_in`: the path to the input data (relative to the root)
- `false_positive_phrases`: phrases to remove from the quality phrase list
- `false_positive_substrings`: substrings to remove from the quality phrase list

The configuration file for the AutoPhrase submodule is `autophrase.json`, which contains the following items:

- `MIN_SUP`: the minimum count of a phrase to include in the training process
- `MODEL`: the path to the output model (relative to the root)
- `RAW_TRAIN`: the path to the raw corpus for training (relative to the root)
- `TEXT_TO_SEG`: the path to the raw corpus for segmentation (relative to the root)
- `THREAD`: the number of threads to use

### `eda` target

Run the EDA pipeline. This target will find the temporal change of quality phrase distributions and generate visualizations to show the findings.

The configuration file for this target is `eda.json` (or `eda_test.json` for `test` target), which contains the following items:

- `data_in`: the path to the input data (relative to the root)
- `data_out`: the path to the output directory (relative to the root)
- `example_movie`: example movie to profile
- `year_start`: the earliest year to analyze
- `year_end`: the latest year to analyze
- `decade_start`: the earliest decade to analyze
- `decade_end`: the latest decade to analyze
- `phrase_count_threshold`: the minimum count of a quality phrase to be included in the analysis
- `stop_words`: the stop words to ignore in the analysis
- `compact`: whether to output a full or compact visualization
- `n_bars`: number of bars to display in the bar plots
- `movie_name_overflow`: number of characters in visualization until ellipses
- `dpi`: subplot dpi (dot per inches)
- `fps`: fps (frame per second) of the bar chart race animation
- `seconds_per_period`: the time each subplot will take in the bar chart race animation

### `classification` target

Run the classification pipeline. This target will transform the data into a TF-IDF matrix, fit a one-vs-rest logistic regression as the classifier and tune the parameters if specified.

The configuration file for this target is `classification.json`, which contains the following items:

- `data`: the path to the input data (relative to the root)
- `baseline`: a boolean indicator to specify running baseline (true-like) or parameter tuning (false-like)
- `top_genre`: a number to specify the number of genres in the final output plot, default is 10 
- `top_phrase`: a number of specify the number of words/phrases in the final output plot, default is 10

### `clustering` target

Run the clustering pipeline. This target will pick representative sentences based on average sublinear TF-IDF score on the quality phrases, calculate document embeddings by average the Sentence-BERT embeddings of the representative sentences, and visualize the clusters.

The configuration file for this target is `clustering.json`, which contains the following items:

- `clu_num_workers`: the number of workers to use
- `clu_rep_sentences_path`: the path to the checkpoint representative sentences file (relative to the root), or an empty string `""""` to disable the checkpoint
- `clu_doc_embeddings_path`: the path to the checkpoint document embeddings file (relative to the root), or an empty string `""""` to disable the checkpoint
- `clu_dim_reduction`: the dimensionality reduction method to apply on the document embeddings for visualization, choose one from `{""PCA"", ""TSNE""}`
- `clu_sbert_base`: the sentence transformer model to use, can be either a pretrained model or a path to the saved model
- `clu_sbert_finetune`: enable finetuning or not
- `clu_sbert_finetune_config`: configurations for finetuning, will only be used if finetuning is enabled
  - `train_size`: total number of training pairs to sample
  - `sample_per_pair`: number of training pairs to sample per sampled document pair
  - `train_batch_size`: batch size for training
  - `epochs`: number of epochs to train
- `clu_num_clusters`: number of clusters to generate
- `clu_num_rep_features`: number of top representative features to store
- `clu_rep_features_min_support`: the minimum support of a feature to be analyzed with summarizing the clusters

## Contributors

- Daniel Lee
- Huilai Miao
- Yuxuan Fan
"
46,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_41,,,"# AutoPhrase for Financial Documents Interpretation 

Our main targets are data preparation, feature encoding, eda (optional), train, report (optional), and test. Users can configure parameters for these targets in the ./config files.


## Data Prep

The data preparation target scrapes, cleans, and consolidates companies' 8-K documents. Furthermore, it curates features such as EPS as well as price movements for the given companies.
<br />
* `data_dir` is the file path to download files: 8-K's, EPS, etc.
* `raw_dir` is the directory to the raw data
* `raw_8k_fp` is the file path with newly downloaded 8-K's (should be the same as to_dir)
* `raw_eps_fp` is the file path with newly downloaded EPS information (should be the same as to_dir)
* `processed_dir` is the directory to the processed data
* `testing` is the status of whether we are doing testing (by default is false)


## Feature Encoding

The feature encoding target creates encoded text vectors for each 8-K: both unigrams and quality phrases outputed by the AutoPhrase method.
<br />
* `data_file` is the file path with all data files from data prep target: processed, raw, models, etc.
* `phrase_file` is the file path to the quality phrases outputted by AutoPhrase
* `n_unigrams` sets the top n unigrams to be encoded based on PMI (may not be exacly `n_unigrams` total due to overlap of top unigrams within each class)
* `threshhold` takes quality phrases with a quality score above it to be encoded


## Train

Trains Random Forest models using 3 feature sets on encoded data: baseline, baseline + unigrams, and baseline + phrases. The selected classifier and set model parameters were decided through comparing validation accuracy.  

* `data_dir` is the file path with all data files from data prep targed: processed, raw, models, etc.
* `input_file` is the file path (from `data_dir`) to outputed files by the feature encoding target
* `output_file` is the desired file path to download trained, outputed models
* `testing` is the status of whether we are doing testing (by default is false)


## EDA and Report (optional)

Exports Jupyter notebooks to HTML with EDA and result analysis from the models.
<br />
* `report_name` is the desired name of report
* `data_dir` is the file path with all data files from data prep target: processed, raw, models, etc.
* `notebook_dir` is the file path containing the repo's notebooks
* `notebook_file` is the desired file path (from `notebook_dir`) of outputed notebook
* `report_dir` is the desired directory our outputed HTML report
* `report_file` is the desired file name of outputed report in `report_dir`


## Test

Test target will run the whole project with only test data


## Correct order of excution

* data_prep: `python run.py data_prep`
* feature_encoding: `python run.py feature_encoding`
* (optional) eda: `python run.py eda`
* train: `python run.py train`
* (optional) report: `python run.py report`


## Project Links

* [Project Website](https://shy218.github.io/dsc180-project/)
* [AutoPhrase](https://github.com/shangjingbo1226/AutoPhrase)
","# AutoPhrase for Financial Documents Interpretation 

Our main targets are data preparation, feature encoding, eda (optional), train, report (optional), and test. Users can configure parameters for these targets in the ./config files.


## Data Prep

The data preparation target scrapes, cleans, and consolidates companies' 8-K documents. Furthermore, it curates features such as EPS as well as price movements for the given companies.
<br />
* `data_dir` is the file path to download files: 8-K's, EPS, etc.
* `raw_dir` is the directory to the raw data
* `raw_8k_fp` is the file path with newly downloaded 8-K's (should be the same as to_dir)
* `raw_eps_fp` is the file path with newly downloaded EPS information (should be the same as to_dir)
* `processed_dir` is the directory to the processed data
* `testing` is the status of whether we are doing testing (by default is false)


## Feature Encoding

The feature encoding target creates encoded text vectors for each 8-K: both unigrams and quality phrases outputed by the AutoPhrase method.
<br />
* `data_file` is the file path with all data files from data prep target: processed, raw, models, etc.
* `phrase_file` is the file path to the quality phrases outputted by AutoPhrase
* `n_unigrams` sets the top n unigrams to be encoded based on PMI (may not be exacly `n_unigrams` total due to overlap of top unigrams within each class)
* `threshhold` takes quality phrases with a quality score above it to be encoded


## Train

Trains Random Forest models using 3 feature sets on encoded data: baseline, baseline + unigrams, and baseline + phrases. The selected classifier and set model parameters were decided through comparing validation accuracy.  

* `data_dir` is the file path with all data files from data prep targed: processed, raw, models, etc.
* `input_file` is the file path (from `data_dir`) to outputed files by the feature encoding target
* `output_file` is the desired file path to download trained, outputed models
* `testing` is the status of whether we are doing testing (by default is false)


## EDA and Report (optional)

Exports Jupyter notebooks to HTML with EDA and result analysis from the models.
<br />
* `report_name` is the desired name of report
* `data_dir` is the file path with all data files from data prep target: processed, raw, models, etc.
* `notebook_dir` is the file path containing the repo's notebooks
* `notebook_file` is the desired file path (from `notebook_dir`) of outputed notebook
* `report_dir` is the desired directory our outputed HTML report
* `report_file` is the desired file name of outputed report in `report_dir`


## Test

Test target will run the whole project with only test data


## Correct order of excution

* data_prep: `python run.py data_prep`
* feature_encoding: `python run.py feature_encoding`
* (optional) eda: `python run.py eda`
* train: `python run.py train`
* (optional) report: `python run.py report`


## Project Links

* [Project Website](https://shy218.github.io/dsc180-project/)
* [AutoPhrase](https://github.com/shangjingbo1226/AutoPhrase)
"
47,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_40,,,"# DSC180B-NER-Project
This project focuses on the task of document classification using a BBC News Dataset and a 20 News Group Dataset. We implemented various feature based classification models and compared the results. We have analysed the advantages and shortcomings of each method.

## Webpage
* https://dsc180b-a04-capstone-group-06.github.io/News-Classification-Webpage/

## Datasets Used
* BBC news: https://www.kaggle.com/pariza/bbc-news-summary </br>
  * Download this dataset
* 20 news group: http://qwone.com/~jason/20Newsgroups/ 
  * This dataset is fetched by using the sklearn package
## Environment Required
* Please use the docker image: ``` littlestone111/dsc180b-ner-project  ```

## Run
```
$ launch-180.sh -i littlestone111/dsc180b-ner-project -G [group]
$ python run.py [all] [preprocessing] [autophrase] [model] [test]
```

```test``` :        target will build the Tf-Idf models on the small subset of 20 new groups dataset and save the models to the model folder.</br>
```all```:          target will run everthing inlcuded in project, and return the final prediction on the test dataset for document classification.</br>
```preprocessing```: target will preprocess 20 news group data for AutoPhrase, so that they can be used for training the model.</br>
```autophrase```:   target will run Professor Shang's Autophrase model to extract quality phrases from the dataset.</br>
```model```:        target will build the SVM+ALL+TF-IDF combined vocab list model for 20 news group dataset. </br>

</br>
Output: <br>

* ```model.pkl```: the parameter of the final model.

## Group Members
* Rachel Ung
* Siyu Dong
* Yang Li

## Our Findings

The BERT classification on the five-class BBC News dataset does not outperform any of our implemented models. From our results table, we observed that our models have F1-Score and Accuracy performances at around 0.95, indicating they are high-performing classifiers. The best of them is the SVM+ALL(TF-IDF) classifier, or the Support Vector Machine with the All Vector Vocabulary List and Tf-Idf Representations, which uses the vocabulary from both NER results and AutoPhrase results. Because the quality phrases between different domains are likely to differ, we expect these results to be optimal features for our predictors. 

For the 20 News Group dataset, the SVM+ALL(TF-IDF) classifer also outperformed the other models, with the F1-Score and Accuracy being 0.84. Considering the classes are huge (i.e. 20 classes), these results verify our model is high-performing. Applying our best model on the five-class BBC News dataset, we attained a F1-Score at 0.9525, and Accuracy at 0.9528; while for the 20 News Group, we yielded a F1-Score at 0.8463 and Accuracy at 0.8478. 




","# DSC180B-NER-Project
This project focuses on the task of document classification using a BBC News Dataset and a 20 News Group Dataset. We implemented various feature based classification models and compared the results. We have analysed the advantages and shortcomings of each method.

## Webpage
* https://dsc180b-a04-capstone-group-06.github.io/News-Classification-Webpage/

## Datasets Used
* BBC news: https://www.kaggle.com/pariza/bbc-news-summary </br>
  * Download this dataset
* 20 news group: http://qwone.com/~jason/20Newsgroups/ 
  * This dataset is fetched by using the sklearn package
## Environment Required
* Please use the docker image: ``` littlestone111/dsc180b-ner-project  ```

## Run
```
$ launch-180.sh -i littlestone111/dsc180b-ner-project -G [group]
$ python run.py [all] [preprocessing] [autophrase] [model] [test]
```

```test``` :        target will build the Tf-Idf models on the small subset of 20 new groups dataset and save the models to the model folder.</br>
```all```:          target will run everthing inlcuded in project, and return the final prediction on the test dataset for document classification.</br>
```preprocessing```: target will preprocess 20 news group data for AutoPhrase, so that they can be used for training the model.</br>
```autophrase```:   target will run Professor Shang's Autophrase model to extract quality phrases from the dataset.</br>
```model```:        target will build the SVM+ALL+TF-IDF combined vocab list model for 20 news group dataset. </br>

</br>
Output: <br>

* ```model.pkl```: the parameter of the final model.

## Group Members
* Rachel Ung
* Siyu Dong
* Yang Li

## Our Findings

The BERT classification on the five-class BBC News dataset does not outperform any of our implemented models. From our results table, we observed that our models have F1-Score and Accuracy performances at around 0.95, indicating they are high-performing classifiers. The best of them is the SVM+ALL(TF-IDF) classifier, or the Support Vector Machine with the All Vector Vocabulary List and Tf-Idf Representations, which uses the vocabulary from both NER results and AutoPhrase results. Because the quality phrases between different domains are likely to differ, we expect these results to be optimal features for our predictors. 

For the 20 News Group dataset, the SVM+ALL(TF-IDF) classifer also outperformed the other models, with the F1-Score and Accuracy being 0.84. Considering the classes are huge (i.e. 20 classes), these results verify our model is high-performing. Applying our best model on the five-class BBC News dataset, we attained a F1-Score at 0.9525, and Accuracy at 0.9528; while for the 20 News Group, we yielded a F1-Score at 0.8463 and Accuracy at 0.8478. 




"
48,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_39,,,"# AutoLibrary - A Personal Digital Library to Find Related Works via Text Analyzer
- Website: https://yichunren.pythonanywhere.com/autolibrary/
- DSC180B - Capstone Project (Winter 2021)
- Section A04 Group03: Yichun Ren, Jiayi Fan, Bingqi Zhou
- Note: This is an application of [AutoPhrase](https://github.com/shangjingbo1226/AutoPhrase) by Jingbo Shang.

## Docker
- The docker repository is `jfan1998/dsc180a-docker`.
- Note: The docker uses dsmlp base container. Please login to a dsmlp jumpbox before entering the command below.
```
launch-scipy-ml.sh -i jfan1998/dsc180a-docker:latest
```
Use port-forwarding on dsmlp to open the website:
  - Instruction: https://docs.google.com/document/d/15ehCaVIKSXwgh2jvH3034l5uSPNLrZRgkczwl-xWNEU/edit?usp=sharing

## Website
- Our published website enables users to upload papers via URLs/Local Machine
- If you are curious about our code for published website, you can switch the branch to website.

## Local Run
- Please refer to `requirements.txt` to check if all the packages and libraries needed are installed.
### Default Run: open AutoLibrary website
```
python run.py
```
- The home page of website: `http://127.0.0.1:8000/autolibrary/`
### Target 1: Convert the input .pdf file into .txt
```
python run.py data
```
### Target 2: Run AutoPhrase on the input file
```
python run.py autophrase
```
### Target 3: Apply weight to the quality scores of phrases according the corresponding quality score in its domain
```
python run.py weight
```
### Target 4: Webscrape the search results on Semantic Scholar with keywords and domains
```
python run.py webscrape
```
### Target 5: Convert jupyter notebooks to html
Note: Files with human annotations are in the references directory.
```
python run.py report
```
### Target 6: Test all previous targets on test data
Note: For the test run, raw test data and domain for search is in test/testdata directory.
```
python run.py test
```
### Target 7: Activating the website
```
python run.py website
```

#### Note for local run:
Since AutoLibary does not have access right to your local documents, if you would like to try other papers, please put the papers in ```~/AutoLibrary/website/autolibrary/documents``` and refresh the local website.

### Responsbilities: 
- Yichun Ren: Dataset, Weight, Website Development
- Jiayi Fan: Data, Result Analysis, Website Development
- Bingqi Zhou: Dataset, Webscrap, Experiments
","# AutoLibrary - A Personal Digital Library to Find Related Works via Text Analyzer
- Website: https://yichunren.pythonanywhere.com/autolibrary/
- DSC180B - Capstone Project (Winter 2021)
- Section A04 Group03: Yichun Ren, Jiayi Fan, Bingqi Zhou
- Note: This is an application of [AutoPhrase](https://github.com/shangjingbo1226/AutoPhrase) by Jingbo Shang.

## Docker
- The docker repository is `jfan1998/dsc180a-docker`.
- Note: The docker uses dsmlp base container. Please login to a dsmlp jumpbox before entering the command below.
```
launch-scipy-ml.sh -i jfan1998/dsc180a-docker:latest
```
Use port-forwarding on dsmlp to open the website:
  - Instruction: https://docs.google.com/document/d/15ehCaVIKSXwgh2jvH3034l5uSPNLrZRgkczwl-xWNEU/edit?usp=sharing

## Website
- Our published website enables users to upload papers via URLs/Local Machine
- If you are curious about our code for published website, you can switch the branch to website.

## Local Run
- Please refer to `requirements.txt` to check if all the packages and libraries needed are installed.
### Default Run: open AutoLibrary website
```
python run.py
```
- The home page of website: `http://127.0.0.1:8000/autolibrary/`
### Target 1: Convert the input .pdf file into .txt
```
python run.py data
```
### Target 2: Run AutoPhrase on the input file
```
python run.py autophrase
```
### Target 3: Apply weight to the quality scores of phrases according the corresponding quality score in its domain
```
python run.py weight
```
### Target 4: Webscrape the search results on Semantic Scholar with keywords and domains
```
python run.py webscrape
```
### Target 5: Convert jupyter notebooks to html
Note: Files with human annotations are in the references directory.
```
python run.py report
```
### Target 6: Test all previous targets on test data
Note: For the test run, raw test data and domain for search is in test/testdata directory.
```
python run.py test
```
### Target 7: Activating the website
```
python run.py website
```

#### Note for local run:
Since AutoLibary does not have access right to your local documents, if you would like to try other papers, please put the papers in ```~/AutoLibrary/website/autolibrary/documents``` and refresh the local website.

### Responsbilities: 
- Yichun Ren: Dataset, Weight, Website Development
- Jiayi Fan: Data, Result Analysis, Website Development
- Bingqi Zhou: Dataset, Webscrap, Experiments
"
49,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_37,,,"# Restaurant Recommender System
There are multiple factors that go into a rating: wait time, service, quality of food, cleanliness, or even atmosphere - for example, a restaurant could have positive sentiment towards the food but negative sentiment towards the service. In order to solve this problem, our aim is to include such sentiments that can be found in the review text and turn that into data which can be used to further improve business recommendations to users.

This repository is a recommender system with a primary focus on the text reviews analysis through TF-IDF (term frequency-inverse document frequency) and targeted sentiment analysis with AutoPhrase to attach sentiments to aspects of a restaurant. In building the recommender system, we learned that review texts can hold the same importance as the numerical statistics because they contain key phrases that characterize how they felt about the review. The ultimate goal is designing a website for deploying our recommender system and showing its functionality.

Visit our `website` branch to try some queries on a preprocessed Las Vegas / Phoenix dataset!

## Important Things:
* This repository is contains two branches. The `main` branch contains the source code for our methods. The `website` branch contains the code to run our recommender sebsite on Flask.
* In our implementation and analysis, we use the Autophrase as our core NLP analysis method by submoduling into our repository.
* The Docker Image and Tag is `launch.sh -i catherinehou99/yelp-recommender-system:latest -c 8 -m 20 -P Always`
- If you would like to learn more details about the AutoPhrase method, please refer to the original github repository: https://github.com/shangjingbo1226/AutoPhrase. Namely, you will find the system requirements, all the tools used and detailed explanation of the output.
- Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, Jiawei Han, ""**[Automated Phrase Mining from Massive Text Corpora](https://arxiv.org/abs/1702.04457)**"", accepted by IEEE Transactions on Knowledge and Data Engineering, Feb. 2018.

## Before You Run:
* In order to use Yelp's academic dataset, you will need to go to their [Website](https://www.yelp.com/dataset) and agree to the Terms of Use Agreement before you download the dataset. Save the dataset to the directory `data/raw`
* This repo uses AutoPhrase as the git submodule, run the command `git submodule update --init` after cloning this repo.

## How to Use this Repository:
1. Run the test target in the `main` branch if you would like to test the targets.
2. In `config/data-params` you can choose which city you would like to subset. For test target, the city can be Las Vegas or Phoenix.
3. Once you successfully run the targets, the generated files will be saved to `data/tmp`. These files need to be used in the `data` folder of the`website` branch.
4. If you would like to run the website on Flask, head over to the website branch!

## Default Run

```
$ python3 run.py -- all
```
The default running file is run.py and can be simply run through the command line: python3 run.py -- all
This will run all the targets below (data, sentiment, eda, tfidf)

For each of the target:
* data: prepares necessary folders, reads in Yelp json files, and filters the dataset to contain rows relevant to the specified city.
* sentiment: performs sentiment analysis on the reviews. It will take in the reviews dataframe and output the positive/negative sentences counts.
* eda: performs the eda analysis of the dataset and autophrase result.
* test: runs the above targets on a test dataset which runs around 3mins.
* clean: removes all the files generated with keeping the html report in the `data/eda` folder.

```
$ python3 run.py -- data
```
The default running file is run.py and can be simply run through the command line: python3 run.py -- data
* Check if the reference folder exists in the user local drive. If not, create all the necessary folder for projects
* Read the dataframes for further analysis.

```
$ python3 run.py -- sentiment
```
The default running file is run.py and can be simply run through the command line: python3 run.py -- sentiment
* Perform sentiment analysis on the reviews text
* Outputs a city_name.csv in the `data/tmp` folder contains, for each restaurant, the positive phrases and the number of times they were mentioned in a review.

```
$ python3 run.py -- eda
```
The default running file is run.py and can be simply run through the command line: python3 run.py -- eda
* Perform the EDA analysis on the AutoPhrase result of individual user
* Perform the EDA analysis on individual city review dataset such as sentiment analysis, feature exploration
* Convert all the EDA analysis into an HTML report stored under data/eda
* After running this tag go to the data/eda directory to see the report.html

```
$ python3 run.py -- tfidf
```
The default running file is run.py and can be simply run through the command line: python3 run.py -- tfidf
* Run restaurant-restaurant based recommendation methods using TF-IDF and cosine similarity score
* Generate the TF-IDF results CSV file of two cities, Las Vegas and Phoenix and store in the reference folder
* Two TF-IDF results CSV are using in the website building as backend database for generating recommendation


```
$ python3 run.py -- clean
```
The default running file is run.py and can be simply run through the command line: python3 run.py -- clean
* Remove all the generated files, plots, dataframes under the reference folder
* Keep the HTML file in the eda/data folder for report visualization
* Be careful: this will clear all the outputs running so far and can not be reversed!!

```
$ python3 run.py -- test
```
The default running file is run.py and can be simply run through the command line: python3 run.py -- test
* Run all the targets on the test data set we generated 

### Responsibilities
* Catherine Hou developed the sentiment/eda/data tag and the food query on the website.
* Vincent Le created the dockerfile and developed the website (not in main branch).
* Shenghan Liu developed TF-IDF/clean/data tag, user AutoPhrase EDA, and the restaurant query on the website.
","# Restaurant Recommender System
There are multiple factors that go into a rating: wait time, service, quality of food, cleanliness, or even atmosphere - for example, a restaurant could have positive sentiment towards the food but negative sentiment towards the service. In order to solve this problem, our aim is to include such sentiments that can be found in the review text and turn that into data which can be used to further improve business recommendations to users.

This repository is a recommender system with a primary focus on the text reviews analysis through TF-IDF (term frequency-inverse document frequency) and targeted sentiment analysis with AutoPhrase to attach sentiments to aspects of a restaurant. In building the recommender system, we learned that review texts can hold the same importance as the numerical statistics because they contain key phrases that characterize how they felt about the review. The ultimate goal is designing a website for deploying our recommender system and showing its functionality.

Visit our `website` branch to try some queries on a preprocessed Las Vegas / Phoenix dataset!

## Important Things:
* This repository is contains two branches. The `main` branch contains the source code for our methods. The `website` branch contains the code to run our recommender sebsite on Flask.
* In our implementation and analysis, we use the Autophrase as our core NLP analysis method by submoduling into our repository.
* The Docker Image and Tag is `launch.sh -i catherinehou99/yelp-recommender-system:latest -c 8 -m 20 -P Always`
- If you would like to learn more details about the AutoPhrase method, please refer to the original github repository: https://github.com/shangjingbo1226/AutoPhrase. Namely, you will find the system requirements, all the tools used and detailed explanation of the output.
- Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, Jiawei Han, ""**[Automated Phrase Mining from Massive Text Corpora](https://arxiv.org/abs/1702.04457)**"", accepted by IEEE Transactions on Knowledge and Data Engineering, Feb. 2018.

## Before You Run:
* In order to use Yelp's academic dataset, you will need to go to their [Website](https://www.yelp.com/dataset) and agree to the Terms of Use Agreement before you download the dataset. Save the dataset to the directory `data/raw`
* This repo uses AutoPhrase as the git submodule, run the command `git submodule update --init` after cloning this repo.

## How to Use this Repository:
1. Run the test target in the `main` branch if you would like to test the targets.
2. In `config/data-params` you can choose which city you would like to subset. For test target, the city can be Las Vegas or Phoenix.
3. Once you successfully run the targets, the generated files will be saved to `data/tmp`. These files need to be used in the `data` folder of the`website` branch.
4. If you would like to run the website on Flask, head over to the website branch!

## Default Run

```
$ python3 run.py -- all
```
The default running file is run.py and can be simply run through the command line: python3 run.py -- all
This will run all the targets below (data, sentiment, eda, tfidf)

For each of the target:
* data: prepares necessary folders, reads in Yelp json files, and filters the dataset to contain rows relevant to the specified city.
* sentiment: performs sentiment analysis on the reviews. It will take in the reviews dataframe and output the positive/negative sentences counts.
* eda: performs the eda analysis of the dataset and autophrase result.
* test: runs the above targets on a test dataset which runs around 3mins.
* clean: removes all the files generated with keeping the html report in the `data/eda` folder.

```
$ python3 run.py -- data
```
The default running file is run.py and can be simply run through the command line: python3 run.py -- data
* Check if the reference folder exists in the user local drive. If not, create all the necessary folder for projects
* Read the dataframes for further analysis.

```
$ python3 run.py -- sentiment
```
The default running file is run.py and can be simply run through the command line: python3 run.py -- sentiment
* Perform sentiment analysis on the reviews text
* Outputs a city_name.csv in the `data/tmp` folder contains, for each restaurant, the positive phrases and the number of times they were mentioned in a review.

```
$ python3 run.py -- eda
```
The default running file is run.py and can be simply run through the command line: python3 run.py -- eda
* Perform the EDA analysis on the AutoPhrase result of individual user
* Perform the EDA analysis on individual city review dataset such as sentiment analysis, feature exploration
* Convert all the EDA analysis into an HTML report stored under data/eda
* After running this tag go to the data/eda directory to see the report.html

```
$ python3 run.py -- tfidf
```
The default running file is run.py and can be simply run through the command line: python3 run.py -- tfidf
* Run restaurant-restaurant based recommendation methods using TF-IDF and cosine similarity score
* Generate the TF-IDF results CSV file of two cities, Las Vegas and Phoenix and store in the reference folder
* Two TF-IDF results CSV are using in the website building as backend database for generating recommendation


```
$ python3 run.py -- clean
```
The default running file is run.py and can be simply run through the command line: python3 run.py -- clean
* Remove all the generated files, plots, dataframes under the reference folder
* Keep the HTML file in the eda/data folder for report visualization
* Be careful: this will clear all the outputs running so far and can not be reversed!!

```
$ python3 run.py -- test
```
The default running file is run.py and can be simply run through the command line: python3 run.py -- test
* Run all the targets on the test data set we generated 

### Responsibilities
* Catherine Hou developed the sentiment/eda/data tag and the food query on the website.
* Vincent Le created the dockerfile and developed the website (not in main branch).
* Shenghan Liu developed TF-IDF/clean/data tag, user AutoPhrase EDA, and the restaurant query on the website.
"
50,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_48,,,"# ForumRec

## Introduction
This repository is for the ForumRec project, a recommendation system, that recommends users questions they are adept to answer on the [Super User](superuser.com) forum of [StackExchange](https://stackexchange.com). The website can be reached at [jackzlin.com](jackzlin.com) and the repository for the website can be reached at [ForumRecWeb](https://github.com/okminz/ForumRecWeb).

##  Files

For this project, we have files for retriving the data, running the models, and processing it into the desired output. The files are described below and explain the purpose of each part of the repository.

> etl.py: Passes in the configs file related to it. The process for taking the data from the data files, extracting the necessary information, and splitting them into questions and answers after a certain date. This is to maintain a higher predictive function and while utilizing as much of the information as e can after extracting.

> api_etl.py: Passes in the configs file path related to it. It uses the [Stack Exchange API](https://api.stackexchange.com/) on the Super User forum to gather recent questions and answers from the Super User forum, concatenates the questions and answers and saves them into a continuously updating file so it can be used for new recommendations.

> run.py: Runs etl on the data. Runs api_etl on the api parameters. Runs the hybrid model (collaborative and content-based filterling) creation and recommendation files creation along with evaluating the model. Runs the baseline file to get the baseline comparison values. Can also run all of these files on test data.

> create_model.py: Passes in the configs file related to it. This file will run create_inputs.py and gather inputs from that file. It will then use those inputs to a generate a model. 

> create_inputs.py: Passes in the configs file related to it. This file will use processed questions and answers to generate the necessary matrices and files and save them so that they may be used in create_model.py to create a model.

> model.py: Passes in the configs file related to it a boolean parameter to determine if running for baselines (default is False). This file the generated model and gather its inputs in order to make recommendations for based on the interactions between users, questions, answers, text, and tags. These recommendations will be returned in a file. It will also produce the evaluations of the model using precision, recall, auc score, and recriprical rank.

> new_user.py: Passes in the configs file related to it. This file will take in user response data from the website in order to gather new and fresh recommendations for the user by fitting partially to the already generated model and replacing it. This script is used for the website's cold start function mostly. 

> create_baseline.py: Passes in the configs file related to it. Baseline file that return the recommendations given to a user using a simple collaborative filtering model so that it can be compared against the model's recommendations and evaluation metrics.

> requirements.txt: Contains the amount of processing resources recommended to run the files and the packages needed and the versions that were used to run all the processes.

> LICENSE: A file that contains the reuse and use licenses for this repository.

> SuperUser EDA.ipynb: Inside the notebook directory. Notebook containing the exploratory data analyses that was taken on the data to further understand and gain insight on the data we were using, and how we can use the data to build the recommendation system we wanted.
> 
> SuperUser API.ipynb: Inside the notebook directory. Notebook containing the explorationa and trials of using the Stack Exchange API to further understand and gain insight on how to get the desire questions and answers from it. It was used to build the api-etl process.

> ETL.ipynb: Inside the notebook directory. Notebook containing the etl processes and the results to look at the etl.

> NLP.ipynb: Inside the notebook directory. Notebook containing the natural language processing code that looks at the text data and creates a model through that code.

##  Directories

The following directories were created by us in order to be able to store and retain the necessary information needed for different purposes.

> config: Contains a list of all the config files that determines the parameters of each file. Use these files according to their use to change the parameters and change which subset of data you are running the processes on. Make sure you are changing the file paths correctly and throughout the entire config file.

> data: Location to put the original raw data in. It also would contain a final directory which would contain the data retrieved after running the repository.

> test: Contains inside the data directory and inside that the data used to run the repository on a small subset of the data to ensure the models and the scripts are running correctly.

> src: Contains inside the src, models, and baselines folders which contain the etl, model creation, input creation, model, new user, and baselines python files that are described above.

> notebooks: Contains multiple notebooks that explore the data. The notebooks are described above.

## Running the Code
Prior to running the code, make sure that you install all the packages listed in *requirements.txt* 

In order to obtain the data, one can follow the processes below:

### Creating the Data

To create the processed data, run this following command on the command line terminal:
```
python run.py data
```
Where the data will be replaced processed and be returned into new files usable by our models in this project and placed in the data directory.

### Gathering data from the API

To get new questions and answers from Super User since the last pull of API data, run this following command on the command line terminal:
```
python run.py api
```
Where the API data will be processed and be returned into a usable continuous file placed in the data directory.

### Running the cosine models

To run the hybrid filtering model on the data, run this following command on the command line terminal:
```
python run.py models
```
Where hybrid filtering model and its inputs will be created and used to generate new recommendations and then evaluated using specific metrics.

### Comparing the model to baselines

To determine how our model compares to the baselines model, run this following command on the command line terminal:
```
python run.py baselines
```
Where the baseline model will be evaluated and recommendations will be returned using the same data as the hybrid filtering model.

### Running all the model targets

If you want to run all of these together, run this following command on the command line terminal:
```
python run.py all
```
Where the all 4 targets (excluding *test*) will run one after another in the order presented above.

### Testing all the model targets

To test how if the repository and all the models and scripts are working, run this following command on the command line terminal
```
python run.py test
```
Where all of the targets ('data', 'api', 'models' and 'baselines') above will all be run one after another in the order presented above, but on small test data so that we can observe how the models and scripts are working.

## Responsibilities
Yo Jerimijenko-Conley: 

Jasraj Johl: Created the ETL process, worked with the Super User API, created the code repository and made sure it was clean and runnable, and created the website's design through HTML and CSS integrating it partially with Flask.

Jack Lin: 
","# ForumRec

## Introduction
This repository is for the ForumRec project, a recommendation system, that recommends users questions they are adept to answer on the [Super User](superuser.com) forum of [StackExchange](https://stackexchange.com). The website can be reached at [jackzlin.com](jackzlin.com) and the repository for the website can be reached at [ForumRecWeb](https://github.com/okminz/ForumRecWeb).

##  Files

For this project, we have files for retriving the data, running the models, and processing it into the desired output. The files are described below and explain the purpose of each part of the repository.

> etl.py: Passes in the configs file related to it. The process for taking the data from the data files, extracting the necessary information, and splitting them into questions and answers after a certain date. This is to maintain a higher predictive function and while utilizing as much of the information as e can after extracting.

> api_etl.py: Passes in the configs file path related to it. It uses the [Stack Exchange API](https://api.stackexchange.com/) on the Super User forum to gather recent questions and answers from the Super User forum, concatenates the questions and answers and saves them into a continuously updating file so it can be used for new recommendations.

> run.py: Runs etl on the data. Runs api_etl on the api parameters. Runs the hybrid model (collaborative and content-based filterling) creation and recommendation files creation along with evaluating the model. Runs the baseline file to get the baseline comparison values. Can also run all of these files on test data.

> create_model.py: Passes in the configs file related to it. This file will run create_inputs.py and gather inputs from that file. It will then use those inputs to a generate a model. 

> create_inputs.py: Passes in the configs file related to it. This file will use processed questions and answers to generate the necessary matrices and files and save them so that they may be used in create_model.py to create a model.

> model.py: Passes in the configs file related to it a boolean parameter to determine if running for baselines (default is False). This file the generated model and gather its inputs in order to make recommendations for based on the interactions between users, questions, answers, text, and tags. These recommendations will be returned in a file. It will also produce the evaluations of the model using precision, recall, auc score, and recriprical rank.

> new_user.py: Passes in the configs file related to it. This file will take in user response data from the website in order to gather new and fresh recommendations for the user by fitting partially to the already generated model and replacing it. This script is used for the website's cold start function mostly. 

> create_baseline.py: Passes in the configs file related to it. Baseline file that return the recommendations given to a user using a simple collaborative filtering model so that it can be compared against the model's recommendations and evaluation metrics.

> requirements.txt: Contains the amount of processing resources recommended to run the files and the packages needed and the versions that were used to run all the processes.

> LICENSE: A file that contains the reuse and use licenses for this repository.

> SuperUser EDA.ipynb: Inside the notebook directory. Notebook containing the exploratory data analyses that was taken on the data to further understand and gain insight on the data we were using, and how we can use the data to build the recommendation system we wanted.
> 
> SuperUser API.ipynb: Inside the notebook directory. Notebook containing the explorationa and trials of using the Stack Exchange API to further understand and gain insight on how to get the desire questions and answers from it. It was used to build the api-etl process.

> ETL.ipynb: Inside the notebook directory. Notebook containing the etl processes and the results to look at the etl.

> NLP.ipynb: Inside the notebook directory. Notebook containing the natural language processing code that looks at the text data and creates a model through that code.

##  Directories

The following directories were created by us in order to be able to store and retain the necessary information needed for different purposes.

> config: Contains a list of all the config files that determines the parameters of each file. Use these files according to their use to change the parameters and change which subset of data you are running the processes on. Make sure you are changing the file paths correctly and throughout the entire config file.

> data: Location to put the original raw data in. It also would contain a final directory which would contain the data retrieved after running the repository.

> test: Contains inside the data directory and inside that the data used to run the repository on a small subset of the data to ensure the models and the scripts are running correctly.

> src: Contains inside the src, models, and baselines folders which contain the etl, model creation, input creation, model, new user, and baselines python files that are described above.

> notebooks: Contains multiple notebooks that explore the data. The notebooks are described above.

## Running the Code
Prior to running the code, make sure that you install all the packages listed in *requirements.txt* 

In order to obtain the data, one can follow the processes below:

### Creating the Data

To create the processed data, run this following command on the command line terminal:
```
python run.py data
```
Where the data will be replaced processed and be returned into new files usable by our models in this project and placed in the data directory.

### Gathering data from the API

To get new questions and answers from Super User since the last pull of API data, run this following command on the command line terminal:
```
python run.py api
```
Where the API data will be processed and be returned into a usable continuous file placed in the data directory.

### Running the cosine models

To run the hybrid filtering model on the data, run this following command on the command line terminal:
```
python run.py models
```
Where hybrid filtering model and its inputs will be created and used to generate new recommendations and then evaluated using specific metrics.

### Comparing the model to baselines

To determine how our model compares to the baselines model, run this following command on the command line terminal:
```
python run.py baselines
```
Where the baseline model will be evaluated and recommendations will be returned using the same data as the hybrid filtering model.

### Running all the model targets

If you want to run all of these together, run this following command on the command line terminal:
```
python run.py all
```
Where the all 4 targets (excluding *test*) will run one after another in the order presented above.

### Testing all the model targets

To test how if the repository and all the models and scripts are working, run this following command on the command line terminal
```
python run.py test
```
Where all of the targets ('data', 'api', 'models' and 'baselines') above will all be run one after another in the order presented above, but on small test data so that we can observe how the models and scripts are working.

## Responsibilities
Yo Jerimijenko-Conley: 

Jasraj Johl: Created the ETL process, worked with the Super User API, created the code repository and made sure it was clean and runnable, and created the website's design through HTML and CSS integrating it partially with Flask.

Jack Lin: 
"
51,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_47,,,"

# OnSight: Outdoor Rock Climbing Recommendations

Recommendations for outdoor rock climbing has historically been limited to word of mouth, guide books, and most popular climbs. With our project OnSight, we believe we can offer personalized recommendations for outdoor rock climbers.

Disclaimer: With rock climbing, especially outdoors, there is an inherent risk that is taken when you decide to climb. Although our recommender tries to offer routes similar to the ones users have done, there is still a risk that the route may be too hard and therefore dangerous. This is not a problem that is solely put on the recommender, but a problem with rock climbing as a whole. There is no standard in climbing grades, but rather it is an agreement among the climbers that have climbed that route. Therefore climbing grades are subjective, and climbs may be harder and more dangerous than a user expects. We realize this, and we encourage everyone to look at the safety information of each climb on its corresponding climbing page on Mountain Project.

## How To Run

We suggest for casual users to simply use our website to run the project. The website URL is https://dsc180b-rc-rec.herokuapp.com/. Note that this project runs on a free dyno, so if you are the first user to open the website in about half an hour, then the website may take a minute to load. Be patient!

However, if you are interested in making changes or diving deep into the code, you can run the project and customize it by either creating your own Heroku project or running the project on the command line.

### Creating your own Heroku project
To have your own version of OnSight running on Heroku, do the following steps:
 1. Fork the OnSight GitHub repository to your own GitHub account
 2. Create a new project on Heroku
 3. In the Heroku app dashboard, go to the ""Deploy"" tab
 4. Under the ""Deployment method"" section, select GitHub and connect the Heroku app to your forked repository
 5. In the Heroku app dashboard, go to the ""Settings"" tab
 6. Under the ""Config Vars"" section, click on ""Reveal Config Vars"" and fill in the config variables as shown in the table below. 

|Config Vars|Value|Description|
|-|-|-|
|PROJECT_PATH|mysite/|Since the django webserver is stored in the ""mysite/"" folder, but the Procfile (tells Heroku how to start the web server) is in the project root, we need to tell Heroku to look in the ""mysite/"" folder for the webserver code|
|GOOGLE_MAPS_API_KEY|Your API key|Your Google Maps API key needs to have the following APIs enabled on the key: ""Maps JavaScript API"", ""Places API"", and ""Maps Embed API"". This key is not strictly necessary, but without the key the map will not work and location can only be entered by manually typing in a latitude and longitude, which is not very UX friendly.|

7. Make sure you deploy the Heroku app again from the ""Deploy"" tab on the Heroku dashboard and you should be all set!

### Running the Project on the Command Line

To run the project, every command must start with ""python run.py"" from the root directory of the project. By default, ""python run.py"" will do absolutely nothing. You must use at least one of the following flags to actually get some response:

|Flag|Type|Default Value|Description|
|-|-|-|-|
|-d, --data|bool|False|Use this flag to run all data scraping code. This will take a very long time, upwards of one week total to scrape all the data. It is recommended *not* to run this. Be aware that this will only store data locally. |
|-c, --clean|bool|False|Use this flag to run all data cleaning code. It is expected that all files defined in the ""state"" key of data_params.json are present in the raw data folder.|
|-\-data-config|str|""config/data_params.json""|The location at which data parameters can be found|
|-\-web-config|str|""config/web_params.json""|The location at which web parameters can be found. These parameters simulate a user using the website.|
|-\-top-pop|bool|False|Use this flag to print the top N most popular climbs. This does not use locally saved data, but rather uses saved data in MongoDB. Additionally, the exact climbs and number of climbs are determined by the web_params.json file.|
|-\-cosine|bool|False|Use this flag to print the top N most similar climbs to the users favorite. This does not use locally saved data, but rather uses saved data in MongoDB. Additionally, the exact climbs and number of climbs are determined by the web_params.json file.|
|-\-test|bool|False|Use this flag to run the two implemented models based on default config files. Using the --test flag will override all other present flags and is equivalent to running ""python run.py --top-pop --cosine --debug"".|
|-\-delete|bool|False|Use this flag to wipe out all data from MongoDB. This will not do anything since the MongoDB login is set to read only.|
|-\-upload|bool|False|Use this flag to upload cleaned data to MongoDB. This will not do anything since the MongoDB login is set to read only.|
|-\-debug|bool|False|Use this flag activate various print statements throughout the project.|

### Description of Parameters

#### Data Parameters

|Parameter Name|Type|Default Value|Description|
|-|-|-|-|
|raw_data_folder|str|""data/raw/""|The location at which raw data will be saved. Note that this path is relative to the project root.|
|clean_data_folder|str|""data/cleaned/""|The location at which clean data will be saved. Note that this path is relative to the project root.|
|states|dict|Too long to copy here...|Although the parameter is called states, this is really just the areas to scrape/clean and the urls at which they can be found. The file will be named based on the key string, and the area url to scrape is the value string. By default this contains all 50 states, with the state name as key and state area url as value.|

#### Web Parameters
Be aware that all these parameters do is simulate a user using the website. Each of the parameters here refer to a form element on the website.

|Parameter Name|Type|Default Value|Description|
|-|-|-|-|
|user_url|str|Too long to copy here...|The ""Mountain Project URL"" form element on the website. This value is only used if the user requests personalized recommendations. The default value is a user with a lot of climbs rated, about 600 in March 2021. You can find the actual default value in the config file.|
|location|[float, float]|[43.444918, -71.707888]|The ""Latitude"" and ""Longitude"" form elements on the website. This location is the center of the circle where climbs will be looked for. The default value is some random location in New Hampshire.|
|max_distance|int|50|The ""Max Distance (mi)"" form element on the website. This value is the radius of the circle where climbs will be looked for. The default value is 50 miles, which should be sufficient to encompass any climbing area.|
|recommender|str|""top_pop""|The ""Recommenders"" form element on the website. This is the recommender to use and will be any of ""top_pop"" or ""cosine_rec"". There is an additional hidden debug recommender that uses the string of ""debug"". The debug recommender is not accessible without modifying the ""mysite/bootstrap4/forms.py"" file|
|num_recs|int|10|The ""Number of Recommendations"" form element on the website. This is the maximum number of recommendations that will be displayed once the user hits submit.|
|difficulty_range|{""boulder"": [int, int], ""route: [int, int]}|{""boulder"": [0, 3], ""route"": [11, 16]}|The ""Boulder"", ""V_-V_"", ""Route"", and ""5.\_-5.\_"" form elements on the website. Due to the way data is cleaned, bouldering V grades and route 5. grades are converted to integers starting at 0 on different scales. You can find the scales defined in code. The two default difficulty ranges correspond to V0-V3 and 5.8-5.10d. Note that if the user does not want boulders or routes, the corresponding difficulty range will be [-1, -1]|

","

# OnSight: Outdoor Rock Climbing Recommendations

Recommendations for outdoor rock climbing has historically been limited to word of mouth, guide books, and most popular climbs. With our project OnSight, we believe we can offer personalized recommendations for outdoor rock climbers.

Disclaimer: With rock climbing, especially outdoors, there is an inherent risk that is taken when you decide to climb. Although our recommender tries to offer routes similar to the ones users have done, there is still a risk that the route may be too hard and therefore dangerous. This is not a problem that is solely put on the recommender, but a problem with rock climbing as a whole. There is no standard in climbing grades, but rather it is an agreement among the climbers that have climbed that route. Therefore climbing grades are subjective, and climbs may be harder and more dangerous than a user expects. We realize this, and we encourage everyone to look at the safety information of each climb on its corresponding climbing page on Mountain Project.

## How To Run

We suggest for casual users to simply use our website to run the project. The website URL is https://dsc180b-rc-rec.herokuapp.com/. Note that this project runs on a free dyno, so if you are the first user to open the website in about half an hour, then the website may take a minute to load. Be patient!

However, if you are interested in making changes or diving deep into the code, you can run the project and customize it by either creating your own Heroku project or running the project on the command line.

### Creating your own Heroku project
To have your own version of OnSight running on Heroku, do the following steps:
 1. Fork the OnSight GitHub repository to your own GitHub account
 2. Create a new project on Heroku
 3. In the Heroku app dashboard, go to the ""Deploy"" tab
 4. Under the ""Deployment method"" section, select GitHub and connect the Heroku app to your forked repository
 5. In the Heroku app dashboard, go to the ""Settings"" tab
 6. Under the ""Config Vars"" section, click on ""Reveal Config Vars"" and fill in the config variables as shown in the table below. 

|Config Vars|Value|Description|
|-|-|-|
|PROJECT_PATH|mysite/|Since the django webserver is stored in the ""mysite/"" folder, but the Procfile (tells Heroku how to start the web server) is in the project root, we need to tell Heroku to look in the ""mysite/"" folder for the webserver code|
|GOOGLE_MAPS_API_KEY|Your API key|Your Google Maps API key needs to have the following APIs enabled on the key: ""Maps JavaScript API"", ""Places API"", and ""Maps Embed API"". This key is not strictly necessary, but without the key the map will not work and location can only be entered by manually typing in a latitude and longitude, which is not very UX friendly.|

7. Make sure you deploy the Heroku app again from the ""Deploy"" tab on the Heroku dashboard and you should be all set!

### Running the Project on the Command Line

To run the project, every command must start with ""python run.py"" from the root directory of the project. By default, ""python run.py"" will do absolutely nothing. You must use at least one of the following flags to actually get some response:

|Flag|Type|Default Value|Description|
|-|-|-|-|
|-d, --data|bool|False|Use this flag to run all data scraping code. This will take a very long time, upwards of one week total to scrape all the data. It is recommended *not* to run this. Be aware that this will only store data locally. |
|-c, --clean|bool|False|Use this flag to run all data cleaning code. It is expected that all files defined in the ""state"" key of data_params.json are present in the raw data folder.|
|-\-data-config|str|""config/data_params.json""|The location at which data parameters can be found|
|-\-web-config|str|""config/web_params.json""|The location at which web parameters can be found. These parameters simulate a user using the website.|
|-\-top-pop|bool|False|Use this flag to print the top N most popular climbs. This does not use locally saved data, but rather uses saved data in MongoDB. Additionally, the exact climbs and number of climbs are determined by the web_params.json file.|
|-\-cosine|bool|False|Use this flag to print the top N most similar climbs to the users favorite. This does not use locally saved data, but rather uses saved data in MongoDB. Additionally, the exact climbs and number of climbs are determined by the web_params.json file.|
|-\-test|bool|False|Use this flag to run the two implemented models based on default config files. Using the --test flag will override all other present flags and is equivalent to running ""python run.py --top-pop --cosine --debug"".|
|-\-delete|bool|False|Use this flag to wipe out all data from MongoDB. This will not do anything since the MongoDB login is set to read only.|
|-\-upload|bool|False|Use this flag to upload cleaned data to MongoDB. This will not do anything since the MongoDB login is set to read only.|
|-\-debug|bool|False|Use this flag activate various print statements throughout the project.|

### Description of Parameters

#### Data Parameters

|Parameter Name|Type|Default Value|Description|
|-|-|-|-|
|raw_data_folder|str|""data/raw/""|The location at which raw data will be saved. Note that this path is relative to the project root.|
|clean_data_folder|str|""data/cleaned/""|The location at which clean data will be saved. Note that this path is relative to the project root.|
|states|dict|Too long to copy here...|Although the parameter is called states, this is really just the areas to scrape/clean and the urls at which they can be found. The file will be named based on the key string, and the area url to scrape is the value string. By default this contains all 50 states, with the state name as key and state area url as value.|

#### Web Parameters
Be aware that all these parameters do is simulate a user using the website. Each of the parameters here refer to a form element on the website.

|Parameter Name|Type|Default Value|Description|
|-|-|-|-|
|user_url|str|Too long to copy here...|The ""Mountain Project URL"" form element on the website. This value is only used if the user requests personalized recommendations. The default value is a user with a lot of climbs rated, about 600 in March 2021. You can find the actual default value in the config file.|
|location|[float, float]|[43.444918, -71.707888]|The ""Latitude"" and ""Longitude"" form elements on the website. This location is the center of the circle where climbs will be looked for. The default value is some random location in New Hampshire.|
|max_distance|int|50|The ""Max Distance (mi)"" form element on the website. This value is the radius of the circle where climbs will be looked for. The default value is 50 miles, which should be sufficient to encompass any climbing area.|
|recommender|str|""top_pop""|The ""Recommenders"" form element on the website. This is the recommender to use and will be any of ""top_pop"" or ""cosine_rec"". There is an additional hidden debug recommender that uses the string of ""debug"". The debug recommender is not accessible without modifying the ""mysite/bootstrap4/forms.py"" file|
|num_recs|int|10|The ""Number of Recommendations"" form element on the website. This is the maximum number of recommendations that will be displayed once the user hits submit.|
|difficulty_range|{""boulder"": [int, int], ""route: [int, int]}|{""boulder"": [0, 3], ""route"": [11, 16]}|The ""Boulder"", ""V_-V_"", ""Route"", and ""5.\_-5.\_"" form elements on the website. Due to the way data is cleaned, bouldering V grades and route 5. grades are converted to integers starting at 0 on different scales. You can find the scales defined in code. The two default difficulty ranges correspond to V0-V3 and 5.8-5.10d. Note that if the user does not want boulders or routes, the corresponding difficulty range will be [-1, -1]|

"
52,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_46,,,"# DSC180b-Capstone

Project repository for Recommender Systems group 3

This project is focused on creating music recommendations for users and their parents.

# HOW TO RUN

Our project's current targets are: load-data, task0, task1, task2, all, test

Our project's current config files are: test.json and run.json

### Targets

load-data: Pulls our training data from a S3 bucket where we store it and creates a new 'data'
repository to store it.

task0: Generates a list of sample parent recommendations and saves them to data/recommendations as a csv file.

task1: Generates a list of parent-user recommendations and saves them to data/recommendations as a csv file.

task2: Generates a list of user-parent recommendations base on a user's listening history. Saves
these recommendations to data/recommendations as a csv file.

all: runs load-data, task0, task1, and task2 in succession

test: Runs through the same load-data, task0, task1, task2 pipeline but uses a pre-stored user access code.
This allows us to 'test' our recommendation models without having to authenticate ourselves every
single time. The test data in this case is a user account that we have permission to read from.

### Configs

Our configuration files are relatively simple given our project's reliance on listening histories and
otherwise limited user information. The values in each file are the same, but we have created two files
so that logic can be quickly tested without having to constantly change the configuration parameters during
development.

username: The username of the Spotify account that we are creating recommendations for
parent_age: The age of the user's parent
genre: The parent's preferred genre of music
artist: The parent's preferred artist

This information would normally be provided by users through a form on our website, but for this situation
we just reference configuration files.


### IN THE FUTURE

We plan on adding a clean-data target that isolates some of the small preprocessing that our code does: dropping na values
small transformations, etc.

Instead of caching an auth_token for a specific user, we want to just directly load listening history. This would be a more
straightforward procedure, the problem is that we inevitably have to authenticate regardless of if we have our user data predownloaded or not. Spotify has another authentication flow that would better suit itself to this situation, and we will be looking into that in the future.

","# DSC180b-Capstone

Project repository for Recommender Systems group 3

This project is focused on creating music recommendations for users and their parents.

# HOW TO RUN

Our project's current targets are: load-data, task0, task1, task2, all, test

Our project's current config files are: test.json and run.json

### Targets

load-data: Pulls our training data from a S3 bucket where we store it and creates a new 'data'
repository to store it.

task0: Generates a list of sample parent recommendations and saves them to data/recommendations as a csv file.

task1: Generates a list of parent-user recommendations and saves them to data/recommendations as a csv file.

task2: Generates a list of user-parent recommendations base on a user's listening history. Saves
these recommendations to data/recommendations as a csv file.

all: runs load-data, task0, task1, and task2 in succession

test: Runs through the same load-data, task0, task1, task2 pipeline but uses a pre-stored user access code.
This allows us to 'test' our recommendation models without having to authenticate ourselves every
single time. The test data in this case is a user account that we have permission to read from.

### Configs

Our configuration files are relatively simple given our project's reliance on listening histories and
otherwise limited user information. The values in each file are the same, but we have created two files
so that logic can be quickly tested without having to constantly change the configuration parameters during
development.

username: The username of the Spotify account that we are creating recommendations for
parent_age: The age of the user's parent
genre: The parent's preferred genre of music
artist: The parent's preferred artist

This information would normally be provided by users through a form on our website, but for this situation
we just reference configuration files.


### IN THE FUTURE

We plan on adding a clean-data target that isolates some of the small preprocessing that our code does: dropping na values
small transformations, etc.

Instead of caching an auth_token for a specific user, we want to just directly load listening history. This would be a more
straightforward procedure, the problem is that we inevitably have to authenticate regardless of if we have our user data predownloaded or not. Spotify has another authentication flow that would better suit itself to this situation, and we will be looking into that in the future.

"
53,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_45,,,"## Asnapp

Asnapp is a workout video recommender web application. 

Authors: Amanda Shu, Peter Peng, Najeem Kanishka

### Website URL
The website is now live on: https://workout-recommender.herokuapp.com/

### Video Demonstration
For a demonstration of the project, visit: https://www.youtube.com/watch?v=QJFg0HguGuI

### Data
The data is scraped from https://www.fitnessblender.com/. We are using the data for academic purposes only.

### Code Organization

- `run.py`: Run to get data and model results.
- `app.py`: Runs flask web application.
- `workout_db.sql`: Contains sql statements for creation of tables in database.
- `requirements.txt`: Python packages required to run project
- `wsgi.py & Procfile & runtime.txt`: Entrypoint for Heroku, used in website deployment

**Source**
- The `src/data` folder contains `scrape.py`, the web-scraping script that writes three data files into `data/raw` folder. `fbpreprocessing.py` takes these raw data files and outputs cleaned/transformed data files into `data/preprocessed` folder. `youtube.py` grabs youtube related data from the Youtube API. `model_preprocessing` reads in preprocessed data and transforms the data into what is needed for model inputs.
- `src/models` contains `run_models.py` which trains and evaluates the models. Models are implemented in `lightm_fm.py` and `top_popular.py`
- The `src/utils` folder has `clean.py` which implements the standard target `clean`.
- The `src/app` folder holds files for the web application. `forms.py` contains wtforms classes for registration/login pages. `recommendations.py` holds code for filtering user preferences and building recommendation lists. `register.py` contains helper functions to create the sql insertion/update statements for registering users and updating their workout preferences.

**Static**:
- The`images` folder holds a gif ([source](https://www.pinterest.at/pin/512495632597411529/)) used for the loading page. No copyright intended.
- The `js` folder contains several javascript files. `overlay.js` is for the display of the popup videos on the recommendation page. `workout_info.js` is for registration and update preferences pages. `rec.js` is for the loading page and recommendation engine logic on the recommendations page.
- `libraries/slick` has several files for the carousel, `styles` has a css file, and `favicon.io` is the dumbbell icon
- `vendor` holds several javascript files (Bootstrap, JQuery) for styling/theming of the website

**Config**: `data-params.json` has file paths outputs for data collection/preprocessing and `test-params.json` has the data paths for the test target. To webscrape, this folder should also include `chromedriver.json`. To gather Youtube data, `api_key.json` specifies the api key. To run the app, `db_config.json` has the database configurations.

**Notebook**: `eda.ipynb` is a notebook with exploratory data analysis on scraped data. `param_comparision.ipynb` is a notebook reporting the recommendation models' performance across a couple parameters. `top_popular_extension.ipynb` is a notebook looking into adding Youtube API data into the top popular recommender. `KNN_collab.ipynb` contains results of a KNN collaborative filtering model from surprise package and a pure collaborative filtering from LightFM.

**Templates**: Holds html files for the various endpoints.

**Testdata/raw**: These are fake datasets meant to be used with the test target.

**Docker**: Docker related files. See [here](https://github.com/amandashu/Workout_Recommender/blob/main/docker/README_DOCKER.md)

**Materials**: Contains pdfs for presentation slides and a report detailing our methods/implementations.

### Set Up Project Environment
There are two ways to run this project: a) Docker (preferred) or b) Locally <br>
a) To Run in Docker:<br>
  1) Pull the container with `docker pull nkanishka/workout-recommender`
  2) Run the container using:
  * General Use: `docker run -it -p 5000:5000 workout-recommender`
  * DSMLP Only: `launch.sh -i nkanishka/workout_recommender_dsmlp -c 4 -m 8` <br><br>`kubectl port-forward <Kubernates Cluster Name> 5000`<br><br> `ssh -N -L 5000:127.0.0.1:5000 <AD Name>@dsmlp-login.ucsd.edu`
  3) Inside container/cluster, type `cd Workout_Recommender`. Note that in the DSMLP environment, you will need to manually clone this repo.
  4) If using website, go to [localhost:5000](localhost:5000)
    <br>

b) To run locally, install requiremnents.txt into a virtualenv. Make sure you have Python 3.8+ and Pip installed.

### Run the Project Stages
- To get the data, run `python run.py data`. This scrapes the data and cleans the data and saves these files into `/data/raw` and `data/preprocessed` respectively.
  - Note: for scraping, this assumes that there is a file `config/chromedriver.json` that specifies where the path to the downloaded chromedriver.exe file for your Chrome version lies in the attribute `chromedriver_path`.
  - Note: for making requests to Youtube API, this assumes that there is a file `config/api_key.json` that specifies the api key in the `api_key` attribute.
- To run model results, run `python run.py model`. This takes in the preprocessed data, trains the models, and prints out the NDCG scores for each model.
- Standard target `clean` is implemented, and it will delete the `data` folder.
- Standard target `all` is implemented, and it equivalent to running `python run.py data model`.
- Standard target `test` is implemented, and runs the data preprocessing and modeling results on the test data. The purpose of this target is purely to check the implementation of the code.
- Use `python app.py` to run the app locally.
  - Note: this assumes that there is a file `config/db_config.json`, which has database host, user, password, and name information.
  - And a file `config/flask_keys.json` which has a Flask secret key
","## Asnapp

Asnapp is a workout video recommender web application. 

Authors: Amanda Shu, Peter Peng, Najeem Kanishka

### Website URL
The website is now live on: https://workout-recommender.herokuapp.com/

### Video Demonstration
For a demonstration of the project, visit: https://www.youtube.com/watch?v=QJFg0HguGuI

### Data
The data is scraped from https://www.fitnessblender.com/. We are using the data for academic purposes only.

### Code Organization

- `run.py`: Run to get data and model results.
- `app.py`: Runs flask web application.
- `workout_db.sql`: Contains sql statements for creation of tables in database.
- `requirements.txt`: Python packages required to run project
- `wsgi.py & Procfile & runtime.txt`: Entrypoint for Heroku, used in website deployment

**Source**
- The `src/data` folder contains `scrape.py`, the web-scraping script that writes three data files into `data/raw` folder. `fbpreprocessing.py` takes these raw data files and outputs cleaned/transformed data files into `data/preprocessed` folder. `youtube.py` grabs youtube related data from the Youtube API. `model_preprocessing` reads in preprocessed data and transforms the data into what is needed for model inputs.
- `src/models` contains `run_models.py` which trains and evaluates the models. Models are implemented in `lightm_fm.py` and `top_popular.py`
- The `src/utils` folder has `clean.py` which implements the standard target `clean`.
- The `src/app` folder holds files for the web application. `forms.py` contains wtforms classes for registration/login pages. `recommendations.py` holds code for filtering user preferences and building recommendation lists. `register.py` contains helper functions to create the sql insertion/update statements for registering users and updating their workout preferences.

**Static**:
- The`images` folder holds a gif ([source](https://www.pinterest.at/pin/512495632597411529/)) used for the loading page. No copyright intended.
- The `js` folder contains several javascript files. `overlay.js` is for the display of the popup videos on the recommendation page. `workout_info.js` is for registration and update preferences pages. `rec.js` is for the loading page and recommendation engine logic on the recommendations page.
- `libraries/slick` has several files for the carousel, `styles` has a css file, and `favicon.io` is the dumbbell icon
- `vendor` holds several javascript files (Bootstrap, JQuery) for styling/theming of the website

**Config**: `data-params.json` has file paths outputs for data collection/preprocessing and `test-params.json` has the data paths for the test target. To webscrape, this folder should also include `chromedriver.json`. To gather Youtube data, `api_key.json` specifies the api key. To run the app, `db_config.json` has the database configurations.

**Notebook**: `eda.ipynb` is a notebook with exploratory data analysis on scraped data. `param_comparision.ipynb` is a notebook reporting the recommendation models' performance across a couple parameters. `top_popular_extension.ipynb` is a notebook looking into adding Youtube API data into the top popular recommender. `KNN_collab.ipynb` contains results of a KNN collaborative filtering model from surprise package and a pure collaborative filtering from LightFM.

**Templates**: Holds html files for the various endpoints.

**Testdata/raw**: These are fake datasets meant to be used with the test target.

**Docker**: Docker related files. See [here](https://github.com/amandashu/Workout_Recommender/blob/main/docker/README_DOCKER.md)

**Materials**: Contains pdfs for presentation slides and a report detailing our methods/implementations.

### Set Up Project Environment
There are two ways to run this project: a) Docker (preferred) or b) Locally <br>
a) To Run in Docker:<br>
  1) Pull the container with `docker pull nkanishka/workout-recommender`
  2) Run the container using:
  * General Use: `docker run -it -p 5000:5000 workout-recommender`
  * DSMLP Only: `launch.sh -i nkanishka/workout_recommender_dsmlp -c 4 -m 8` <br><br>`kubectl port-forward <Kubernates Cluster Name> 5000`<br><br> `ssh -N -L 5000:127.0.0.1:5000 <AD Name>@dsmlp-login.ucsd.edu`
  3) Inside container/cluster, type `cd Workout_Recommender`. Note that in the DSMLP environment, you will need to manually clone this repo.
  4) If using website, go to [localhost:5000](localhost:5000)
    <br>

b) To run locally, install requiremnents.txt into a virtualenv. Make sure you have Python 3.8+ and Pip installed.

### Run the Project Stages
- To get the data, run `python run.py data`. This scrapes the data and cleans the data and saves these files into `/data/raw` and `data/preprocessed` respectively.
  - Note: for scraping, this assumes that there is a file `config/chromedriver.json` that specifies where the path to the downloaded chromedriver.exe file for your Chrome version lies in the attribute `chromedriver_path`.
  - Note: for making requests to Youtube API, this assumes that there is a file `config/api_key.json` that specifies the api key in the `api_key` attribute.
- To run model results, run `python run.py model`. This takes in the preprocessed data, trains the models, and prints out the NDCG scores for each model.
- Standard target `clean` is implemented, and it will delete the `data` folder.
- Standard target `all` is implemented, and it equivalent to running `python run.py data model`.
- Standard target `test` is implemented, and runs the data preprocessing and modeling results on the test data. The purpose of this target is purely to check the implementation of the code.
- Use `python app.py` to run the app locally.
  - Note: this assumes that there is a file `config/db_config.json`, which has database host, user, password, and name information.
  - And a file `config/flask_keys.json` which has a Flask secret key
"
54,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_43,,,"# Recipe-Recommendation

## Introduction
This repository contains the code and models needed to create a recommender system for recipe recommendations. Included in the repository are a few simple baselines as well as the final model utilized by our recommender system for recipe recommendation. The data for this project was pulled from Kaggle (https://www.kaggle.com/shuyangli94/food-com-recipes-and-user-interactions) and (https://www.kaggle.com/kaggle/recipe-ingredients-dataset/home).

##  Files

For this project, we have files for running the code, retriving the data, and processing it into the desired output.

There are several files that will be obtained from Kaggle, but are not part of this repository. This is because they are data files and are too large to be version controlled.

The following files were created by us in order to create and run our baselines and final model.

> run.py: Passes in the location of the data folder. Runs etl on the Kaggle data and stores the data in the data output folder. Runs baselines and model on the dataset. Evaluates models to see how well the baselines did in comparison to the final model.

> mostPop.py: Baseline file that uses a top popular model to determine what users would rate certain recipes.

> randomFor.py: Baseline file that uses a random forest model to determine what cuisine type each recipe would fall under.

> conBased.py: Baseline file that uses a content based model with cosine similarity to determine what recipe to recommend based on what ingredients are listed by the user.

> requirements.txt: Contains the amount of processing resources recommended to run the files within a few hours each, and the packages needed and the versions that were used to run all the processes.

> Preliminary EDA.ipynb: Inside the notebook directory. Notebook containing the exploratory data analyses that was taken on the data to further understand and gain insight on the data we were using.

##  Directories

The following directories were created by us in order to be able to store and retain the necessary information needed for different purposes.

> config: Contains a list of all the config files that determines the parameters of each file. Use these files according to their use to change the parameters and change which subset of data you are running the processes on. Make sure you are changing the file paths correctly and throughout the entire config file.

> data: Contains the data retrieved after running the repository, and the cleaned and processed data by us.

> testData: Contains the randomly generated testData that we would run our models against to see how well they performed. Much smaller than full dataset, and allows for easy tracking to gain the most insight from how the model works.

## Running the Code
Prior to running the code, make sure that you install all the packages listed in *requirements.txt* 

In order to obtain the data, one can simply run the run.py python file with the command data or all. This will prompt the script to download the data and store it in the proper place for you.

### Creating the Data

To create the processed data, run this following command on the command line terminal:
```
python run.py data
```
Where the data will be returned into new files usable by our models in this project and placed in the data directory.

### Running all the model targets

If you want to run all of these together, run this following command on the command line terminal:
```
python run.py all
```
Where the all targets (excluding *test*) will run one after another.

### Testing all the model targets

To test how if the repository and all the models and scripts are working, run this following command on the command line terminal
```
python run.py test
```
Where the targets above will all be run one after another, but on small test data so that we can observe how the models and scripts are working.

## Repository Organization

To ensure the code runs properly, it must have the same folders and files locations. etl.py must be inside src and data folders, and data must be in the data folder. mostPop.py, must be in the src and baselines folders.
","# Recipe-Recommendation

## Introduction
This repository contains the code and models needed to create a recommender system for recipe recommendations. Included in the repository are a few simple baselines as well as the final model utilized by our recommender system for recipe recommendation. The data for this project was pulled from Kaggle (https://www.kaggle.com/shuyangli94/food-com-recipes-and-user-interactions) and (https://www.kaggle.com/kaggle/recipe-ingredients-dataset/home).

##  Files

For this project, we have files for running the code, retriving the data, and processing it into the desired output.

There are several files that will be obtained from Kaggle, but are not part of this repository. This is because they are data files and are too large to be version controlled.

The following files were created by us in order to create and run our baselines and final model.

> run.py: Passes in the location of the data folder. Runs etl on the Kaggle data and stores the data in the data output folder. Runs baselines and model on the dataset. Evaluates models to see how well the baselines did in comparison to the final model.

> mostPop.py: Baseline file that uses a top popular model to determine what users would rate certain recipes.

> randomFor.py: Baseline file that uses a random forest model to determine what cuisine type each recipe would fall under.

> conBased.py: Baseline file that uses a content based model with cosine similarity to determine what recipe to recommend based on what ingredients are listed by the user.

> requirements.txt: Contains the amount of processing resources recommended to run the files within a few hours each, and the packages needed and the versions that were used to run all the processes.

> Preliminary EDA.ipynb: Inside the notebook directory. Notebook containing the exploratory data analyses that was taken on the data to further understand and gain insight on the data we were using.

##  Directories

The following directories were created by us in order to be able to store and retain the necessary information needed for different purposes.

> config: Contains a list of all the config files that determines the parameters of each file. Use these files according to their use to change the parameters and change which subset of data you are running the processes on. Make sure you are changing the file paths correctly and throughout the entire config file.

> data: Contains the data retrieved after running the repository, and the cleaned and processed data by us.

> testData: Contains the randomly generated testData that we would run our models against to see how well they performed. Much smaller than full dataset, and allows for easy tracking to gain the most insight from how the model works.

## Running the Code
Prior to running the code, make sure that you install all the packages listed in *requirements.txt* 

In order to obtain the data, one can simply run the run.py python file with the command data or all. This will prompt the script to download the data and store it in the proper place for you.

### Creating the Data

To create the processed data, run this following command on the command line terminal:
```
python run.py data
```
Where the data will be returned into new files usable by our models in this project and placed in the data directory.

### Running all the model targets

If you want to run all of these together, run this following command on the command line terminal:
```
python run.py all
```
Where the all targets (excluding *test*) will run one after another.

### Testing all the model targets

To test how if the repository and all the models and scripts are working, run this following command on the command line terminal
```
python run.py test
```
Where the targets above will all be run one after another, but on small test data so that we can observe how the models and scripts are working.

## Repository Organization

To ensure the code runs properly, it must have the same folders and files locations. etl.py must be inside src and data folders, and data must be in the data folder. mostPop.py, must be in the src and baselines folders.
"
55,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_44,,,"# Makeup Recommender

This project is a recommender that will provide a one-stop shop experience where a user will get recommended an array of products to create an entire makeup look based on similar products that the user enjoys, products that similar users have purchased, as well as products that are personalized to the user including skin type, skin tone, allergies, and budget. Our project aims to utilize collaborative filtering recommendations along with content-based filtering to ensure user satisfaction and success when creating their desired look.


## How To Run

To install all dependencies, run the following command from the root directory of the project:
> ```pip install -r requirements.txt```

### Running the Project

To run the project, each command must start from the root directory of the project with:
> ```python run.py```

This base command can be modified with various different flags:
| Flag                | Type | Default Value             | Description                                                       |
|---------------------|------|---------------------------|-------------------------------------------------------------------|
| --item_data              |      |                      | Scrape item dataset from www.sephora.com.                              |
| --review_data              |      |                      | Scrape review dataset from www.sephora.com.                              |
| --features          |      |                           | Clean dataset and create features.                                |
| --model             |      |                           | Run model to create recommendations.                              |
| --accuracy          |      |                           | Evaluate accuracy of model.                                       |
| --test              |      |                           | Train model on a test dataset.                                    |
| --all               |      |                           | Train and evaluate accuracy of baseline model and our recommender model.      |


## Website

To visit the webpage copy and paste this URL into your browser: https://makeup-recommender.herokuapp.com/


## Document History

**Project Proposal**: https://docs.google.com/document/d/1bAXSUrQHcss8uU_eeqIJ4ewX3N-I8RLAjGJi77Zqw6c/edit

**Check-in**: https://docs.google.com/document/d/18rve8FbhRN8VXoO99ixB6nh96uWYtR-DNQMrjIwCr8Y/edit

**Report Checkpoint**: https://docs.google.com/document/d/1Xh3Ddskyy4niA7ZbzBUc9hoaR0bsrLKWgni9YzXTtZY/edit#

**Final Report**: https://docs.google.com/document/d/1WRj9ukUa-ozK3xtao-khCdradUQsR1k23tn4ELvQE3U/edit#

**Presentation Slides**: https://docs.google.com/presentation/d/1WYmy2IKTuVGE193Pq5t9v2BRTVCKxFHXy5zfLZN1K_I/edit?usp=sharing


## Credits

### For Usage of Scraping Script

https://github.com/jjone36/Cosmetic



### Responsibilities

**Alex Kim**:
* Scraped eye products and reviews
* Designed website
* Cleaned dataset
* Encode ingredient data
* Write rough draft of report (Abstract, Description of Data, Method, Metrics, Results)
* Deploy website on Heroku
* Use Flask to connect model to website (not implemented in final product)
* Fix UI with Streamlit
* Filter recommendations
* Edit report

**Justin Lee**:
* Fixed script to scrape data from Sephora
* Scraped lip products and reviews
* Coded data ingestion pipeline
* Incorporate run.py
* Code collaborative filtering model
* Create docker container
* Deploy website on Heroku
* Use Flask to connect model to website (not implemented in final product)
* Organize code and documentation

**Shayal Singh**:
* Fixed script to scrape data from Sephora
* Scraped face and cheek products and reviews
* Coded website layout
* Top popular baseline
* Write rough draft of report (Website, Conclusion)
* Deploy website on Heroku
* Use Flask to connect model to website (not implemented in final product)
* Display recommendations
* Visual presentation slides
* Fix UI with Streamlit
* Filter recommendations
* Finalize website
","# Makeup Recommender

This project is a recommender that will provide a one-stop shop experience where a user will get recommended an array of products to create an entire makeup look based on similar products that the user enjoys, products that similar users have purchased, as well as products that are personalized to the user including skin type, skin tone, allergies, and budget. Our project aims to utilize collaborative filtering recommendations along with content-based filtering to ensure user satisfaction and success when creating their desired look.


## How To Run

To install all dependencies, run the following command from the root directory of the project:
> ```pip install -r requirements.txt```

### Running the Project

To run the project, each command must start from the root directory of the project with:
> ```python run.py```

This base command can be modified with various different flags:
| Flag                | Type | Default Value             | Description                                                       |
|---------------------|------|---------------------------|-------------------------------------------------------------------|
| --item_data              |      |                      | Scrape item dataset from www.sephora.com.                              |
| --review_data              |      |                      | Scrape review dataset from www.sephora.com.                              |
| --features          |      |                           | Clean dataset and create features.                                |
| --model             |      |                           | Run model to create recommendations.                              |
| --accuracy          |      |                           | Evaluate accuracy of model.                                       |
| --test              |      |                           | Train model on a test dataset.                                    |
| --all               |      |                           | Train and evaluate accuracy of baseline model and our recommender model.      |


## Website

To visit the webpage copy and paste this URL into your browser: https://makeup-recommender.herokuapp.com/


## Document History

**Project Proposal**: https://docs.google.com/document/d/1bAXSUrQHcss8uU_eeqIJ4ewX3N-I8RLAjGJi77Zqw6c/edit

**Check-in**: https://docs.google.com/document/d/18rve8FbhRN8VXoO99ixB6nh96uWYtR-DNQMrjIwCr8Y/edit

**Report Checkpoint**: https://docs.google.com/document/d/1Xh3Ddskyy4niA7ZbzBUc9hoaR0bsrLKWgni9YzXTtZY/edit#

**Final Report**: https://docs.google.com/document/d/1WRj9ukUa-ozK3xtao-khCdradUQsR1k23tn4ELvQE3U/edit#

**Presentation Slides**: https://docs.google.com/presentation/d/1WYmy2IKTuVGE193Pq5t9v2BRTVCKxFHXy5zfLZN1K_I/edit?usp=sharing


## Credits

### For Usage of Scraping Script

https://github.com/jjone36/Cosmetic



### Responsibilities

**Alex Kim**:
* Scraped eye products and reviews
* Designed website
* Cleaned dataset
* Encode ingredient data
* Write rough draft of report (Abstract, Description of Data, Method, Metrics, Results)
* Deploy website on Heroku
* Use Flask to connect model to website (not implemented in final product)
* Fix UI with Streamlit
* Filter recommendations
* Edit report

**Justin Lee**:
* Fixed script to scrape data from Sephora
* Scraped lip products and reviews
* Coded data ingestion pipeline
* Incorporate run.py
* Code collaborative filtering model
* Create docker container
* Deploy website on Heroku
* Use Flask to connect model to website (not implemented in final product)
* Organize code and documentation

**Shayal Singh**:
* Fixed script to scrape data from Sephora
* Scraped face and cheek products and reviews
* Coded website layout
* Top popular baseline
* Write rough draft of report (Website, Conclusion)
* Deploy website on Heroku
* Use Flask to connect model to website (not implemented in final product)
* Display recommendations
* Visual presentation slides
* Fix UI with Streamlit
* Filter recommendations
* Finalize website
"
56,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_71,,,"# Opioid-Use Prevalance Analysis Project 
```
### 
* Author: Flory Huang
* Date: 03.19.2021
```

This repository contains code for extraction mentioned drug terms and emotions of drug use discussion in Reddit data. 
The code takes in reddit post that are discussing drugs and ontology list(RxNorm and Mesh),used to extract drug names. 
The emotion in reddit post will be analyzed.

The data_reddit.py takes care of loading data and formating data
The analysis.py conduct the similar matching to extract drug terms.
The emotion.py will analyze emotion in the reddit post
The model.py procude a report of matching result

The code can be excuted by 
    - python run.py test 
    - python run.py data 
    - python run.py analysis

Target explaination 
    0: test: test the whole process of code
    1: data： read in, text, and ontology data. And process them into format (e.g.parse nouns from text and put into a dictionary with ontology terms) that can be used for following analysis steps.
    2: analysis: use similar matching to extract drug terms and perform emotion analysis and store result into output csv files. 

","# Opioid-Use Prevalance Analysis Project 
```
### 
* Author: Flory Huang
* Date: 03.19.2021
```

This repository contains code for extraction mentioned drug terms and emotions of drug use discussion in Reddit data. 
The code takes in reddit post that are discussing drugs and ontology list(RxNorm and Mesh),used to extract drug names. 
The emotion in reddit post will be analyzed.

The data_reddit.py takes care of loading data and formating data
The analysis.py conduct the similar matching to extract drug terms.
The emotion.py will analyze emotion in the reddit post
The model.py procude a report of matching result

The code can be excuted by 
    - python run.py test 
    - python run.py data 
    - python run.py analysis

Target explaination 
    0: test: test the whole process of code
    1: data： read in, text, and ontology data. And process them into format (e.g.parse nouns from text and put into a dictionary with ontology terms) that can be used for following analysis steps.
    2: analysis: use similar matching to extract drug terms and perform emotion analysis and store result into output csv files. 

"
57,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_63,,,"# DSC180B-Capstone-Project
- dataset available for download at https://www.kaggle.com/crawford/gene-expression
  - #Make sure to unzip files into 'data/raw' folder#
- get data: in the command line enter `Rscript run-data.R data`
- analysis: in the command line enter `Rscript run-data.R analysis`
  - the resulting graphs will be in the data/out folder 

- data: contains the raw and cleaned versions of the datasets we're working with. Also will hold the graphs from analysis
- src: contains the analysis, cleaning, and data etl scripts.
  - analysis: golubAnalysis.R contains the script we used to do tests and generate plots
  - cleaning: golubCleaning.R contains the script we used to clean the raw datasets found in data/raw
  - data: etl.R contains the scirpt to extract the data for run-data.R
  
Acknowledgements
- Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression

  - Science 286:531-537. (1999). Published: 1999.10.14

  - T.R. Golub, D.K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J.P. Mesirov, H. Coller, M. Loh, J.R. Downing, M.A. Caligiuri, C.D. Bloomfield, and E.S. Lander
","# DSC180B-Capstone-Project
- dataset available for download at https://www.kaggle.com/crawford/gene-expression
  - #Make sure to unzip files into 'data/raw' folder#
- get data: in the command line enter `Rscript run-data.R data`
- analysis: in the command line enter `Rscript run-data.R analysis`
  - the resulting graphs will be in the data/out folder 

- data: contains the raw and cleaned versions of the datasets we're working with. Also will hold the graphs from analysis
- src: contains the analysis, cleaning, and data etl scripts.
  - analysis: golubAnalysis.R contains the script we used to do tests and generate plots
  - cleaning: golubCleaning.R contains the script we used to clean the raw datasets found in data/raw
  - data: etl.R contains the scirpt to extract the data for run-data.R
  
Acknowledgements
- Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression

  - Science 286:531-537. (1999). Published: 1999.10.14

  - T.R. Golub, D.K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J.P. Mesirov, H. Coller, M. Loh, J.R. Downing, M.A. Caligiuri, C.D. Bloomfield, and E.S. Lander
"
58,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_77,,,"# DSC180B_A07

This repository contains the completed codes for this project that focuses on multiple testing. The src folder contains the main python scripts, with all the functions inside, and the data folder, where all the datasets are held. 


The function will return the eda statistic, various plots that show the differences between the healthy people and cardial patients

The congfig folder contains one json files for the use of functions' parameters. 

URL to our webpage: https://larryzly.github.io/CardiovascularClassifier/
```
### Responsibilities

All the work and code are produced after our group discussion.
* Wentaon Chen developed the run.py and found out how to set the paths for the scripts
* Leyang Zhang developed some functions of model.py and upload the reports and datasets
* Zimin Dai developed functions in model.py and built the structure for the repository, docker image
```

","# DSC180B_A07

This repository contains the completed codes for this project that focuses on multiple testing. The src folder contains the main python scripts, with all the functions inside, and the data folder, where all the datasets are held. 


The function will return the eda statistic, various plots that show the differences between the healthy people and cardial patients

The congfig folder contains one json files for the use of functions' parameters. 

URL to our webpage: https://larryzly.github.io/CardiovascularClassifier/
```
### Responsibilities

All the work and code are produced after our group discussion.
* Wentaon Chen developed the run.py and found out how to set the paths for the scripts
* Leyang Zhang developed some functions of model.py and upload the reports and datasets
* Zimin Dai developed functions in model.py and built the structure for the repository, docker image
```

"
59,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_29,,,"

# Using Epidemiology Model To Predict Case Numbers for COVID-19

## Table of contents
* [General info](#general-info)
* [Technologies](#technologies)
* [Setup](#setup)
* [Directions](#directions)
* [Processing](#in_processing)
## General info
- Use covid-19 datasets provided by JHU to fit epidemiology model to U.S.. After figuring out the infection parameter, we can then predict 

## Introduction
Fitting Epidemiology Model with Covid-19 JHU U.S. Data
## Technology
Project is created with:
* Image : https://hub.docker.com/repository/docker/caw062/test
## Setup
- Before running, use `pip install -r requirements.txt` to install all the required packages
- on terminal, run `python run.py data` to retrieve the most current data from JHU & Apple Data

## Directions
`python run.py test` to first download test data, and then build epidemiology model on the test data. 
It will return the beta (infection rate), and D (infection duration) for the entire United States.
It will also return a prediction for counties in Southern California on 1/22/2021 based on case counts on 1/21/2021 (previous day)
","

# Using Epidemiology Model To Predict Case Numbers for COVID-19

## Table of contents
* [General info](#general-info)
* [Technologies](#technologies)
* [Setup](#setup)
* [Directions](#directions)
* [Processing](#in_processing)
## General info
- Use covid-19 datasets provided by JHU to fit epidemiology model to U.S.. After figuring out the infection parameter, we can then predict 

## Introduction
Fitting Epidemiology Model with Covid-19 JHU U.S. Data
## Technology
Project is created with:
* Image : https://hub.docker.com/repository/docker/caw062/test
## Setup
- Before running, use `pip install -r requirements.txt` to install all the required packages
- on terminal, run `python run.py data` to retrieve the most current data from JHU & Apple Data

## Directions
`python run.py test` to first download test data, and then build epidemiology model on the test data. 
It will return the beta (infection rate), and D (infection duration) for the entire United States.
It will also return a prediction for counties in Southern California on 1/22/2021 based on case counts on 1/21/2021 (previous day)
"
60,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_22,,,"# Political Analysis on Senatorial Twitter Account Using Machine Learning
 
### Project Description
The modern American political landscape often seems void of bipartisanship. Nowhere is this stark divide between red and blue more evident than in the halls of the US Capitol, where the Senate and House of Representatives convene to carry out the duties of the legislative branch. While us average Americans rarely watch the daily proceedings of the Senate or House, Twitter has given us a unique window into the debates and discourse that shape our democracy. In fact, the 116th Congress, which served from January 3, 2019 to January 3, 2021 broke records by tweeting a total of 2.3 million tweets! As such, it is clear that Twitter is quickly becoming a digital public forum for American politicians. This surplus of tweets from the 116th Congress enables us to analyze the Twitter (following-follower) relationships between politicians on and across the two sides of the aisle. This project’s main inquiry is into whether there is a tangible difference in the way that Democrat members of Congress speak and interact on social media in comparison to Republican members of Congress. If there are such differences, this project will leverage them to train a suitable ML model on this data for node classification. That is to say, this project aims to determine a Senator’s political affiliation based off of a) their Twitter relationships to other Senators b) their speech patterns, and c) other mine-able features on Twitter. In order to truly utilize the complex implicit relationships hidden in the Twitter graph, we can use models such as Graph Convolutional Networks, which apply the concept of “convolutions” from CNNs to a graph network-oriented framework, and GraphSage model.

### run.py
We implement the GCN and GraphSage models as our main models for training and comparison.

- parameters:
  - model: The choice of models. We only implement the GCN and GraphSage. 
  - dataset: The choice of datasets. There are multiple datasets, including data_voting, data_voting_senti. The differences between these datasets are features. The adjacency matrix of each dataset stays the same.
  - output_path: The output of project will be stored in json file.
  - agg_func: The choice of aggregated function in the graphSage model. We only support MEAN aggregator. The default is MEAN.
  - num_neigh: The number of neighbors in the graphSage model. The default is 10.
  - n: The number of hidden layers in the GCN model. This can be tuned to reach higher accuracy.
  - self_weight: The weight of self-loop in the GCN model.
  - hidden_neurons: The number of hidden neurons in the GCN model. The default is 200 and it can be tuned to reach higher accuracy.
  - device: The device for training the model. We only support cuda.
  - epochs: The number of epochs for both models. The default is 200 epochs.
  - lr: The learning rate for both models. The default is 1e-4. This can be tuned for higher accuracy.
  - val_size: The size of testing data. The default is 0.3.
  - test: The parameter for running test data on models.

- some examples for using the project:
  - python run.py
  - python run.py --test
  - python run.py --n_GCN
  - python run.py --dataset data_voting
  - python run.py --model n_GCN --n 2 --self_weight 20

  

# Our Project Website

Please view our website [here](https://anuragpamuru.github.io/dsc-180b-capstone-b03/)


### Contributers: 
Yimei Zhao, Anurag Pamuru, Yueting Wu
","# Political Analysis on Senatorial Twitter Account Using Machine Learning
 
### Project Description
The modern American political landscape often seems void of bipartisanship. Nowhere is this stark divide between red and blue more evident than in the halls of the US Capitol, where the Senate and House of Representatives convene to carry out the duties of the legislative branch. While us average Americans rarely watch the daily proceedings of the Senate or House, Twitter has given us a unique window into the debates and discourse that shape our democracy. In fact, the 116th Congress, which served from January 3, 2019 to January 3, 2021 broke records by tweeting a total of 2.3 million tweets! As such, it is clear that Twitter is quickly becoming a digital public forum for American politicians. This surplus of tweets from the 116th Congress enables us to analyze the Twitter (following-follower) relationships between politicians on and across the two sides of the aisle. This project’s main inquiry is into whether there is a tangible difference in the way that Democrat members of Congress speak and interact on social media in comparison to Republican members of Congress. If there are such differences, this project will leverage them to train a suitable ML model on this data for node classification. That is to say, this project aims to determine a Senator’s political affiliation based off of a) their Twitter relationships to other Senators b) their speech patterns, and c) other mine-able features on Twitter. In order to truly utilize the complex implicit relationships hidden in the Twitter graph, we can use models such as Graph Convolutional Networks, which apply the concept of “convolutions” from CNNs to a graph network-oriented framework, and GraphSage model.

### run.py
We implement the GCN and GraphSage models as our main models for training and comparison.

- parameters:
  - model: The choice of models. We only implement the GCN and GraphSage. 
  - dataset: The choice of datasets. There are multiple datasets, including data_voting, data_voting_senti. The differences between these datasets are features. The adjacency matrix of each dataset stays the same.
  - output_path: The output of project will be stored in json file.
  - agg_func: The choice of aggregated function in the graphSage model. We only support MEAN aggregator. The default is MEAN.
  - num_neigh: The number of neighbors in the graphSage model. The default is 10.
  - n: The number of hidden layers in the GCN model. This can be tuned to reach higher accuracy.
  - self_weight: The weight of self-loop in the GCN model.
  - hidden_neurons: The number of hidden neurons in the GCN model. The default is 200 and it can be tuned to reach higher accuracy.
  - device: The device for training the model. We only support cuda.
  - epochs: The number of epochs for both models. The default is 200 epochs.
  - lr: The learning rate for both models. The default is 1e-4. This can be tuned for higher accuracy.
  - val_size: The size of testing data. The default is 0.3.
  - test: The parameter for running test data on models.

- some examples for using the project:
  - python run.py
  - python run.py --test
  - python run.py --n_GCN
  - python run.py --dataset data_voting
  - python run.py --model n_GCN --n 2 --self_weight 20

  

# Our Project Website

Please view our website [here](https://anuragpamuru.github.io/dsc-180b-capstone-b03/)


### Contributers: 
Yimei Zhao, Anurag Pamuru, Yueting Wu
"
61,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_20,,,"# GNN-on-3d-points

### Abstract:
   This research focuses on 3D shape classification. Our goal is to predict the category of shapes consisting of 3D data points. We aim to implement Graph Neural Network models and compare the performances with PointNet, a popular architecture for 3d points cloud classification tasks. Not only will we compare standard metrics such as accuracy and confusion matrix, we will also explore the model's resilience of data transformation. Besides, we also tried combining PointNet with graph pooling layers.
   
   
See project website here: https://ctwayen.github.io/Graph-Neural-Network-on-3D-Points/

Docker name: ctwayen/project_docker

Docker web path: https://hub.docker.com/repository/docker/ctwayen/project_docker

### Instruction:

   If it is the first time you running this project, please download the data through python run.py all --mode download; You could also use the parameter --method to choose knn or fix-radius to construct the graph you like. Their corresponding parameters are --k and --r.The raw dataset would automatically download into the path 'data/modelnet/ModelNet40'. The points data is stored in 'data/modelnet/modelnet_points'. The consturcted graph trainning data is in 'data/modelnet/modelnet_(knn/fix_radius)(k/r){your param}'. Do not move those files. It may cause problems
   
   We support training Pointnet and GCN two models.Using --model to choose which one you want to train: GCN or pointNet.
   
   Shared paramaters are (default values are the best combination we found):
   
   --lr：Learning rate
   
   --bs: batch size
   
   --base: dataset path. If you are using default data, you do not need to specify this.Otherwise, write the graph dataset you just constructed
   
   --data: 10 or 40. Choose 10 to run 10 categories classfication and 40 for 40 categories
   
   --epoch: epoch
   
   --val_size: validation size. For example, 0.2 will have 20 % data as validation data
   
   --model_path: The path to store trained_model. Default is 'trained_models'
   
   --ouput_path: The path to store the ouput csv file
   
   Parameters that only used in GCN:
   
   --pool: Which pooling to use. SAG or ASA
   
   --ratio: The pooling ratio. For example, 0.4 will pool out 60% of data each time
   
#### Important! Training GCN will take about 7-10 minutes for one epoch. Training PointNet will take about 50s for one epoch. Please manage your time for training process.

   
### Notebooks and results:

   Besides training your own models, we also offered few trained models. You could check notebooks/Analyzing results to see our training results for different hyperparamters setting. You could also check how to use a trained_model there.
   
   You could also check the data augmentation's effects on models in the notebook/analyzing resistence file
   
   You could check how pooling layer affect result in the notebook/analyzing pooling file

Author: @Xinrui Zhan.If you find any bug, contact me through the email: ctwayen@outlook.com 
   
   
","# GNN-on-3d-points

### Abstract:
   This research focuses on 3D shape classification. Our goal is to predict the category of shapes consisting of 3D data points. We aim to implement Graph Neural Network models and compare the performances with PointNet, a popular architecture for 3d points cloud classification tasks. Not only will we compare standard metrics such as accuracy and confusion matrix, we will also explore the model's resilience of data transformation. Besides, we also tried combining PointNet with graph pooling layers.
   
   
See project website here: https://ctwayen.github.io/Graph-Neural-Network-on-3D-Points/

Docker name: ctwayen/project_docker

Docker web path: https://hub.docker.com/repository/docker/ctwayen/project_docker

### Instruction:

   If it is the first time you running this project, please download the data through python run.py all --mode download; You could also use the parameter --method to choose knn or fix-radius to construct the graph you like. Their corresponding parameters are --k and --r.The raw dataset would automatically download into the path 'data/modelnet/ModelNet40'. The points data is stored in 'data/modelnet/modelnet_points'. The consturcted graph trainning data is in 'data/modelnet/modelnet_(knn/fix_radius)(k/r){your param}'. Do not move those files. It may cause problems
   
   We support training Pointnet and GCN two models.Using --model to choose which one you want to train: GCN or pointNet.
   
   Shared paramaters are (default values are the best combination we found):
   
   --lr：Learning rate
   
   --bs: batch size
   
   --base: dataset path. If you are using default data, you do not need to specify this.Otherwise, write the graph dataset you just constructed
   
   --data: 10 or 40. Choose 10 to run 10 categories classfication and 40 for 40 categories
   
   --epoch: epoch
   
   --val_size: validation size. For example, 0.2 will have 20 % data as validation data
   
   --model_path: The path to store trained_model. Default is 'trained_models'
   
   --ouput_path: The path to store the ouput csv file
   
   Parameters that only used in GCN:
   
   --pool: Which pooling to use. SAG or ASA
   
   --ratio: The pooling ratio. For example, 0.4 will pool out 60% of data each time
   
#### Important! Training GCN will take about 7-10 minutes for one epoch. Training PointNet will take about 50s for one epoch. Please manage your time for training process.

   
### Notebooks and results:

   Besides training your own models, we also offered few trained models. You could check notebooks/Analyzing results to see our training results for different hyperparamters setting. You could also check how to use a trained_model there.
   
   You could also check the data augmentation's effects on models in the notebook/analyzing resistence file
   
   You could check how pooling layer affect result in the notebook/analyzing pooling file

Author: @Xinrui Zhan.If you find any bug, contact me through the email: ctwayen@outlook.com 
   
   
"
62,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_19,,,"# Graph-based Product Recommendation
DSC180B Capstone Project on Graph Data Analysis

Project Website: https://nhtsai.github.io/graph-rec/

## Project
Amazon Product Recommendation using a graph neural network approach.

### Requirements
- dask
- pandas
- torch
- torchtext
- dgl

## Data
### Datasets
Amazon Product Dataset from Professor Julian McAuley ([link](http://jmcauley.ucsd.edu/data/amazon/links.html))
* Product Reviews (5-core)
* Product Metadata
* Product Image Features

## GraphSAGE Model

## PinSAGE

### Graph & Features
The graph is a heterogeneous, bipartite user-product graph, connected by reviews.
 * Product Nodes (`ASIN`)
   * Features: `title`, `price`, image representation
 * User Nodes (`reviewerID`)
 * Edges (`user`, `reviewed`, `product`) and (`product`, `reviewed-by`, `user`)
   * Features: `helpful`, `overall`

### Data Configuration (`config/data-params.json`)

### Model
We use an unsupervised PinSage model (adapted from [DGL](https://github.com/dmlc/dgl/tree/master/examples/pytorch/pinsage)).

### Model Configuration (`config/pinsage-model-params.json`)
- `name`: model configuration name
- `random-walk-length`: maximum number traversals for a single random walk, `default: 2`
- `random-walk-restart-prob`: termination probability after each random walk traversal, `default: 0.5`
- `num-random-walks`:  number of random walks to try for each given node, `default: 10`
- `num-neighbors`: number of neighbors to select for each given node, `default: 3`
- `num-layers`: number of sampling layers, `default: 2`
- `hidden-dims`: dimension of product embedding, `default: 64 or 128`
- `batch-size`: batch size, `default: 64`
- `num-epochs`: number of training epochs, `default: 500`
- `batches-per-epoch`: number of batches per training epoch, `default: 512`
- `num-workers`: number of workers, `default: 3 or (#cores - 1)
- `lr`: learning rate, `default: 3e-4`
- `k`: number of recommendations, `default: 500`
- `model-dir`: directory of existing model to continue training
- `existing-model`: filename of existing model to continue training, `default: null`
- `id-as-features`: use id as features, makes model transductive
- `eval-freq`: evaluates model on validation set when `epoch % eval-freq == 0`, also evaluates model after last training epoch
- `save-freq`: saves model when `epoch % save-freq == 0`, also saves model after last training epoch

## References
* [GraphSAGE Homepage](http://snap.stanford.edu/graphsage/)
* [GraphSAGE Research Paper](https://arxiv.org/abs/1706.02216)
* [PinSage Article](https://medium.com/pinterest-engineering/pinsage-a-new-graph-convolutional-neural-network-for-web-scale-recommender-systems-88795a107f48)
* [PinSage Research Paper](https://arxiv.org/abs/1806.01973)
","# Graph-based Product Recommendation
DSC180B Capstone Project on Graph Data Analysis

Project Website: https://nhtsai.github.io/graph-rec/

## Project
Amazon Product Recommendation using a graph neural network approach.

### Requirements
- dask
- pandas
- torch
- torchtext
- dgl

## Data
### Datasets
Amazon Product Dataset from Professor Julian McAuley ([link](http://jmcauley.ucsd.edu/data/amazon/links.html))
* Product Reviews (5-core)
* Product Metadata
* Product Image Features

## GraphSAGE Model

## PinSAGE

### Graph & Features
The graph is a heterogeneous, bipartite user-product graph, connected by reviews.
 * Product Nodes (`ASIN`)
   * Features: `title`, `price`, image representation
 * User Nodes (`reviewerID`)
 * Edges (`user`, `reviewed`, `product`) and (`product`, `reviewed-by`, `user`)
   * Features: `helpful`, `overall`

### Data Configuration (`config/data-params.json`)

### Model
We use an unsupervised PinSage model (adapted from [DGL](https://github.com/dmlc/dgl/tree/master/examples/pytorch/pinsage)).

### Model Configuration (`config/pinsage-model-params.json`)
- `name`: model configuration name
- `random-walk-length`: maximum number traversals for a single random walk, `default: 2`
- `random-walk-restart-prob`: termination probability after each random walk traversal, `default: 0.5`
- `num-random-walks`:  number of random walks to try for each given node, `default: 10`
- `num-neighbors`: number of neighbors to select for each given node, `default: 3`
- `num-layers`: number of sampling layers, `default: 2`
- `hidden-dims`: dimension of product embedding, `default: 64 or 128`
- `batch-size`: batch size, `default: 64`
- `num-epochs`: number of training epochs, `default: 500`
- `batches-per-epoch`: number of batches per training epoch, `default: 512`
- `num-workers`: number of workers, `default: 3 or (#cores - 1)
- `lr`: learning rate, `default: 3e-4`
- `k`: number of recommendations, `default: 500`
- `model-dir`: directory of existing model to continue training
- `existing-model`: filename of existing model to continue training, `default: null`
- `id-as-features`: use id as features, makes model transductive
- `eval-freq`: evaluates model on validation set when `epoch % eval-freq == 0`, also evaluates model after last training epoch
- `save-freq`: saves model when `epoch % save-freq == 0`, also saves model after last training epoch

## References
* [GraphSAGE Homepage](http://snap.stanford.edu/graphsage/)
* [GraphSAGE Research Paper](https://arxiv.org/abs/1706.02216)
* [PinSage Article](https://medium.com/pinterest-engineering/pinsage-a-new-graph-convolutional-neural-network-for-web-scale-recommender-systems-88795a107f48)
* [PinSage Research Paper](https://arxiv.org/abs/1806.01973)
"
63,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_18,,,"# NBA-Game-Prediction
Project Group: MengYuan Shi, Austin Le

This repository contains a data science project that discover the NBA Game Prediction. We investigate the social network for individual NBA players and the relationship between each team. We will use team's statistics and players' statistics and analysis for predicting who wins the games by leveraging the team's statistics and players' statistics from 2015 season to 2019 season. We will use GraphSAGE which is a generalizable embedding framework to create a graph classification.


### Warning
Our group has altered and used the graphsage implementation that can be received from https://github.com/williamleif/GraphSAGE . We made minor changes within the model and inputs to align with the goals of our project, but we would like to cite them as a source for the main graphsage implementation.


### Running the project
- `python run.py` can be run from the command line to ingest data, train a model, and present relevant statistics for model performance to the shell
  - Reads in CSV file from eightthirtyfour for play by play data
  - Runs algorithm to create network between each player based on their playing time
  - Appends all player edges from all season onto a single graph 
  - Embedd player categorical statistics onto each node 
  - runs graphSage model to learn over features

### Outputs
  - The outputs printed will be the corresponding accuracies obtained after the training
  - ~5min runtime

### Responsibility 
- Austin Le: Responsible for the data cleaning and data scraping and the coding part as well as the report.
- MengYuan Shi: Responsible for the paper researching and writing the report part as well as the visualization.
","# NBA-Game-Prediction
Project Group: MengYuan Shi, Austin Le

This repository contains a data science project that discover the NBA Game Prediction. We investigate the social network for individual NBA players and the relationship between each team. We will use team's statistics and players' statistics and analysis for predicting who wins the games by leveraging the team's statistics and players' statistics from 2015 season to 2019 season. We will use GraphSAGE which is a generalizable embedding framework to create a graph classification.


### Warning
Our group has altered and used the graphsage implementation that can be received from https://github.com/williamleif/GraphSAGE . We made minor changes within the model and inputs to align with the goals of our project, but we would like to cite them as a source for the main graphsage implementation.


### Running the project
- `python run.py` can be run from the command line to ingest data, train a model, and present relevant statistics for model performance to the shell
  - Reads in CSV file from eightthirtyfour for play by play data
  - Runs algorithm to create network between each player based on their playing time
  - Appends all player edges from all season onto a single graph 
  - Embedd player categorical statistics onto each node 
  - runs graphSage model to learn over features

### Outputs
  - The outputs printed will be the corresponding accuracies obtained after the training
  - ~5min runtime

### Responsibility 
- Austin Le: Responsible for the data cleaning and data scraping and the coding part as well as the report.
- MengYuan Shi: Responsible for the paper researching and writing the report part as well as the visualization.
"
64,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_21,,,"# DSC180B_Project

URL: https://sdevinl.github.io/DSC180B_Project/

**Disclaimer:**   
  We want to make it clear that the graphsage implentation found in this repo is not our own. We have made minor alterations to the code in order to better serve our overall project in regards to NBA team rankings. The original graphsage implementation can be found here https://github.com/williamleif/GraphSAGE . We would also like to cite their paper:
  
     @inproceedings{hamilton2017inductive,
	     author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	     title = {Inductive Representation Learning on Large Graphs},
	     booktitle = {NIPS},
	     year = {2017}
	   }
  For more information on how to run graphsage as well as the requirements for grapshage be sure to checkout the original graphsage's implementation.

**About:**  
  This repository contains an implementation of a GraphSAGE for node classification on an NBA dataset. The goal being able to classify the ranks of NBA teams using player stats and a graph representation of matchups between teams in a season. 
  
**Setting Up Docker Image**  
  The docker image that was created in order to have an environment able to run this project is found on the repo at aubarrio/graphsage . 
    
**Model**  
  data: The data we use in this project is a compound of multiple webscraped data found on https://www.basketball-reference.com we used stats such as player and team stats, along with team schedules for the season. The seasons for which we collected data range from 2011 to the 2019 season.  

**Basic Parameters**  
  Since we are in early stages of developing we only have one parameter of choice and that is [train, test]. This is to distinguish the data being input into the model.  
    train: Parameter train will train the model on all available features (181 different features)  
    test: Parameter test will train the model on 2 features (Rank and Id) which is meant to use as an evaluation of how our data is performing  
    
**Examples run.py**  
  python run.py test  
  python run.py  
  
  The run.py file will run a graphsage model with the mean aggregator on our NBA data. For more information on how to change the overall model or change the parameters of a model refer to the original graphsage implementation.
  
**Output**  
  Direct terminal output outlining the training, validation and test accuracies of our model.  
  In the sage_outputs folder you will find files that contain data on the model:
  	The rawOutputs folder contains a json file that includes the raw output probabilites of our NBA test set.
	The stats folder will contain all accuracies and losses captured whilst training your model.
	
**Acknowledgements**   
  As mentioned in the disclaimer the original version of this code can be found here https://github.com/williamleif/GraphSAGE. An appreciation to the original creators of this code and a thank you for allowing this code to be available and ready to use on projects such as ours. 
","# DSC180B_Project

URL: https://sdevinl.github.io/DSC180B_Project/

**Disclaimer:**   
  We want to make it clear that the graphsage implentation found in this repo is not our own. We have made minor alterations to the code in order to better serve our overall project in regards to NBA team rankings. The original graphsage implementation can be found here https://github.com/williamleif/GraphSAGE . We would also like to cite their paper:
  
     @inproceedings{hamilton2017inductive,
	     author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	     title = {Inductive Representation Learning on Large Graphs},
	     booktitle = {NIPS},
	     year = {2017}
	   }
  For more information on how to run graphsage as well as the requirements for grapshage be sure to checkout the original graphsage's implementation.

**About:**  
  This repository contains an implementation of a GraphSAGE for node classification on an NBA dataset. The goal being able to classify the ranks of NBA teams using player stats and a graph representation of matchups between teams in a season. 
  
**Setting Up Docker Image**  
  The docker image that was created in order to have an environment able to run this project is found on the repo at aubarrio/graphsage . 
    
**Model**  
  data: The data we use in this project is a compound of multiple webscraped data found on https://www.basketball-reference.com we used stats such as player and team stats, along with team schedules for the season. The seasons for which we collected data range from 2011 to the 2019 season.  

**Basic Parameters**  
  Since we are in early stages of developing we only have one parameter of choice and that is [train, test]. This is to distinguish the data being input into the model.  
    train: Parameter train will train the model on all available features (181 different features)  
    test: Parameter test will train the model on 2 features (Rank and Id) which is meant to use as an evaluation of how our data is performing  
    
**Examples run.py**  
  python run.py test  
  python run.py  
  
  The run.py file will run a graphsage model with the mean aggregator on our NBA data. For more information on how to change the overall model or change the parameters of a model refer to the original graphsage implementation.
  
**Output**  
  Direct terminal output outlining the training, validation and test accuracies of our model.  
  In the sage_outputs folder you will find files that contain data on the model:
  	The rawOutputs folder contains a json file that includes the raw output probabilites of our NBA test set.
	The stats folder will contain all accuracies and losses captured whilst training your model.
	
**Acknowledgements**   
  As mentioned in the disclaimer the original version of this code can be found here https://github.com/williamleif/GraphSAGE. An appreciation to the original creators of this code and a thank you for allowing this code to be available and ready to use on projects such as ours. 
"
65,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_76,,,"Name: Jason Chau, Sung-Lin Chang, Dylan Loe

Welcome to our Stock Predictor. 

IMPORTANT!
For running actual models:

In order to run our stock predictor, just make sure that you are in the current directory and run the command 

python run.py all - this will let you scrape the data, as well as running our model


python run.py test - this will let you run our model on the data pulled from the webscraper, as well as predicting which stocks will
be bullish or bearish in Dow Jones 30.

python run.py fcn - this will let you run our Fully connected network model on the data pulled from the webscraper

python run.py build - This command builds the test portion of the code. Please keep in mind this will pull in data from the yahoo finance api,
so it will require an internet connection to pull in the data and calculate it.


------ CONFIG FILE--------------

""NUM_EPOCHS"" : 100, ( this is the number of trials you want to use)
""LEARNING_RATE"" : 0.001, ( this is the learning rate)
""NUM_HIDDEN"" : 32, ( number of hidden features)
""num_days"": 5, (number of lag days you will have, we chose 5 because there are 5 trading days)
""nfeat"" : 20, ( this is 4 * num_days)
""nclass"" : 1, 
""dataset"" : ""./data/dowJonescorrelation0.4graph.csv"", (this is the correlation graph we use for our model)
""thresh"" : 0.4, ( the threshold for the node adjacency)
""filepath"" : ""./data/12modowJonesData/"", (the dataset to use)
""timeframe"" : ""12mo"" ( which time period, there is 12mo and 6 mo

Required packages

yfinance == 0.1.55
pandas-datareader = 0.9.0
beautifulsoup4 4.9.3
tensorflow == 1.12.0
networkx == 2.1
numpy == 1.14.3
scipy == 1.1.0
sklearn == 0.19.1
matplotlib == 2.2.2
","Name: Jason Chau, Sung-Lin Chang, Dylan Loe

Welcome to our Stock Predictor. 

IMPORTANT!
For running actual models:

In order to run our stock predictor, just make sure that you are in the current directory and run the command 

python run.py all - this will let you scrape the data, as well as running our model


python run.py test - this will let you run our model on the data pulled from the webscraper, as well as predicting which stocks will
be bullish or bearish in Dow Jones 30.

python run.py fcn - this will let you run our Fully connected network model on the data pulled from the webscraper

python run.py build - This command builds the test portion of the code. Please keep in mind this will pull in data from the yahoo finance api,
so it will require an internet connection to pull in the data and calculate it.


------ CONFIG FILE--------------

""NUM_EPOCHS"" : 100, ( this is the number of trials you want to use)
""LEARNING_RATE"" : 0.001, ( this is the learning rate)
""NUM_HIDDEN"" : 32, ( number of hidden features)
""num_days"": 5, (number of lag days you will have, we chose 5 because there are 5 trading days)
""nfeat"" : 20, ( this is 4 * num_days)
""nclass"" : 1, 
""dataset"" : ""./data/dowJonescorrelation0.4graph.csv"", (this is the correlation graph we use for our model)
""thresh"" : 0.4, ( the threshold for the node adjacency)
""filepath"" : ""./data/12modowJonesData/"", (the dataset to use)
""timeframe"" : ""12mo"" ( which time period, there is 12mo and 6 mo

Required packages

yfinance == 0.1.55
pandas-datareader = 0.9.0
beautifulsoup4 4.9.3
tensorflow == 1.12.0
networkx == 2.1
numpy == 1.14.3
scipy == 1.1.0
sklearn == 0.19.1
matplotlib == 2.2.2
"
66,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_0,,,"# Political Popularity of Misinformation
- This project looks at the popularity and influence of politicians on Twitter by analyzing the engagement ratios as well as the rolling and cumulative maxes of likes and retweets over time.

### Note
- To get the data necessary to replicate this project, access to the Twitter API is needed. 
- You must have configured twarc using `twarc configure` in the terminal with your API credentials in order to run the data pipeline. Additional information on how to do so can be found here, https://github.com/DocNow/twarc.
- In addition, to run the data pipeline you must obtain a bearer token from Twitter’s API and store it in a config.py file in the root directory. Information on using and generating a bearer token can be found here, https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens. bearer_token = “...”

### Obtaining the txt files
- We obtain the tweet IDs that compose our politicians’ timeline found in `src/data` from George Washington University’s TweetSets database found here, https://tweetsets.library.gwu.edu/datasets.
- We chose to focus on politicians who served in the 116th United States Congress, which corresponds to two datasets, Congress: Representatives of the 116th Congress and Congress: Senators of the 116th Congress.
- After identifying our politicians, we gathered the user IDs for their Twitter accounts using Tweepy, which are then used to query the two Congress datasets. The datasets also contain a file of the House and Senate members along with their user IDs which is an alternative way to obtain these IDs. The files can be found here, [Senate](https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/MBOJNS/8VQVWT&version=1.0) and [Representative](https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/MBOJNS/WXZE5N&version=1.0).
- To query the datasets, for each politician, we selected either the Representative or Senator dataset depending on their position and inputted their user ID in the “Contains any user id” box under the “Posted by” section. This process gives us a txt file of tweet IDs for each politician which is then stored in the `src/data/misinformation` and `src/data/scientific` folders depending on the group the politician is assigned to. 

### Using `run.py`
- To get the data, from the project root directory, run `python run.py data`
    * This downloads the data using the tweet IDs found in `src/data/misinformation` and `src/data/scientific`.
    * This also downloads the engagement metrics needed for ratio analysis using the same tweet IDs.
    * The politicians to analyze can be specified in `config/data-params.json`.
    * The name of the txt file containing the tweet IDs must match the name specified in `config/data-params.json`.
    * The output is a json file for each politician containing their collection of tweets as well as a csv file containing the engagement metrics and text of the tweet ID.

- To calculate the ratios for the tweets, from the project root directory, run `python run.py ratio`
    * This calculates the ratios for the tweets found in the csv files in `src/data/misinformation` and `src/data/scientific`.
    * The politicians to analyze can be specified in `config/data-params.json`.
    * Output is an updated csv file for each politician containing the engagement metrics of their tweets and their ratio values.

- To calculate the popularity estimate metrics, from the projecy root directory, run `python run.py metrics`
    * This will create a JSON file for each estimate metric you wish to analyze. 
    * The outputs are stored in `src/out`.
    * These JSON files will then be used to create visualizations using the visualization target.

- To create the visualizations, from the project root directory, run `python run.py visualization`
    * This creates visualizations from the JSON files created from the metrics target.
    * The outputs are stored in `src/out`.

- To run the permutation test on our groups, from the project root directory, run `python run.py permute`
    * This will run a permutation test within our two groups as well as between our groups.
    * The outputs are stored in `src/out`.

- To run all of the above targets, from the project root directory, run `python run.py all`

- To run the test target, from the project root directory, run `python run.py test`
    * This runs most of the above targets on a small, fake dataset.","# Political Popularity of Misinformation
- This project looks at the popularity and influence of politicians on Twitter by analyzing the engagement ratios as well as the rolling and cumulative maxes of likes and retweets over time.

### Note
- To get the data necessary to replicate this project, access to the Twitter API is needed. 
- You must have configured twarc using `twarc configure` in the terminal with your API credentials in order to run the data pipeline. Additional information on how to do so can be found here, https://github.com/DocNow/twarc.
- In addition, to run the data pipeline you must obtain a bearer token from Twitter’s API and store it in a config.py file in the root directory. Information on using and generating a bearer token can be found here, https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens. bearer_token = “...”

### Obtaining the txt files
- We obtain the tweet IDs that compose our politicians’ timeline found in `src/data` from George Washington University’s TweetSets database found here, https://tweetsets.library.gwu.edu/datasets.
- We chose to focus on politicians who served in the 116th United States Congress, which corresponds to two datasets, Congress: Representatives of the 116th Congress and Congress: Senators of the 116th Congress.
- After identifying our politicians, we gathered the user IDs for their Twitter accounts using Tweepy, which are then used to query the two Congress datasets. The datasets also contain a file of the House and Senate members along with their user IDs which is an alternative way to obtain these IDs. The files can be found here, [Senate](https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/MBOJNS/8VQVWT&version=1.0) and [Representative](https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/MBOJNS/WXZE5N&version=1.0).
- To query the datasets, for each politician, we selected either the Representative or Senator dataset depending on their position and inputted their user ID in the “Contains any user id” box under the “Posted by” section. This process gives us a txt file of tweet IDs for each politician which is then stored in the `src/data/misinformation` and `src/data/scientific` folders depending on the group the politician is assigned to. 

### Using `run.py`
- To get the data, from the project root directory, run `python run.py data`
    * This downloads the data using the tweet IDs found in `src/data/misinformation` and `src/data/scientific`.
    * This also downloads the engagement metrics needed for ratio analysis using the same tweet IDs.
    * The politicians to analyze can be specified in `config/data-params.json`.
    * The name of the txt file containing the tweet IDs must match the name specified in `config/data-params.json`.
    * The output is a json file for each politician containing their collection of tweets as well as a csv file containing the engagement metrics and text of the tweet ID.

- To calculate the ratios for the tweets, from the project root directory, run `python run.py ratio`
    * This calculates the ratios for the tweets found in the csv files in `src/data/misinformation` and `src/data/scientific`.
    * The politicians to analyze can be specified in `config/data-params.json`.
    * Output is an updated csv file for each politician containing the engagement metrics of their tweets and their ratio values.

- To calculate the popularity estimate metrics, from the projecy root directory, run `python run.py metrics`
    * This will create a JSON file for each estimate metric you wish to analyze. 
    * The outputs are stored in `src/out`.
    * These JSON files will then be used to create visualizations using the visualization target.

- To create the visualizations, from the project root directory, run `python run.py visualization`
    * This creates visualizations from the JSON files created from the metrics target.
    * The outputs are stored in `src/out`.

- To run the permutation test on our groups, from the project root directory, run `python run.py permute`
    * This will run a permutation test within our two groups as well as between our groups.
    * The outputs are stored in `src/out`.

- To run all of the above targets, from the project root directory, run `python run.py all`

- To run the test target, from the project root directory, run `python run.py test`
    * This runs most of the above targets on a small, fake dataset."
67,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_1,,,"# The Sentiment of U.S. Presidential Elections on Twitter
This project investigates the public sentiment on Twitter regarding the 2016 and 2020 U.S. Presidential Elections. Political tensions in the United States came to a head in 2020 as people disputed President Donald Trump's handling of various major events that the year brought such as the COVID-19 pandemic and the killing of George Floyd and subsequent racial protests, and we aim to identify if this was quantifiably reflected in people's behavior on social media. To do this, we perform sentiment analysis on tweets related to the elections and conduct permutation testing to analyze how sentiment may differ between the two years and between and within politically left- and right-leaning groups of users. 


### Running The Project
- All commands specified here are to be run from within this project's root directory
- To install necessary dependencies, run `pip install -r requirements.txt`
- Note: to get the data necessary to replicate this project, access to the Twitter API is needed

### Using `run.py`
- This project uses publicly available 2016 and 2020 presidential election datasets located at https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/PDI7IN and https://github.com/echen102/us-pres-elections-2020. Given the 2016 dataset is not uniformly structured, you must manually download the txt files of tweet ids from the dataset's website to the directory `data/raw/2016`. The 2020 dataset can be downloaded programmatically using the `data` target, as follows.

- To get hydrated tweets for the 2016 and 2020 tweet ids, run the command `python run.py data`
    * This samples from the 2016 tweet ids located in `data/raw/2016` and stores them in txt files in `data/temp/2020/tweet_ids/`
    * It also directly downloads tweets for the 2020 election from the dataset's GitHub page falling within the date range specified in `config/etl-params.json`, samples from them, and stores them in txt files in `data/temp/2020/tweet_ids/`
    * It then rehydrates the tweets using `twarc` and saves the them in jsonl format in the `hydrated_tweets/` directory within each year's respective data directory

- To clean the rehydrated tweets, run the command `python run.py clean`
    * This takes the rehydrated tweets obtained from running `python run.py data` and creates a single csv file of tweets for each year with fields for the features of interest specified in `config/clean-params.json`
    * For the purpose of performing sentiment analysis later, tweets in languages other than English are filtered out.
    * The resulting csvs are stored as `clean/clean_tweets.csv` within each year's data directory.

- To run the main analysis, run `python run.py compute` from the project root directory
    * For each year, this subsets the tweets into left and right leaning, classifies the the different types of dialogue, performs sentiment analysis on the various subsets of data, and conducts permutation testing on the subsets of the data between the two years, producing plots of the results.
    
- To run the `clean` and `compute` targets on fake test data, run the command `python run.py test`

","# The Sentiment of U.S. Presidential Elections on Twitter
This project investigates the public sentiment on Twitter regarding the 2016 and 2020 U.S. Presidential Elections. Political tensions in the United States came to a head in 2020 as people disputed President Donald Trump's handling of various major events that the year brought such as the COVID-19 pandemic and the killing of George Floyd and subsequent racial protests, and we aim to identify if this was quantifiably reflected in people's behavior on social media. To do this, we perform sentiment analysis on tweets related to the elections and conduct permutation testing to analyze how sentiment may differ between the two years and between and within politically left- and right-leaning groups of users. 


### Running The Project
- All commands specified here are to be run from within this project's root directory
- To install necessary dependencies, run `pip install -r requirements.txt`
- Note: to get the data necessary to replicate this project, access to the Twitter API is needed

### Using `run.py`
- This project uses publicly available 2016 and 2020 presidential election datasets located at https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/PDI7IN and https://github.com/echen102/us-pres-elections-2020. Given the 2016 dataset is not uniformly structured, you must manually download the txt files of tweet ids from the dataset's website to the directory `data/raw/2016`. The 2020 dataset can be downloaded programmatically using the `data` target, as follows.

- To get hydrated tweets for the 2016 and 2020 tweet ids, run the command `python run.py data`
    * This samples from the 2016 tweet ids located in `data/raw/2016` and stores them in txt files in `data/temp/2020/tweet_ids/`
    * It also directly downloads tweets for the 2020 election from the dataset's GitHub page falling within the date range specified in `config/etl-params.json`, samples from them, and stores them in txt files in `data/temp/2020/tweet_ids/`
    * It then rehydrates the tweets using `twarc` and saves the them in jsonl format in the `hydrated_tweets/` directory within each year's respective data directory

- To clean the rehydrated tweets, run the command `python run.py clean`
    * This takes the rehydrated tweets obtained from running `python run.py data` and creates a single csv file of tweets for each year with fields for the features of interest specified in `config/clean-params.json`
    * For the purpose of performing sentiment analysis later, tweets in languages other than English are filtered out.
    * The resulting csvs are stored as `clean/clean_tweets.csv` within each year's data directory.

- To run the main analysis, run `python run.py compute` from the project root directory
    * For each year, this subsets the tweets into left and right leaning, classifies the the different types of dialogue, performs sentiment analysis on the various subsets of data, and conducts permutation testing on the subsets of the data between the two years, producing plots of the results.
    
- To run the `clean` and `compute` targets on fake test data, run the command `python run.py test`

"
68,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_2,,,"# Data Science Capstone Project

## Info
* The data being used here is Tweets from various news sources.
## Preqrequisites
* Ensure libraries are installed. (pandas, requests, os, gzip, shutil, json, flatten).
* Download repo: https://github.com/thepanacealab/covid19_twitter.
* Docker container id: tmpankaj/example-docker
## How to Run
* All parameters are of type str unless specified otherwise
* Set twitter API Keys in config/twitter-api-keys.json
#### Test
* run 'python run.py test' in root directory of repo
* look in test/visualizations for the test targets
#### Data 
* Go inside docker container
* Add .txt files with Tweet IDs from https://tweetsets.library.gwu.edu/ to some directory where preprocessed data will be stored. (E.g. cnn.txt in /data/preprocessed directory)
* Use this hydrator https://github.com/DocNow/hydrator to hydrate these tweets and make sure there is a .csv file in the same directory (E.g. cnn.csv in /data/preprocessed)
* Set data parameters in config/data-params.json
   * preprocessed_data_path: path to directory of preprocessed data
   * training_data_path: path to directory to output training data
   * dims (list of str): list of the names of the dimensions that polarities will be eventually calculated on (E.g. [""moderacy"", ""misinformation""])
   * labels (dict): dictionary with the keys including the news sources and each value being a list with a polarity for each dimension. Every news source that will be used in your data must have a label for every dimension. (E.g. {""cnn"": [0, 1], ""fox"": [1, 0]})
   * user_data_path: path to directory to output user data
   * exclude_replies (bool): If true, will exclude replies when collecting user tweets.
   * include_rts (bool): If true, will include retweets when collecting user tweets.
   * max_recent_tweets (int): maximum recent number of tweets to obtain from a user
   * tweet_ids (list of str): list of tweet IDs to collect to analyze flagged vs unflagged retweeters
* Make sure paths to directories already exist
* run 'python run.py data' in root directory of repo
* This will only collect the data
#### Train
* Go inside docker container and make sure data has been collected
* Set train parameters in config/train-params.json
   * training_data_path: path to directory of the training data (should be same as data-params)
   * model_path: path to directory to output models to be trained
   * dims (list of str): list of the names of the dimensions that polarities should be calculated on (E.g. [""moderacy"", ""misinformation""])
   * fit_priors (list of bools): hyperparemeter for Naive Bayes classifier (1 for each dimension) - Whether to learn class prior probabilities or not. If false, a uniform prior will be used.
   * max_dfs (list of floats/ints): hyperparameter for CountVectorizer (1 for each dimension) - When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts.
   * min_dfs (list of floats/ints): hyperparameter for CountVectorizer (1 for each dimension) - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. 
   * n_splits (int): number of folds to use for K-fold cross validation
   * outdir: path to directory to output a notebook of the results
* Make sure paths to directories already exist
* run 'python run.py train' in root directory of repo 
* Look in the outdir you specified for an html file of the results
#### Analysis
* Go inside docker container and make sure data has been collected and models have been trained
* Set analysis parameters in config/analysis-params.json
   * model_path: path to directory of trained models (should be same as train-params)
   * user_data_path: path to directory of user data (should be same as data-params)
   * dims (list of str): list of the names of the dimensions that polarities should be calculated on (E.g. [""moderacy"", ""misinformation""])
   * tweet_ids (list of str): list of tweet IDs to analyze
   * flagged (dict): dictionary should have a key for every tweet to be analyzed and a boolean for whether or not the tweet was flagged (E.g. {""123"": true, ""456"": false})
   * outdir: path to directory to output a notebook of the results
* Make sure paths to directories already exist
* run 'python run.py analysis' in root directory of repo
* Look in the outdir you specified for an html file of the results
#### Results
* Go inside docker container and make sure data has been collected, models have been trained, and analysis has been ran.
* Set results parameters in config/results-params.json
   * user_data_path: path to directory of user data (should be same as data-params)
   * dims (list of str): list of the names of the dimensions that results should be calculated on (E.g. [""moderacy"", ""misinformation""])
   * outdir: path to directory to output a notebook of the results
* Make sure paths to directories already exist
* run 'python run.py results' in root directory of repo
* Look in the outdir you specified for an html file of the results
","# Data Science Capstone Project

## Info
* The data being used here is Tweets from various news sources.
## Preqrequisites
* Ensure libraries are installed. (pandas, requests, os, gzip, shutil, json, flatten).
* Download repo: https://github.com/thepanacealab/covid19_twitter.
* Docker container id: tmpankaj/example-docker
## How to Run
* All parameters are of type str unless specified otherwise
* Set twitter API Keys in config/twitter-api-keys.json
#### Test
* run 'python run.py test' in root directory of repo
* look in test/visualizations for the test targets
#### Data 
* Go inside docker container
* Add .txt files with Tweet IDs from https://tweetsets.library.gwu.edu/ to some directory where preprocessed data will be stored. (E.g. cnn.txt in /data/preprocessed directory)
* Use this hydrator https://github.com/DocNow/hydrator to hydrate these tweets and make sure there is a .csv file in the same directory (E.g. cnn.csv in /data/preprocessed)
* Set data parameters in config/data-params.json
   * preprocessed_data_path: path to directory of preprocessed data
   * training_data_path: path to directory to output training data
   * dims (list of str): list of the names of the dimensions that polarities will be eventually calculated on (E.g. [""moderacy"", ""misinformation""])
   * labels (dict): dictionary with the keys including the news sources and each value being a list with a polarity for each dimension. Every news source that will be used in your data must have a label for every dimension. (E.g. {""cnn"": [0, 1], ""fox"": [1, 0]})
   * user_data_path: path to directory to output user data
   * exclude_replies (bool): If true, will exclude replies when collecting user tweets.
   * include_rts (bool): If true, will include retweets when collecting user tweets.
   * max_recent_tweets (int): maximum recent number of tweets to obtain from a user
   * tweet_ids (list of str): list of tweet IDs to collect to analyze flagged vs unflagged retweeters
* Make sure paths to directories already exist
* run 'python run.py data' in root directory of repo
* This will only collect the data
#### Train
* Go inside docker container and make sure data has been collected
* Set train parameters in config/train-params.json
   * training_data_path: path to directory of the training data (should be same as data-params)
   * model_path: path to directory to output models to be trained
   * dims (list of str): list of the names of the dimensions that polarities should be calculated on (E.g. [""moderacy"", ""misinformation""])
   * fit_priors (list of bools): hyperparemeter for Naive Bayes classifier (1 for each dimension) - Whether to learn class prior probabilities or not. If false, a uniform prior will be used.
   * max_dfs (list of floats/ints): hyperparameter for CountVectorizer (1 for each dimension) - When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts.
   * min_dfs (list of floats/ints): hyperparameter for CountVectorizer (1 for each dimension) - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. 
   * n_splits (int): number of folds to use for K-fold cross validation
   * outdir: path to directory to output a notebook of the results
* Make sure paths to directories already exist
* run 'python run.py train' in root directory of repo 
* Look in the outdir you specified for an html file of the results
#### Analysis
* Go inside docker container and make sure data has been collected and models have been trained
* Set analysis parameters in config/analysis-params.json
   * model_path: path to directory of trained models (should be same as train-params)
   * user_data_path: path to directory of user data (should be same as data-params)
   * dims (list of str): list of the names of the dimensions that polarities should be calculated on (E.g. [""moderacy"", ""misinformation""])
   * tweet_ids (list of str): list of tweet IDs to analyze
   * flagged (dict): dictionary should have a key for every tweet to be analyzed and a boolean for whether or not the tweet was flagged (E.g. {""123"": true, ""456"": false})
   * outdir: path to directory to output a notebook of the results
* Make sure paths to directories already exist
* run 'python run.py analysis' in root directory of repo
* Look in the outdir you specified for an html file of the results
#### Results
* Go inside docker container and make sure data has been collected, models have been trained, and analysis has been ran.
* Set results parameters in config/results-params.json
   * user_data_path: path to directory of user data (should be same as data-params)
   * dims (list of str): list of the names of the dimensions that results should be calculated on (E.g. [""moderacy"", ""misinformation""])
   * outdir: path to directory to output a notebook of the results
* Make sure paths to directories already exist
* run 'python run.py results' in root directory of repo
* Look in the outdir you specified for an html file of the results
"
69,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_3,,,"## Where to begin?
### Begin by uploading your twitter API credentials into a json file as under a new .env director. The file path should look like this: .env/twitter_credentials.json

The json file should be structured as

```json
{
   ""CONSUMER_KEY"":""your-consumer-key-here"",
   ""CONSUMER_SECRET"":""your-consumer-secret-here"",
   ""ACCESS_TOKEN"":""your-access-token-here"",
   ""ACCESS_TOKEN_SECRET"":""your-access-token-secret-here""
}
```

If you do not have twitter API credentials, please visit https://developer.twitter.com/en/docs/twitter-api to apply for a developer account.

To install the dependencies, run the following command from the root directory of the project: pip install ```-r requirements.txt```

## How to use run.py:
run.py takes in one argument, a choice between *data*, *eda*, *test*

## Directories
* A directory titled *data* will be created with 4 subdirectories: *graphs, raw, processed*
   * *graphs* will hold any charts from eda functions
   * *processed* will hold any statistic data from eda functions
   * *raw* will hold raw tweet data
* Each of the above directories will be split into two additional subdirectories, *news* and *election*
   * *news* will hold any data related to news stations
   * *election* will hold any data related to the election dataset

## Description of arguments (targets)

### data
* Your twitter API credentials for use in downloading data to be used in our project.
* The target will download all tweets as specified in the config file *news_params.json*

### eda
* The eda target will generate statistics and visualizations after data has been gathered from the *data* target
* Currently we have built a wordcloud visualization that will be stored in *graphs* and a statistic of most common hashtags per news station stored in *processed*

### compile and embed
* Performs graph embedding calculations as described in the methodology section of the report

### test
* The test target is designed for grading functionality in the DSC180B capstone course and will test three functionalities:
   * *etl_news* checks that test data is available for use
   * *eda* generates visualizations and statistics based on the test data, stores in *test/testreport*
   * *similarity* will generate similarity hashtag vectors to be used in our main analysis *test/testreport*

","## Where to begin?
### Begin by uploading your twitter API credentials into a json file as under a new .env director. The file path should look like this: .env/twitter_credentials.json

The json file should be structured as

```json
{
   ""CONSUMER_KEY"":""your-consumer-key-here"",
   ""CONSUMER_SECRET"":""your-consumer-secret-here"",
   ""ACCESS_TOKEN"":""your-access-token-here"",
   ""ACCESS_TOKEN_SECRET"":""your-access-token-secret-here""
}
```

If you do not have twitter API credentials, please visit https://developer.twitter.com/en/docs/twitter-api to apply for a developer account.

To install the dependencies, run the following command from the root directory of the project: pip install ```-r requirements.txt```

## How to use run.py:
run.py takes in one argument, a choice between *data*, *eda*, *test*

## Directories
* A directory titled *data* will be created with 4 subdirectories: *graphs, raw, processed*
   * *graphs* will hold any charts from eda functions
   * *processed* will hold any statistic data from eda functions
   * *raw* will hold raw tweet data
* Each of the above directories will be split into two additional subdirectories, *news* and *election*
   * *news* will hold any data related to news stations
   * *election* will hold any data related to the election dataset

## Description of arguments (targets)

### data
* Your twitter API credentials for use in downloading data to be used in our project.
* The target will download all tweets as specified in the config file *news_params.json*

### eda
* The eda target will generate statistics and visualizations after data has been gathered from the *data* target
* Currently we have built a wordcloud visualization that will be stored in *graphs* and a statistic of most common hashtags per news station stored in *processed*

### compile and embed
* Performs graph embedding calculations as described in the methodology section of the report

### test
* The test target is designed for grading functionality in the DSC180B capstone course and will test three functionalities:
   * *etl_news* checks that test data is available for use
   * *eda* generates visualizations and statistics based on the test data, stores in *test/testreport*
   * *similarity* will generate similarity hashtag vectors to be used in our main analysis *test/testreport*

"
70,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_4,,,"# Election-Sentiment
An analysis of views towards the US 2020 Presidential election using Twitter data.

Contained in this repository are a few notebooks that contain our analyses in this investigation on Election Sentiment analysis as well as an investigation into how Twitter impacts election results.

### Building the preoject using `run.py`

Running the test target via the command ""python run.py test"" will produce images related to the distribution of discussion levels of the two elections. In order to customize the data that this script is run on, replace the data in the test folder with data of your choice.

Provided in the scripts folder, are scripts to donwload tweets from the github repository that we collected tweets on the 2020 election for. The links to the 2020 election and 2016 election tweet ID's are below:

2016:
https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/PDI7IN

2020:
https://github.com/echen102/us-pres-elections-2020

In order to run the data download scripts, you need to have twarc installed. However, building the project with the provided dockerfile in this repository will download all of the extra packages not native to most Machine Learning and Data Science platforms.


## Running the project
* To get the data from Twitter, create a developer account and get your developer keys
* Configure `twarc`
  - On the terminal, run `twarc configure`
  - Supply keys made earlier


## Groupmate Responsibilities

### Chris

Chris was responsible for the EDA and sentiment analysis and those respective portions of the report. Her work was focused heavily on understanding the sentiment as time progressed and how that related to the individual elections. 

### Prem

Prem was involved heavily in creating the scripts that could be ran by anyone in order to perform ETL on the data. In addition Prem worked on developing the discussion metric that was critical to this investigation.
","# Election-Sentiment
An analysis of views towards the US 2020 Presidential election using Twitter data.

Contained in this repository are a few notebooks that contain our analyses in this investigation on Election Sentiment analysis as well as an investigation into how Twitter impacts election results.

### Building the preoject using `run.py`

Running the test target via the command ""python run.py test"" will produce images related to the distribution of discussion levels of the two elections. In order to customize the data that this script is run on, replace the data in the test folder with data of your choice.

Provided in the scripts folder, are scripts to donwload tweets from the github repository that we collected tweets on the 2020 election for. The links to the 2020 election and 2016 election tweet ID's are below:

2016:
https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/PDI7IN

2020:
https://github.com/echen102/us-pres-elections-2020

In order to run the data download scripts, you need to have twarc installed. However, building the project with the provided dockerfile in this repository will download all of the extra packages not native to most Machine Learning and Data Science platforms.


## Running the project
* To get the data from Twitter, create a developer account and get your developer keys
* Configure `twarc`
  - On the terminal, run `twarc configure`
  - Supply keys made earlier


## Groupmate Responsibilities

### Chris

Chris was responsible for the EDA and sentiment analysis and those respective portions of the report. Her work was focused heavily on understanding the sentiment as time progressed and how that related to the individual elections. 

### Prem

Prem was involved heavily in creating the scripts that could be ran by anyone in order to perform ETL on the data. In addition Prem worked on developing the discussion metric that was critical to this investigation.
"
71,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_5,,,"
# The Spread of Misinformation on Reddit
Observing how different forms of misinformation and conspiracies are spread through social media.

## Overview
With the amount of actively spread misinformation circulating popular social media platforms, our goal is to explore if various forms of misinformation follow varying patterns of diffusion. The scope of our project will be limited to two misinformation types -- myth and political misinformation, and focused on Reddit. Our data will be obtained from Reddit archive [pushshift.io](http://pushshift.io).

## Contents
- `src` contains the source code of our project, including algorithms for data extraction, analysis, and modelling.
- `notebooks` contain some examples of the models this code will generate, detailing our findings under the circumstances in which we conducted our testing.
- `config` contains easily changable parameters to test the data under various circumstances or change directories as needed.
- `run.py` will build and run different the different parts of the source code, as needed by the user.
- `references` cite the sources we used to construct this project.
- `requirements.txt` lists the Python package dependencies of which the code relies on. 

## How to Run
- Install the dependencies by running `pip install -r requirements.txt` from the root directory of the project.
- Alternatively, you may reference our Docker image to recreate our environment, located [here](https://hub.docker.com/r/cindyhuynh/reddit-misinformation).
- Due to the open source nature of the PushShift archive, there is no need for any API use or developer account. 

### Building the project stages using `run.py`
- To download the data, run `python run.py data`
	- This downloads reddit comments from specified subreddits between a certain time period. The subreddits and time period are specified in `config/data_params.json`
- To create visualizations of EDA charts, run `python run.py eda`
	- This creates bar charts representing the dataset we have collected. It also shows visualizes statitics of one-time posters and average number of posts in each category and subreddit.
- To get user polarities, run `python run.py user_polarity`
	- This generates a metric for all users collected in the data, getting filepaths from `config/user_polarity_params.json`
- To generate common user matrices, run `python run.py matrices`
	- This creates two matrices demonstrating, for every possible pair of subreddits, the number and average user polarity of the users in that subset. Filepaths are specified in `config/matrix_params.json`. 
	- NOTE: user_polarities should be run at least once before running `matrices`.
- To create visualizations of user polarities and matrices, run `python run.py visualize`
	- This creates bar charts representing the general user polarity spread, as well as charts showing how users of different types cross into other subreddits. These bar charts will be replaced by heatmaps in a future update for easier visualization. Filepaths are specified in `config/visualize_params.json`. 
	- NOTE: `user_polarities` and `matrices` should be run at least once before running `visualize`.
- To run the full pipeline, run `python run.py all`
	- This will run all the targets. These targets include: `data`, `eda`, `user_polarity`, `matrices`, and `visualize`
- To run the full pipeline on test data, run `python run.py test`
	- This will run all the targets on test data. These targets include: `data`, `eda`, `user_polarity`, `matrices`, and `visualize`
","
# The Spread of Misinformation on Reddit
Observing how different forms of misinformation and conspiracies are spread through social media.

## Overview
With the amount of actively spread misinformation circulating popular social media platforms, our goal is to explore if various forms of misinformation follow varying patterns of diffusion. The scope of our project will be limited to two misinformation types -- myth and political misinformation, and focused on Reddit. Our data will be obtained from Reddit archive [pushshift.io](http://pushshift.io).

## Contents
- `src` contains the source code of our project, including algorithms for data extraction, analysis, and modelling.
- `notebooks` contain some examples of the models this code will generate, detailing our findings under the circumstances in which we conducted our testing.
- `config` contains easily changable parameters to test the data under various circumstances or change directories as needed.
- `run.py` will build and run different the different parts of the source code, as needed by the user.
- `references` cite the sources we used to construct this project.
- `requirements.txt` lists the Python package dependencies of which the code relies on. 

## How to Run
- Install the dependencies by running `pip install -r requirements.txt` from the root directory of the project.
- Alternatively, you may reference our Docker image to recreate our environment, located [here](https://hub.docker.com/r/cindyhuynh/reddit-misinformation).
- Due to the open source nature of the PushShift archive, there is no need for any API use or developer account. 

### Building the project stages using `run.py`
- To download the data, run `python run.py data`
	- This downloads reddit comments from specified subreddits between a certain time period. The subreddits and time period are specified in `config/data_params.json`
- To create visualizations of EDA charts, run `python run.py eda`
	- This creates bar charts representing the dataset we have collected. It also shows visualizes statitics of one-time posters and average number of posts in each category and subreddit.
- To get user polarities, run `python run.py user_polarity`
	- This generates a metric for all users collected in the data, getting filepaths from `config/user_polarity_params.json`
- To generate common user matrices, run `python run.py matrices`
	- This creates two matrices demonstrating, for every possible pair of subreddits, the number and average user polarity of the users in that subset. Filepaths are specified in `config/matrix_params.json`. 
	- NOTE: user_polarities should be run at least once before running `matrices`.
- To create visualizations of user polarities and matrices, run `python run.py visualize`
	- This creates bar charts representing the general user polarity spread, as well as charts showing how users of different types cross into other subreddits. These bar charts will be replaced by heatmaps in a future update for easier visualization. Filepaths are specified in `config/visualize_params.json`. 
	- NOTE: `user_polarities` and `matrices` should be run at least once before running `visualize`.
- To run the full pipeline, run `python run.py all`
	- This will run all the targets. These targets include: `data`, `eda`, `user_polarity`, `matrices`, and `visualize`
- To run the full pipeline on test data, run `python run.py test`
	- This will run all the targets on test data. These targets include: `data`, `eda`, `user_polarity`, `matrices`, and `visualize`
"
72,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_6,,,"# DSC180B-Project

## Table of Contents

- [Introduction](#introduction)
- [Requirement](#requirement)
- [Steps for Building](#building)
- [Data](#data)
- [EDA](#EDA)
- [Features and Models](#features_and_models)
- [Contributors](#responsibilities)


## Introduction

Covid-19 changed everyone, from the way we interact, to how we work, and our methods of communication, especially through social media. Under this pandemic period, social media becomes a huge and important part of people’s daily lives. It provides mobile users a convenient way to connect to each other around the world and acquire the updated and trending information about the topic of covid-19. Beside these, people can also express their thoughts and feelings toward certain topics by posting on social media. Throughout the studying of this quarter, we noticed that there are numbers of posts in our Twitter dataset that are related to the topic of covid-19 having some strong emotions and sentiments. In the meantime, a previous study has shown that more people are experiencing negative emotions such as anxiety and panic under this pandemic period. Therefore, we are interested in analyzing the posts that are related to the topic of covid-19 on social media and investigating the underlying causes of the negative emotions implied in these posts.


## Requirement

- python 3
- install the used modules included in the requirements.txt:
```
cd references
pip install -r requirements.txt
```
- In order to rehydrate the twitter data, you need to create a Twitter Developer Api account and get the API(costumer) key, API secret key, Access Token, and Access Token Secret.
- Also a Kaggle account with username and key to access the kaggle dataset.
- Save these keys into `.env` file in the project root directory. You can modify the `.env.example` with your own information and save it as `.env`.


## Building

There are six targets available in order to build the projects:
* data
    - run **`python run.py data`** for downloading and making datasets (three in total).
    - data will be saved in the paths `data/raw` and `data/inteirm`.
* analysis
    - run **`python run.py analysis`** for cleaning and analyzing the collected data
    - results (plots and tables) will be saved in the paths `data/analysis`.
    - you can directly view the EDA report in `notebooks/analysis.ipynb`.
* feature
    - run **`python run.py feature`** for building prediction models by using the pre-trained Kaggle dataset in order to label our own tweet dataset.
    - the mean sentiment scores per day will be saved in `data/final`.
* model
    - run **`python run.py model`** for using time series models to analyze the sentiment scores and daily new cases.
    - results will be saved in `data/final`.
* test
    - run **`python run.py test`** for building steps on our made-up test data.
    - this target is equivalent to **`python run.py test-data analysis feature model`**.
    - **IMPORTANT**: If you want to run any specific target on the test data, please include the target `test-data` in the command. For example, `python run.py test-data analysis` and `python run.py test-data feature`. And please make sure run the command in this order: test-data, analysis, feature, model.
* all
    - run **`python run.py all`** for building the complete project.
    - this command is equivalent to **`python run.py data analysis feature model`**.


## Data

We use data from [thepanacealab](https://github.com/thepanacealab) which gathers COVID-19 twitter data daily. Our data generation script enables us to input a certain date and automatically download the corresponding tweets on that date from the Panacea Lab, unzip the tab-delimited file (tsv), generate a list full of tweets IDs and rehydrate them using twarc, a command line tool and Python library that archives Twitter data.

We also obtain the Covid-19 daily cases dataset from [Our World in Data](https://github.com/owid/covid-19-data/tree/master/public/data) then sum up the daily cases numbers for all countries listed per day. In addition, in order to build efficient machine learning models, we use the tweet text dataset with sentiment labels from [Kaggle](https://www.kaggle.com/kazanova/sentiment140).

```
.
├──src
│  ├── data              
│      ├── collect_data.sh  # changing directory, unzipping file, curl and twarc
│      ├── get_IDs.py   # table processing function that extracts the tweetID and output txt
       |—— extract_to_csv.py # convert all the json files into cleaner csv formats
       |—— clean_text.py # clean all the tweet text
       |—— case_download.py # dowanload and clean the daily cases dataset 
       |—— train_dataset.py # dowanload and clean the labeled tweet sentiment dataset
└── ...
```

In order to successfully obtain data from Twitter and Kaggle, you need to have your Twitter and Kaggle Api information saved in the `.env` file. The script will download all of the tweetsIDs and hydrate them using `twarc`. The data will be saved under the `data/raw` folder with date being the subfolder name. Note that the `data/raw` folder is not being version controlled to avoid data corruption. The script will create the raw folder if it does not already exist. All cleaned datasets are saved in `data/interim`.


## EDA

Use the command `python run.py analysis` to generate statistics and graphs of the twitter data. These files will be generated in the `data/analysis` folder. The `notebooks/report.ipynb` displays a analysis and results report alongside those statistics.


## Features and Models

By using the command `python run.py feature`, you can build machine learning models on the pre-labeled Kaggle dataset to label the collected tweet dataset. There are two models that can be used: svc and logreg. In order to change the model options, you can modify the configuration file `config/feature-params.json`: change the value of `model` to either `logreg` or `svc`. The results (mean sentiment score for each day from 03/22 to 11/30) will be saved in `data/final`.


## Test

All test data are saved in `test/testdata`. Our test data contains no personal information as all of the details have been replaced. Only some tweets in the test set has hashtags of COVID19 and specific words which ensures it to run efficiently.


## Responsibilities
```
* Jiawei Zheng developed
* Yunlin Tang developed
* Zhou Li developed
```
","# DSC180B-Project

## Table of Contents

- [Introduction](#introduction)
- [Requirement](#requirement)
- [Steps for Building](#building)
- [Data](#data)
- [EDA](#EDA)
- [Features and Models](#features_and_models)
- [Contributors](#responsibilities)


## Introduction

Covid-19 changed everyone, from the way we interact, to how we work, and our methods of communication, especially through social media. Under this pandemic period, social media becomes a huge and important part of people’s daily lives. It provides mobile users a convenient way to connect to each other around the world and acquire the updated and trending information about the topic of covid-19. Beside these, people can also express their thoughts and feelings toward certain topics by posting on social media. Throughout the studying of this quarter, we noticed that there are numbers of posts in our Twitter dataset that are related to the topic of covid-19 having some strong emotions and sentiments. In the meantime, a previous study has shown that more people are experiencing negative emotions such as anxiety and panic under this pandemic period. Therefore, we are interested in analyzing the posts that are related to the topic of covid-19 on social media and investigating the underlying causes of the negative emotions implied in these posts.


## Requirement

- python 3
- install the used modules included in the requirements.txt:
```
cd references
pip install -r requirements.txt
```
- In order to rehydrate the twitter data, you need to create a Twitter Developer Api account and get the API(costumer) key, API secret key, Access Token, and Access Token Secret.
- Also a Kaggle account with username and key to access the kaggle dataset.
- Save these keys into `.env` file in the project root directory. You can modify the `.env.example` with your own information and save it as `.env`.


## Building

There are six targets available in order to build the projects:
* data
    - run **`python run.py data`** for downloading and making datasets (three in total).
    - data will be saved in the paths `data/raw` and `data/inteirm`.
* analysis
    - run **`python run.py analysis`** for cleaning and analyzing the collected data
    - results (plots and tables) will be saved in the paths `data/analysis`.
    - you can directly view the EDA report in `notebooks/analysis.ipynb`.
* feature
    - run **`python run.py feature`** for building prediction models by using the pre-trained Kaggle dataset in order to label our own tweet dataset.
    - the mean sentiment scores per day will be saved in `data/final`.
* model
    - run **`python run.py model`** for using time series models to analyze the sentiment scores and daily new cases.
    - results will be saved in `data/final`.
* test
    - run **`python run.py test`** for building steps on our made-up test data.
    - this target is equivalent to **`python run.py test-data analysis feature model`**.
    - **IMPORTANT**: If you want to run any specific target on the test data, please include the target `test-data` in the command. For example, `python run.py test-data analysis` and `python run.py test-data feature`. And please make sure run the command in this order: test-data, analysis, feature, model.
* all
    - run **`python run.py all`** for building the complete project.
    - this command is equivalent to **`python run.py data analysis feature model`**.


## Data

We use data from [thepanacealab](https://github.com/thepanacealab) which gathers COVID-19 twitter data daily. Our data generation script enables us to input a certain date and automatically download the corresponding tweets on that date from the Panacea Lab, unzip the tab-delimited file (tsv), generate a list full of tweets IDs and rehydrate them using twarc, a command line tool and Python library that archives Twitter data.

We also obtain the Covid-19 daily cases dataset from [Our World in Data](https://github.com/owid/covid-19-data/tree/master/public/data) then sum up the daily cases numbers for all countries listed per day. In addition, in order to build efficient machine learning models, we use the tweet text dataset with sentiment labels from [Kaggle](https://www.kaggle.com/kazanova/sentiment140).

```
.
├──src
│  ├── data              
│      ├── collect_data.sh  # changing directory, unzipping file, curl and twarc
│      ├── get_IDs.py   # table processing function that extracts the tweetID and output txt
       |—— extract_to_csv.py # convert all the json files into cleaner csv formats
       |—— clean_text.py # clean all the tweet text
       |—— case_download.py # dowanload and clean the daily cases dataset 
       |—— train_dataset.py # dowanload and clean the labeled tweet sentiment dataset
└── ...
```

In order to successfully obtain data from Twitter and Kaggle, you need to have your Twitter and Kaggle Api information saved in the `.env` file. The script will download all of the tweetsIDs and hydrate them using `twarc`. The data will be saved under the `data/raw` folder with date being the subfolder name. Note that the `data/raw` folder is not being version controlled to avoid data corruption. The script will create the raw folder if it does not already exist. All cleaned datasets are saved in `data/interim`.


## EDA

Use the command `python run.py analysis` to generate statistics and graphs of the twitter data. These files will be generated in the `data/analysis` folder. The `notebooks/report.ipynb` displays a analysis and results report alongside those statistics.


## Features and Models

By using the command `python run.py feature`, you can build machine learning models on the pre-labeled Kaggle dataset to label the collected tweet dataset. There are two models that can be used: svc and logreg. In order to change the model options, you can modify the configuration file `config/feature-params.json`: change the value of `model` to either `logreg` or `svc`. The results (mean sentiment score for each day from 03/22 to 11/30) will be saved in `data/final`.


## Test

All test data are saved in `test/testdata`. Our test data contains no personal information as all of the details have been replaced. Only some tweets in the test set has hashtags of COVID19 and specific words which ensures it to run efficiently.


## Responsibilities
```
* Jiawei Zheng developed
* Yunlin Tang developed
* Zhou Li developed
```
"
73,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_25,,,"# DSC180B

There are “wars” going on every day online, but instead of cities, they are defending their options, and perspects. This phenomenon is especially common on the Wikipedia platform where users are free to edit others' revisions. In fact, there are “about 12% of discussions are devoted to reverts and vandalism, suggesting that the WP development process is highly contentious.” As Wikipedia has become a trusted source of information and knowledge which is freely accessible, It is important to investigate how editors collaborate and controvert each other in such a platform. This repository will show our coding methods to discuss a new method of measuring controvisality in Wikipedia articles. We have found out that controversiality is highly related to the number of revert edits, the sentiment level among one article comments, and the view counts of that article. Thus we developed a weighted sum formula, which combines those three factors to accurately measure the controversy level within articles in Wikipedia. 


# Coding part
From the run.py file, you can notice that we have 2 targets, which is ""All"" and ""Test"". In the following part, We will discuss about the details of those two targets:
    
For the ""ALL"" target, it uses datasets from the Wikimedia Data Archives and English Light Dump. Then it used the functions that are listed in the process to generate our final analysis results. There are the purpose for different functions:

1. For the get_data.py and deal_withcomment.py, we use those coding files to download XML files from the Wikimedia Data Archives and then convert those raw XML file to dataframe, which is a better form to let us doing the analysis. 
    
2. For the english_lighdump.py, the function for the python file is to download the English Light dump file from the WikiWarMonitor and convert this dataset to a dataframe. It also merge the English Lightdump Dataframe with the comment dataframe that is generated by get_data.py and deal_withcomment.py. 
    
3. For the page_view.py, the function for this python file is to use the titles in the generated dataframe to find the raw description number of views on English Wikipedia of articles in the merged dataframe from those articles' start dates to 20210101. 
    
4. For the sentiment_analysis.py, the function for this python file is to use the comments content in the generated dataframe and the Vader model to generate the sentiment score for each comment. Finally, in this python file we will generate a final dataframe that contains M score, page view count, article title, date, comment and sentiment score to use for the future analysis. 
    
5. For the generatefinaldatf.py, the function for this python file is to generate two dataframes that we will use in our analysis part and generate some graphs from analysis. Those two dataframes are dataframe with M is zero and dataframe for all M. 
    
6. For the Analysis.py, the function for this python file is to make some analysis. We generate four graphs in this analysis part:
  
    a. first one is analysis for corr between M and sentiment score
    b. second one is analysis for example of Wooster Ohio
    c. third one is relationship between pageview and sentiment score
    d. fourth one is view counts with M
   For each graph, we save as one figures and use those figures in our report. 

7. For the Weighted_sum_formula.py, the function for this python file is to generate our final weighed sum formula. And we will use the new scores which are generated by weighted sum formula to make some comparisons with the scores that are generated by M-statistics, and make some analysis on this comparison.  


For the 'Test' target, it mainly runs our test dataset, which means that the result that is generated by our ""test"" target is not representative. And there are some special function for this target, such as generatefinal_dataf_test.py and page_view_test.py, we generate those files because we need to use our test dataset. However, by using this ""test"" target, the analysis result will not be representative. 

# Notebook

For the content of the notebook, we put our analysis graphs which are generated by 'ALL' target and we will use those graphs in our report. 

# Resource

English light dump data from WikiWarMonitor: http://wwm.phy.bme.hu/light.html

XML file from the Wikimedia Data Archives: https://dumps.wikimedia.org/enwiki/20210220/

And the pageview API from: https://github.com/Commonists/pageview-api

# Responsibility

Coding: Xingyu Jiang, Xiangchen Zhao
Notebook: Xingyu Jiang, Xiangchen Zhao
Report: Xingyu Jiang, Xiangchen Zhao and Hengyu Liu
","# DSC180B

There are “wars” going on every day online, but instead of cities, they are defending their options, and perspects. This phenomenon is especially common on the Wikipedia platform where users are free to edit others' revisions. In fact, there are “about 12% of discussions are devoted to reverts and vandalism, suggesting that the WP development process is highly contentious.” As Wikipedia has become a trusted source of information and knowledge which is freely accessible, It is important to investigate how editors collaborate and controvert each other in such a platform. This repository will show our coding methods to discuss a new method of measuring controvisality in Wikipedia articles. We have found out that controversiality is highly related to the number of revert edits, the sentiment level among one article comments, and the view counts of that article. Thus we developed a weighted sum formula, which combines those three factors to accurately measure the controversy level within articles in Wikipedia. 


# Coding part
From the run.py file, you can notice that we have 2 targets, which is ""All"" and ""Test"". In the following part, We will discuss about the details of those two targets:
    
For the ""ALL"" target, it uses datasets from the Wikimedia Data Archives and English Light Dump. Then it used the functions that are listed in the process to generate our final analysis results. There are the purpose for different functions:

1. For the get_data.py and deal_withcomment.py, we use those coding files to download XML files from the Wikimedia Data Archives and then convert those raw XML file to dataframe, which is a better form to let us doing the analysis. 
    
2. For the english_lighdump.py, the function for the python file is to download the English Light dump file from the WikiWarMonitor and convert this dataset to a dataframe. It also merge the English Lightdump Dataframe with the comment dataframe that is generated by get_data.py and deal_withcomment.py. 
    
3. For the page_view.py, the function for this python file is to use the titles in the generated dataframe to find the raw description number of views on English Wikipedia of articles in the merged dataframe from those articles' start dates to 20210101. 
    
4. For the sentiment_analysis.py, the function for this python file is to use the comments content in the generated dataframe and the Vader model to generate the sentiment score for each comment. Finally, in this python file we will generate a final dataframe that contains M score, page view count, article title, date, comment and sentiment score to use for the future analysis. 
    
5. For the generatefinaldatf.py, the function for this python file is to generate two dataframes that we will use in our analysis part and generate some graphs from analysis. Those two dataframes are dataframe with M is zero and dataframe for all M. 
    
6. For the Analysis.py, the function for this python file is to make some analysis. We generate four graphs in this analysis part:
  
    a. first one is analysis for corr between M and sentiment score
    b. second one is analysis for example of Wooster Ohio
    c. third one is relationship between pageview and sentiment score
    d. fourth one is view counts with M
   For each graph, we save as one figures and use those figures in our report. 

7. For the Weighted_sum_formula.py, the function for this python file is to generate our final weighed sum formula. And we will use the new scores which are generated by weighted sum formula to make some comparisons with the scores that are generated by M-statistics, and make some analysis on this comparison.  


For the 'Test' target, it mainly runs our test dataset, which means that the result that is generated by our ""test"" target is not representative. And there are some special function for this target, such as generatefinal_dataf_test.py and page_view_test.py, we generate those files because we need to use our test dataset. However, by using this ""test"" target, the analysis result will not be representative. 

# Notebook

For the content of the notebook, we put our analysis graphs which are generated by 'ALL' target and we will use those graphs in our report. 

# Resource

English light dump data from WikiWarMonitor: http://wwm.phy.bme.hu/light.html

XML file from the Wikimedia Data Archives: https://dumps.wikimedia.org/enwiki/20210220/

And the pageview API from: https://github.com/Commonists/pageview-api

# Responsibility

Coding: Xingyu Jiang, Xiangchen Zhao
Notebook: Xingyu Jiang, Xiangchen Zhao
Report: Xingyu Jiang, Xiangchen Zhao and Hengyu Liu
"
74,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_26,,,"# The Large-Scale Collaborative Presence of Online Fandoms

Fan communities exist within every industry, and there has been little study on understanding their scale and how they influence the media and their industries. As technology and social media have made it easier than ever for fans to connect with their favorite influencers and find like-minded fans, we’ve seen a rise in fan culture or “fandom”. These individuals form fan groups and communities, which have become increasingly popular online and have rallied behind their favorite artists for different causes.<br><br>
This repository contains library code to explore the similarities and differences in collaboration efforts among fans on two primary online social platforms, Twitter and Wikipedia. It contains methods to quantify the scale, strength, and influence of online fan communities—with a focus on the K-pop fanbase—and how this online collaboration affects outside audiences.

## Materials
- [Website](https://kyleepeng.github.io/Fandom-Online-Collaboration/)
- [Report](https://raw.githubusercontent.com/dliu9999/artifact-directory-template/main/report.pdf)

## Usage

- Install dependencies (Navigate to the directory you cloned to)
`pip install -r requirements.txt`

- Run (test) script:
`python run.py test` Runs the (all) script on test data found in `test/testdata`. Plots data and aggregate stats to `data/eda`","# The Large-Scale Collaborative Presence of Online Fandoms

Fan communities exist within every industry, and there has been little study on understanding their scale and how they influence the media and their industries. As technology and social media have made it easier than ever for fans to connect with their favorite influencers and find like-minded fans, we’ve seen a rise in fan culture or “fandom”. These individuals form fan groups and communities, which have become increasingly popular online and have rallied behind their favorite artists for different causes.<br><br>
This repository contains library code to explore the similarities and differences in collaboration efforts among fans on two primary online social platforms, Twitter and Wikipedia. It contains methods to quantify the scale, strength, and influence of online fan communities—with a focus on the K-pop fanbase—and how this online collaboration affects outside audiences.

## Materials
- [Website](https://kyleepeng.github.io/Fandom-Online-Collaboration/)
- [Report](https://raw.githubusercontent.com/dliu9999/artifact-directory-template/main/report.pdf)

## Usage

- Install dependencies (Navigate to the directory you cloned to)
`pip install -r requirements.txt`

- Run (test) script:
`python run.py test` Runs the (all) script on test data found in `test/testdata`. Plots data and aggregate stats to `data/eda`"
75,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_27,,,"# DSC180BProject: Wikipedia’s Response to the COVID-19 Pandemic 


This is the Wikipedia project working on its performance on providing COVID-19 pandemic information. Most of our data generated can be seen using certain targets, but 
there are also some analysis we made through notebook and we will specified those notebooks in the notebooks seciton.

### Project Team Members:
- Yiheng Ye, yiy291@ucsd.edu
- Gabrielle Avila, ggavila@ucsd.edu
- Michael Lam, mel157@ucsd.edu

### Requirements:
- python 3.8
- pandas 1.1.0
- wordcloud 1.8.1
- wikipedia 1.4.0
- sklearn 0.24.1
- gensim 3.8.3
- nltk 3.5

### Code, Purpose, and Guideline:

- run.py: If target='data': Get top 1000 popular articles relating COVID-19 from Wikipedia. Get the pageview data for them in 2020.
          If target='eda': Get top10 article with top average daily pageview and plot their daily views
          If target='revision"": Get revision history for important pages and doing analysis with LDA model on them.
          if target='word': Generates word cloud for Wikipedia, JHU, and WHO
          If target='test': Runs test program about data: getting pageview on the test data and eda, getting revision data and doing LDA model on them, and 
          generating word cloud.
- elt.py: the library for the data pipeline, see the documentation for detailed functions of every function writtened. Basically
          these functions are used to fulfill the job done in run.py.
- eda.py: the library for doing eda on data.
- revision.py: the library for analysis revision data
- word.py: the library for creating wordcloud
- config/data-params.json: it stores the links of the source data as well as the output path for raw data.
- code in src/data: the source code to fulfill the functions about processing data. The current usable files are get_data.py(getting top1000 articles'
  basic information) and get_apipageview.py(getting pageview from given article information csvs)

### Notebooks
The notebook file is primary serving as our original test base for code development. Additionally, it also has a notebook called Project EDA Single Webpage.ipynb which we investigate ""COVID-19 pandemic data"" page deeply.

There is also another notebook called ""Word Clouds.ipynb"" which produces word clouds on Wikipedia Coronavirus page, JHU page, and WHO page.

The ""top_model.ipynb"" generated LDA model for the LDA model on article 'Coronavirus"", and this model needed to be open in a notebook to get visualization.

## Responsibilities:
- Yiheng Ye set up the structure of the project and the structure of run.py. He also wrote get_data.py and get_apipageview.py and put them into the etl.py. He also 
  wrote eda.py and eda_pageview.py
- Gabrielle Avila constructed our report and made deep analysis into the ""COVID-19 pandemic data"" page. She also made the ""Word Clouds.ipynb""
- Michael Lam made LDA model analysis on the page ""Coronavirus"" and put them into the notebook ""top_model.ipynb"".","# DSC180BProject: Wikipedia’s Response to the COVID-19 Pandemic 


This is the Wikipedia project working on its performance on providing COVID-19 pandemic information. Most of our data generated can be seen using certain targets, but 
there are also some analysis we made through notebook and we will specified those notebooks in the notebooks seciton.

### Project Team Members:
- Yiheng Ye, yiy291@ucsd.edu
- Gabrielle Avila, ggavila@ucsd.edu
- Michael Lam, mel157@ucsd.edu

### Requirements:
- python 3.8
- pandas 1.1.0
- wordcloud 1.8.1
- wikipedia 1.4.0
- sklearn 0.24.1
- gensim 3.8.3
- nltk 3.5

### Code, Purpose, and Guideline:

- run.py: If target='data': Get top 1000 popular articles relating COVID-19 from Wikipedia. Get the pageview data for them in 2020.
          If target='eda': Get top10 article with top average daily pageview and plot their daily views
          If target='revision"": Get revision history for important pages and doing analysis with LDA model on them.
          if target='word': Generates word cloud for Wikipedia, JHU, and WHO
          If target='test': Runs test program about data: getting pageview on the test data and eda, getting revision data and doing LDA model on them, and 
          generating word cloud.
- elt.py: the library for the data pipeline, see the documentation for detailed functions of every function writtened. Basically
          these functions are used to fulfill the job done in run.py.
- eda.py: the library for doing eda on data.
- revision.py: the library for analysis revision data
- word.py: the library for creating wordcloud
- config/data-params.json: it stores the links of the source data as well as the output path for raw data.
- code in src/data: the source code to fulfill the functions about processing data. The current usable files are get_data.py(getting top1000 articles'
  basic information) and get_apipageview.py(getting pageview from given article information csvs)

### Notebooks
The notebook file is primary serving as our original test base for code development. Additionally, it also has a notebook called Project EDA Single Webpage.ipynb which we investigate ""COVID-19 pandemic data"" page deeply.

There is also another notebook called ""Word Clouds.ipynb"" which produces word clouds on Wikipedia Coronavirus page, JHU page, and WHO page.

The ""top_model.ipynb"" generated LDA model for the LDA model on article 'Coronavirus"", and this model needed to be open in a notebook to get visualization.

## Responsibilities:
- Yiheng Ye set up the structure of the project and the structure of run.py. He also wrote get_data.py and get_apipageview.py and put them into the etl.py. He also 
  wrote eda.py and eda_pageview.py
- Gabrielle Avila constructed our report and made deep analysis into the ""COVID-19 pandemic data"" page. She also made the ""Word Clouds.ipynb""
- Michael Lam made LDA model analysis on the page ""Coronavirus"" and put them into the notebook ""top_model.ipynb""."
76,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_23,,,"# DSC 180B Final Pipeline
---
## How to run

```
usage: python run.py [-h] [-p] [-s] [-r]

optional arguments:
  -h, --help       show help message and exit
  -p, --pages      obtain list of pages to analyze from Wikipedia
  -s, --sentiment  run sentiment analysis on pages from list
  -r, --results    obtain stats and visuals from sentiment analysis
  -t, --test       runs test suite
```

## Purpose
This program collects wikipedia data from URLs to analyze the sentiment of articles over time.

## Config Formats
The configuration .json files in the config folder can be used to change the program operation
### Pages
* language:      English, Spanish, or Chinese
* targets:       Wikipedia categories from which to analyze articles
* skip_cats:     Wikipedia categories to skip due to abundance of unnecessary articles
* output:        data file to write list of articles to
### Sentiment
* language:      English, Spanish, or Chinese
* infile:        data file to read in from
* outfile:       data file to write to
### Results
* language:      English, Spanish, or Chinese
* infile:        data file to read in from
* outfile:       data file to write to
### Test - Pages
* language:      English, Spanish, or Chinese
* targets:       Wikipedia categories from which to analyze articles
* skip_cats:     Wikipedia categories to skip due to abundance of unnecessary articles
* output:        data file to write list of articles to
### Test - Sentiment
* language:      English, Spanish, or Chinese
* infile:        data file to read in from
* outfile:       data file to write to
### Test - Results
* language:      English, Spanish, or Chinese
* infile:        data file to read in from
* outfile:       data file to write to

---
Yuanbo Shi

Henry Lozada

Parth Patel

Emma Logomasini

UCSD Winter 2021
","# DSC 180B Final Pipeline
---
## How to run

```
usage: python run.py [-h] [-p] [-s] [-r]

optional arguments:
  -h, --help       show help message and exit
  -p, --pages      obtain list of pages to analyze from Wikipedia
  -s, --sentiment  run sentiment analysis on pages from list
  -r, --results    obtain stats and visuals from sentiment analysis
  -t, --test       runs test suite
```

## Purpose
This program collects wikipedia data from URLs to analyze the sentiment of articles over time.

## Config Formats
The configuration .json files in the config folder can be used to change the program operation
### Pages
* language:      English, Spanish, or Chinese
* targets:       Wikipedia categories from which to analyze articles
* skip_cats:     Wikipedia categories to skip due to abundance of unnecessary articles
* output:        data file to write list of articles to
### Sentiment
* language:      English, Spanish, or Chinese
* infile:        data file to read in from
* outfile:       data file to write to
### Results
* language:      English, Spanish, or Chinese
* infile:        data file to read in from
* outfile:       data file to write to
### Test - Pages
* language:      English, Spanish, or Chinese
* targets:       Wikipedia categories from which to analyze articles
* skip_cats:     Wikipedia categories to skip due to abundance of unnecessary articles
* output:        data file to write list of articles to
### Test - Sentiment
* language:      English, Spanish, or Chinese
* infile:        data file to read in from
* outfile:       data file to write to
### Test - Results
* language:      English, Spanish, or Chinese
* infile:        data file to read in from
* outfile:       data file to write to

---
Yuanbo Shi

Henry Lozada

Parth Patel

Emma Logomasini

UCSD Winter 2021
"
77,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_24,,,"# Politics on Wikipedia
This project is focused on detecting political controversy in online communities. We use a bag-of-words model and a party-embed model, trained on the ideological books corpus (Sim et al, 2013) as well as congressional record data (api.govinfo.gov), and attempt to generalize this to Wikipedia articles, validating it on edit comments which explicitly mention reverting bias.


## Usage

This code is intended to be run with the dockerfile vasyasha/pow_docker

It relies on data from the ideological books corpus (Sim et al., 2013) with sub-sentential annotations (Iyyer et al., 2014). To download this data please visit https://people.cs.umass.edu/~miyyer/ibc/index.html where you can send an email to the address in order to obtain the full dataset.

Once obtained, please extract the dataset to **/data/full_ibc/**

Once this is done, please alter the config in **/config/get_ibc_params** accordingly.

To run, in terminal type:
```
python run.py *target*
```

## Description of Contents

### `run.py`

Main driver for running the project. The targets and their functions are:
* `scrape_anames` : scrapes political article names
* `retrieve_anames` : obtains political articles
* `ibc` : downloads test IBC data
* `interpret_ibc` : runs partyembed model on IBC data
* `revision_xmls` : downloads XML files for nine political Wikipedia articles
* `partyembed` : runs Rheault and Cochrane model on current-page Wikipedia articles
* `partyembed_time` : runs Rheault and Cochrane model on Wikipedia edit histories
* `all` : Runs the whole pipeline.
* `test`: Runs the pipeline with pre-loaded test data.

### `config/`

* `get_ibc_params.json` : Input parameters for running the ibc target.

* `interpret_ibc_params.json` : Input parameters for running the interpret_ibc target.

### `notebooks/`

* `Partyembed+IBC_EDA.ipynb` : Jupyter notebook for the exploratory data analysis on Party_embed and IBC.

### `src/`

* `libcode.py` : Library code.

### `src/etl/`

* `bias.py` : Preliminary function for extracting bias from Rheault and Cochrane model.
* `get_anames.py` : Scrapes relevant article names.
* `get_atexts.py` : Scrapes article contents for the list gathered above.
* `get_ibc.py` : Downloads sample IBC data. For the full dataset, please see **Usage** above.
* `get_revision_xmls.py` : Downloads xml files using Wikipedia API for our time series analysis

### `src/models/`

* `difflib_bigrams.py` : Finds text difference between two article states
* `get_gns_scores.py` : Assigns scores to the article texts according to Gentzkow, Shapiro, Taddy 2019.
* `get_x2_scores.py` : Gets x2 scores based on the formula from Gentzkow and Shapiro 2010 from the IBC.
* `gns_histories.py` : Applies Gentzkow and Shapiro 2010 approach on edit histories
* `loadIBC.py` : This project uses code from (Sim et al., 2013) and (Iyyer et al., 2014). As this was written in a previous version of python, these updated versions replace downloads made during the building process.
* `partyembed_current_pages.py` : Applies partyembed model to get scores for the current pages of political Wikipedia articles
* `partyembed_ibc.py` : This file extracts from the partyembed .issue() function the ideological leanings of each word in each sentence of the ideological books corpus. After applying an aggregate function on this data, it writes this to a csv.
* `partyembed_revisions.py` : Applies partyembed model on edit histories to find change over time.
* `treeUtil.py` : This project uses code from (Sim et al., 2013) and (Iyyer et al., 2014). As this was written in a previous version of python, these updated versions replace downloads made during the building process.


## Sources

Papers Referenced
* https://siepr.stanford.edu/sites/default/files/publications/16-028.pdf

* https://www.cs.toronto.edu/~gh/2528/RheaultCochraneOct2018.pdf

Data
* https://people.cs.umass.edu/~miyyer/ibc/index.html

* https://data.stanford.edu/congress_text

* https://dumps.wikimedia.org

","# Politics on Wikipedia
This project is focused on detecting political controversy in online communities. We use a bag-of-words model and a party-embed model, trained on the ideological books corpus (Sim et al, 2013) as well as congressional record data (api.govinfo.gov), and attempt to generalize this to Wikipedia articles, validating it on edit comments which explicitly mention reverting bias.


## Usage

This code is intended to be run with the dockerfile vasyasha/pow_docker

It relies on data from the ideological books corpus (Sim et al., 2013) with sub-sentential annotations (Iyyer et al., 2014). To download this data please visit https://people.cs.umass.edu/~miyyer/ibc/index.html where you can send an email to the address in order to obtain the full dataset.

Once obtained, please extract the dataset to **/data/full_ibc/**

Once this is done, please alter the config in **/config/get_ibc_params** accordingly.

To run, in terminal type:
```
python run.py *target*
```

## Description of Contents

### `run.py`

Main driver for running the project. The targets and their functions are:
* `scrape_anames` : scrapes political article names
* `retrieve_anames` : obtains political articles
* `ibc` : downloads test IBC data
* `interpret_ibc` : runs partyembed model on IBC data
* `revision_xmls` : downloads XML files for nine political Wikipedia articles
* `partyembed` : runs Rheault and Cochrane model on current-page Wikipedia articles
* `partyembed_time` : runs Rheault and Cochrane model on Wikipedia edit histories
* `all` : Runs the whole pipeline.
* `test`: Runs the pipeline with pre-loaded test data.

### `config/`

* `get_ibc_params.json` : Input parameters for running the ibc target.

* `interpret_ibc_params.json` : Input parameters for running the interpret_ibc target.

### `notebooks/`

* `Partyembed+IBC_EDA.ipynb` : Jupyter notebook for the exploratory data analysis on Party_embed and IBC.

### `src/`

* `libcode.py` : Library code.

### `src/etl/`

* `bias.py` : Preliminary function for extracting bias from Rheault and Cochrane model.
* `get_anames.py` : Scrapes relevant article names.
* `get_atexts.py` : Scrapes article contents for the list gathered above.
* `get_ibc.py` : Downloads sample IBC data. For the full dataset, please see **Usage** above.
* `get_revision_xmls.py` : Downloads xml files using Wikipedia API for our time series analysis

### `src/models/`

* `difflib_bigrams.py` : Finds text difference between two article states
* `get_gns_scores.py` : Assigns scores to the article texts according to Gentzkow, Shapiro, Taddy 2019.
* `get_x2_scores.py` : Gets x2 scores based on the formula from Gentzkow and Shapiro 2010 from the IBC.
* `gns_histories.py` : Applies Gentzkow and Shapiro 2010 approach on edit histories
* `loadIBC.py` : This project uses code from (Sim et al., 2013) and (Iyyer et al., 2014). As this was written in a previous version of python, these updated versions replace downloads made during the building process.
* `partyembed_current_pages.py` : Applies partyembed model to get scores for the current pages of political Wikipedia articles
* `partyembed_ibc.py` : This file extracts from the partyembed .issue() function the ideological leanings of each word in each sentence of the ideological books corpus. After applying an aggregate function on this data, it writes this to a csv.
* `partyembed_revisions.py` : Applies partyembed model on edit histories to find change over time.
* `treeUtil.py` : This project uses code from (Sim et al., 2013) and (Iyyer et al., 2014). As this was written in a previous version of python, these updated versions replace downloads made during the building process.


## Sources

Papers Referenced
* https://siepr.stanford.edu/sites/default/files/publications/16-028.pdf

* https://www.cs.toronto.edu/~gh/2528/RheaultCochraneOct2018.pdf

Data
* https://people.cs.umass.edu/~miyyer/ibc/index.html

* https://data.stanford.edu/congress_text

* https://dumps.wikimedia.org

"
78,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_70,,,"Opioids Overdose Genome Analysis
==============================

## Project Overview
Opioids are now one of the most common causes of accidental death in the US. According to statistics, two out of three drug overdose deaths in 2018 involved an opioid, so opioid abuse can not only affect people physically and mentally but can also deprive their lives (https://docs.google.com/document/d/1JXWb1Bla8iqvyKl3EUxJcGAWBWTvqfO6rRhn1kzrOr4/edit#bookmark=id.p3zzn76i4h3). Opioid addiction has a unique background in that a large reason for why people become addicted is that patients in hospitals are often prescribed opioids to treat pain, however these patients wind up misusing their prescriptions and become addicted.
This is a data science project curated by Cathleen Peña, Zhaoyi Guo, and Dennis Wu. This github repo contains the codes that are essential to conduct the explicit visualization on the raw data gather from NCBI. 

## Website

The website that introduces the project is under https://genetics.denncc.com/

## Running the Project 

### Pull the Docker Image
To test on the project, in DSMLP, simply pull the image we've generated exclusively for this project by inputting:

    $ launch-180.sh -i dencc/opioids-od:dw -G B04_Genetics
    
### Clone the Repository
In the directory you want to run the project in, run

    $ mkdir temp
    $ cd ./temp 
    $ git clone https://github.com/denncc/opioids-od-genome-analysis

Then you will be able to run the project the project after cloning

### Test the project
To test on the project, simply input

    $ python run.py test

You will be able to see the testing procedure to run

## Project Organization
```
📦opioids-od-genome-analysis
 ┣ 📂config
 ┃ ┣ 📜data_config.json
 ┃ ┣ 📜feature_config.json
 ┃ ┣ 📜model_config.json
 ┃ ┣ 📜submission.json
 ┃ ┗ 📜test_config.json
 ┣ 📂data
 ┃ ┣ 📂external
 ┃ ┃ ┣ 📂bam
 ┃ ┃ ┣ 📜.gitkeep
 ┃ ┃ ┣ 📜GRCh38_latest_rna.fna
 ┃ ┃ ┣ 📜Log.out
 ┃ ┃ ┣ 📜SRA_case_table.csv
 ┃ ┃ ┣ 📜chrLength.txt
 ┃ ┃ ┣ 📜chrName.txt
 ┃ ┃ ┣ 📜chrNameLength.txt
 ┃ ┃ ┣ 📜chrStart.txt
 ┃ ┃ ┣ 📜gencode.v24.annotation.gff3
 ┃ ┃ ┣ 📜gencode.v24.annotation.gtf
 ┃ ┃ ┣ 📜gencode.v24.annotation_mrna.gff
 ┃ ┃ ┗ 📜genomeParameters.txt
 ┃ ┣ 📂interim
 ┃ ┃ ┣ 📜.gitkeep
 ┃ ┃ ┣ 📜cts.tsv
 ┃ ┃ ┗ 📜dds_res_before_filter.csv
 ┃ ┣ 📂processed
 ┃ ┃ ┣ 📂duplicates_removed
 ┃ ┃ ┣ 📂htseq
 ┃ ┃ ┣ 📂kallisto
 ┃ ┃ ┣ 📂merged
 ┃ ┃ ┣ 📂sorted
 ┃ ┃ ┣ 📂temp
 ┃ ┃ ┣ 📜.gitkeep
 ┃ ┃ ┣ 📜htseq_cts.csv
 ┃ ┃ ┣ 📜htseq_cts_1.csv
 ┃ ┃ ┣ 📜htseq_cts_gene.csv
 ┃ ┃ ┣ 📜htseq_cts_gene_filtered.csv
 ┃ ┃ ┣ 📜kallisto_transcripts.idx
 ┃ ┃ ┗ 📜test_gene_counts.csv
 ┃ ┣ 📂raw
 ┃ ┣ 📂test
 ┃ ┃ ┣ 📂SRR7949794
 ┃ ┃ ┃ ┣ 📜abundance.h5
 ┃ ┃ ┃ ┣ 📜abundance.tsv
 ┃ ┃ ┃ ┣ 📜pseudoalignments.bam
 ┃ ┃ ┃ ┗ 📜run_info.json
 ┃ ┃ ┣ 📜SRR7949794_1.fastq.gz
 ┃ ┃ ┗ 📜SRR7949794_2.fastq.gz
 ┃ ┗ 📜SRA_case_table.csv
 ┣ 📂docs
 ┃ ┣ 📜Makefile
 ┃ ┣ 📜commands.rst
 ┃ ┣ 📜conf.py
 ┃ ┣ 📜getting-started.rst
 ┃ ┣ 📜index.rst
 ┃ ┗ 📜make.bat
 ┣ 📂models
 ┃ ┗ 📜.gitkeep
 ┣ 📂notebooks
 ┃ ┣ 📜.gitkeep
 ┃ ┣ 📜EDA_python.ipynb
 ┃ ┣ 📜EDA_r.ipynb
 ┃ ┣ 📜HTSeq.ipynb
 ┃ ┣ 📜SRA_eda.ipynb
 ┃ ┗ 📜htseq_cts.py
 ┣ 📂references
 ┃ ┗ 📜.gitkeep
 ┣ 📂reports
 ┃ ┣ 📂figures
 ┃ ┃ ┣ 📜.gitkeep
 ┃ ┃ ┣ 📜Dist_of_Age.pdf
 ┃ ┃ ┣ 📜Scatterplot_Matrix_All.pdf
 ┃ ┃ ┣ 📜Scatterplot_Matrix_Users.pdf
 ┃ ┃ ┣ 📜cocaine_use_diff_means.pdf
 ┃ ┃ ┣ 📜cocaine_use_means.pdf
 ┃ ┃ ┣ 📜diff_group_means.pdf
 ┃ ┃ ┣ 📜drug_use_pie.pdf
 ┃ ┃ ┣ 📜group_means.pdf
 ┃ ┃ ┗ 📜race_pie.pdf
 ┃ ┗ 📜.gitkeep
 ┣ 📂src
 ┃ ┣ 📂__pycache__
 ┃ ┃ ┣ 📜__init__.cpython-36.pyc
 ┃ ┃ ┗ 📜__init__.cpython-37.pyc
 ┃ ┣ 📂data
 ┃ ┃ ┣ 📂__pycache__
 ┃ ┃ ┃ ┣ 📜__init__.cpython-36.pyc
 ┃ ┃ ┃ ┣ 📜__init__.cpython-37.pyc
 ┃ ┃ ┃ ┣ 📜import_data.cpython-36.pyc
 ┃ ┃ ┃ ┗ 📜import_data.cpython-37.pyc
 ┃ ┃ ┣ 📜.gitkeep
 ┃ ┃ ┣ 📜__init__.py
 ┃ ┃ ┣ 📜__init__.pyc
 ┃ ┃ ┣ 📜import_data.py
 ┃ ┃ ┗ 📜make_dataset.py
 ┃ ┣ 📂features
 ┃ ┃ ┣ 📂__pycache__
 ┃ ┃ ┃ ┣ 📜__init__.cpython-36.pyc
 ┃ ┃ ┃ ┣ 📜__init__.cpython-37.pyc
 ┃ ┃ ┃ ┣ 📜build_features.cpython-36.pyc
 ┃ ┃ ┃ ┗ 📜build_features.cpython-37.pyc
 ┃ ┃ ┣ 📂r_scripts
 ┃ ┃ ┃ ┗ 📜main.R
 ┃ ┃ ┣ 📜.gitkeep
 ┃ ┃ ┣ 📜__init__.py
 ┃ ┃ ┗ 📜build_features.py
 ┃ ┣ 📂models
 ┃ ┃ ┣ 📂__pycache__
 ┃ ┃ ┃ ┣ 📜__init__.cpython-37.pyc
 ┃ ┃ ┃ ┣ 📜build_model.cpython-37.pyc
 ┃ ┃ ┃ ┗ 📜htseq_cts.cpython-37.pyc
 ┃ ┃ ┣ 📂r_scripts
 ┃ ┃ ┃ ┣ 📜deseq2.R
 ┃ ┃ ┃ ┣ 📜visualization.R
 ┃ ┃ ┃ ┗ 📜wgcna.R
 ┃ ┃ ┣ 📂sh_scripts
 ┃ ┃ ┃ ┗ 📜samtools.sh
 ┃ ┃ ┣ 📜.Rhistory
 ┃ ┃ ┣ 📜.gitkeep
 ┃ ┃ ┣ 📜__init__.py
 ┃ ┃ ┣ 📜build_model.py
 ┃ ┃ ┗ 📜htseq_cts.py
 ┃ ┣ 📂visualization
 ┃ ┃ ┣ 📜.gitkeep
 ┃ ┃ ┣ 📜__init__.py
 ┃ ┃ ┗ 📜visualize.py
 ┃ ┣ 📜__init__.py
 ┃ ┗ 📜__init__.pyc
 ┣ 📜.gitignore
 ┣ 📜Dockerfile
 ┣ 📜LICENSE
 ┣ 📜Makefile
 ┣ 📜README.md
 ┣ 📜command-line-htseq.txt
 ┣ 📜r-bio.yaml
 ┣ 📜requirements.txt
 ┣ 📜run.py
 ┣ 📜setup.py
 ┣ 📜test_environment.py
 ┗ 📜tox.ini
```
","Opioids Overdose Genome Analysis
==============================

## Project Overview
Opioids are now one of the most common causes of accidental death in the US. According to statistics, two out of three drug overdose deaths in 2018 involved an opioid, so opioid abuse can not only affect people physically and mentally but can also deprive their lives (https://docs.google.com/document/d/1JXWb1Bla8iqvyKl3EUxJcGAWBWTvqfO6rRhn1kzrOr4/edit#bookmark=id.p3zzn76i4h3). Opioid addiction has a unique background in that a large reason for why people become addicted is that patients in hospitals are often prescribed opioids to treat pain, however these patients wind up misusing their prescriptions and become addicted.
This is a data science project curated by Cathleen Peña, Zhaoyi Guo, and Dennis Wu. This github repo contains the codes that are essential to conduct the explicit visualization on the raw data gather from NCBI. 

## Website

The website that introduces the project is under https://genetics.denncc.com/

## Running the Project 

### Pull the Docker Image
To test on the project, in DSMLP, simply pull the image we've generated exclusively for this project by inputting:

    $ launch-180.sh -i dencc/opioids-od:dw -G B04_Genetics
    
### Clone the Repository
In the directory you want to run the project in, run

    $ mkdir temp
    $ cd ./temp 
    $ git clone https://github.com/denncc/opioids-od-genome-analysis

Then you will be able to run the project the project after cloning

### Test the project
To test on the project, simply input

    $ python run.py test

You will be able to see the testing procedure to run

## Project Organization
```
📦opioids-od-genome-analysis
 ┣ 📂config
 ┃ ┣ 📜data_config.json
 ┃ ┣ 📜feature_config.json
 ┃ ┣ 📜model_config.json
 ┃ ┣ 📜submission.json
 ┃ ┗ 📜test_config.json
 ┣ 📂data
 ┃ ┣ 📂external
 ┃ ┃ ┣ 📂bam
 ┃ ┃ ┣ 📜.gitkeep
 ┃ ┃ ┣ 📜GRCh38_latest_rna.fna
 ┃ ┃ ┣ 📜Log.out
 ┃ ┃ ┣ 📜SRA_case_table.csv
 ┃ ┃ ┣ 📜chrLength.txt
 ┃ ┃ ┣ 📜chrName.txt
 ┃ ┃ ┣ 📜chrNameLength.txt
 ┃ ┃ ┣ 📜chrStart.txt
 ┃ ┃ ┣ 📜gencode.v24.annotation.gff3
 ┃ ┃ ┣ 📜gencode.v24.annotation.gtf
 ┃ ┃ ┣ 📜gencode.v24.annotation_mrna.gff
 ┃ ┃ ┗ 📜genomeParameters.txt
 ┃ ┣ 📂interim
 ┃ ┃ ┣ 📜.gitkeep
 ┃ ┃ ┣ 📜cts.tsv
 ┃ ┃ ┗ 📜dds_res_before_filter.csv
 ┃ ┣ 📂processed
 ┃ ┃ ┣ 📂duplicates_removed
 ┃ ┃ ┣ 📂htseq
 ┃ ┃ ┣ 📂kallisto
 ┃ ┃ ┣ 📂merged
 ┃ ┃ ┣ 📂sorted
 ┃ ┃ ┣ 📂temp
 ┃ ┃ ┣ 📜.gitkeep
 ┃ ┃ ┣ 📜htseq_cts.csv
 ┃ ┃ ┣ 📜htseq_cts_1.csv
 ┃ ┃ ┣ 📜htseq_cts_gene.csv
 ┃ ┃ ┣ 📜htseq_cts_gene_filtered.csv
 ┃ ┃ ┣ 📜kallisto_transcripts.idx
 ┃ ┃ ┗ 📜test_gene_counts.csv
 ┃ ┣ 📂raw
 ┃ ┣ 📂test
 ┃ ┃ ┣ 📂SRR7949794
 ┃ ┃ ┃ ┣ 📜abundance.h5
 ┃ ┃ ┃ ┣ 📜abundance.tsv
 ┃ ┃ ┃ ┣ 📜pseudoalignments.bam
 ┃ ┃ ┃ ┗ 📜run_info.json
 ┃ ┃ ┣ 📜SRR7949794_1.fastq.gz
 ┃ ┃ ┗ 📜SRR7949794_2.fastq.gz
 ┃ ┗ 📜SRA_case_table.csv
 ┣ 📂docs
 ┃ ┣ 📜Makefile
 ┃ ┣ 📜commands.rst
 ┃ ┣ 📜conf.py
 ┃ ┣ 📜getting-started.rst
 ┃ ┣ 📜index.rst
 ┃ ┗ 📜make.bat
 ┣ 📂models
 ┃ ┗ 📜.gitkeep
 ┣ 📂notebooks
 ┃ ┣ 📜.gitkeep
 ┃ ┣ 📜EDA_python.ipynb
 ┃ ┣ 📜EDA_r.ipynb
 ┃ ┣ 📜HTSeq.ipynb
 ┃ ┣ 📜SRA_eda.ipynb
 ┃ ┗ 📜htseq_cts.py
 ┣ 📂references
 ┃ ┗ 📜.gitkeep
 ┣ 📂reports
 ┃ ┣ 📂figures
 ┃ ┃ ┣ 📜.gitkeep
 ┃ ┃ ┣ 📜Dist_of_Age.pdf
 ┃ ┃ ┣ 📜Scatterplot_Matrix_All.pdf
 ┃ ┃ ┣ 📜Scatterplot_Matrix_Users.pdf
 ┃ ┃ ┣ 📜cocaine_use_diff_means.pdf
 ┃ ┃ ┣ 📜cocaine_use_means.pdf
 ┃ ┃ ┣ 📜diff_group_means.pdf
 ┃ ┃ ┣ 📜drug_use_pie.pdf
 ┃ ┃ ┣ 📜group_means.pdf
 ┃ ┃ ┗ 📜race_pie.pdf
 ┃ ┗ 📜.gitkeep
 ┣ 📂src
 ┃ ┣ 📂__pycache__
 ┃ ┃ ┣ 📜__init__.cpython-36.pyc
 ┃ ┃ ┗ 📜__init__.cpython-37.pyc
 ┃ ┣ 📂data
 ┃ ┃ ┣ 📂__pycache__
 ┃ ┃ ┃ ┣ 📜__init__.cpython-36.pyc
 ┃ ┃ ┃ ┣ 📜__init__.cpython-37.pyc
 ┃ ┃ ┃ ┣ 📜import_data.cpython-36.pyc
 ┃ ┃ ┃ ┗ 📜import_data.cpython-37.pyc
 ┃ ┃ ┣ 📜.gitkeep
 ┃ ┃ ┣ 📜__init__.py
 ┃ ┃ ┣ 📜__init__.pyc
 ┃ ┃ ┣ 📜import_data.py
 ┃ ┃ ┗ 📜make_dataset.py
 ┃ ┣ 📂features
 ┃ ┃ ┣ 📂__pycache__
 ┃ ┃ ┃ ┣ 📜__init__.cpython-36.pyc
 ┃ ┃ ┃ ┣ 📜__init__.cpython-37.pyc
 ┃ ┃ ┃ ┣ 📜build_features.cpython-36.pyc
 ┃ ┃ ┃ ┗ 📜build_features.cpython-37.pyc
 ┃ ┃ ┣ 📂r_scripts
 ┃ ┃ ┃ ┗ 📜main.R
 ┃ ┃ ┣ 📜.gitkeep
 ┃ ┃ ┣ 📜__init__.py
 ┃ ┃ ┗ 📜build_features.py
 ┃ ┣ 📂models
 ┃ ┃ ┣ 📂__pycache__
 ┃ ┃ ┃ ┣ 📜__init__.cpython-37.pyc
 ┃ ┃ ┃ ┣ 📜build_model.cpython-37.pyc
 ┃ ┃ ┃ ┗ 📜htseq_cts.cpython-37.pyc
 ┃ ┃ ┣ 📂r_scripts
 ┃ ┃ ┃ ┣ 📜deseq2.R
 ┃ ┃ ┃ ┣ 📜visualization.R
 ┃ ┃ ┃ ┗ 📜wgcna.R
 ┃ ┃ ┣ 📂sh_scripts
 ┃ ┃ ┃ ┗ 📜samtools.sh
 ┃ ┃ ┣ 📜.Rhistory
 ┃ ┃ ┣ 📜.gitkeep
 ┃ ┃ ┣ 📜__init__.py
 ┃ ┃ ┣ 📜build_model.py
 ┃ ┃ ┗ 📜htseq_cts.py
 ┃ ┣ 📂visualization
 ┃ ┃ ┣ 📜.gitkeep
 ┃ ┃ ┣ 📜__init__.py
 ┃ ┃ ┗ 📜visualize.py
 ┃ ┣ 📜__init__.py
 ┃ ┗ 📜__init__.pyc
 ┣ 📜.gitignore
 ┣ 📜Dockerfile
 ┣ 📜LICENSE
 ┣ 📜Makefile
 ┣ 📜README.md
 ┣ 📜command-line-htseq.txt
 ┣ 📜r-bio.yaml
 ┣ 📜requirements.txt
 ┣ 📜run.py
 ┣ 📜setup.py
 ┣ 📜test_environment.py
 ┗ 📜tox.ini
```
"
79,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_68,,,"# antibiotic-resistance
 Identifying the genetic basis of antibiotic resistance in E. Coli
","# antibiotic-resistance
 Identifying the genetic basis of antibiotic resistance in E. Coli
"
80,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_67,,,"# DSC180B_Capstone_Project
Ryan Cummings,
Gregory Thein,
Justin Kang,
Prof. Shannon Ellis,
Code Artifact Checkpoint

#### In this you will find our Checkpoint Code, We are in the B04 Genetics domain and this is our Capstone Project. For our Capstone Project we are looking at Alzheimer's Diseased Patient's Blood miRNA Data. Our Pipeline functions are seen in the all_pipeline.py file. Running the full pipeline takes multiple hours to run and implements the tools in our Genetics Pipeline (FastQC, CutAdapt, Kallisto, DESeq2). Our project implements both python and R to perform successful analysis on our dataset of blood based miRNA in which we find miRNAs with significantly changed expression level.

#### Our repo consists of 4 folders, and 3 files (a .gitignore, the README, and the run.py). The 4 folders consist of: config, notebooks, references, src. Inside config is our data-params.json file, eda-params.json file, report-params.json, analyze-params.json, viz-params.json, and test-params.json. These files specifies the data-input and output locations/file paths that is necessary for this Checkpoint's data retrieval. The eda-params file specifies the input and output of the report generated by the `eda` call, while the test-params has the names of the samples that we run the `test` keyword argument on, report-params has the input and output locations of the full report that is generated at the end of the `all` call, analyze-params has the the filepaths for the input/output of the analyze notebook that is ran when `analyze` is passed as a target, viz-params has the locations for the notebook that is generated when the `viz` param is called. Notebooks folder consists of all of our .ipynb files that we used for testing, and as a dev tool (to see what we did along the way). It also contains the notebooks that are converted for each of the targets that can be passed into our program. References has our SRARunTable from the patients we used in our project, and also contains static images that are loaded for some of the notebooks when converting to output report. The data folder is where we created the symlink between our folder and the dataset on DSMLP. The data folder (and test/testdata) will also consist of the data/out information once the `test` keyword is ran, specifically the output from Kallisto is stored here. The contents of our src folder contains our etl.py file, eda.py file, utils.py file, test_pipeline.py, and all_pipeline.py. Our etl.py file is where our file is extracting the dataset from the DSMLP's /teams dataset. Utils.py is where we created a function that turns a notebook into an HTML format, which then outputs that HTML file as a report. test_pipeline and all_pipeline contain the pipeline that is created for our project, varying slightly since test is only ran on a portion while all is ran on the entire dataset!

### Project Decisions

- Our project focus shifted from looking at gene expression data for Alzheimer's Disease patients, to observing blood sample data of patients diagnosed with Alzheimer's Disease and a control group. This was done in large part because of the lack of access to the databases we initially wanted to retrieve data from
- After spending time searching for a viable replacement dataset on Recount2, we set on data from SRA Study SRP022043 and downloaded the data onto DSMLP from the SRA Run Selector Tool 
- We initially implemented the dockerfile for this project based on the dockerfile used in last quarters replication and had hoped to implement TrimGalore as a new tool into our pipeline. Incompatibility issues, however, led us to drop TrimGalore as tool and stick with running Cutadapt and FastQC separately.
- The Kallisto reference file was originally stored in our data file in our Github but the `.gitignore` was hiding that file when we would pull the repo. We need it in order to run Kallisto so we moved it to our teams directory on DSMLP.


### Project Targets:
#### all
Runs entire pipeline on all of the data. Running `all` will run the full pipeline from scratch, this does take hours and sometimes even days to run, it can be ran from scratch but is not needed to be ran from scratch to see our results!
```
{
    ""outdir"": ""data/report"",
    ""report_in_path"": ""notebooks/Alzheimers-Biomarker-Analysis.ipynb"",
    ""report_out_path"": ""report/Alzheimers-Biomarker-Analysis.html""
}
```
#### test
Runs part of pipeline on a couple fastq files. Implements fastqc and kallisto. Then generates this report!
```
{
  ""test_1"": ""SRR837440.fastq.gz"",
  ""test_2"": ""SRR837444.fastq.gz""
}
```
#### data
In Progress! Gets and outputs the data and generates the report as well!
```
{
  ""file_path"": ""/teams/DSC180A_FA20_A00/b04genetics/group_1/raw_data""
}
```
#### eda
Runs EDA process. Makes report with data and plots figures.
```
{
    ""outdir"": ""data/report"",
    ""report_in_path"": ""notebooks/EDA.ipynb"",
    ""report_out_path"": ""notebooks/EDA.html""
}
```
#### viz
Runs Visualization process. Simply outputs all the charts and graphs used in the project.
```
{
    ""outdir"": ""data/report"",
    ""report_in_path"": ""notebooks/Viz.ipynb"",
    ""report_out_path"": ""notebooks/Viz.html""
}
```

#### analyze
Runs the Notebook used for our Analysis portion of the project. Generating the plots that are used to explain our results.
```
{
    ""outdir"": ""data/report"",
    ""report_in_path"": ""notebooks/analyze.ipynb"",
    ""report_out_path"": ""notebooks/analyze.html""
}
```


#### Running `python run.py all` will run the full pipeline from scrath, this does take hours and sometimes even days to run, it can be ran from scratch but is not needed to be ran from scratch to see our results! Other keywords that can be passed into the funciton are `test eda data viz analyze`. Running `python run.py test` is actually the most recommended one, this gives you the full pipeline experience on a fraction of the data, running in just a few minutes. Portions of the code can also be ran with `python run.py data` or `python run.py eda` or a combination of these: `python run.py data eda` etc. We also printed steps along the way to notify the user what is currently running in the pipeline. Our code assumes it is ran on the DSMLP Servers! Without running on the DSMLP Servers we would not be able to access the data, which is why it is important to be connected to the server.



### Responsibilities

Ryan: 
Ryan created the Pipeline that we are using for our project so far: FastQC, Cutadapt, FastQC (2), and Kallisto. Along with formatting the Github repo to the Cookiecutter Data Science standard. 

Justin: 
Justin worked mainly on getting the report side of the project complete. He, alongside Gregory, spent time researching what MicroRNA and biomarkers are to include as part of our background. Researching additional information about Alzheimer’s Disease was also completed. He also worked on getting the initial structure of the report completed prior to the checkpoint. 

Gregory: 
Gregory, alongside Justin worked on the researching miRNA and biomarkers, and their relation to AD. Furthermore, he helped research various parameters and settings for parts of the pipeline. 

All assisted in the implementation of the pipeline alongside editing/reviewing each other’s work. 
","# DSC180B_Capstone_Project
Ryan Cummings,
Gregory Thein,
Justin Kang,
Prof. Shannon Ellis,
Code Artifact Checkpoint

#### In this you will find our Checkpoint Code, We are in the B04 Genetics domain and this is our Capstone Project. For our Capstone Project we are looking at Alzheimer's Diseased Patient's Blood miRNA Data. Our Pipeline functions are seen in the all_pipeline.py file. Running the full pipeline takes multiple hours to run and implements the tools in our Genetics Pipeline (FastQC, CutAdapt, Kallisto, DESeq2). Our project implements both python and R to perform successful analysis on our dataset of blood based miRNA in which we find miRNAs with significantly changed expression level.

#### Our repo consists of 4 folders, and 3 files (a .gitignore, the README, and the run.py). The 4 folders consist of: config, notebooks, references, src. Inside config is our data-params.json file, eda-params.json file, report-params.json, analyze-params.json, viz-params.json, and test-params.json. These files specifies the data-input and output locations/file paths that is necessary for this Checkpoint's data retrieval. The eda-params file specifies the input and output of the report generated by the `eda` call, while the test-params has the names of the samples that we run the `test` keyword argument on, report-params has the input and output locations of the full report that is generated at the end of the `all` call, analyze-params has the the filepaths for the input/output of the analyze notebook that is ran when `analyze` is passed as a target, viz-params has the locations for the notebook that is generated when the `viz` param is called. Notebooks folder consists of all of our .ipynb files that we used for testing, and as a dev tool (to see what we did along the way). It also contains the notebooks that are converted for each of the targets that can be passed into our program. References has our SRARunTable from the patients we used in our project, and also contains static images that are loaded for some of the notebooks when converting to output report. The data folder is where we created the symlink between our folder and the dataset on DSMLP. The data folder (and test/testdata) will also consist of the data/out information once the `test` keyword is ran, specifically the output from Kallisto is stored here. The contents of our src folder contains our etl.py file, eda.py file, utils.py file, test_pipeline.py, and all_pipeline.py. Our etl.py file is where our file is extracting the dataset from the DSMLP's /teams dataset. Utils.py is where we created a function that turns a notebook into an HTML format, which then outputs that HTML file as a report. test_pipeline and all_pipeline contain the pipeline that is created for our project, varying slightly since test is only ran on a portion while all is ran on the entire dataset!

### Project Decisions

- Our project focus shifted from looking at gene expression data for Alzheimer's Disease patients, to observing blood sample data of patients diagnosed with Alzheimer's Disease and a control group. This was done in large part because of the lack of access to the databases we initially wanted to retrieve data from
- After spending time searching for a viable replacement dataset on Recount2, we set on data from SRA Study SRP022043 and downloaded the data onto DSMLP from the SRA Run Selector Tool 
- We initially implemented the dockerfile for this project based on the dockerfile used in last quarters replication and had hoped to implement TrimGalore as a new tool into our pipeline. Incompatibility issues, however, led us to drop TrimGalore as tool and stick with running Cutadapt and FastQC separately.
- The Kallisto reference file was originally stored in our data file in our Github but the `.gitignore` was hiding that file when we would pull the repo. We need it in order to run Kallisto so we moved it to our teams directory on DSMLP.


### Project Targets:
#### all
Runs entire pipeline on all of the data. Running `all` will run the full pipeline from scratch, this does take hours and sometimes even days to run, it can be ran from scratch but is not needed to be ran from scratch to see our results!
```
{
    ""outdir"": ""data/report"",
    ""report_in_path"": ""notebooks/Alzheimers-Biomarker-Analysis.ipynb"",
    ""report_out_path"": ""report/Alzheimers-Biomarker-Analysis.html""
}
```
#### test
Runs part of pipeline on a couple fastq files. Implements fastqc and kallisto. Then generates this report!
```
{
  ""test_1"": ""SRR837440.fastq.gz"",
  ""test_2"": ""SRR837444.fastq.gz""
}
```
#### data
In Progress! Gets and outputs the data and generates the report as well!
```
{
  ""file_path"": ""/teams/DSC180A_FA20_A00/b04genetics/group_1/raw_data""
}
```
#### eda
Runs EDA process. Makes report with data and plots figures.
```
{
    ""outdir"": ""data/report"",
    ""report_in_path"": ""notebooks/EDA.ipynb"",
    ""report_out_path"": ""notebooks/EDA.html""
}
```
#### viz
Runs Visualization process. Simply outputs all the charts and graphs used in the project.
```
{
    ""outdir"": ""data/report"",
    ""report_in_path"": ""notebooks/Viz.ipynb"",
    ""report_out_path"": ""notebooks/Viz.html""
}
```

#### analyze
Runs the Notebook used for our Analysis portion of the project. Generating the plots that are used to explain our results.
```
{
    ""outdir"": ""data/report"",
    ""report_in_path"": ""notebooks/analyze.ipynb"",
    ""report_out_path"": ""notebooks/analyze.html""
}
```


#### Running `python run.py all` will run the full pipeline from scrath, this does take hours and sometimes even days to run, it can be ran from scratch but is not needed to be ran from scratch to see our results! Other keywords that can be passed into the funciton are `test eda data viz analyze`. Running `python run.py test` is actually the most recommended one, this gives you the full pipeline experience on a fraction of the data, running in just a few minutes. Portions of the code can also be ran with `python run.py data` or `python run.py eda` or a combination of these: `python run.py data eda` etc. We also printed steps along the way to notify the user what is currently running in the pipeline. Our code assumes it is ran on the DSMLP Servers! Without running on the DSMLP Servers we would not be able to access the data, which is why it is important to be connected to the server.



### Responsibilities

Ryan: 
Ryan created the Pipeline that we are using for our project so far: FastQC, Cutadapt, FastQC (2), and Kallisto. Along with formatting the Github repo to the Cookiecutter Data Science standard. 

Justin: 
Justin worked mainly on getting the report side of the project complete. He, alongside Gregory, spent time researching what MicroRNA and biomarkers are to include as part of our background. Researching additional information about Alzheimer’s Disease was also completed. He also worked on getting the initial structure of the report completed prior to the checkpoint. 

Gregory: 
Gregory, alongside Justin worked on the researching miRNA and biomarkers, and their relation to AD. Furthermore, he helped research various parameters and settings for parts of the pipeline. 

All assisted in the implementation of the pipeline alongside editing/reviewing each other’s work. 
"
81,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_66,,,"# RNASeqToolComparison
Data Science Senior Capstone Project: Comparing RNA Sequencing Differential Gene Expression Analysis Tools

In this project, we want to compare distinct differential gene expression analysis tools on simulated data created with different numbers of genes differentially expressed.

## Running the project
* Use the command `launch.sh -i buijoseph21/rna-seq-tool-comparison:v1 -m 6 -P Always` in order to have the necessary software from `compcodeR` (e.g., `generateSyntheticData`, `runDiffExp`, `ABSSeq`, `PoissonSeq`, etc.) to generate the synthetic data & perform differential gene expression analysis. The `-m 6` specifies the number of RAM which is needed to run tools that require more memory. 

## Building the project using `run.py`
* Use the command `python run.py build` to generate the synthetic data in `data/data<N>.rds`, where N represents the dataset number, using `generateSyntheticData`
* Use the command `python run.py analysis` to perform `DESeq2`, `edgeR.exact`, `NOISeq`, `PoissonSeq`, `ttest`, `ABSSeq`, and `voom.limma` on the synthetic data created in `data` folder which returns the results in `out/data<N>_<tool>.rds`, where N represents the dataset number & tool represents the software. The output of each tool will be organized in its respective `<tool_name><synthetic_data_num>` folders in the `<tool_name>` folders. 
* Use the command `python run.py graph` to build area under the curves (AUC), type I error rates, accuracy, sensitivity, specificity, and False Discovery Rates (FDR) graphs to compare how well the tools performed with each other. The output of each of the graphs will be stored in `rna_graphs`.
* Use the command `python run.py real` to run the whole pipeline mentioned above on the real life dataset: Post-Mortem Molecular Profiling of Schizophrenia, Bipolar Disorder, and Major Depressive Disorder (https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-017-0458-5). 

## Running the project on test data
* `ssh` into dsmlp and `git clone` the repository
* Use the command `launch.sh -i buijoseph21/rna-seq-tool-comparison:v1 -m 6` in order to have the necessary software from `compcodeR` (e.g., `generateSyntheticData`, `runDiffExp`, `PoissonSeq`, etc.) to generate the test synthetic data & perform differential gene expression analysis. The `-m 6` specifies the number of RAM which is needed to run tools that require more memory. 
* Use the command `python run.py test` to create a test synthetic dataset of 100 genes differentially expressed where 50 are differentially expressed in condition 1 and 50 are differentially expressed in condition 2. This test dataset contains 5 samples per condition and is a baseline model with no outliers containing abnormal counts. When running `python run.py test`, it should first create a synthetic dataset which will be stored in the `data` folder and named as `test.rds`. Next, the 7 tools will be performed on the `test.rds` where the outputs will be stored in the `out/test` folder which is created when ran. Each tool will produce different results which is stored in `test_<tool>.rds`. For the next step of our pipeline, we want to produce the area under the curve for this test data, which will be stored in `/out/test/test_auc_plot.png` & the summary of all the metrics in `statistics.csv`.

## Group Contributions
* Joseph built the dockerfile/container. He also created the starter code for building the synthetic datasets and performing differential expression analysis tools in `compcodeR` such as `DESeq2`, `edgeR`, `NOISeq`, `voom.limma`, and `ttest`. He was able to create an R script that read all the outputs from each tool mentioned previously to write out to a `num_expressed_by_tool.csv`. He created the notebook that illustrates the timings/duration for each of the tools performed on each synthetic dataset. As for the report, he helped write the Abstract, Background, Dataset (Figure 1), Methods (Creating the synthetic data, DESeq2, NOISeq, edgeR.exact), and briefly explained Figure 3. He also looked over other sections to add onto or revise. 
* Brandon was responsible for creating the `random` outlier synthetic datasets and performing differential expression analysis tools not built-in `compcodeR`, including `ABSSeq` and `PoissonSeq`. Brandon helped write out to the `num_expressed_by_tool.csv` for his outputs produced by `ABSSeq` and `PoissonSeq`. As for the report, he helped write the Dataset and analysis of Figure 3. He also looked over other sections to add onto or revise. Brandon mainly focused on creating the graphs for the comparisons. 
* Luigi was responsible for creating the `single` outlier and `poisson` synthetic datasets and helped perform `NOISeq` and `voom.limma` on the synthetic datasets. As for the report, Luigi created the citations, made revisions based on comments suggested by Shannon, and briefly described Figure 2. He wrote the Methods section for ABSSeq and PoissonSeq. Luigi was mainly responsible for the creation of the website and running/creating the pipeline for the real life dataset. 
","# RNASeqToolComparison
Data Science Senior Capstone Project: Comparing RNA Sequencing Differential Gene Expression Analysis Tools

In this project, we want to compare distinct differential gene expression analysis tools on simulated data created with different numbers of genes differentially expressed.

## Running the project
* Use the command `launch.sh -i buijoseph21/rna-seq-tool-comparison:v1 -m 6 -P Always` in order to have the necessary software from `compcodeR` (e.g., `generateSyntheticData`, `runDiffExp`, `ABSSeq`, `PoissonSeq`, etc.) to generate the synthetic data & perform differential gene expression analysis. The `-m 6` specifies the number of RAM which is needed to run tools that require more memory. 

## Building the project using `run.py`
* Use the command `python run.py build` to generate the synthetic data in `data/data<N>.rds`, where N represents the dataset number, using `generateSyntheticData`
* Use the command `python run.py analysis` to perform `DESeq2`, `edgeR.exact`, `NOISeq`, `PoissonSeq`, `ttest`, `ABSSeq`, and `voom.limma` on the synthetic data created in `data` folder which returns the results in `out/data<N>_<tool>.rds`, where N represents the dataset number & tool represents the software. The output of each tool will be organized in its respective `<tool_name><synthetic_data_num>` folders in the `<tool_name>` folders. 
* Use the command `python run.py graph` to build area under the curves (AUC), type I error rates, accuracy, sensitivity, specificity, and False Discovery Rates (FDR) graphs to compare how well the tools performed with each other. The output of each of the graphs will be stored in `rna_graphs`.
* Use the command `python run.py real` to run the whole pipeline mentioned above on the real life dataset: Post-Mortem Molecular Profiling of Schizophrenia, Bipolar Disorder, and Major Depressive Disorder (https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-017-0458-5). 

## Running the project on test data
* `ssh` into dsmlp and `git clone` the repository
* Use the command `launch.sh -i buijoseph21/rna-seq-tool-comparison:v1 -m 6` in order to have the necessary software from `compcodeR` (e.g., `generateSyntheticData`, `runDiffExp`, `PoissonSeq`, etc.) to generate the test synthetic data & perform differential gene expression analysis. The `-m 6` specifies the number of RAM which is needed to run tools that require more memory. 
* Use the command `python run.py test` to create a test synthetic dataset of 100 genes differentially expressed where 50 are differentially expressed in condition 1 and 50 are differentially expressed in condition 2. This test dataset contains 5 samples per condition and is a baseline model with no outliers containing abnormal counts. When running `python run.py test`, it should first create a synthetic dataset which will be stored in the `data` folder and named as `test.rds`. Next, the 7 tools will be performed on the `test.rds` where the outputs will be stored in the `out/test` folder which is created when ran. Each tool will produce different results which is stored in `test_<tool>.rds`. For the next step of our pipeline, we want to produce the area under the curve for this test data, which will be stored in `/out/test/test_auc_plot.png` & the summary of all the metrics in `statistics.csv`.

## Group Contributions
* Joseph built the dockerfile/container. He also created the starter code for building the synthetic datasets and performing differential expression analysis tools in `compcodeR` such as `DESeq2`, `edgeR`, `NOISeq`, `voom.limma`, and `ttest`. He was able to create an R script that read all the outputs from each tool mentioned previously to write out to a `num_expressed_by_tool.csv`. He created the notebook that illustrates the timings/duration for each of the tools performed on each synthetic dataset. As for the report, he helped write the Abstract, Background, Dataset (Figure 1), Methods (Creating the synthetic data, DESeq2, NOISeq, edgeR.exact), and briefly explained Figure 3. He also looked over other sections to add onto or revise. 
* Brandon was responsible for creating the `random` outlier synthetic datasets and performing differential expression analysis tools not built-in `compcodeR`, including `ABSSeq` and `PoissonSeq`. Brandon helped write out to the `num_expressed_by_tool.csv` for his outputs produced by `ABSSeq` and `PoissonSeq`. As for the report, he helped write the Dataset and analysis of Figure 3. He also looked over other sections to add onto or revise. Brandon mainly focused on creating the graphs for the comparisons. 
* Luigi was responsible for creating the `single` outlier and `poisson` synthetic datasets and helped perform `NOISeq` and `voom.limma` on the synthetic datasets. As for the report, Luigi created the citations, made revisions based on comments suggested by Shannon, and briefly described Figure 2. He wrote the Methods section for ABSSeq and PoissonSeq. Luigi was mainly responsible for the creation of the website and running/creating the pipeline for the real life dataset. 
"
82,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_69,,,"# Title: Genetic Overlap between Alzheimer's, Parkinson’s, and healthy patients

#### Capstone Project: Data Science DSC180B

#### Section B04: Genetics

#### Authors: Saroop Samra, Justin Lu, Xuanyu Wu

#### Date : 2/2/2021

### Overview

This repository code is for the replication project for the paper: Profiles of Extracellular miRNA in Cerebrospinal Fluid and Serum from Patients with Alzheimer’s and Parkinson’s Diseases Correlate with Disease Status and Features of Pathology (https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0094839). The data includes that miRNA sequences from tissues from two biofluids (serum and cerebrospinal fluid), and is from 69 patients with Alzheimer's disease, 67 with Parkinson's disease and 78 neurologically normal controls using next generation small RNA sequencing (NGS).


### Running the project

•	To install the dependencies, run the following command from the root directory of the project: 

    pip install -r requirements.txt


### target: data
•	To process the data, from the root project directory run the command:

    python3 run.py data

•   The data pipeline step takes the .fastq compressed files as input and then applies two transformations: process and align

•	This pipeline step also uses an additional CSV file that is the SRA run database, a sample looks like as follows:

    Run expired_age    CONDITION    BIOFLUID     
    SRR1568567  40  Parkinson's Disease Cerebrospinal 



•   The configuration files for the data step are stored in config/data-params.json. These include the parameters for the tools as well as the directories used for storing the raw, temporary and output files.

    ""raw_data_directory"": ""./data/raw"",
    ""tmp_data_directory"": ""./data/tmp"",
    ""out_data_directory"": ""./data/out"",

•   The configuration also includes an attribute to the SRA run input database (described above), and an attribute of where to store that in the data folder. Additional filter attributes are included for ease of use to avoid processing all patients, if this filter_enable is set it will only process a subset of SRA rows (filter_start_row to filter_start_row + filter_num_rows).

    ""sra_runs"" : {
        ""input_database"" : ""/datasets/SRP046292/exRNA_Atlas_CORE_Results.csv"",
        ""input_database2"" : ""/datasets/SRP046292/SraRunTable.csv"",
        ""input_database3"" : ""/datasets/SRP046292/Table_S1.csv"",
        ""output_database"" : ""data/raw/exRNA_Atlas_CORE_Results.csv"",
        ""filter_enable"" : 0,
        ""filter_start_row"" : 120,
        ""filter_num_rows"" : 10   
    },
    

•	An optional transformation of the data is ""process"" that uses the following data configuration below that will invoke cutadapt which finds and removes adapter sequences. The attributes include the adapters (r1 and r2) to identify the start and end of pairs are a JSON array. The attribute enable allows to disable this cleaning step, instead it will simply copy the paired files from the source dataset. The arguments attribute allows flexible setting of any additional attribute to the cutadapt process. Finally, we have two wildcard paths that indicate the location of the SRA fastq pair files (fastq1 and fastq2).

    ""process"" : {
        ""enable"" : 1,
        ""tool"" : ""/opt/conda/bin/cutadapt"",
        ""r1_adapters"" : [""AAAAA"", ""GGGG""],
        ""r2_adapters"" : [""CCCCC"", ""TTTT""],
        ""arguments"" : ""--pair-adapters --cores=4"",
        ""fastq1_path"" : ""/datasets/srp073813/%run_1.fastq.gz"", 
        ""fastq2_path"" : ""/datasets/srp073813/%run_2.fastq.gz""
    },
    
•   The second transformation of the data is ""aligncount"" that can be set to either use download, STAR or Kallisto. The choice is controlled by the aligncount attribute:

    ""aligncount"" : ""download"",

•   download step will use the ftp location of the gzip file in the Sra table and download using the curl command and unzips and the extracts the readCounts_gencode_sense.txt which represents thae gene counts for the sample. 

    ""download"" : {
        ""enable"" : 1,
        ""tool"" : ""curl"",
        ""arguments"" : ""-L -R"",
        ""read_counts_file"" : ""readCounts_gencode_sense.txt""
    },

•   kallisto uses the index_file attribute is the location of the directory of the reference genome, which for this replication project was GRCh37_E75. The arguments attribute allows flexible setting of any additional attribute to the kallisto process. Including the bootstaro samples.The attribute enable allows to disable this alignment step, this is useful for debugging the process prior step, for example, you can run quality checks on the processed fastq files before proceeding to alignment. 

    ""kallisto"" : {
        ""enable"" : 1,
        ""tool"" : ""/opt/kallisto_linux-v0.42.4/kallisto"",
        ""index_file"" : ""/datasets/srp073813/reference/kallisto_transcripts.idx"",
        ""arguments"" : ""quant -b 8 -t 8""
    },

•   STAR uses the gene_path attribute is the location of the directory of the reference genome, which for this replication project was GRCh37_E75 as described in the reference_gene attribute. The arguments attribute allows flexible setting of any additional attribute to the STAR process. Including TranscriptomeSAM in the quantMode arguments will also output bam files. Additionally, the log file gets outputted which has PRUA (percentage of reads uniquely aligned). The attribute enable allows to disable this alignment step, this is useful for debugging the process prior step, for example, you can run quality checks on the processed fastq files before proceeding to alignment. 

    ""STAR"" : {
        ""enable"" : 1,
        ""tool"" : ""/opt/STAR-2.5.2b/bin/Linux_x86_64_static/STAR"",
        ""reference_gene"" : ""GRCh37_E75"",
        ""gene_path"" : ""/path/to/genomeDir"",
        ""arguments"" : ""--runMode alignReads --quantMode GeneCounts --genomeLoad LoadAndKeep --readFilesCommand zcat --runThreadN 8""
    },




•   The process and align transformation work on each of the samples. After each sample iteration, the temporary fastq files will be deleted to reduce storage requirements.


•   Example processing:

    python3 run.py data

    # ---------------------------------------------------
    # Process
    # ---------------------------------------------------
    # ---------------------------------------------------
    # Starting sample # 1 out of 1
    # ---------------------------------------------------
    # Starting sample # 1 out of 343
    curl-proxy -L -R -o ./data/tmp/SRR1568613.tgz ftp://ftp.genboree.org/exRNA-atlas/grp/Extracellular%20RNA%20Atlas/db/exRNA%20Repository%20-%20hg19/file/exRNA-atlas/exceRptPipeline_v4.6.2/KJENS1-Alzheimers_Parkinsons-2016-10-17/sample_SAMPLE_1022_CONTROL_SER_fastq/CORE_RESULTS/sample_SAMPLE_1022_CONTROL_SER_fastq_KJENS1-Alzheimers_Parkinsons-2016-10-17_CORE_RESULTS_v4.6.2.tgz
    sh: curl-proxy: command not found
    mkdir ./data/tmp/SRR1568613
    tar -C ./data/tmp/SRR1568613 -xzf ./data/tmp/SRR1568613.tgz
    cp ./data/tmp/SRR1568613/data/readCounts_gencode_sense.txt ./data/tmp/SRR1568613_ReadsPerGene.out.tab
    # ---------------------------------------------------
    # Starting sample # 2 out of 343
    curl-proxy -L -R -o ./data/tmp/SRR1568457.tgz ftp://ftp.genboree.org/exRNA-atlas/grp/Extracellular%20RNA%20Atlas/db/exRNA%20Repository%20-%20hg19/file/exRNA-atlas/exceRptPipeline_v4.6.2/KJENS1-Alzheimers_Parkinsons-2016-10-17/sample_SAMPLE_0427_PD_CSF_fastq/CORE_RESULTS/sample_SAMPLE_0427_PD_CSF_fastq_KJENS1-Alzheimers_Parkinsons-2016-10-17_CORE_RESULTS_v4.6.2.tgz
    sh: curl-proxy: command not found
    mkdir ./data/tmp/SRR1568457
    tar -C ./data/tmp/SRR1568457 -xzf ./data/tmp/SRR1568457.tgz
    cp ./data/tmp/SRR1568457/data/readCounts_gencode_sense.txt ./data/tmp/SRR1568457_ReadsPerGene.out.tab
    # ---------------------------------------------------


### target: merge
•   To merge gene count and/or BAM files generated from the data target, from the root project directory run the command:

    python3 run.py merge

•   The configuration files for the data step are stored in config/count-params.json. These include the parameters for the count merge and bam merge and it's associated arguments.

•   The format attrbute informs if to process downlload, kallisto (or STAR) files. The gene counts are merged into a TSV file and as well as a feature table based on the SRA run table. Additional STAR attributes in the JSON allow you to specify skiprows used when processing the  gene count files as well as identifying the column from the  gene matrix file to use as the column used to. There is an additional imputes attribute that allows you to impute any column with missing data. The attributes also include an optional ""filter_names"" gene table used to remove genes as well as removing false-positive genes. Finally, we can rename the feature columns before we save out the feature table.

    ""count"" : {
        ""enable"" : 1,
        ""format"" : ""download"",
        ""skiprows"" : 4,
        ""column_count"" : 1,
        ""skip_samples"" : [""SRR1568391""],
        ""enable_filter"" : 0,
        ""filter_keep_genes"" : ""NM_"",
        ""filter_remove_genes"" : [""chrX"", ""chrY""],
        ""filter_names"" : ""/datasets/srp073813/reference/Gene_Naming.csv"",
        ""run_database"" : ""data/raw/exRNA_Atlas_CORE_Results.csv"",
        ""imputes"" : [""TangleTotal""],
        ""features"" : [""Run"", ""CONDITION"", ""expired_age"", ""BIOFLUID"", ""sex"", ""PMI"", ""sn_depigmentation"", ""Braak score"", ""TangleTotal"", ""Plaque density"", ""PlaqueTotal""],
        ""rename"" : {""CONDITION"" : ""Disorder"", ""BIOFLUID"" : ""Biofluid"", ""Braak score"" : ""Braak_Score"", ""Plaque density"" : ""Plaque_density""},
        ""replace"" : {""from"":[""Parkinson's Disease"", ""Alzheimer's Disease"", ""Cerebrospinal fluid"", ""Healthy Control""], ""to"":[""Parkinson"", ""Alzheimer"", ""Cerebrospinal"", ""Control""]},
        ""output_matrix"" : ""data/out/gene_matrix.tsv"",
        ""output_features"" : ""data/out/features.tsv""
    },

•   For bam merging, which should not be enabled by default, we use the ""samtools"" merge feature that takes all the BAM files and combine them into one merged BAM file. 


    ""bam"" : {
        ""enable"" : 0,
        ""output"" : ""data/tmp/merged.bam"",
        ""tool"" : ""/usr/local/bin/samtools"",
        ""arguments"" : ""merge --threads 8""
    },


•   Example processing:

    python3 run.py merge

    # ---------------------------------------------------
    # Merge
    Input: SRR3438605_ReadsPerGene.out.tab
    Input: SRR3438604_ReadsPerGene.out.tab
    Output: data/out/gene_matrix.tsv data/out/features.tsv
    # Finished
    # ---------------------------------------------------



### target: normalize
•   To normalize the aligned merge counts, from the root project directory run the command:

    python3 run.py normalize

•   The configuration files for the data step are stored in config/normalize-params.json. 

•   We use a custom R script which uses the DESeq2 module to take the input merged gene counts and the experiment features and outputs two normalized counts files. The analysis is done for all samples in the SRA run table. The output_dir sets the output location for the normalized count matrix files. One file is the standard normalized counts using the DESeq2 module, and the second normalized count file is after a Variable Stablization Transform (LRT). We also have a ""max_genes"" attribute that will filter the genes and removes ones that have little to no variance across disorder vesus control.

•   The data JSON configuration file also holds an array of samples, a sample looks like as follows:
    
    {
        ""output_dir"" : ""data/out"",
        ""DESeq2"" : {
            ""Rscript"" : ""/opt/conda/envs/r-bio/bin/Rscript"",
            ""source"" : ""src/data/normalize.r"",
            ""input_counts"" : ""data/out/gene_matrix.tsv"",
            ""input_features"" : ""data/out/features.tsv"",
            ""max_genes"" : 8000
        },
        ""cleanup"" : 0,
        ""verbose"": 1
    }


•   Example processing:

    python3 run.py normalize

    # ---------------------------------------------------
    # Normalize
    Rscript  src/data/normalize.r data/out/gene_matrix.tsv data/out/features.tsv data/out/
    [1] ""Output data/out/normalized_counts.tsv data/out/vst_transformed_counts.tsv""
    # Finished
    # ---------------------------------------------------


### target: analysis
•   To perform the analysis for the gene counts, from the root project directory run the command:

    python3 run.py analysis

•   The configuration files for the data step are stored in config/analysis-params.json. 

•   We use a custom R script which uses the DESeq2 module to take the input merged gene counts and the experiment features and outputs 2 sets of files for each biofluid region. Each biofluid region will compare a disorder versus Control. This will result in a total of 4 sets of files (2 biofluid regions x 2 disorder pair comparisons). Each output set includes a Likelihood Ratio Test (LRT) using the full and reduced model as specified in the attributes below as well as a MA-Plot and Heatmap. The additional attributes include the property of doing parallel processing for DESeq2.
    
    {
        ""output_prefix"" : ""data/out/%biofluid_region%"",
        ""DESeq2"" : {
            ""Rscript"" : ""/opt/conda/envs/r-bio/bin/Rscript"",
            ""biofluid_regions"" : [""Cerebrospinal"", ""Serum""],
            ""disorders"" : [""Parkinson"", ""Alzheimer""],
            ""control"" : ""Control"",
            ""input_counts"" : ""data/out/pca_normalized_counts.tsv"",
            ""input_features"" : ""data/out/features.tsv"",
            ""source"" : ""src/analysis/analysis.r"",
            ""full"" : ""expired_age+sex+PMI+sn_depigmentation+Braak_Score+TangleTotal+Plaque_density+PlaqueTotal+Disorder"",
            ""reduced"" : ""expired_age+sex+PMI+sn_depigmentation+Braak_Score+TangleTotal+Plaque_density+PlaqueTotal"",
            ""parallel"" : 0
        },
        ""cleanup"" : 0,
        ""verbose"": 1
    }


•   Example processing:

    python3 run.py analysis

    # ---------------------------------------------------
    # Analysis
    Cerebrospinal x Parkinson vs Control
    Rscript src/analysis/analysis.r data/out/Cerebrospinal/Parkinson/gene_matrix.tsv data/out/Cerebrospinal/Parkinson/features.tsv data/out/Cerebrospinal/Parkinson/ full=expired_age+sex+PMI+sn_depigmentation+Braak_Score+TangleTotal+Plaque_density+PlaqueTotal+Disorder reduced=expired_age+sex+PMI+sn_depigmentation+Braak_Score+TangleTotal+Plaque_density+PlaqueTotal charts=1 parallel=0


### target: visualize

•   The visualize pipeline step can be invoked as follows:

    python3 run.py visualize

•   The configuration files for the data step are stored in config/visualize-params.json. The output will include multiple sets of charts: Gene Spread Variance Histogram, SRA Linear Correlation between SRA chart, MA-Plot 2x2 chart, Heat Map 2x2 chart, 2x2 Histogram, 4x4 Correlation Matrix and a Disorder Venn Diagram. Each chart type has flexible settings to control the input and layout for the charts as shown below:

    ""gene_hist"" : {
        ""enable"" : 1,
        ""max_genes"" : 8000,
        ""nbins"" : 100,
        ""title"" : ""Distribution of Genes Based on Spread Metric: All vs Top Genes""
    },
    ""missing_plot"" : {
        ""enable"" : 1,
        ""title"" : ""Percentage of Missing Genes over""
    },
    ""sra_lm"" : {
        ""enable"" : 1,
        ""sra"" : [""SRR1568567"", ""SRR1568584""],
        ""normalized_counts"" : ""data/out/normalized_counts.tsv"",
        ""vst_counts"" : ""data/out/vst_transformed_counts.tsv"",
        ""title"" : ""%sra% Regression Log(Norm) v VST counts""
    },
    ""ma_plot"" : {
        ""enable"" : 1,
        ""biofluid_regions"" : [""Cerebrospinal"", ""Serum""],
        ""disorders"" : [""Parkinson"", ""Alzheimer""],
        ""src_image"" : ""MAplot.png"",
        ""title"" : ""MA Plot: Biofluid Region vs Disorder""
    },
    ""heat_map"" : {
        ""enable"" : 1,
        ""biofluid_regions"" : [""Cerebrospinal"", ""Serum""],
        ""disorders"" : [""Parkinson"", ""Alzheimer""],
        ""src_image"" : ""heatmap.png"",
        ""title"" : ""Heat Map: Biofluid Region vs Disorder""
    },
    ""histogram"" : {
        ""enable"" : 1,
        ""biofluid_regions"" : [""Cerebrospinal"", ""Serum""],
        ""disorders"" : [""Parkinson"", ""Alzheimer""],
        ""title"" : ""Histograms Differential Gene Expression vs Control"",
        ""ylim"" : 55
    },
    ""corrmatrix"" : {
        ""enable"" : 1,
        ""title"" : ""Spearman Correlations of log2 fold gene expression""
    },
    ""venn"" : {
        ""enable"" : 1,
        ""biofluid_regions"" : [""Cerebrospinal"", ""Serum""],
        ""disorders"" : [""Parkinson"", ""Alzheimer""],
        ""pvalue_cutoff"" : 0.05,
        ""title"" : ""Venn Diagram Disorders""
    },


•   Example processing:

    python3 run.py visualize

    # ---------------------------------------------------
    # Visualize
    # Finished
    # ---------------------------------------------------


### target: qc

•   The quality pipeline step can be invoked as follows:

    python3 run.py qc

•   The configuration files for the data step are stored in config/qc-params.json. These include the parameters for the output directory where the quality HTML reports will be outputted. 

    ""outdir"" : ""data/out"",
    ""inputs"" : ""data/tmp"",

•   For fastq files, the quality tool attribute is set to fastqc and that includes attributes to extract reports or keep them in a zip file. To enable this quality check make sure you set the cleanup to 0 in the data configuration pipeline as well as to disable the STAR processing, this will retain the fastq.qz files after the data pipeline step is executed.

    ""fastq"" : {
        ""enable"" : 1,
        ""tool"" : ""/opt/FastQC/fastqc"",
        ""extract"" : 1   
    },

•   For bam files, the quality tool attribute is set to picard and that includes attributes such as collecting alignment summary metrics. To enable this quality check make sure you set the cleanup to 0 in the data configuration pipeline and add 'TranscriptomeSAM' to the arguments for STAR which will then output BAM files that will be retained after the data pipeline step is executed.

    ""bam"" : {
        ""enable"" : 1,
        ""tool"" : ""java"",
        ""jar"" : ""/opt/picard-tools-1.88/CollectAlignmentSummaryMetrics.jar""
    },
    

•   Example processing:

    python3 run.py qc

    # ---------------------------------------------------
    # Quality Check
    fastqc data/tmp/out.1.fastq.gz --outdir=data/out --extract
    fastqc data/tmp/out.2.fastq.gz --outdir=data/out --extract
    java -jar /opt/picard-tools-1.88/CollectAlignmentSummaryMetrics.jar INPUT=data/tmp/SRR3438604_Aligned.bam OUTPUT=data/out/SRR3438604_Aligned.bam.txt
    java -jar /opt/picard-tools-1.88/CollectAlignmentSummaryMetrics.jar INPUT=data/tmp/SRR3438605_Aligned.bam OUTPUT=data/out/SRR3438605_Aligned.bam.txt
    # Finished
    # ---------------------------------------------------


### target: report
•   To generate the report from the notebook, run this command:

    python3 run.py report

•   The configuration files for the data step are stored in config/report-params.json. 

    {
        ""tool"": ""jupyter"",
        ""args"": ""nbconvert --no-input --to html --output report.html notebooks/report.ipynb"",
        ""verbose"" : 1
    }


### target: clean 

•	To clean the data (remove it from the working project), from the root project directory run the command:

python3 run.py clean


### target: all 

•   The all target will execute the following steps in sequence: data, merge, normalize, analysis and visualize. It can be executed as follows:

python3 run.py all


### Future Work

•	New pipeline step: predict. This step will use the model to predict the classification for a given miRNA sequences on the test data and reporting the classification errors



### Major Change History


Date:  2/2/2021

Work completed:

- Created new visualization, Volcano Plot, wrote the code and implemented it into our pipeline in the visualize step 
- Updated the code pipeline to make the correlation matrix more meaningful by adding color
- Finished descriptions for EDA plots


Date:  1/19/2021

Work completed:

- Got all steps in pipeline to work with new data (data, merge, normalize, analysis, visualize) 
- Used LRT Hypothesis Testing and have updated all previous quarter visualizations to work for our new data set
- Compared the outputs of 2 samples that failed FastQC/ERCC quality check with 2 samples that passed
- Developed and organized EDA code for gene matrix (missingness, correlation between sequence count and numerical features of the samples) 


Date:  1/12/2020

Work completed:

- Created repo, initial version from the DSC180A Genetics project
- Added new download step and modified data target to use new SRA
- Wrote out background information/introduction sections of the report, researched our diseases (Alzheimer’s/Parkinson’s) and data sources (miRNA, serum/CSF)
- Developed and organized EDA code for features in the SRA run table (box plots, histograms, bar plots, etc)



### Responsibilities


* Saroop Samra, developed the original codebase based on the DSC180A genetics replication project. Saroop ported the code to support the new miRNA dataset including adding a new download step in the data target. She worked on modifying the code and configuration files for the merge, normalize, analysis and visualize targets to process and generate the visualizations from the DSC180A project. She got the new Volcano Plot to work for our dataset and wrote basic descriptions for the visualizations including what significant patterns exists (MA plot, heatmap, histogram, venn diagram, correlation matrix).

* Justin Lu, wrote out background information/introduction sections of the report. Justin did the data quality control check with FastQC (focused on the FastQC report outputs that we acquired instead of actually running FastQC since we still do not have access to the raw .fastq data), wrote in descriptions for visualizations and some of the EDA in our final report notebook. He updated the code pipeline with colored correlation matrix.

* Xuanyu Wu, generated around 20 EDA plots for features that describe our merged dataset (incl. box plots, histograms, bar plots, etc) Xuanyu created EDA plots to explore the missingness of the gene count matrix and the basic correlation of each sequence with the numerical features we have selected. She also finished the descriptions for EDA plots and analysis.




","# Title: Genetic Overlap between Alzheimer's, Parkinson’s, and healthy patients

#### Capstone Project: Data Science DSC180B

#### Section B04: Genetics

#### Authors: Saroop Samra, Justin Lu, Xuanyu Wu

#### Date : 2/2/2021

### Overview

This repository code is for the replication project for the paper: Profiles of Extracellular miRNA in Cerebrospinal Fluid and Serum from Patients with Alzheimer’s and Parkinson’s Diseases Correlate with Disease Status and Features of Pathology (https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0094839). The data includes that miRNA sequences from tissues from two biofluids (serum and cerebrospinal fluid), and is from 69 patients with Alzheimer's disease, 67 with Parkinson's disease and 78 neurologically normal controls using next generation small RNA sequencing (NGS).


### Running the project

•	To install the dependencies, run the following command from the root directory of the project: 

    pip install -r requirements.txt


### target: data
•	To process the data, from the root project directory run the command:

    python3 run.py data

•   The data pipeline step takes the .fastq compressed files as input and then applies two transformations: process and align

•	This pipeline step also uses an additional CSV file that is the SRA run database, a sample looks like as follows:

    Run expired_age    CONDITION    BIOFLUID     
    SRR1568567  40  Parkinson's Disease Cerebrospinal 



•   The configuration files for the data step are stored in config/data-params.json. These include the parameters for the tools as well as the directories used for storing the raw, temporary and output files.

    ""raw_data_directory"": ""./data/raw"",
    ""tmp_data_directory"": ""./data/tmp"",
    ""out_data_directory"": ""./data/out"",

•   The configuration also includes an attribute to the SRA run input database (described above), and an attribute of where to store that in the data folder. Additional filter attributes are included for ease of use to avoid processing all patients, if this filter_enable is set it will only process a subset of SRA rows (filter_start_row to filter_start_row + filter_num_rows).

    ""sra_runs"" : {
        ""input_database"" : ""/datasets/SRP046292/exRNA_Atlas_CORE_Results.csv"",
        ""input_database2"" : ""/datasets/SRP046292/SraRunTable.csv"",
        ""input_database3"" : ""/datasets/SRP046292/Table_S1.csv"",
        ""output_database"" : ""data/raw/exRNA_Atlas_CORE_Results.csv"",
        ""filter_enable"" : 0,
        ""filter_start_row"" : 120,
        ""filter_num_rows"" : 10   
    },
    

•	An optional transformation of the data is ""process"" that uses the following data configuration below that will invoke cutadapt which finds and removes adapter sequences. The attributes include the adapters (r1 and r2) to identify the start and end of pairs are a JSON array. The attribute enable allows to disable this cleaning step, instead it will simply copy the paired files from the source dataset. The arguments attribute allows flexible setting of any additional attribute to the cutadapt process. Finally, we have two wildcard paths that indicate the location of the SRA fastq pair files (fastq1 and fastq2).

    ""process"" : {
        ""enable"" : 1,
        ""tool"" : ""/opt/conda/bin/cutadapt"",
        ""r1_adapters"" : [""AAAAA"", ""GGGG""],
        ""r2_adapters"" : [""CCCCC"", ""TTTT""],
        ""arguments"" : ""--pair-adapters --cores=4"",
        ""fastq1_path"" : ""/datasets/srp073813/%run_1.fastq.gz"", 
        ""fastq2_path"" : ""/datasets/srp073813/%run_2.fastq.gz""
    },
    
•   The second transformation of the data is ""aligncount"" that can be set to either use download, STAR or Kallisto. The choice is controlled by the aligncount attribute:

    ""aligncount"" : ""download"",

•   download step will use the ftp location of the gzip file in the Sra table and download using the curl command and unzips and the extracts the readCounts_gencode_sense.txt which represents thae gene counts for the sample. 

    ""download"" : {
        ""enable"" : 1,
        ""tool"" : ""curl"",
        ""arguments"" : ""-L -R"",
        ""read_counts_file"" : ""readCounts_gencode_sense.txt""
    },

•   kallisto uses the index_file attribute is the location of the directory of the reference genome, which for this replication project was GRCh37_E75. The arguments attribute allows flexible setting of any additional attribute to the kallisto process. Including the bootstaro samples.The attribute enable allows to disable this alignment step, this is useful for debugging the process prior step, for example, you can run quality checks on the processed fastq files before proceeding to alignment. 

    ""kallisto"" : {
        ""enable"" : 1,
        ""tool"" : ""/opt/kallisto_linux-v0.42.4/kallisto"",
        ""index_file"" : ""/datasets/srp073813/reference/kallisto_transcripts.idx"",
        ""arguments"" : ""quant -b 8 -t 8""
    },

•   STAR uses the gene_path attribute is the location of the directory of the reference genome, which for this replication project was GRCh37_E75 as described in the reference_gene attribute. The arguments attribute allows flexible setting of any additional attribute to the STAR process. Including TranscriptomeSAM in the quantMode arguments will also output bam files. Additionally, the log file gets outputted which has PRUA (percentage of reads uniquely aligned). The attribute enable allows to disable this alignment step, this is useful for debugging the process prior step, for example, you can run quality checks on the processed fastq files before proceeding to alignment. 

    ""STAR"" : {
        ""enable"" : 1,
        ""tool"" : ""/opt/STAR-2.5.2b/bin/Linux_x86_64_static/STAR"",
        ""reference_gene"" : ""GRCh37_E75"",
        ""gene_path"" : ""/path/to/genomeDir"",
        ""arguments"" : ""--runMode alignReads --quantMode GeneCounts --genomeLoad LoadAndKeep --readFilesCommand zcat --runThreadN 8""
    },




•   The process and align transformation work on each of the samples. After each sample iteration, the temporary fastq files will be deleted to reduce storage requirements.


•   Example processing:

    python3 run.py data

    # ---------------------------------------------------
    # Process
    # ---------------------------------------------------
    # ---------------------------------------------------
    # Starting sample # 1 out of 1
    # ---------------------------------------------------
    # Starting sample # 1 out of 343
    curl-proxy -L -R -o ./data/tmp/SRR1568613.tgz ftp://ftp.genboree.org/exRNA-atlas/grp/Extracellular%20RNA%20Atlas/db/exRNA%20Repository%20-%20hg19/file/exRNA-atlas/exceRptPipeline_v4.6.2/KJENS1-Alzheimers_Parkinsons-2016-10-17/sample_SAMPLE_1022_CONTROL_SER_fastq/CORE_RESULTS/sample_SAMPLE_1022_CONTROL_SER_fastq_KJENS1-Alzheimers_Parkinsons-2016-10-17_CORE_RESULTS_v4.6.2.tgz
    sh: curl-proxy: command not found
    mkdir ./data/tmp/SRR1568613
    tar -C ./data/tmp/SRR1568613 -xzf ./data/tmp/SRR1568613.tgz
    cp ./data/tmp/SRR1568613/data/readCounts_gencode_sense.txt ./data/tmp/SRR1568613_ReadsPerGene.out.tab
    # ---------------------------------------------------
    # Starting sample # 2 out of 343
    curl-proxy -L -R -o ./data/tmp/SRR1568457.tgz ftp://ftp.genboree.org/exRNA-atlas/grp/Extracellular%20RNA%20Atlas/db/exRNA%20Repository%20-%20hg19/file/exRNA-atlas/exceRptPipeline_v4.6.2/KJENS1-Alzheimers_Parkinsons-2016-10-17/sample_SAMPLE_0427_PD_CSF_fastq/CORE_RESULTS/sample_SAMPLE_0427_PD_CSF_fastq_KJENS1-Alzheimers_Parkinsons-2016-10-17_CORE_RESULTS_v4.6.2.tgz
    sh: curl-proxy: command not found
    mkdir ./data/tmp/SRR1568457
    tar -C ./data/tmp/SRR1568457 -xzf ./data/tmp/SRR1568457.tgz
    cp ./data/tmp/SRR1568457/data/readCounts_gencode_sense.txt ./data/tmp/SRR1568457_ReadsPerGene.out.tab
    # ---------------------------------------------------


### target: merge
•   To merge gene count and/or BAM files generated from the data target, from the root project directory run the command:

    python3 run.py merge

•   The configuration files for the data step are stored in config/count-params.json. These include the parameters for the count merge and bam merge and it's associated arguments.

•   The format attrbute informs if to process downlload, kallisto (or STAR) files. The gene counts are merged into a TSV file and as well as a feature table based on the SRA run table. Additional STAR attributes in the JSON allow you to specify skiprows used when processing the  gene count files as well as identifying the column from the  gene matrix file to use as the column used to. There is an additional imputes attribute that allows you to impute any column with missing data. The attributes also include an optional ""filter_names"" gene table used to remove genes as well as removing false-positive genes. Finally, we can rename the feature columns before we save out the feature table.

    ""count"" : {
        ""enable"" : 1,
        ""format"" : ""download"",
        ""skiprows"" : 4,
        ""column_count"" : 1,
        ""skip_samples"" : [""SRR1568391""],
        ""enable_filter"" : 0,
        ""filter_keep_genes"" : ""NM_"",
        ""filter_remove_genes"" : [""chrX"", ""chrY""],
        ""filter_names"" : ""/datasets/srp073813/reference/Gene_Naming.csv"",
        ""run_database"" : ""data/raw/exRNA_Atlas_CORE_Results.csv"",
        ""imputes"" : [""TangleTotal""],
        ""features"" : [""Run"", ""CONDITION"", ""expired_age"", ""BIOFLUID"", ""sex"", ""PMI"", ""sn_depigmentation"", ""Braak score"", ""TangleTotal"", ""Plaque density"", ""PlaqueTotal""],
        ""rename"" : {""CONDITION"" : ""Disorder"", ""BIOFLUID"" : ""Biofluid"", ""Braak score"" : ""Braak_Score"", ""Plaque density"" : ""Plaque_density""},
        ""replace"" : {""from"":[""Parkinson's Disease"", ""Alzheimer's Disease"", ""Cerebrospinal fluid"", ""Healthy Control""], ""to"":[""Parkinson"", ""Alzheimer"", ""Cerebrospinal"", ""Control""]},
        ""output_matrix"" : ""data/out/gene_matrix.tsv"",
        ""output_features"" : ""data/out/features.tsv""
    },

•   For bam merging, which should not be enabled by default, we use the ""samtools"" merge feature that takes all the BAM files and combine them into one merged BAM file. 


    ""bam"" : {
        ""enable"" : 0,
        ""output"" : ""data/tmp/merged.bam"",
        ""tool"" : ""/usr/local/bin/samtools"",
        ""arguments"" : ""merge --threads 8""
    },


•   Example processing:

    python3 run.py merge

    # ---------------------------------------------------
    # Merge
    Input: SRR3438605_ReadsPerGene.out.tab
    Input: SRR3438604_ReadsPerGene.out.tab
    Output: data/out/gene_matrix.tsv data/out/features.tsv
    # Finished
    # ---------------------------------------------------



### target: normalize
•   To normalize the aligned merge counts, from the root project directory run the command:

    python3 run.py normalize

•   The configuration files for the data step are stored in config/normalize-params.json. 

•   We use a custom R script which uses the DESeq2 module to take the input merged gene counts and the experiment features and outputs two normalized counts files. The analysis is done for all samples in the SRA run table. The output_dir sets the output location for the normalized count matrix files. One file is the standard normalized counts using the DESeq2 module, and the second normalized count file is after a Variable Stablization Transform (LRT). We also have a ""max_genes"" attribute that will filter the genes and removes ones that have little to no variance across disorder vesus control.

•   The data JSON configuration file also holds an array of samples, a sample looks like as follows:
    
    {
        ""output_dir"" : ""data/out"",
        ""DESeq2"" : {
            ""Rscript"" : ""/opt/conda/envs/r-bio/bin/Rscript"",
            ""source"" : ""src/data/normalize.r"",
            ""input_counts"" : ""data/out/gene_matrix.tsv"",
            ""input_features"" : ""data/out/features.tsv"",
            ""max_genes"" : 8000
        },
        ""cleanup"" : 0,
        ""verbose"": 1
    }


•   Example processing:

    python3 run.py normalize

    # ---------------------------------------------------
    # Normalize
    Rscript  src/data/normalize.r data/out/gene_matrix.tsv data/out/features.tsv data/out/
    [1] ""Output data/out/normalized_counts.tsv data/out/vst_transformed_counts.tsv""
    # Finished
    # ---------------------------------------------------


### target: analysis
•   To perform the analysis for the gene counts, from the root project directory run the command:

    python3 run.py analysis

•   The configuration files for the data step are stored in config/analysis-params.json. 

•   We use a custom R script which uses the DESeq2 module to take the input merged gene counts and the experiment features and outputs 2 sets of files for each biofluid region. Each biofluid region will compare a disorder versus Control. This will result in a total of 4 sets of files (2 biofluid regions x 2 disorder pair comparisons). Each output set includes a Likelihood Ratio Test (LRT) using the full and reduced model as specified in the attributes below as well as a MA-Plot and Heatmap. The additional attributes include the property of doing parallel processing for DESeq2.
    
    {
        ""output_prefix"" : ""data/out/%biofluid_region%"",
        ""DESeq2"" : {
            ""Rscript"" : ""/opt/conda/envs/r-bio/bin/Rscript"",
            ""biofluid_regions"" : [""Cerebrospinal"", ""Serum""],
            ""disorders"" : [""Parkinson"", ""Alzheimer""],
            ""control"" : ""Control"",
            ""input_counts"" : ""data/out/pca_normalized_counts.tsv"",
            ""input_features"" : ""data/out/features.tsv"",
            ""source"" : ""src/analysis/analysis.r"",
            ""full"" : ""expired_age+sex+PMI+sn_depigmentation+Braak_Score+TangleTotal+Plaque_density+PlaqueTotal+Disorder"",
            ""reduced"" : ""expired_age+sex+PMI+sn_depigmentation+Braak_Score+TangleTotal+Plaque_density+PlaqueTotal"",
            ""parallel"" : 0
        },
        ""cleanup"" : 0,
        ""verbose"": 1
    }


•   Example processing:

    python3 run.py analysis

    # ---------------------------------------------------
    # Analysis
    Cerebrospinal x Parkinson vs Control
    Rscript src/analysis/analysis.r data/out/Cerebrospinal/Parkinson/gene_matrix.tsv data/out/Cerebrospinal/Parkinson/features.tsv data/out/Cerebrospinal/Parkinson/ full=expired_age+sex+PMI+sn_depigmentation+Braak_Score+TangleTotal+Plaque_density+PlaqueTotal+Disorder reduced=expired_age+sex+PMI+sn_depigmentation+Braak_Score+TangleTotal+Plaque_density+PlaqueTotal charts=1 parallel=0


### target: visualize

•   The visualize pipeline step can be invoked as follows:

    python3 run.py visualize

•   The configuration files for the data step are stored in config/visualize-params.json. The output will include multiple sets of charts: Gene Spread Variance Histogram, SRA Linear Correlation between SRA chart, MA-Plot 2x2 chart, Heat Map 2x2 chart, 2x2 Histogram, 4x4 Correlation Matrix and a Disorder Venn Diagram. Each chart type has flexible settings to control the input and layout for the charts as shown below:

    ""gene_hist"" : {
        ""enable"" : 1,
        ""max_genes"" : 8000,
        ""nbins"" : 100,
        ""title"" : ""Distribution of Genes Based on Spread Metric: All vs Top Genes""
    },
    ""missing_plot"" : {
        ""enable"" : 1,
        ""title"" : ""Percentage of Missing Genes over""
    },
    ""sra_lm"" : {
        ""enable"" : 1,
        ""sra"" : [""SRR1568567"", ""SRR1568584""],
        ""normalized_counts"" : ""data/out/normalized_counts.tsv"",
        ""vst_counts"" : ""data/out/vst_transformed_counts.tsv"",
        ""title"" : ""%sra% Regression Log(Norm) v VST counts""
    },
    ""ma_plot"" : {
        ""enable"" : 1,
        ""biofluid_regions"" : [""Cerebrospinal"", ""Serum""],
        ""disorders"" : [""Parkinson"", ""Alzheimer""],
        ""src_image"" : ""MAplot.png"",
        ""title"" : ""MA Plot: Biofluid Region vs Disorder""
    },
    ""heat_map"" : {
        ""enable"" : 1,
        ""biofluid_regions"" : [""Cerebrospinal"", ""Serum""],
        ""disorders"" : [""Parkinson"", ""Alzheimer""],
        ""src_image"" : ""heatmap.png"",
        ""title"" : ""Heat Map: Biofluid Region vs Disorder""
    },
    ""histogram"" : {
        ""enable"" : 1,
        ""biofluid_regions"" : [""Cerebrospinal"", ""Serum""],
        ""disorders"" : [""Parkinson"", ""Alzheimer""],
        ""title"" : ""Histograms Differential Gene Expression vs Control"",
        ""ylim"" : 55
    },
    ""corrmatrix"" : {
        ""enable"" : 1,
        ""title"" : ""Spearman Correlations of log2 fold gene expression""
    },
    ""venn"" : {
        ""enable"" : 1,
        ""biofluid_regions"" : [""Cerebrospinal"", ""Serum""],
        ""disorders"" : [""Parkinson"", ""Alzheimer""],
        ""pvalue_cutoff"" : 0.05,
        ""title"" : ""Venn Diagram Disorders""
    },


•   Example processing:

    python3 run.py visualize

    # ---------------------------------------------------
    # Visualize
    # Finished
    # ---------------------------------------------------


### target: qc

•   The quality pipeline step can be invoked as follows:

    python3 run.py qc

•   The configuration files for the data step are stored in config/qc-params.json. These include the parameters for the output directory where the quality HTML reports will be outputted. 

    ""outdir"" : ""data/out"",
    ""inputs"" : ""data/tmp"",

•   For fastq files, the quality tool attribute is set to fastqc and that includes attributes to extract reports or keep them in a zip file. To enable this quality check make sure you set the cleanup to 0 in the data configuration pipeline as well as to disable the STAR processing, this will retain the fastq.qz files after the data pipeline step is executed.

    ""fastq"" : {
        ""enable"" : 1,
        ""tool"" : ""/opt/FastQC/fastqc"",
        ""extract"" : 1   
    },

•   For bam files, the quality tool attribute is set to picard and that includes attributes such as collecting alignment summary metrics. To enable this quality check make sure you set the cleanup to 0 in the data configuration pipeline and add 'TranscriptomeSAM' to the arguments for STAR which will then output BAM files that will be retained after the data pipeline step is executed.

    ""bam"" : {
        ""enable"" : 1,
        ""tool"" : ""java"",
        ""jar"" : ""/opt/picard-tools-1.88/CollectAlignmentSummaryMetrics.jar""
    },
    

•   Example processing:

    python3 run.py qc

    # ---------------------------------------------------
    # Quality Check
    fastqc data/tmp/out.1.fastq.gz --outdir=data/out --extract
    fastqc data/tmp/out.2.fastq.gz --outdir=data/out --extract
    java -jar /opt/picard-tools-1.88/CollectAlignmentSummaryMetrics.jar INPUT=data/tmp/SRR3438604_Aligned.bam OUTPUT=data/out/SRR3438604_Aligned.bam.txt
    java -jar /opt/picard-tools-1.88/CollectAlignmentSummaryMetrics.jar INPUT=data/tmp/SRR3438605_Aligned.bam OUTPUT=data/out/SRR3438605_Aligned.bam.txt
    # Finished
    # ---------------------------------------------------


### target: report
•   To generate the report from the notebook, run this command:

    python3 run.py report

•   The configuration files for the data step are stored in config/report-params.json. 

    {
        ""tool"": ""jupyter"",
        ""args"": ""nbconvert --no-input --to html --output report.html notebooks/report.ipynb"",
        ""verbose"" : 1
    }


### target: clean 

•	To clean the data (remove it from the working project), from the root project directory run the command:

python3 run.py clean


### target: all 

•   The all target will execute the following steps in sequence: data, merge, normalize, analysis and visualize. It can be executed as follows:

python3 run.py all


### Future Work

•	New pipeline step: predict. This step will use the model to predict the classification for a given miRNA sequences on the test data and reporting the classification errors



### Major Change History


Date:  2/2/2021

Work completed:

- Created new visualization, Volcano Plot, wrote the code and implemented it into our pipeline in the visualize step 
- Updated the code pipeline to make the correlation matrix more meaningful by adding color
- Finished descriptions for EDA plots


Date:  1/19/2021

Work completed:

- Got all steps in pipeline to work with new data (data, merge, normalize, analysis, visualize) 
- Used LRT Hypothesis Testing and have updated all previous quarter visualizations to work for our new data set
- Compared the outputs of 2 samples that failed FastQC/ERCC quality check with 2 samples that passed
- Developed and organized EDA code for gene matrix (missingness, correlation between sequence count and numerical features of the samples) 


Date:  1/12/2020

Work completed:

- Created repo, initial version from the DSC180A Genetics project
- Added new download step and modified data target to use new SRA
- Wrote out background information/introduction sections of the report, researched our diseases (Alzheimer’s/Parkinson’s) and data sources (miRNA, serum/CSF)
- Developed and organized EDA code for features in the SRA run table (box plots, histograms, bar plots, etc)



### Responsibilities


* Saroop Samra, developed the original codebase based on the DSC180A genetics replication project. Saroop ported the code to support the new miRNA dataset including adding a new download step in the data target. She worked on modifying the code and configuration files for the merge, normalize, analysis and visualize targets to process and generate the visualizations from the DSC180A project. She got the new Volcano Plot to work for our dataset and wrote basic descriptions for the visualizations including what significant patterns exists (MA plot, heatmap, histogram, venn diagram, correlation matrix).

* Justin Lu, wrote out background information/introduction sections of the report. Justin did the data quality control check with FastQC (focused on the FastQC report outputs that we acquired instead of actually running FastQC since we still do not have access to the raw .fastq data), wrote in descriptions for visualizations and some of the EDA in our final report notebook. He updated the code pipeline with colored correlation matrix.

* Xuanyu Wu, generated around 20 EDA plots for features that describe our merged dataset (incl. box plots, histograms, bar plots, etc) Xuanyu created EDA plots to explore the missingness of the gene count matrix and the basic correlation of each sequence with the numerical features we have selected. She also finished the descriptions for EDA plots and analysis.




"
83,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_9,,,"# Live_vs_Vod_Project
## Group Name: Live

Due to the variety, affordability and convenience of online video streaming, there are more subscribers than ever to video streaming platforms. Moreover, the decreased operation of non-essential businesses and increase in the number of people working from home in this past year has further compounded this effect. More people are streaming live lectures, sports, news, and video calls via the internet at home today than we have ever seen before. Internet Service Providers, such as Viasat, are tasked with optimizing  internet connections and tailoring their allocation of resources to fit each unique customer’s needs. With this increase in internet activity, it would be especially beneficial for Viasat to understand what issues arise when customers stream various forms of video. In general, different internet activities require different resources to optimize the connection. For example, if a customer watches a lot of live video they may prefer a connection with lower latency and higher bandwidth. Although we are able to identify the genre of an activity when a user is not using a VPN, the challenge arises when a user chooses to surf the web through a VPN. When it comes to VPN use cases we can’t identify a user’s unique activity when they experience issues, thus making us unable to successfully troubleshoot  those problems. This is where a tool that could identify various internet activities, specifically live or uploaded video streaming, within a VPN tunnel would be extremely useful for an Internet Service Provider. 

## Project Report: 
Found within the **references** folder. The file name is Final_Report.

## Guide for Launching this Project:
Note: These instructions assume that the user has access to the DSMLP server to be able to run this project. Open terminal, run these commands in the associated order:

1.) **ssh user@dsmlp-login.ucsd.edu** (user refers to your school username). Enter credentials.

2.) **launch-180-gid.sh -G 100011655 -P Always -i apristin99/live_vs_vod_project**

3.) **git clone https://github.com/pristinsky1/live_vs_video_on_demand_VPN_detection.git**

4.) **cd live_vs_video_on_demand_VPN_detection**

Now you are within the right directory with the environment already set up! Start running the files!

5.) For files needed to be predict, you have to put them in the ""data/in"" directory. Acceptable files are files generated network-stats tool provided by Viasat.

6.) If you wish to train a new classifier based on new data or new type of model and parameters, you have to change the location of training data and other settings in the ""config/train-parmas.json"". Otherwise, you can directly run **python run.py predict** using the trained model contained in the project.


## Guide for Pipeline Testing:
Run **python run.py test** and the results will be in **test/out**. Within that folder, it contains the output dataframes, model, and reports of accuracies for the test data.


## Contents:
There are three parts of contents:
1. src folder - Contains all library code.
2. config folder - Contains directory of each target.
3. run.py - Main Program for this project.
4. notebooks folder - stores notebooks for this project.
5. data/out folder - stores results of this project.

The two files to look at for results under data/out:

training_report.json: Json file contains the report of model's basic information and its performance on validation set.
predictions.csv: DataFrame contains basic information of one record generated by network-stats and its prediction result.



## How to run it?
Warning: The feature, train and predict has to be run in fixed order. If you want to run everything at once, you can use the all script.

Use console to run **python run.py eda** as a script. This will run the eda notebook and store the html version for easy accessibility under ""/notebooks"". 

Use console to run **python run.py feature** as a script. This will create the features. The output dataframe will be stored in ""data/out"" directory in csv format.

Use console to run **python run.py train** as a script. This will train the model. The output model and report will be stored in ""data/out"" directory in csv format.

Use console to run **python run.py predict** as a script. This will classify the input dataset as live or streaming. The output dataframe will be stored in ""data/out"" directory in csv format.

Use console to run **python run.py all** as a script. This will run everything listed above. The output dataframes and model and report will be stored in ""data/out"" directory in csv format.


## Description of Each Params Files
""feature-params.json"" -- ""indir: the input directory of training set, outdir: the output directory of generated dataframe, output: 1 means output dataframe containing features information, 0 means only return it as a dataframe(Must be 1 in feature-params.json)""

""train-params.json"" -- 
    :param: indir: file directory where extracted features stored.
    :param: outdir: file directory where output of this funcition stored.
    :param: testsize: the portion of train dataset used for validation.
    :param: randomstate: the randomstate number to random split train and valid set.
    :param: method: the classifier name used for training.
    :param: method_parameters: the parameter used for training.

""predict-params.json"" -- ""indir: the input directory of stored model, indir2: the input directory of testset, outdir: the output directory of test result.""



```
### Responsibilities

* Da Gong developed the structure of this project.
* Zishun Jin worked on the prediction model and the model features of this project.
* Tianran Qiu worked on the prediction model and the model features of this project.
* Andrey developed the environment and the model feature creation for this project.
* Mariam worked on the final report and model feature creation for this project. 
```




### Website

Link to the webpage: https://pristinsky1.github.io/live_vs_video_on_demand_VPN_detection/
","# Live_vs_Vod_Project
## Group Name: Live

Due to the variety, affordability and convenience of online video streaming, there are more subscribers than ever to video streaming platforms. Moreover, the decreased operation of non-essential businesses and increase in the number of people working from home in this past year has further compounded this effect. More people are streaming live lectures, sports, news, and video calls via the internet at home today than we have ever seen before. Internet Service Providers, such as Viasat, are tasked with optimizing  internet connections and tailoring their allocation of resources to fit each unique customer’s needs. With this increase in internet activity, it would be especially beneficial for Viasat to understand what issues arise when customers stream various forms of video. In general, different internet activities require different resources to optimize the connection. For example, if a customer watches a lot of live video they may prefer a connection with lower latency and higher bandwidth. Although we are able to identify the genre of an activity when a user is not using a VPN, the challenge arises when a user chooses to surf the web through a VPN. When it comes to VPN use cases we can’t identify a user’s unique activity when they experience issues, thus making us unable to successfully troubleshoot  those problems. This is where a tool that could identify various internet activities, specifically live or uploaded video streaming, within a VPN tunnel would be extremely useful for an Internet Service Provider. 

## Project Report: 
Found within the **references** folder. The file name is Final_Report.

## Guide for Launching this Project:
Note: These instructions assume that the user has access to the DSMLP server to be able to run this project. Open terminal, run these commands in the associated order:

1.) **ssh user@dsmlp-login.ucsd.edu** (user refers to your school username). Enter credentials.

2.) **launch-180-gid.sh -G 100011655 -P Always -i apristin99/live_vs_vod_project**

3.) **git clone https://github.com/pristinsky1/live_vs_video_on_demand_VPN_detection.git**

4.) **cd live_vs_video_on_demand_VPN_detection**

Now you are within the right directory with the environment already set up! Start running the files!

5.) For files needed to be predict, you have to put them in the ""data/in"" directory. Acceptable files are files generated network-stats tool provided by Viasat.

6.) If you wish to train a new classifier based on new data or new type of model and parameters, you have to change the location of training data and other settings in the ""config/train-parmas.json"". Otherwise, you can directly run **python run.py predict** using the trained model contained in the project.


## Guide for Pipeline Testing:
Run **python run.py test** and the results will be in **test/out**. Within that folder, it contains the output dataframes, model, and reports of accuracies for the test data.


## Contents:
There are three parts of contents:
1. src folder - Contains all library code.
2. config folder - Contains directory of each target.
3. run.py - Main Program for this project.
4. notebooks folder - stores notebooks for this project.
5. data/out folder - stores results of this project.

The two files to look at for results under data/out:

training_report.json: Json file contains the report of model's basic information and its performance on validation set.
predictions.csv: DataFrame contains basic information of one record generated by network-stats and its prediction result.



## How to run it?
Warning: The feature, train and predict has to be run in fixed order. If you want to run everything at once, you can use the all script.

Use console to run **python run.py eda** as a script. This will run the eda notebook and store the html version for easy accessibility under ""/notebooks"". 

Use console to run **python run.py feature** as a script. This will create the features. The output dataframe will be stored in ""data/out"" directory in csv format.

Use console to run **python run.py train** as a script. This will train the model. The output model and report will be stored in ""data/out"" directory in csv format.

Use console to run **python run.py predict** as a script. This will classify the input dataset as live or streaming. The output dataframe will be stored in ""data/out"" directory in csv format.

Use console to run **python run.py all** as a script. This will run everything listed above. The output dataframes and model and report will be stored in ""data/out"" directory in csv format.


## Description of Each Params Files
""feature-params.json"" -- ""indir: the input directory of training set, outdir: the output directory of generated dataframe, output: 1 means output dataframe containing features information, 0 means only return it as a dataframe(Must be 1 in feature-params.json)""

""train-params.json"" -- 
    :param: indir: file directory where extracted features stored.
    :param: outdir: file directory where output of this funcition stored.
    :param: testsize: the portion of train dataset used for validation.
    :param: randomstate: the randomstate number to random split train and valid set.
    :param: method: the classifier name used for training.
    :param: method_parameters: the parameter used for training.

""predict-params.json"" -- ""indir: the input directory of stored model, indir2: the input directory of testset, outdir: the output directory of test result.""



```
### Responsibilities

* Da Gong developed the structure of this project.
* Zishun Jin worked on the prediction model and the model features of this project.
* Tianran Qiu worked on the prediction model and the model features of this project.
* Andrey developed the environment and the model feature creation for this project.
* Mariam worked on the final report and model feature creation for this project. 
```




### Website

Link to the webpage: https://pristinsky1.github.io/live_vs_video_on_demand_VPN_detection/
"
84,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_8,,,"# DANE - Data Automation and Network Emulation Tool

<img align='right' src='docs/media/dane-transparent-small.png' height=248>

DANE is a hackable dataset generation tool to collect network traffic in a variety of configurable network conditions.

It runs on Windows, Mac, and Linux.

**Table of contents**
- [Why use DANE?](#why-use-dane)
- [Documentation](#documentation)
- [Contributing](#contributing)
- [Acknowledgements](#acknowledgements)


## Why use DANE?

DANE provides two core functionalities:

1. Automatically collect network traffic datasets in a parallelized manner

   Manual data collection for network traffic datasets is a long and tedious process—run the tool and you can easily collect multiple hours of data in one hour of time (magic!) with one or many desired 'user' behaviors.
   
2. Emulate a diverse range of network conditions that are representative of the real world

   Data representation is an increasingly relevant issue in all fields of data science, but generating a dataset while connected to a fixed network doesn't capture diversity in network conditions—in a single file, you can configure DANE to emulate a variety of network conditions, including latency and bandwidth.

You can easily hack the tool to run custom scripts, custom data collection tools, and other custom software dependencies which support your particular research interest.

## Documentation

For all documentation, including a [quick start](https://dane-tool.github.io/dane/guide/quickstart.html), details about the [technical approach](https://dane-tool.github.io/dane/guide/approach.html), and [FAQs](https://dane-tool.github.io/dane/guide/faq.html), please consult the [**website 📖**](https://dane-tool.github.io/dane).  
https://dane-tool.github.io/dane

## Contributing

See something you'd like improved? Better yet, have some improvements coded up locally you'd like to contribute?

We welcome you to **submit an Issue** or **make a Pull Request** detailing your ideas!

## Acknowledgements

This project was originally created in affiliation with the **Halıcıoğlu Data Science Institute**'s data science program at UC San Diego.  
https://hdsi.ucsd.edu/, https://dsc-capstone.github.io/

DANE was motivated and developed with the generous support of **Viasat**.  
https://viasat.com/
","# DANE - Data Automation and Network Emulation Tool

<img align='right' src='docs/media/dane-transparent-small.png' height=248>

DANE is a hackable dataset generation tool to collect network traffic in a variety of configurable network conditions.

It runs on Windows, Mac, and Linux.

**Table of contents**
- [Why use DANE?](#why-use-dane)
- [Documentation](#documentation)
- [Contributing](#contributing)
- [Acknowledgements](#acknowledgements)


## Why use DANE?

DANE provides two core functionalities:

1. Automatically collect network traffic datasets in a parallelized manner

   Manual data collection for network traffic datasets is a long and tedious process—run the tool and you can easily collect multiple hours of data in one hour of time (magic!) with one or many desired 'user' behaviors.
   
2. Emulate a diverse range of network conditions that are representative of the real world

   Data representation is an increasingly relevant issue in all fields of data science, but generating a dataset while connected to a fixed network doesn't capture diversity in network conditions—in a single file, you can configure DANE to emulate a variety of network conditions, including latency and bandwidth.

You can easily hack the tool to run custom scripts, custom data collection tools, and other custom software dependencies which support your particular research interest.

## Documentation

For all documentation, including a [quick start](https://dane-tool.github.io/dane/guide/quickstart.html), details about the [technical approach](https://dane-tool.github.io/dane/guide/approach.html), and [FAQs](https://dane-tool.github.io/dane/guide/faq.html), please consult the [**website 📖**](https://dane-tool.github.io/dane).  
https://dane-tool.github.io/dane

## Contributing

See something you'd like improved? Better yet, have some improvements coded up locally you'd like to contribute?

We welcome you to **submit an Issue** or **make a Pull Request** detailing your ideas!

## Acknowledgements

This project was originally created in affiliation with the **Halıcıoğlu Data Science Institute**'s data science program at UC San Diego.  
https://hdsi.ucsd.edu/, https://dsc-capstone.github.io/

DANE was motivated and developed with the generous support of **Viasat**.  
https://viasat.com/
"
85,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_7,,,"# ResRecovery

Website: https://stephdoan.github.io/ResRecovery/

# Table of Contents

1. [Abstract](#Abstract)
2. [Config Files](#config)
   - [`train-params.json`](#train)
   - [`model-params.json`](#model)
   - [`user-data.json`](#user)
   - [`generate-data.json`](#generate)
3. [Running the Project](#running)

## Abstract

Virtual private networks, or VPNs, have seen a growth in popularity as more of the general population has come to realize the importance of maintaining data privacy and security while browsing the Internet. In previous works, our domain developed robust classifiers that could identify when a user was streaming video. As an extension, our group has developed a Random Forest model that determines the resolution at the time of video streaming. Our final model has an overall accuracy of **87%**.

<a name=""config""></a>

## Configuration Files

<a name=""train""></a>

### `train-params.json`

Allows users to adjust some parameters of the training data creation process. The main point of focus is the `{interval}` argument. This allows users to adjust how big of a chunk size they would like their model to be trained on. The default is 300 seconds as it allows replication of our project.

| Parameter     | Description                                                                                                     |
| ------------- | --------------------------------------------------------------------------------------------------------------- |
| folder_path   | path to where all of the raw data is stored; please refer to the folder structure below to achieve best results |
| interval      | chunk size                                                                                                      |
| threshold     | minimum megabit value; used in peak feature creation                                                            |
| prominence_fs | sampling rate to find the max peak prominence                                                                   |
| binned_fs     | deprecated parameter                                                                                            |

##### Data Folder Structure

All of the training data should be stored in an accessible `data` folder. CSV files should be categorized into folders according to their resolution below.

```
+-- data
 |
 +-- 144p
 +-- 240p
 +-- 360p
 +-- 480p
 +-- 720p
 +-- 1080p
```

<a name=""model""></a>

### `model-params.json`

Allows users to adjust hyperparameters of the random forest classifier. The default values are the values we utilized in our original project.

| Parameter         | Description                                                              |
| ----------------- | ------------------------------------------------------------------------ |
| training_data     | path to where training data is stored; data must be stored as a CSV file |
| n_estimators      | number of trees in the forest model                                      |
| max_depth         | max depth of the tree                                                    |
| min_samples_split | minimum number of samples required to split an internal node             |

<a name=""user""></a>

### `user-data.json`

Allows users to input their own data to be classified by the model.

| Parameter     | Description                                                                                                                     |
| ------------- | ------------------------------------------------------------------------------------------------------------------------------- |
| path          | path to where all of the raw user data is stored; must be an output of [network-stats](https://github.com/viasat/network-stats) |
| interval      | chunk size                                                                                                                      |
| threshold     | minimum megabit value; used in peak feature creation                                                                            |
| prominence_fs | sampling rate to find the max peak prominence                                                                                   |
| binned_fs     | deprecated parameter                                                                                                            |

<a name=""generate""></a>

### `generate-data.json`

Parameters used by the `generate_data.py` script. Please refer to [Selenium](https://www.selenium.dev/documentation/en/) documentation to install the appropriate `webdriver.exe`. For best use, please configure the `PATH` variable in the `generate-data.py` file to the correct file path of the webdriver. This script was developed using Google Chrome.

| Parameter          | Description                                                                                                     |
| ------------------ | --------------------------------------------------------------------------------------------------------------- |
| network_stats_path | location of network-stats.py                                                                                    |
| interface          | user interface to collect from; refer to [network-stats](https://github.com/viasat/network-stats) documentation |
| playlist           | link to YouTube playlist                                                                                        |
| outdir             | to be implemented                                                                                               |
| resolutions        | list of resolutions to be collected                                                                             |

<a name=""running""></a>

## Running the Project

The project is current set to the assumption that users will collect their own training data. There is a repository of available training hosted on the DSMLP server located at `/teams/DSC180A_FA20_A00/b05vpnxray/personal_stdoan/data`. If not accessible, please refer to the [`generate_data.json`](#generate) configurations to automate collection of a training set.

#### Running on the DSMLP Server

The project was mean to be run on the UCSD DSMLP server. Below are instructions if user has access to DSMLP resources.

1. Open up a terminal and run the command below to log onto the server. Users will need to provide appropriate identification when asked.

> `ssh [username]@dsmlp-login.ucsd.edu`

2. Launch a docker container to ensure package dependencies are fulfilled by running the command:

> `launch-180-gid.sh -G 100011652 -P Always stdoan/viasat-q1`

3. Clone this repository.

4. Adjust config files as necessary and then run the targets!

#### Targets

- `python run.py test` will test the various targets to ensure that all methods are running properly.

- `python run.py clean` will delete files created from running various targets. The folder and files are deleted from the local machine.

- `python run.py features` will create features from data specified in `train-params.json`.

- `python run.py predict` will either create training data to create a model or utilize a Pickle'd model that we have included. Output is an array of resolution label for each chunk in the data.
","# ResRecovery

Website: https://stephdoan.github.io/ResRecovery/

# Table of Contents

1. [Abstract](#Abstract)
2. [Config Files](#config)
   - [`train-params.json`](#train)
   - [`model-params.json`](#model)
   - [`user-data.json`](#user)
   - [`generate-data.json`](#generate)
3. [Running the Project](#running)

## Abstract

Virtual private networks, or VPNs, have seen a growth in popularity as more of the general population has come to realize the importance of maintaining data privacy and security while browsing the Internet. In previous works, our domain developed robust classifiers that could identify when a user was streaming video. As an extension, our group has developed a Random Forest model that determines the resolution at the time of video streaming. Our final model has an overall accuracy of **87%**.

<a name=""config""></a>

## Configuration Files

<a name=""train""></a>

### `train-params.json`

Allows users to adjust some parameters of the training data creation process. The main point of focus is the `{interval}` argument. This allows users to adjust how big of a chunk size they would like their model to be trained on. The default is 300 seconds as it allows replication of our project.

| Parameter     | Description                                                                                                     |
| ------------- | --------------------------------------------------------------------------------------------------------------- |
| folder_path   | path to where all of the raw data is stored; please refer to the folder structure below to achieve best results |
| interval      | chunk size                                                                                                      |
| threshold     | minimum megabit value; used in peak feature creation                                                            |
| prominence_fs | sampling rate to find the max peak prominence                                                                   |
| binned_fs     | deprecated parameter                                                                                            |

##### Data Folder Structure

All of the training data should be stored in an accessible `data` folder. CSV files should be categorized into folders according to their resolution below.

```
+-- data
 |
 +-- 144p
 +-- 240p
 +-- 360p
 +-- 480p
 +-- 720p
 +-- 1080p
```

<a name=""model""></a>

### `model-params.json`

Allows users to adjust hyperparameters of the random forest classifier. The default values are the values we utilized in our original project.

| Parameter         | Description                                                              |
| ----------------- | ------------------------------------------------------------------------ |
| training_data     | path to where training data is stored; data must be stored as a CSV file |
| n_estimators      | number of trees in the forest model                                      |
| max_depth         | max depth of the tree                                                    |
| min_samples_split | minimum number of samples required to split an internal node             |

<a name=""user""></a>

### `user-data.json`

Allows users to input their own data to be classified by the model.

| Parameter     | Description                                                                                                                     |
| ------------- | ------------------------------------------------------------------------------------------------------------------------------- |
| path          | path to where all of the raw user data is stored; must be an output of [network-stats](https://github.com/viasat/network-stats) |
| interval      | chunk size                                                                                                                      |
| threshold     | minimum megabit value; used in peak feature creation                                                                            |
| prominence_fs | sampling rate to find the max peak prominence                                                                                   |
| binned_fs     | deprecated parameter                                                                                                            |

<a name=""generate""></a>

### `generate-data.json`

Parameters used by the `generate_data.py` script. Please refer to [Selenium](https://www.selenium.dev/documentation/en/) documentation to install the appropriate `webdriver.exe`. For best use, please configure the `PATH` variable in the `generate-data.py` file to the correct file path of the webdriver. This script was developed using Google Chrome.

| Parameter          | Description                                                                                                     |
| ------------------ | --------------------------------------------------------------------------------------------------------------- |
| network_stats_path | location of network-stats.py                                                                                    |
| interface          | user interface to collect from; refer to [network-stats](https://github.com/viasat/network-stats) documentation |
| playlist           | link to YouTube playlist                                                                                        |
| outdir             | to be implemented                                                                                               |
| resolutions        | list of resolutions to be collected                                                                             |

<a name=""running""></a>

## Running the Project

The project is current set to the assumption that users will collect their own training data. There is a repository of available training hosted on the DSMLP server located at `/teams/DSC180A_FA20_A00/b05vpnxray/personal_stdoan/data`. If not accessible, please refer to the [`generate_data.json`](#generate) configurations to automate collection of a training set.

#### Running on the DSMLP Server

The project was mean to be run on the UCSD DSMLP server. Below are instructions if user has access to DSMLP resources.

1. Open up a terminal and run the command below to log onto the server. Users will need to provide appropriate identification when asked.

> `ssh [username]@dsmlp-login.ucsd.edu`

2. Launch a docker container to ensure package dependencies are fulfilled by running the command:

> `launch-180-gid.sh -G 100011652 -P Always stdoan/viasat-q1`

3. Clone this repository.

4. Adjust config files as necessary and then run the targets!

#### Targets

- `python run.py test` will test the various targets to ensure that all methods are running properly.

- `python run.py clean` will delete files created from running various targets. The folder and files are deleted from the local machine.

- `python run.py features` will create features from data specified in `train-params.json`.

- `python run.py predict` will either create training data to create a model or utilize a Pickle'd model that we have included. Output is an array of resolution label for each chunk in the data.
"
86,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_10,,,"# Data Science Senior Capstone - Viasat VPN Analysis

**Table of Contents**:
- [Link to Blog page](#blog-page)
- [Abstract](#abstract)
- [Approach](#approach)
- [Running](#running)
  - [Setup](#setup)
  - [Logging](#logging)
  - [Target `data`](#target-data)
  - [Target `features`](#target-features)
  - [Target `train`](#target-train)
- [Report](#report)

## Link to Blog page
https://mhrowlan.github.io/streaming_provider_classifier_inside_vpn/
## Abstract

Whether to access another country's Netflix library or for privacy, more people are using Virtual Private Networks (VPN) to stream videos than ever before. However, many of the different service providers offer different user experiences that can lead to differences in the network transmissions. This repository contains the implementation of our classifying model to determine what streaming service provider was being used over a VPN. The streaming providers that the model identifies are Amazon Prime, Youtube, Netflix, Youtube Live, Twitch, and an other category consiting of Disney+, Discovery+, and Hulu. This is valuable in understanding the differences in the network work patterns for the different streaming service providers. We achieve an average accuracy of 96.5% on our Random Forest model.

## Approach

We utilize Viasat's [`network-stats`](https://github.com/Viasat/network-stats) to collect network traffic on a per-second, per-connection basis while we are connected to a VPN, then engage in either internet browsing or video streaming behavior.

Utilizing the output of network-stats, we extract packet-level measurements and engineer features based on the packet sizes, arrival times, and directions.

We leverage these features in a classification model to determine whether or not a network-stats output contains video streaming activity.

## Running

The following targets can be run by calling `python run.py <target_name>`. These targets perform various aspects of the data collection, cleaning, engineering, training, and predicting pipeline.

### Setup

To leverage the existing dataset, you must be a member of DSMLP and have access to the shared /teams/ directory.

Log on to DSMLP via `ssh <username>@dsmlp-login.ucsd.edu`

Launch a Docker container with the necessary components via `launch-180.sh -i jeq004/streaming_provider_classifier_inside_vpn -G B05_VPN_XRAY -c 8 -g 1 -m 64`

Clone this repository: `git clone https://github.com/mhrowlan/streaming_provider_classifier_inside_vpn.git`

Navigate to this repository `cd streaming_provider_classifier_inside_vpn`

Now, you are ready to configure targets for our project build. Details are specified below.

### Logging

Logging behavior can be configured in `config/logging.json`.
| Key | Description |
| --- | --- |
| produce_logs | Boolean. Whether or not to write to the log file. Default: `true` |
| log_file | Path to the log file. Default: `data/logs/project_run.log` |

### Target `data`

Loads data from a source directory then performs cleaning and preprocessing steps on each file. Saves the preprocessed data to a intermediate directory.

If on DSMLP with the proper group membership, this target will symlink existing data from the shared /teams/ directory.

See `config/data-params.json` for configuration:
| Key | Description |
| --- | --- |
| source_dir | Path to directory containing raw data. Default: `data/raw/` |
| out_dir | Path to store preprocessed data. Default: `data/preprocessed/` |

### Target `features`

Engineers features on the preprocessed data with configurable parameters and saves to an output directory.

See `config/features-params.json` for configuration:
| Key | Description |
| --- | --- |
| source_dir | Path to directory containing preprocessed data. Default: `data/preprocessed/` |
| out_dir | Path to directory to store feature engineered data. Default: `data/features/` |
| chunk_size | Milliseconds. We split our variable length data into smaller chunks with consistent time spans. Default: `90000` |
| rolling_window_1 | Milliseconds. We generate a smoothed mean using rolling windows of multiple lengths, this is the first length. Default `10000` |
| rolling_window_1 | Milliseconds. This is the second length for our smoothed means. Default `60000` |
| resample_rate | Time offset. We resample our packet measurements to produce a consistent sample-rate signal for spectral analysis. Default `500ms` |
| frequency | Hertz. We compute a power spectral density feature on a signal of this sample rate. Default `2.0` |

### Target `train`

Trains a classifier based on the new features and outputs the accuracy between the predicted and true labels. In other words, it prints out the percentage of cases that were correctly classified as streaming.

See `config/train-params.json` for configuration:
| Key | Description |
| --- | --- |
| source | Path to csv containing feature engineered data. Default: `data/features/features.csv` |
| out | Path to .pkl file which will save the trained model. Default: `data/out/model.pkl` |
| validation_size | Proportion. This amount of training data will be withheld to evaluate the performance of the trained classifier. Default: `0.3` |
| classifier | String name of scikit-learn classifier to use. One of 'RandomForest', 'KNN', or 'LogisticRegression'. Default: `RandomForest` |
| model_params | Scikit-Learn hyperparameters for the chosen model. See scikit-learn documentation. |

### Target `all`

Runs `data`, `features`, `train` in order.

### Target `test`

Runs `data`, `features`, `train` with configuration found in `test/config/`.

Can optionally specify targets after test to only run that target. For example `python run.py test data` will only run the data target with the test config.

## Report

An academic report on the exploration and model built in this repository can be found at [`SPICIVPN_report.pdf`](SPICIVPN_report.pdf)
","# Data Science Senior Capstone - Viasat VPN Analysis

**Table of Contents**:
- [Link to Blog page](#blog-page)
- [Abstract](#abstract)
- [Approach](#approach)
- [Running](#running)
  - [Setup](#setup)
  - [Logging](#logging)
  - [Target `data`](#target-data)
  - [Target `features`](#target-features)
  - [Target `train`](#target-train)
- [Report](#report)

## Link to Blog page
https://mhrowlan.github.io/streaming_provider_classifier_inside_vpn/
## Abstract

Whether to access another country's Netflix library or for privacy, more people are using Virtual Private Networks (VPN) to stream videos than ever before. However, many of the different service providers offer different user experiences that can lead to differences in the network transmissions. This repository contains the implementation of our classifying model to determine what streaming service provider was being used over a VPN. The streaming providers that the model identifies are Amazon Prime, Youtube, Netflix, Youtube Live, Twitch, and an other category consiting of Disney+, Discovery+, and Hulu. This is valuable in understanding the differences in the network work patterns for the different streaming service providers. We achieve an average accuracy of 96.5% on our Random Forest model.

## Approach

We utilize Viasat's [`network-stats`](https://github.com/Viasat/network-stats) to collect network traffic on a per-second, per-connection basis while we are connected to a VPN, then engage in either internet browsing or video streaming behavior.

Utilizing the output of network-stats, we extract packet-level measurements and engineer features based on the packet sizes, arrival times, and directions.

We leverage these features in a classification model to determine whether or not a network-stats output contains video streaming activity.

## Running

The following targets can be run by calling `python run.py <target_name>`. These targets perform various aspects of the data collection, cleaning, engineering, training, and predicting pipeline.

### Setup

To leverage the existing dataset, you must be a member of DSMLP and have access to the shared /teams/ directory.

Log on to DSMLP via `ssh <username>@dsmlp-login.ucsd.edu`

Launch a Docker container with the necessary components via `launch-180.sh -i jeq004/streaming_provider_classifier_inside_vpn -G B05_VPN_XRAY -c 8 -g 1 -m 64`

Clone this repository: `git clone https://github.com/mhrowlan/streaming_provider_classifier_inside_vpn.git`

Navigate to this repository `cd streaming_provider_classifier_inside_vpn`

Now, you are ready to configure targets for our project build. Details are specified below.

### Logging

Logging behavior can be configured in `config/logging.json`.
| Key | Description |
| --- | --- |
| produce_logs | Boolean. Whether or not to write to the log file. Default: `true` |
| log_file | Path to the log file. Default: `data/logs/project_run.log` |

### Target `data`

Loads data from a source directory then performs cleaning and preprocessing steps on each file. Saves the preprocessed data to a intermediate directory.

If on DSMLP with the proper group membership, this target will symlink existing data from the shared /teams/ directory.

See `config/data-params.json` for configuration:
| Key | Description |
| --- | --- |
| source_dir | Path to directory containing raw data. Default: `data/raw/` |
| out_dir | Path to store preprocessed data. Default: `data/preprocessed/` |

### Target `features`

Engineers features on the preprocessed data with configurable parameters and saves to an output directory.

See `config/features-params.json` for configuration:
| Key | Description |
| --- | --- |
| source_dir | Path to directory containing preprocessed data. Default: `data/preprocessed/` |
| out_dir | Path to directory to store feature engineered data. Default: `data/features/` |
| chunk_size | Milliseconds. We split our variable length data into smaller chunks with consistent time spans. Default: `90000` |
| rolling_window_1 | Milliseconds. We generate a smoothed mean using rolling windows of multiple lengths, this is the first length. Default `10000` |
| rolling_window_1 | Milliseconds. This is the second length for our smoothed means. Default `60000` |
| resample_rate | Time offset. We resample our packet measurements to produce a consistent sample-rate signal for spectral analysis. Default `500ms` |
| frequency | Hertz. We compute a power spectral density feature on a signal of this sample rate. Default `2.0` |

### Target `train`

Trains a classifier based on the new features and outputs the accuracy between the predicted and true labels. In other words, it prints out the percentage of cases that were correctly classified as streaming.

See `config/train-params.json` for configuration:
| Key | Description |
| --- | --- |
| source | Path to csv containing feature engineered data. Default: `data/features/features.csv` |
| out | Path to .pkl file which will save the trained model. Default: `data/out/model.pkl` |
| validation_size | Proportion. This amount of training data will be withheld to evaluate the performance of the trained classifier. Default: `0.3` |
| classifier | String name of scikit-learn classifier to use. One of 'RandomForest', 'KNN', or 'LogisticRegression'. Default: `RandomForest` |
| model_params | Scikit-Learn hyperparameters for the chosen model. See scikit-learn documentation. |

### Target `all`

Runs `data`, `features`, `train` in order.

### Target `test`

Runs `data`, `features`, `train` with configuration found in `test/config/`.

Can optionally specify targets after test to only run that target. For example `python run.py test data` will only run the data target with the test config.

## Report

An academic report on the exploration and model built in this repository can be found at [`SPICIVPN_report.pdf`](SPICIVPN_report.pdf)
"
87,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_65,,,"## Interpreting Higgs Interaction Network with Layerwise Relevance Propagation

#### Abstract

While graph interaction networks achieve exceptional results in Higgs boson identification, GNN explainer methodology is still in its infancy. To introduce GNN interpretation to the particle physics domain, we apply layerwise relevance propagation (LRP) to our existing Higgs boson interaction network (HIN) to calculate relevance scores and reveal what features, nodes, and connections are most influential in prediction. We call this application HIN-LRP. The synergy between the LRP interpretation and the inherent structure of the HIN is such that HIN-LRP is able to illuminate which particles and particle features in a given jet are most significant in Higgs boson identification. The resulting interpretations are ultimately congruent with extant particle physics theory, with the model demonstrably learning the importance of concepts like the presence of muons, characteristics of secondary decay, and salient features such as impact parameter and momentum.

#### [READ PAPER](report.pdf)
<hr>

## Description of contents


* `notebooks`:
    * `relevance_heatmap.ipynb`: notebook that contains example plots of this project
* `src`:
    * `model`
        * `GraphDataset.py`
        * `InteractionNetwork.py`
    * `sanity_check`
        * `make_data.py`
    * `util`: utility functions such as I/O or plotting
        * `copy.py`
        * `data_io.py`
        * `model_io.py`
        * `plot.py`
    * `LRP.py`: core component of this project

* `run.py`: Entry point for running different targets of this project
* `test`: directory for storing Dev data
    * `test.root`: the generated root file for testing purpose
* `data`
    * `model`: contains a trained IN state dictionary to start with
    * `definitions.yml`: contains metadata definition of the data used in this project
<hr>

## Build Environment
* [Docker image](https://hub.docker.com/repository/docker/shiro0x19a/higgs-interaction-network) used for this project

<hr>


## Usage
To use `run.py`, a list of supported arguments are provided below

For sanity check of the explanation,
```
python run.py sc <arguments>
```
|arguments|purpose|
|-|-|
|`all`|build all targets, equivalent to using [`data` `train` `plot`] as argument|
|`data`| generate dummy data for sanity check|
|`train`| train a dummy IN on the sythesized data|
|`plot`|create static heatmap plots of precomptued relevance scores|



For explaining a pre-trained Higgs boson Interaction Network,
```
python run.py <arguments>
```
|arguments|purpose|
|-|-|
|`test`| build all targets, equivalent to using [`data` `train` `plot`] as argument on Dev data|
|`all`| similar to `test`, but build on actual data|
|`explain`| generate relevance score for given data|
|`plot`| create static heatmap plots of precomptued relevance scores|

<br>

","## Interpreting Higgs Interaction Network with Layerwise Relevance Propagation

#### Abstract

While graph interaction networks achieve exceptional results in Higgs boson identification, GNN explainer methodology is still in its infancy. To introduce GNN interpretation to the particle physics domain, we apply layerwise relevance propagation (LRP) to our existing Higgs boson interaction network (HIN) to calculate relevance scores and reveal what features, nodes, and connections are most influential in prediction. We call this application HIN-LRP. The synergy between the LRP interpretation and the inherent structure of the HIN is such that HIN-LRP is able to illuminate which particles and particle features in a given jet are most significant in Higgs boson identification. The resulting interpretations are ultimately congruent with extant particle physics theory, with the model demonstrably learning the importance of concepts like the presence of muons, characteristics of secondary decay, and salient features such as impact parameter and momentum.

#### [READ PAPER](report.pdf)
<hr>

## Description of contents


* `notebooks`:
    * `relevance_heatmap.ipynb`: notebook that contains example plots of this project
* `src`:
    * `model`
        * `GraphDataset.py`
        * `InteractionNetwork.py`
    * `sanity_check`
        * `make_data.py`
    * `util`: utility functions such as I/O or plotting
        * `copy.py`
        * `data_io.py`
        * `model_io.py`
        * `plot.py`
    * `LRP.py`: core component of this project

* `run.py`: Entry point for running different targets of this project
* `test`: directory for storing Dev data
    * `test.root`: the generated root file for testing purpose
* `data`
    * `model`: contains a trained IN state dictionary to start with
    * `definitions.yml`: contains metadata definition of the data used in this project
<hr>

## Build Environment
* [Docker image](https://hub.docker.com/repository/docker/shiro0x19a/higgs-interaction-network) used for this project

<hr>


## Usage
To use `run.py`, a list of supported arguments are provided below

For sanity check of the explanation,
```
python run.py sc <arguments>
```
|arguments|purpose|
|-|-|
|`all`|build all targets, equivalent to using [`data` `train` `plot`] as argument|
|`data`| generate dummy data for sanity check|
|`train`| train a dummy IN on the sythesized data|
|`plot`|create static heatmap plots of precomptued relevance scores|



For explaining a pre-trained Higgs boson Interaction Network,
```
python run.py <arguments>
```
|arguments|purpose|
|-|-|
|`test`| build all targets, equivalent to using [`data` `train` `plot`] as argument on Dev data|
|`all`| similar to `test`, but build on actual data|
|`explain`| generate relevance score for given data|
|`plot`| create static heatmap plots of precomptued relevance scores|

<br>

"
88,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_64,,,"# Particle Physics Result Replication
## Checkpoint #1

#### Brief Description of Structure

##### config
###### data-params.json
This file includes the parameters for data extraction.

##### src/data
###### etl.py
This file contains the functions necessary for data extraction.

##### run.py
This file successfully extracts the data.

#### In order to run the file run.py, execute the following command:
##### python run.py data","# Particle Physics Result Replication
## Checkpoint #1

#### Brief Description of Structure

##### config
###### data-params.json
This file includes the parameters for data extraction.

##### src/data
###### etl.py
This file contains the functions necessary for data extraction.

##### run.py
This file successfully extracts the data.

#### In order to run the file run.py, execute the following command:
##### python run.py data"
89,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_72,,,"## Project MAI
### Project Description
MAI refers to Microservice-based Auto Infrastructure, a serverless architecture.
This project aims to build a lightweight, effective, and robust system that helps with the automations of several workflows of the campus COVID detection team.

### Underlying Architecture
![alt text](mai.png)

### Supported Functions
#### AUM(auto-update microservice)
1. Automate excel parsing => cross-reference => update the sheet 
2. All in one script(rewrote excel parse used to parse raw cases data)
3. One upload and a POST request
4. Improved code quality
for more information, go to [repository](https://github.com/CrisZong/AUM)


#### statsTool
1. Prediction (autoregression model by buildings)
2. Autocorrelations (autocorrelation by building)
3. Stats(Cases by building sorted by number of cases)
for more information, go to [repository](https://github.com/CrisZong/statsTool)

#### Manhole Graph
the project supports creating a graph of the manhole downstream that can be consumed by the campus notification team and running standard graph routines for paths searching or manipulations.

#### Functions for graph
To build a graph and see the resulting csv, please run `python run.py graph`

### Dataset
The data used was provided by the Knight Lab team. One data source is the daily wastewater data on the Google Spreadsheet. The other data source is the network folder which contains all the shape files about sewer and manhole connections on campus.

","## Project MAI
### Project Description
MAI refers to Microservice-based Auto Infrastructure, a serverless architecture.
This project aims to build a lightweight, effective, and robust system that helps with the automations of several workflows of the campus COVID detection team.

### Underlying Architecture
![alt text](mai.png)

### Supported Functions
#### AUM(auto-update microservice)
1. Automate excel parsing => cross-reference => update the sheet 
2. All in one script(rewrote excel parse used to parse raw cases data)
3. One upload and a POST request
4. Improved code quality
for more information, go to [repository](https://github.com/CrisZong/AUM)


#### statsTool
1. Prediction (autoregression model by buildings)
2. Autocorrelations (autocorrelation by building)
3. Stats(Cases by building sorted by number of cases)
for more information, go to [repository](https://github.com/CrisZong/statsTool)

#### Manhole Graph
the project supports creating a graph of the manhole downstream that can be consumed by the campus notification team and running standard graph routines for paths searching or manipulations.

#### Functions for graph
To build a graph and see the resulting csv, please run `python run.py graph`

### Dataset
The data used was provided by the Knight Lab team. One data source is the daily wastewater data on the Google Spreadsheet. The other data source is the network folder which contains all the shape files about sewer and manhole connections on campus.

"
90,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_60,,,"# AutoBrick: A system for end-to-end automation of building point labels to Brick turtle files

![alt text](https://github.com/Advitya17/AutoBrickify/blob/main/autobrick_workflow.png?raw=true)

## Setup

Clone the repository and cd into the root directory.

`git clone https://github.com/Advitya17/AutoBrickify` & `cd AutoBrickify`

Then run the command below to setup the tool environment.

`python run.py env-setup` (alternatively you can build with the Dockerfile!)

This'll print a message to the console at the end to confirm setup.

#### `python run.py test` (Only for 180B Submission, can ignore otherwise)

## Instructions

### Step 1
Specify your configurations in config/data-params.json. 

Detailed instructions are available in the `config/README.md` file in this repository.


### Step 2
Run the project from the root directory.

`python run.py`

Your Turtle object file (`output.ttl`) will be generated in the root directory!
","# AutoBrick: A system for end-to-end automation of building point labels to Brick turtle files

![alt text](https://github.com/Advitya17/AutoBrickify/blob/main/autobrick_workflow.png?raw=true)

## Setup

Clone the repository and cd into the root directory.

`git clone https://github.com/Advitya17/AutoBrickify` & `cd AutoBrickify`

Then run the command below to setup the tool environment.

`python run.py env-setup` (alternatively you can build with the Dockerfile!)

This'll print a message to the console at the end to confirm setup.

#### `python run.py test` (Only for 180B Submission, can ignore otherwise)

## Instructions

### Step 1
Specify your configurations in config/data-params.json. 

Detailed instructions are available in the `config/README.md` file in this repository.


### Step 2
Run the project from the root directory.

`python run.py`

Your Turtle object file (`output.ttl`) will be generated in the root directory!
"
91,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_61,,,"## Infection Risk App
In this project, We propose an application which estimates infection risk of COVID-19 in buildings. The application accepts building data and a set of parameters regarding occupants and infection rates of the surrounding community. Code and assumptions made in the algorithm are clearly explained to users for transparency, which those explainations are included in the project in our last quarter. 

Opening up the project
The source codes for the calculator are located in /src/calculator. To see the notebook containing the underlying logic and sample runnings for the calculator, open presentation in /notebooks. Note there are actual codes in those notebooks, because the purpose of this project is to create an infection estimation algorithm that's clear to users and easily understanable by users. It's important to show what the algorithm does for each steps. To see our UI demo, open Website in /notebooks. To visit our website, visit https://hinanawits.github.io/DSC-180B-Presentation-Website/ Notice we are constantly updating our website with newest data so if the Website notebooks and the website itself are inconsistant it means we updated something. 

To use run.py in command line, input python run.py [targets].

We currently have the following targets available:

test: which runs the calculator using sample parameters.
More information about those sample runs can be found in the report notebooks mentioned above.

Responsibilities

Etienne Doidic built the structure and underlying logic of the calculator, and also the notebooks for walk through.

Nicholas Kho helped developing the application and migrated the codes to src and project structure.

Zhexu Li added features to the application migrating the codes, updated the project structure and developed configs and run.py.
","## Infection Risk App
In this project, We propose an application which estimates infection risk of COVID-19 in buildings. The application accepts building data and a set of parameters regarding occupants and infection rates of the surrounding community. Code and assumptions made in the algorithm are clearly explained to users for transparency, which those explainations are included in the project in our last quarter. 

Opening up the project
The source codes for the calculator are located in /src/calculator. To see the notebook containing the underlying logic and sample runnings for the calculator, open presentation in /notebooks. Note there are actual codes in those notebooks, because the purpose of this project is to create an infection estimation algorithm that's clear to users and easily understanable by users. It's important to show what the algorithm does for each steps. To see our UI demo, open Website in /notebooks. To visit our website, visit https://hinanawits.github.io/DSC-180B-Presentation-Website/ Notice we are constantly updating our website with newest data so if the Website notebooks and the website itself are inconsistant it means we updated something. 

To use run.py in command line, input python run.py [targets].

We currently have the following targets available:

test: which runs the calculator using sample parameters.
More information about those sample runs can be found in the report notebooks mentioned above.

Responsibilities

Etienne Doidic built the structure and underlying logic of the calculator, and also the notebooks for walk through.

Nicholas Kho helped developing the application and migrated the codes to src and project structure.

Zhexu Li added features to the application migrating the codes, updated the project structure and developed configs and run.py.
"
92,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_34,,,"# Project: User Wait

## Introduction

Activity monitor in the computer visualizes the system performance, but we don't know when our system gets slow or halted. If we are able to predict the mouse wait time, users could terminate their processes ahead of time to avoid waiting. Currently, there is little research conducted on the mouse wait prediction.

## Running the project

- To get the data run the following command line located inside run.py file.
  This will call specific dll files to collect the data regard to your laptop.

- To get the data, from the project root dir, you should see the data folder.
  Inside the data folder, you should see a new database file being generated.

## Type Of Data

Data provided by Intel includes 14,534,433 rows with 29,587 unique GUID within the 2020 interval. Since the complete dataset is fairly large, we sample 1/14 of the dataset, which leaves 27,014 GUID for our model. For the target, we divide the wait time into 0-3s, 3-5s, 5-7s, and 7+s as a preparation for the classification model. After exploring the correlation between potential features and the mouse wait time, we find that dynamic features, including CPU utilization, disk utilization, hard page faults, and static features, including the number of cores, RAM, model type, etc, could influence the mouse wait time. These features are then used in the model.

The data set we use is ”mousewaitall.csv001”,which is provided by the Intel teams. This data setrecords kinds of system usage before and after mousewait happens. Each feature in this data set consists ofprefix, infix and suffix. Prefix has ”before” and ”after”.It represent is this feature recorded before or after mousewait event. Infix has ”CPUUtil”, ”harddpf”, ”diskutil”and ”networkutil”. This represents what kind of systemusage this feature records. Suffix has ”min”, ”max” and”mean”. This represents the way this feature computesstatistics.

The second data set we use is ”system sysinfo uniquenormalized.csv000”, which is provided by the Intel teams. It contain 32 different features and 100,000unique systems.This data set provides informationabout the system hardware like CPU model, GPU,ram, etc.
","# Project: User Wait

## Introduction

Activity monitor in the computer visualizes the system performance, but we don't know when our system gets slow or halted. If we are able to predict the mouse wait time, users could terminate their processes ahead of time to avoid waiting. Currently, there is little research conducted on the mouse wait prediction.

## Running the project

- To get the data run the following command line located inside run.py file.
  This will call specific dll files to collect the data regard to your laptop.

- To get the data, from the project root dir, you should see the data folder.
  Inside the data folder, you should see a new database file being generated.

## Type Of Data

Data provided by Intel includes 14,534,433 rows with 29,587 unique GUID within the 2020 interval. Since the complete dataset is fairly large, we sample 1/14 of the dataset, which leaves 27,014 GUID for our model. For the target, we divide the wait time into 0-3s, 3-5s, 5-7s, and 7+s as a preparation for the classification model. After exploring the correlation between potential features and the mouse wait time, we find that dynamic features, including CPU utilization, disk utilization, hard page faults, and static features, including the number of cores, RAM, model type, etc, could influence the mouse wait time. These features are then used in the model.

The data set we use is ”mousewaitall.csv001”,which is provided by the Intel teams. This data setrecords kinds of system usage before and after mousewait happens. Each feature in this data set consists ofprefix, infix and suffix. Prefix has ”before” and ”after”.It represent is this feature recorded before or after mousewait event. Infix has ”CPUUtil”, ”harddpf”, ”diskutil”and ”networkutil”. This represents what kind of systemusage this feature records. Suffix has ”min”, ”max” and”mean”. This represents the way this feature computesstatistics.

The second data set we use is ”system sysinfo uniquenormalized.csv000”, which is provided by the Intel teams. It contain 32 different features and 100,000unique systems.This data set provides informationabout the system hardware like CPU model, GPU,ram, etc.
"
93,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_33,,,"# DSC180B-Capstone-Project
DSC Capstpne Project: A Prediction Model for Battery Remaining Time
## Usage Instructions
We provided 4 targets for your usage. They are `data`, `eda`, `model`, and `test`.

`test` would run our project on test data, which provides a miniature of what we have done on a smaller dataset. 

An exmple for running our project through terminal is `python run.py data`, which will show our main dataset. To run other branches, just replace `data` with `eda`, `model` or`test`.


## Description of Contents
```
PROJECT
├── config
    └── data-params.json
    └── inputs.json
├── notebooks
    └── DSC180B_Presentation.ipynb
├── references
    └── README.md
└── src
    ├── EDA
        └──  feature_selection.py
    ├── data
        ├── Loading_Data.py
        ├── minimini_battery_event2.csv
        ├── minimini_battery_info2.csv
        ├── minimini_device_use1.csv
        ├── minimini_device_use2.csv
        ├── minimini_hw1.csv
        ├── minimini_hw2.csv
        ├── minimini_process1.csv
        └──minimini_process2.csv
    └── model
        └──hypothesis_testing.py
├── .gitignore
├── README.md
└── run.py
└── submission.json
```


### `config/`
* `data-params.json`: It contains the file path for our dataset
* `inputs.json`: It contains the argument inputs

### `notebooks/`
* `DSC180B_Code.ipynb`: EDA, Hypothesis Testing and Visual Presentation on our project

### `references/`
* `README.md`: External sources

### `src/`
* `EDA/feature_selection.py`: code for selecting desired features for our prediction model
* `data/Loading_Data.py`: code for load our dataset
* `data/minimini_[DIFFERENT FILE NAME]_csv`: sample dataset
* `model/hypothesis_testing.py` : code for prediction model and hypothesis testing

### `run.py`
* Main driver for this project.
","# DSC180B-Capstone-Project
DSC Capstpne Project: A Prediction Model for Battery Remaining Time
## Usage Instructions
We provided 4 targets for your usage. They are `data`, `eda`, `model`, and `test`.

`test` would run our project on test data, which provides a miniature of what we have done on a smaller dataset. 

An exmple for running our project through terminal is `python run.py data`, which will show our main dataset. To run other branches, just replace `data` with `eda`, `model` or`test`.


## Description of Contents
```
PROJECT
├── config
    └── data-params.json
    └── inputs.json
├── notebooks
    └── DSC180B_Presentation.ipynb
├── references
    └── README.md
└── src
    ├── EDA
        └──  feature_selection.py
    ├── data
        ├── Loading_Data.py
        ├── minimini_battery_event2.csv
        ├── minimini_battery_info2.csv
        ├── minimini_device_use1.csv
        ├── minimini_device_use2.csv
        ├── minimini_hw1.csv
        ├── minimini_hw2.csv
        ├── minimini_process1.csv
        └──minimini_process2.csv
    └── model
        └──hypothesis_testing.py
├── .gitignore
├── README.md
└── run.py
└── submission.json
```


### `config/`
* `data-params.json`: It contains the file path for our dataset
* `inputs.json`: It contains the argument inputs

### `notebooks/`
* `DSC180B_Code.ipynb`: EDA, Hypothesis Testing and Visual Presentation on our project

### `references/`
* `README.md`: External sources

### `src/`
* `EDA/feature_selection.py`: code for selecting desired features for our prediction model
* `data/Loading_Data.py`: code for load our dataset
* `data/minimini_[DIFFERENT FILE NAME]_csv`: sample dataset
* `model/hypothesis_testing.py` : code for prediction model and hypothesis testing

### `run.py`
* Main driver for this project.
"
94,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_35,,,"# DSC180B_Project checkpoint_1

# Background
In the PC industry, there are different computer setups for omnifarious PC users. Different types of customers have different needs and budget for their computers. For instance, we think that gamers prefer desktops or laptops with high-end GPU and CPU while office users prefer the ones with decent CPU and long battery life. Hence, being aware of the different needs from different customers could help computer retailers dramatically with marketing and resource allocation. With this background, we decided to build a machine learning model that can predict a users’ persona based on the information of their computers.

# How To Run: Classification of Your Own Audio Files
1. Clone this repository.
2. On the command line, navigate to this repository locally.
3. on the command line, use

    *python run.py test*     runs the pipeline with the test-project target. This will run the test build classifier. THIS PROCESS WOULD LAST HOURS!!!
    
    *python run.py build-classifier*    to build all classifier. THIS PROCESS WOULD LAST HOURS!!!
    
    *python run.py hypo-test*      to run the hypothesis test on column 'ram' with other columns.
    
    *python run.py chi-square-test*     to run the hypothesis test on column 'ram' with other columns.
","# DSC180B_Project checkpoint_1

# Background
In the PC industry, there are different computer setups for omnifarious PC users. Different types of customers have different needs and budget for their computers. For instance, we think that gamers prefer desktops or laptops with high-end GPU and CPU while office users prefer the ones with decent CPU and long battery life. Hence, being aware of the different needs from different customers could help computer retailers dramatically with marketing and resource allocation. With this background, we decided to build a machine learning model that can predict a users’ persona based on the information of their computers.

# How To Run: Classification of Your Own Audio Files
1. Clone this repository.
2. On the command line, navigate to this repository locally.
3. on the command line, use

    *python run.py test*     runs the pipeline with the test-project target. This will run the test build classifier. THIS PROCESS WOULD LAST HOURS!!!
    
    *python run.py build-classifier*    to build all classifier. THIS PROCESS WOULD LAST HOURS!!!
    
    *python run.py hypo-test*      to run the hypothesis test on column 'ram' with other columns.
    
    *python run.py chi-square-test*     to run the hypothesis test on column 'ram' with other columns.
"
95,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_36,,,"# Prediction Task: Utilizing CPU Statistics and Application Usage to Predict a User’s Persona



## Homepage
https://vlw003.github.io

## Medium Blog
https://predicting-persona-b09group04.medium.com/

## Usage
```
git clone https://github.com/jonxsong/DSC180AB-Capstone.git
cd DSC180AB-Capstone
python run.py test
```



## Files

**./config/data-params.json** - directory where data should be output to

**./config/hw-metric-histo-data-params.json** - description of the dataset and features we utilize

**./config/systems-sysinfo-unique-normalized-data-params.json** - description of the dataset and features we utilize

**./config/ucsd-apps-execlass-data-params.json** - description of the dataset and features we utilize

**./config/frgnd_backgrnd_apps-data-params.json** - description of the dataset and features we utilize

**./notebooks/eda.ipynb** - notebook containing data explorations from DSC180B

**./notebooks/dsc180a-notebook.ipynb** - notebook containing data explorations from DSC180A

**./src/data_exploration.py** - file containing relevant methods for data exploration

**./src/model.py** - file containing relevant methods for data modelling

**./requirements.txt** - required packages

**./run.py** - call run.py to run data analysis



## Data/Output Files

**./data/out/...** - this location should hold all the outputted pictures generated from methods

**./data/raw/...** - this location should hold all the datasets downloaded below



## Link to download the datasets:
https://drive.google.com/drive/folders/1nNpwhzrbKUJd0ZwbCYLGQH49CKkKLTQ4?usp=sharing

The datasets we are using are too large for github. The datasets should be stored in /data/raw/.



## Sources
Link to Project Report: https://docs.google.com/document/d/1IpWfuG2IxurT5LOMyudWpn3UOLsKYKdjbbwqNhPGlYk/edit?usp=sharing



## Responsibilities:

Jon:
    - Report + main ideas
    - data analysis - code breakdown
    - repository structuring
    - notebook outlining
    - script writing

Vince:
    - data modeling
    - Report + targets
    - data cleaning
    - data explorations
    - classifications
    - Visual Presentation Checkpoint
    - Website
    - Final Report
    - Slides

Keshan:
    - data preparation
    - tabled data
    - key notes all throughout notebook
    - graphs + graph analysis
    - ATL work
","# Prediction Task: Utilizing CPU Statistics and Application Usage to Predict a User’s Persona



## Homepage
https://vlw003.github.io

## Medium Blog
https://predicting-persona-b09group04.medium.com/

## Usage
```
git clone https://github.com/jonxsong/DSC180AB-Capstone.git
cd DSC180AB-Capstone
python run.py test
```



## Files

**./config/data-params.json** - directory where data should be output to

**./config/hw-metric-histo-data-params.json** - description of the dataset and features we utilize

**./config/systems-sysinfo-unique-normalized-data-params.json** - description of the dataset and features we utilize

**./config/ucsd-apps-execlass-data-params.json** - description of the dataset and features we utilize

**./config/frgnd_backgrnd_apps-data-params.json** - description of the dataset and features we utilize

**./notebooks/eda.ipynb** - notebook containing data explorations from DSC180B

**./notebooks/dsc180a-notebook.ipynb** - notebook containing data explorations from DSC180A

**./src/data_exploration.py** - file containing relevant methods for data exploration

**./src/model.py** - file containing relevant methods for data modelling

**./requirements.txt** - required packages

**./run.py** - call run.py to run data analysis



## Data/Output Files

**./data/out/...** - this location should hold all the outputted pictures generated from methods

**./data/raw/...** - this location should hold all the datasets downloaded below



## Link to download the datasets:
https://drive.google.com/drive/folders/1nNpwhzrbKUJd0ZwbCYLGQH49CKkKLTQ4?usp=sharing

The datasets we are using are too large for github. The datasets should be stored in /data/raw/.



## Sources
Link to Project Report: https://docs.google.com/document/d/1IpWfuG2IxurT5LOMyudWpn3UOLsKYKdjbbwqNhPGlYk/edit?usp=sharing



## Responsibilities:

Jon:
    - Report + main ideas
    - data analysis - code breakdown
    - repository structuring
    - notebook outlining
    - script writing

Vince:
    - data modeling
    - Report + targets
    - data cleaning
    - data explorations
    - classifications
    - Visual Presentation Checkpoint
    - Website
    - Final Report
    - Slides

Keshan:
    - data preparation
    - tabled data
    - key notes all throughout notebook
    - graphs + graph analysis
    - ATL work
"
96,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_32,,,"Scale model Group
The purpose of this project is to provide a testing ground for the transmission code in SchoolABM. By scaling down the parameters to a single room of students, effects of elements such as distance and ventilation can be seen more easily.
This also provides a testing environment that has a significantly faster runtime and has been useful in verifying our math for transmission both through droplets and aerosols

Test Simulation can be run using: python -m run 'test'

Visualization can be run using: python -m run 'visualize'

Visualization includes:
-Color viz indicating which students are infected
-Distribution plot of transmission rates post-processing (for each 5 minute step and unique infected individual)
-Time series plot of when infection occurs

TODO:
Docker Image (due thursday)
Airavata (due wednesday)

Well-Mixed room (due thursday)
","Scale model Group
The purpose of this project is to provide a testing ground for the transmission code in SchoolABM. By scaling down the parameters to a single room of students, effects of elements such as distance and ventilation can be seen more easily.
This also provides a testing environment that has a significantly faster runtime and has been useful in verifying our math for transmission both through droplets and aerosols

Test Simulation can be run using: python -m run 'test'

Visualization can be run using: python -m run 'visualize'

Visualization includes:
-Color viz indicating which students are infected
-Distribution plot of transmission rates post-processing (for each 5 minute step and unique infected individual)
-Time series plot of when infection occurs

TODO:
Docker Image (due thursday)
Airavata (due wednesday)

Well-Mixed room (due thursday)
"
97,https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_31,,,"# Capstone Project dsc180B Covid-19 Transmission in Buses

In order to install mesa-geo and rtree, we have copied Johnny Lei's README file from our domain repo. Here it is:

### Install MESA
#### Install ABM package MESA with:

> pip install mesa

#### Install MESA-geo
  If you are using a Mac Machine:
    Install rtree FIRST with conda (there seems to exist a distribution issue with pip specificly to Mac):

> conda install rtree

  Install geospatial-enabled MESA extension mesa-geo with:

> pip install mesa-geo

end of credit to J.L


### How to run

Open terminal, change to the directory of the code, then type in the following command 

> python run.py

or if you want to run the code with your own parameters in config/test.json

> python run.py test


## Logistics

We are using Agent-Based-Modeling to simulate covid-19 transmission in Buses.
In order to run this simulation, you could change the parameters in config/test.json to simulate different scenarios.

This simulation start with an empty bus. The bus is scheduled to stop at a certain number of stops at certain times, picking up certain number of students. 
All these parameters are adjustable. As the bus continues, the sick students in the bus transmit the virus by normal activities like talking and breathing, and also coughing, sneezing, etc. This model stops the simulation at the end of a trip where the bus reaches the school (destination). Then this model creates a graph of the number of healthy and sick(if they received the virus) students every minute. There is also a gif of the simulation that shows the position of each student, the time since start, and the layout of the bus.
","# Capstone Project dsc180B Covid-19 Transmission in Buses

In order to install mesa-geo and rtree, we have copied Johnny Lei's README file from our domain repo. Here it is:

### Install MESA
#### Install ABM package MESA with:

> pip install mesa

#### Install MESA-geo
  If you are using a Mac Machine:
    Install rtree FIRST with conda (there seems to exist a distribution issue with pip specificly to Mac):

> conda install rtree

  Install geospatial-enabled MESA extension mesa-geo with:

> pip install mesa-geo

end of credit to J.L


### How to run

Open terminal, change to the directory of the code, then type in the following command 

> python run.py

or if you want to run the code with your own parameters in config/test.json

> python run.py test


## Logistics

We are using Agent-Based-Modeling to simulate covid-19 transmission in Buses.
In order to run this simulation, you could change the parameters in config/test.json to simulate different scenarios.

This simulation start with an empty bus. The bus is scheduled to stop at a certain number of stops at certain times, picking up certain number of students. 
All these parameters are adjustable. As the bus continues, the sick students in the bus transmit the virus by normal activities like talking and breathing, and also coughing, sneezing, etc. This model stops the simulation at the end of a trip where the bus reaches the school (destination). Then this model creates a graph of the number of healthy and sick(if they received the virus) students every minute. There is also a gif of the simulation that shows the position of each student, the time since start, and the layout of the bus.
"
99,https://github.com/leonkuoDSC/Policing_and_Income,{'leonkuoDSC': 'https://github.com/leonkuoDSC'},"{'Jupyter Notebook': 1.0, 'Python': 0.0, 'Dockerfile': 0.0}","# Policing_and_Income
Notebooks include work on the car makes and models, using the San Diego Open Policing database, and getting the data by service area.

Using run.py with target test will add the car make_model, age and price to the tx_50.csv test dataset and create a new test.csv file with the added columns.
","# Policing_and_Income
Notebooks include work on the car makes and models, using the San Diego Open Policing database, and getting the data by service area.

Using run.py with target test will add the car make_model, age and price to the tx_50.csv test dataset and create a new test.csv file with the added columns.
"
100,https://github.com/lizheng1226/DSC180B_Project_AutoFiNews,"{'mingjiazhu': 'https://github.com/mingjiazhu', 'lizheng1226': 'https://github.com/lizheng1226', 'yuz047': 'https://github.com/yuz047'}","{'Jupyter Notebook': 0.87, 'Python': 0.13, 'Dockerfile': 0.0}","# Model Analysis of Stock Price Trend Predictions based on Financial News

This is a data analysis project that aims to use different methods to predict the change of stock price from financial news and compare the performances of those methods.

We build following models:
- Baseline model: use Bag-of-Words to extract frequent words in news and determine their attitudes.
- AutoPhrase model: use AutoPhrase to extract high quality phrases and determine their attitudes.
- Doc2vec and LSTM model: use Doc2vec to create numberical representation of documents and use LSTM to predict the result.
- BERT model: use BERT to make prediction.

The code and results of different methods can be found in this repository.

## Local Run
```
$ python run.py [test] [eda] [etl] [doc2vec_lstm_model] [doc2vec_lstm_model2] [baseline_model] [autophrase_model] [bert_model]
```

### `test` target
This target runs a subset of pre-trained AutoPhrase dataset on the Word2vec model using the data in `test/testdata` and test the configurations in `config`.
### `eda` target
This target generate the EDA analysis and wordclouds of the positive and negative word bank and on the Apple stock price, and the results are saved to `data/eda_data`
### `etl` target
This target cleans and outputs the training dataset from the Apple stock price and corresponding financial news release dates, and the results are saved to `data/etl_data`
### `doc2vec_lstm_model` target
This target runs the doc2vec and lstm model to give prediction on Apple's stock price movement, and the model and results are saved to `data/d2v_lstm_model_data`
### `doc2vec_lstm_model2` target
This target runs the doc2vec and lstm model to give the sentiment analysis labels prediction based on the financial news title. The model and results are saved to `data/d2v_lstm_model_data2`
### `baseline_model` target
This target runs the baseline_model using the Bag-of-Words model.
### `autophrase_model` target
This target runs the Autophrase model to predict the stock price changes.
### `bert_model` target
This target runs the BERT model to predict the stock price changes.


## Docker
- The docker repository is `lizheng1226/dsc180_autofinews`.
- Please use the following command to run a DSMLP container using docker:
```
launch.sh -c 2 -m 4 -g 1 -i lizheng1226/dsc180_autofinews:latest
```

## Webpage
* https://aponyua991.github.io/AutoFiNews/


## Group Members
- Liuyang Zheng
- Mingjia Zhu
- Yunhan Zhang","# Model Analysis of Stock Price Trend Predictions based on Financial News

This is a data analysis project that aims to use different methods to predict the change of stock price from financial news and compare the performances of those methods.

We build following models:
- Baseline model: use Bag-of-Words to extract frequent words in news and determine their attitudes.
- AutoPhrase model: use AutoPhrase to extract high quality phrases and determine their attitudes.
- Doc2vec and LSTM model: use Doc2vec to create numberical representation of documents and use LSTM to predict the result.
- BERT model: use BERT to make prediction.

The code and results of different methods can be found in this repository.

## Local Run
```
$ python run.py [test] [eda] [etl] [doc2vec_lstm_model] [doc2vec_lstm_model2] [baseline_model] [autophrase_model] [bert_model]
```

### `test` target
This target runs a subset of pre-trained AutoPhrase dataset on the Word2vec model using the data in `test/testdata` and test the configurations in `config`.
### `eda` target
This target generate the EDA analysis and wordclouds of the positive and negative word bank and on the Apple stock price, and the results are saved to `data/eda_data`
### `etl` target
This target cleans and outputs the training dataset from the Apple stock price and corresponding financial news release dates, and the results are saved to `data/etl_data`
### `doc2vec_lstm_model` target
This target runs the doc2vec and lstm model to give prediction on Apple's stock price movement, and the model and results are saved to `data/d2v_lstm_model_data`
### `doc2vec_lstm_model2` target
This target runs the doc2vec and lstm model to give the sentiment analysis labels prediction based on the financial news title. The model and results are saved to `data/d2v_lstm_model_data2`
### `baseline_model` target
This target runs the baseline_model using the Bag-of-Words model.
### `autophrase_model` target
This target runs the Autophrase model to predict the stock price changes.
### `bert_model` target
This target runs the BERT model to predict the stock price changes.


## Docker
- The docker repository is `lizheng1226/dsc180_autofinews`.
- Please use the following command to run a DSMLP container using docker:
```
launch.sh -c 2 -m 4 -g 1 -i lizheng1226/dsc180_autofinews:latest
```

## Webpage
* https://aponyua991.github.io/AutoFiNews/


## Group Members
- Liuyang Zheng
- Mingjia Zhu
- Yunhan Zhang"
101,https://github.com/jamesjaeyu/autophrase_over_time,"{'jamesjaeyu': 'https://github.com/jamesjaeyu', 'sheildship': 'https://github.com/sheildship', 'cabrody': 'https://github.com/cabrody'}","{'Python': 0.99, 'Dockerfile': 0.01}","# Utilizing AutoPhrase on Computer Science papers over time
### DSC 180B Capstone Project

### Group Members: Cameron Brody, Jason Lin, James Yu

### Link to website: https://jamesjaeyu.github.io/autophrase_over_time/

<br />

[Link to DBLP v10 dataset download](https://lfs.aminer.cn/lab-datasets/citation/dblp.v10.zip)

- Direct download link from [aminer.org/citation](https://www.aminer.org/citation)
- zip file size is 1.7 GB, 4.08 GB when extracted

File & folder descriptions:
- `config` folder: Contains configuration files for run.py

- `data` folder: Directory for full data when running 'all' target on run.py

- `docs` folder: Contains GitHub Pages files for the visual presentation website

- `results` folder: Contains results of EDA figures and AutoPhrase

- `src` folder: Contains .py files for processing datasets, performing EDA, and model generation

- `test` folder: Contains `testdata` and `testresults` for running 'test' target on run.py

- `run.py`: Run file for the project. Targets are 'data', 'eda', 'model', 'analysis'
    - 'all' will run all targets with full dataset
    - 'test' will run 'data', 'model', and 'analysis' with test data. 'eda' will run the same
","# Utilizing AutoPhrase on Computer Science papers over time
### DSC 180B Capstone Project

### Group Members: Cameron Brody, Jason Lin, James Yu

### Link to website: https://jamesjaeyu.github.io/autophrase_over_time/

<br />

[Link to DBLP v10 dataset download](https://lfs.aminer.cn/lab-datasets/citation/dblp.v10.zip)

- Direct download link from [aminer.org/citation](https://www.aminer.org/citation)
- zip file size is 1.7 GB, 4.08 GB when extracted

File & folder descriptions:
- `config` folder: Contains configuration files for run.py

- `data` folder: Directory for full data when running 'all' target on run.py

- `docs` folder: Contains GitHub Pages files for the visual presentation website

- `results` folder: Contains results of EDA figures and AutoPhrase

- `src` folder: Contains .py files for processing datasets, performing EDA, and model generation

- `test` folder: Contains `testdata` and `testresults` for running 'test' target on run.py

- `run.py`: Run file for the project. Targets are 'data', 'eda', 'model', 'analysis'
    - 'all' will run all targets with full dataset
    - 'test' will run 'data', 'model', and 'analysis' with test data. 'eda' will run the same
"
102,https://github.com/YongqingLi14/codenames-ai-analysis,{'YongqingLi14': 'https://github.com/YongqingLi14'},"{'Jupyter Notebook': 0.96, 'Python': 0.04, 'Dockerfile': 0.0}","# codenames-ai-analysis

## Introduction
This project includes the experiments and analysis on the performance of Codenames AI, a system capable of replacing human efforts in the game Codenames.
* To play the game, please visit https://github.com/XueweiYan/codenames-game-ai
* For more details on the project, please visit https://xueweiyan.github.io/codenames-ai-website/


## Instructions
* Download the datasets from the below link and store them on the same level as this repository in a file named ""AI_dataset"":
  * https://drive.google.com/drive/folders/1FqEHYL_uTQDQ_MFw8T4gdD4VHaa2r0jv?usp=sharing
* Clone the game repo and place it on the same level as this repository:
  * https://github.com/XueweiYan/codenames-game-ai
* Resulting set up should look like this:

<p align=""center"">
  <img src=""https://github.com/YongqingLi14/codenames-ai-analysis/blob/main/file_organization.png"" />
</p>


## Running the Repository
* Run the following docker image inside a container: 
  * yongqingli/codenames_ai
* To test the pipeline of the project: 
  * `python3 run.py test`
* To view full results (total time will take ~15 hours): 
  * `python3 run.py all` 
* To revert the repo back to its original state: 
  * `python3 run.py clean` 
* Results will be availale in Report.ipynb and Report.html


## Datasets
We put 3 datasets to the test in this experiment:
* GloVe: 
  * pretrained word embeddings from Wikipedia
  * cosine similarity
* Word2Vec
  * word embeddings trained from the English Simple Wiki using the gensim word2vec model
  * cosine similarity
* WordNet
  * word embeddings from the WordNet dataset
  * Wu-Palmer similiarity


## Testing Metrics
* Average turns taken to finish the game
* Number of assassins triggered
* Accuracy in correctly guessing the intended words
","# codenames-ai-analysis

## Introduction
This project includes the experiments and analysis on the performance of Codenames AI, a system capable of replacing human efforts in the game Codenames.
* To play the game, please visit https://github.com/XueweiYan/codenames-game-ai
* For more details on the project, please visit https://xueweiyan.github.io/codenames-ai-website/


## Instructions
* Download the datasets from the below link and store them on the same level as this repository in a file named ""AI_dataset"":
  * https://drive.google.com/drive/folders/1FqEHYL_uTQDQ_MFw8T4gdD4VHaa2r0jv?usp=sharing
* Clone the game repo and place it on the same level as this repository:
  * https://github.com/XueweiYan/codenames-game-ai
* Resulting set up should look like this:

<p align=""center"">
  <img src=""https://github.com/YongqingLi14/codenames-ai-analysis/blob/main/file_organization.png"" />
</p>


## Running the Repository
* Run the following docker image inside a container: 
  * yongqingli/codenames_ai
* To test the pipeline of the project: 
  * `python3 run.py test`
* To view full results (total time will take ~15 hours): 
  * `python3 run.py all` 
* To revert the repo back to its original state: 
  * `python3 run.py clean` 
* Results will be availale in Report.ipynb and Report.html


## Datasets
We put 3 datasets to the test in this experiment:
* GloVe: 
  * pretrained word embeddings from Wikipedia
  * cosine similarity
* Word2Vec
  * word embeddings trained from the English Simple Wiki using the gensim word2vec model
  * cosine similarity
* WordNet
  * word embeddings from the WordNet dataset
  * Wu-Palmer similiarity


## Testing Metrics
* Average turns taken to finish the game
* Number of assassins triggered
* Accuracy in correctly guessing the intended words
"
103,https://github.com/jonathantanoto/spam_detection_180B/,{'jonathantanoto': 'https://github.com/jonathantanoto'},"{'Jupyter Notebook': 0.98, 'Python': 0.02}","# Spam Detection Using Natural Language Processing

Building a spam detection algorithm by utilizing Natural Language Processing to extract features associated with spam emails. Deep Learning methods as well as word-to-vector transformation are used to create a spam email classifier.

## Usage Instructions

Potential run.py arguments:
* data: downloads and populates data folder from source.
* build: feature extraction and bi-LSTM model building, requires data to be run.
* predict: runs script to predict phrases whether it is a spam or not, requires data and build to be run.

## Project Contents

```
ROOT FOLDER
├── .gitignore
├── .gitmodules
├── AutoPhrase (forked submodule repository)
├── run.py
├── README.md
├── data (populated by calling data argument of run.py)
├── models (populated by calling build argument of run.py)
│   ├── model
│   │   └── ...
│   └── tokenizer.pickle
├── notebooks
│   └── report.ipynb
└── src
    └── generate_dataset.py
    └── process_build.py
    └── spam_or_not.py
```

### `src`

* `generate_dataset.py`: Code that pulls dataset used for training from data source, and combines data into a dataframe.
* `process_build.py`: Code that extracts features from data, processes data to be used for training deep learning models. Trains Bidirectional Long Short-Term Memory model.
* `spam_or_not.py`: Code that loads model from process_build and runs a script to predict input text's probability of being a spam message.


### `notebooks`

* Jupyter notebooks for Reports and line-by-line executed code.
* Final Report in PDF form.


## Links

* Website: https://jonathantanoto.github.io/spamdetection/
* Presentation Video: https://youtu.be/wkc7R0J4_VM
","# Spam Detection Using Natural Language Processing

Building a spam detection algorithm by utilizing Natural Language Processing to extract features associated with spam emails. Deep Learning methods as well as word-to-vector transformation are used to create a spam email classifier.

## Usage Instructions

Potential run.py arguments:
* data: downloads and populates data folder from source.
* build: feature extraction and bi-LSTM model building, requires data to be run.
* predict: runs script to predict phrases whether it is a spam or not, requires data and build to be run.

## Project Contents

```
ROOT FOLDER
├── .gitignore
├── .gitmodules
├── AutoPhrase (forked submodule repository)
├── run.py
├── README.md
├── data (populated by calling data argument of run.py)
├── models (populated by calling build argument of run.py)
│   ├── model
│   │   └── ...
│   └── tokenizer.pickle
├── notebooks
│   └── report.ipynb
└── src
    └── generate_dataset.py
    └── process_build.py
    └── spam_or_not.py
```

### `src`

* `generate_dataset.py`: Code that pulls dataset used for training from data source, and combines data into a dataframe.
* `process_build.py`: Code that extracts features from data, processes data to be used for training deep learning models. Trains Bidirectional Long Short-Term Memory model.
* `spam_or_not.py`: Code that loads model from process_build and runs a script to predict input text's probability of being a spam message.


### `notebooks`

* Jupyter notebooks for Reports and line-by-line executed code.
* Final Report in PDF form.


## Links

* Website: https://jonathantanoto.github.io/spamdetection/
* Presentation Video: https://youtu.be/wkc7R0J4_VM
"
104,https://github.com/mcelz/MedCoin-Authorization-Smart-Contract,"{'mcelz': 'https://github.com/mcelz', 'ellawan': 'https://github.com/ellawan'}",{'Solidity': 1.0},"# MedCoin Authorization Contract
## Overview
EHR(Electronic Health Records) was never intended to manage and preserve the complications of cross-institutional and lifelong medical records: Medical information for a patient comes from a variety of places, and different pieces of that information must be put together for clinicians to make efficient healthcare decisions. Because of storage constraints, EHRs frequently store health data at a single location for a few years rather than keeping all-time records for patients. EHR systems used by different hospitals are frequently incompatible. Patients who seek medical treatment at several locations must frequently retype their personal information and request data transfers across these health providers, and they encounter considerable issues accessing their reports, correcting incorrect information, and authorizing medical data. <br />
<br />
Another concern in this area is the permission of medical records. To regulate the health industry, patient data protection procedure protocols such as HIPAA and EPHI were established, and different medical information sources have distinct authorization requirements that must be met before patient data can be shared with someone else. Sensitive data, such as the patient's gender, name, residence, zip code, and age, should not be leaked to a third party without authority; similarly, generally non-sensitive medical data should be examined with caution. No information can simply be aired or made available to the general public. Often, a physician will have all of the information they require, as well as others that they may not be aware of but are necessary to care for. <br />
<br />
We propose to implement the authorization smart contracts for EHR-related medical blockchains. This auditing layer of medical blockchain has featured a security design that would prevent data breaches in the medical records and separate sensitive data from non-sensitive data: We separate sensitive and non-sensitive data when patients log their medical information and we would show legal statements (HIPAA, EPHI) to notify the patients when they are authorizing their private data to a third party. Then, the smart contracts would allow different levels of authorization of access to data. The smart contracts would also allow for the protection of private data while delivering useful medical data to health providers and doctors. 
<br /><br />
This authorization contract would therefore provide the medical record requesters(doctors, researchers, providers) with stratified access to medical information for research use, clinical use, or kept private; Patients could choose different authorization levels for people to access their medical records enabling a distributed system that provides layered and use for info users and info providers. This authorization smart contract would be the core construct in our medical data encryption, authorization stratification, and medical records transfer aggregation pipeline.
<br /><br />

## Contributors
Yifei Wang and Ruiwei Wan

## Directory Structure
<pre>
├── README.md
├── notebooks/
│   ├── MedCoin White Paper.pdf
│   └── report.pdf
├── references/
│   └── README.md
├── report.pdf
└── src/
    ├── smart contract/
    │   ├── Patient_Registry.sol
    │   └── Relationship_Management.sol
    └── test/
        ├── Patient_Registry_Test.sol
        └── Relationship_Management_Test.sol
</pre>

## Data Use Explanation
Since our project is not concerned with data ingestion and preprocessing,
we are only concerned with authorization application in the smart contract,
we therefore does not include the data in the data part.
For testing purposes, we only rely on remix website, which is https://remix.ethereum.org/, 
because implementing blockchain infrastructure would take months or even years.
So if our test solidity file and smart contract solidity files were put on the remix and use the test resources there,
it would suffice the testing purpose of our contract.

## Docker Use Explanation
We also will not use docker, because it is hard to deploy docker for bloakchain project.

## Testing website
For testing purposes, we only rely on remix website, which is https://remix.ethereum.org/

First we should download the solidity files in the src folder:

- `Patient_Registry.sol`: the contract which is in charge of register new patients for authorization
- `Relationship_Management.sol`: the contract which manages the authorization pairs of patients and third parties
- `Patient_Registry_Test.sol`: the test contract for Patient_Registry.sol
- `Relationship_Management_Test.sol`: the test contract for Relationship_Management.sol

Then we put the source files and test files onto remix website(https://remix.ethereum.org/) to test. 
We also connected to MetaMask Ropsten testnetwork, however, due to the limitations of blockchain infrastructure, our project stops here.

## Project Report
We also include the Project report in the notebook folder.
Our project website is: https://ellawan.github.io/.  
","# MedCoin Authorization Contract
## Overview
EHR(Electronic Health Records) was never intended to manage and preserve the complications of cross-institutional and lifelong medical records: Medical information for a patient comes from a variety of places, and different pieces of that information must be put together for clinicians to make efficient healthcare decisions. Because of storage constraints, EHRs frequently store health data at a single location for a few years rather than keeping all-time records for patients. EHR systems used by different hospitals are frequently incompatible. Patients who seek medical treatment at several locations must frequently retype their personal information and request data transfers across these health providers, and they encounter considerable issues accessing their reports, correcting incorrect information, and authorizing medical data. <br />
<br />
Another concern in this area is the permission of medical records. To regulate the health industry, patient data protection procedure protocols such as HIPAA and EPHI were established, and different medical information sources have distinct authorization requirements that must be met before patient data can be shared with someone else. Sensitive data, such as the patient's gender, name, residence, zip code, and age, should not be leaked to a third party without authority; similarly, generally non-sensitive medical data should be examined with caution. No information can simply be aired or made available to the general public. Often, a physician will have all of the information they require, as well as others that they may not be aware of but are necessary to care for. <br />
<br />
We propose to implement the authorization smart contracts for EHR-related medical blockchains. This auditing layer of medical blockchain has featured a security design that would prevent data breaches in the medical records and separate sensitive data from non-sensitive data: We separate sensitive and non-sensitive data when patients log their medical information and we would show legal statements (HIPAA, EPHI) to notify the patients when they are authorizing their private data to a third party. Then, the smart contracts would allow different levels of authorization of access to data. The smart contracts would also allow for the protection of private data while delivering useful medical data to health providers and doctors. 
<br /><br />
This authorization contract would therefore provide the medical record requesters(doctors, researchers, providers) with stratified access to medical information for research use, clinical use, or kept private; Patients could choose different authorization levels for people to access their medical records enabling a distributed system that provides layered and use for info users and info providers. This authorization smart contract would be the core construct in our medical data encryption, authorization stratification, and medical records transfer aggregation pipeline.
<br /><br />

## Contributors
Yifei Wang and Ruiwei Wan

## Directory Structure
<pre>
├── README.md
├── notebooks/
│   ├── MedCoin White Paper.pdf
│   └── report.pdf
├── references/
│   └── README.md
├── report.pdf
└── src/
    ├── smart contract/
    │   ├── Patient_Registry.sol
    │   └── Relationship_Management.sol
    └── test/
        ├── Patient_Registry_Test.sol
        └── Relationship_Management_Test.sol
</pre>

## Data Use Explanation
Since our project is not concerned with data ingestion and preprocessing,
we are only concerned with authorization application in the smart contract,
we therefore does not include the data in the data part.
For testing purposes, we only rely on remix website, which is https://remix.ethereum.org/, 
because implementing blockchain infrastructure would take months or even years.
So if our test solidity file and smart contract solidity files were put on the remix and use the test resources there,
it would suffice the testing purpose of our contract.

## Docker Use Explanation
We also will not use docker, because it is hard to deploy docker for bloakchain project.

## Testing website
For testing purposes, we only rely on remix website, which is https://remix.ethereum.org/

First we should download the solidity files in the src folder:

- `Patient_Registry.sol`: the contract which is in charge of register new patients for authorization
- `Relationship_Management.sol`: the contract which manages the authorization pairs of patients and third parties
- `Patient_Registry_Test.sol`: the test contract for Patient_Registry.sol
- `Relationship_Management_Test.sol`: the test contract for Relationship_Management.sol

Then we put the source files and test files onto remix website(https://remix.ethereum.org/) to test. 
We also connected to MetaMask Ropsten testnetwork, however, due to the limitations of blockchain infrastructure, our project stops here.

## Project Report
We also include the Project report in the notebook folder.
Our project website is: https://ellawan.github.io/.  
"
105,https://github.com/DSC180A-A04/spatiotemporal_analysis,"{'kailingding': 'https://github.com/kailingding', 'MilesLabrador': 'https://github.com/MilesLabrador', 'JudyJin': 'https://github.com/JudyJin', 'djleung': 'https://github.com/djleung'}","{'Jupyter Notebook': 0.94, 'Python': 0.06, 'Shell': 0.0, 'Dockerfile': 0.0}","# Spatiotemporal Analysis

## Webpage
https://dsc180a-a04.github.io/spatiotemporal_analysis/


The analysis in this repo uses uncertainty quantification feature that we have developed in [torchTS](https://github.com/Rose-STL-Lab/torchTS).

Example output:

Use conformal prediction to construct a conformal confidence band consisting of quantiles `[0.025, 0.5, 0.975]` for a 95% confidence level.
![uncertainty_quantification](./static/uncertainty_quantification.png)

## Getting Started

1. Create a virtual environment

```bash
python3 -m venv venv
```

2. Activate the virtual environment

```bash
source venv/bin/activate # for mac
```
```bash
venv/Scripts/activate # for windows
```
3. Install dependencies

```bash
pip install -r requirements.txt
```

4. Train models and make predictions. This will generate a `conformal_prediction.png` plot in the root directory.

```bash
python run.py
```","# Spatiotemporal Analysis

## Webpage
https://dsc180a-a04.github.io/spatiotemporal_analysis/


The analysis in this repo uses uncertainty quantification feature that we have developed in [torchTS](https://github.com/Rose-STL-Lab/torchTS).

Example output:

Use conformal prediction to construct a conformal confidence band consisting of quantiles `[0.025, 0.5, 0.975]` for a 95% confidence level.
![uncertainty_quantification](./static/uncertainty_quantification.png)

## Getting Started

1. Create a virtual environment

```bash
python3 -m venv venv
```

2. Activate the virtual environment

```bash
source venv/bin/activate # for mac
```
```bash
venv/Scripts/activate # for windows
```
3. Install dependencies

```bash
pip install -r requirements.txt
```

4. Train models and make predictions. This will generate a `conformal_prediction.png` plot in the root directory.

```bash
python run.py
```"
106,https://github.com/dhaar-data/DSC180B-project,"{'alicegunawan': 'https://github.com/alicegunawan', 'dhaar-data': 'https://github.com/dhaar-data'}","{'Python': 0.95, 'Dockerfile': 0.05}","# DSC180B-project
This is an exploration of the bootstrap post-prediction inference approach introduced by Wang et al. in [Methods for correcting inference based on outcomes predicted by machine learning](https://www.pnas.org/content/117/48/30266/tab-article-info) through a study of tweets and their corresponding political alignment in the US. When we look at a tweet, what are the kinds of words or phrases that most strongly indicate the tweet's alignment to Democrats or Republicans? In today's political climate, what topics or figures are most heavily scrutinized by one party or another? We seek to find these key figures, phrases, and topics through a statistical analyses of political tweets.

As said above, this statistical analyses also functions as an investigation of the bootstrap post-prediction inference approach. Post-prediction--or postpi, as Wang et al. calls it--is the use of predicted outcomes in lieu of observed outcomes during inferential analysis. If postpi is conducted without accounting for the use of predicted outcomes as is often the case, this leads to issues with bias and standard , among other things. While the aforementioned bootstrap post-prediction inference approach corrects these issues for a wide range of datasets, we seek to study its applicability specifically towards text data, particularly in political science. 

## Build Instructions
This project has already provided pre-scraped Twitter data in `data/out/raw`.
* Data Collection: To clean and split the data into train-test-validation sets, run `python run.py data`. The datasets will be stored in `data/out/clean` in separate csvs for covariates and outcomes. 
* EDA: To conduct EDA on the data, run `python run.py data eda`. The output will be three figures in `results/figures` as .png files.
* Prediction: To build the prediction model and predict the outcomes of the datasets, run `python run.py data predict`. The predicted values will be stored in `data/out/predicted` in txt files.
* Relationship: To build the relationship model based on the predicted and observed outcomes from the test dataset, run `python run.py data predict rel`.
* Inference/Bootstrapping: To conduct the bootstrap post-prediction inference on selected features, run `python run.py data predict rel inference`, equivalent to running all targets (see last bullet point). Features you want to conduct inference for can be adjusted in `config/inference-params.json`. This will output estimators, standard errors, and t-statistics of the features in a csv file located at `results/inference`.
* Run all:
    * To run the entire process on the dataset specified in `config/data-params.json`, run `python run.py all`
    * To run the entire process on test data in `test/testdata`, run `python run.py test`
    
## Running the Project
1. Clone this repo
    ```
    git clone https://github.com/dhaar-data/DSC180B-project.git
    ```
2. Build and run the docker image
    ```
    docker build -t ##
    docker run --rm -it ## /bin/bash.
    ```
3. Run the project according to build instructions above. As an example:
    ```
    python run.py all
    ```
    
## Project Website
```
https://dhaar-data.github.io/DSC180B-project/
```
","# DSC180B-project
This is an exploration of the bootstrap post-prediction inference approach introduced by Wang et al. in [Methods for correcting inference based on outcomes predicted by machine learning](https://www.pnas.org/content/117/48/30266/tab-article-info) through a study of tweets and their corresponding political alignment in the US. When we look at a tweet, what are the kinds of words or phrases that most strongly indicate the tweet's alignment to Democrats or Republicans? In today's political climate, what topics or figures are most heavily scrutinized by one party or another? We seek to find these key figures, phrases, and topics through a statistical analyses of political tweets.

As said above, this statistical analyses also functions as an investigation of the bootstrap post-prediction inference approach. Post-prediction--or postpi, as Wang et al. calls it--is the use of predicted outcomes in lieu of observed outcomes during inferential analysis. If postpi is conducted without accounting for the use of predicted outcomes as is often the case, this leads to issues with bias and standard , among other things. While the aforementioned bootstrap post-prediction inference approach corrects these issues for a wide range of datasets, we seek to study its applicability specifically towards text data, particularly in political science. 

## Build Instructions
This project has already provided pre-scraped Twitter data in `data/out/raw`.
* Data Collection: To clean and split the data into train-test-validation sets, run `python run.py data`. The datasets will be stored in `data/out/clean` in separate csvs for covariates and outcomes. 
* EDA: To conduct EDA on the data, run `python run.py data eda`. The output will be three figures in `results/figures` as .png files.
* Prediction: To build the prediction model and predict the outcomes of the datasets, run `python run.py data predict`. The predicted values will be stored in `data/out/predicted` in txt files.
* Relationship: To build the relationship model based on the predicted and observed outcomes from the test dataset, run `python run.py data predict rel`.
* Inference/Bootstrapping: To conduct the bootstrap post-prediction inference on selected features, run `python run.py data predict rel inference`, equivalent to running all targets (see last bullet point). Features you want to conduct inference for can be adjusted in `config/inference-params.json`. This will output estimators, standard errors, and t-statistics of the features in a csv file located at `results/inference`.
* Run all:
    * To run the entire process on the dataset specified in `config/data-params.json`, run `python run.py all`
    * To run the entire process on test data in `test/testdata`, run `python run.py test`
    
## Running the Project
1. Clone this repo
    ```
    git clone https://github.com/dhaar-data/DSC180B-project.git
    ```
2. Build and run the docker image
    ```
    docker build -t ##
    docker run --rm -it ## /bin/bash.
    ```
3. Run the project according to build instructions above. As an example:
    ```
    python run.py all
    ```
    
## Project Website
```
https://dhaar-data.github.io/DSC180B-project/
```
"
108,https://github.com/duha-aldebakel/DSC180B-LDACode,"{'duha-aldebakel': 'https://github.com/duha-aldebakel', 'a1limon': 'https://github.com/a1limon'}","{'Jupyter Notebook': 0.99, 'Python': 0.01, 'Dockerfile': 0.0}","# Overview & Setup

This repository serves as the central user code repository for Group A06's DSC180B Capstone project.

## Title:
Exploration of Variational Inference and Monte Carlo Markov Chain Models for Latent Dirichlet Allocation of Wikipedia Corpus
## Abstract:
Topic modeling allows us to fulfill algorithmic needs to organize, understand, and annotate documents according to the discovered structure. Given the vast troves of data and the lack of specialized skillsets, it is helpful to extract topics in an unsupervised manner using Latent Dirichlet Allocation (LDA). LDA is a generative probabilistic topic model for discrete data, but unfortunately, solving for the posterior distribution of LDA is intractable, given the numerous latent variables that have cross dependencies. It is widely acknowledged that inference methods such Markov Chain Monte Carlo and Variational Inference are a good way forward to achieve suitable approximate solutions for LDA. In this report, we will explore both these methods to solve the LDA problem on the Wikipedia corpus. We find that better performance can be achieved via preprocessing the data to filter only certain parts-of-speech via lemmatization, and also exclude extremely rare or common words. We improved on the Expectations-Maximization (EM) Algorithm used for variational inference by limiting the number of iterations in the E step even if sub-optimal. This leads to benefit of faster runtimes and better convergences due to fewer iterations and avoidance of local minima. Finally, we explore early stopping runtimes on under-parameterized LDA models to infer the true dimensionality of the Wikipedia vocabulary to solve for topics. While the English language has around a million words, our findings are that it only takes around fifteen thousand words to infer around twenty major topics in the dataset.

# To get it up and running:
## 1) Set up python environment
### Option 1: (Easiest) Pulling our Docker from Dockerhub
- Just run ""`docker run -it --rm daldebak/dsc180b bash`""
- If the docker is not on your machine locally, the command should pull it from docker hub automatically. Here is how to force the action, ""`docker pull daldebak/dsc180b`""
- Run the docker using 'docker run -it --rm daldebak/dsc180b bash '
### Option 2: Rebuilding a Docker
- Build from Dockerfile using the docker CLI command
- Type ""`docker build -t <image-fullname> .`"" and hit \<Enter\>, notice the ""period/dot"" at the end of the command, which denotes the current directory. Docker will then build the image in the current directory's context. The resulting image will be labeled `<image-fullname>`. Monitor the build process for errors.
- For example, a command could be 'docker build -t daldebak/dsc180b .'
- Run the docker using 'docker run -it --rm daldebak/dsc180b bash ', or replace with your own <image-fullname>
### Option 3: If using DSMLP
- SSH to `dsmlp-login.ucsd.edu`. (Note if working outside the school, you would need to first connect via VPN)
- Run ""`launch.sh -i daldebak/dsc180b:latest`""
### For local development:
- Make sure, preferably, you have python3.7+ installed and assuming you have configured the `PATH` and `PATHEXT` variables upon installation:
- `python3.7 -m venv env`
- `source env/bin/activate`
- `pip install -r requirements_pip.txt`

To interact with jupyter notebooks (make sure virtual environment is activated and requirements_pip.txt are installed):
- `cd DSC180B-LDACode`
- `ipython kernel install --user --name=env` (assuming you named your virtual environment `env`)
-  `jupyter notebook`
- When notebook server is running: Navigate to `Kernel` > `Change kernel` > select `env`
  
## 2) Getting the repository from Github
- ""`git clone https://github.com/duha-aldebakel/DSC180B-LDACode.git`""
- ""`cd DSC180B-LDACode`""
- ""`python run.py test-gensim`"" to run gensim on test data
- ""`python run.py gensim`"" to run gensim on production data 
- ""`python run.py test-lda-cgs`"" to run gibbs on test data
- ""`python run.py lda-cgs`"" to run gibbs on production data 
- ""`python run.py onlineldavb`"" to run blei MFVI LDA on production data   
","# Overview & Setup

This repository serves as the central user code repository for Group A06's DSC180B Capstone project.

## Title:
Exploration of Variational Inference and Monte Carlo Markov Chain Models for Latent Dirichlet Allocation of Wikipedia Corpus
## Abstract:
Topic modeling allows us to fulfill algorithmic needs to organize, understand, and annotate documents according to the discovered structure. Given the vast troves of data and the lack of specialized skillsets, it is helpful to extract topics in an unsupervised manner using Latent Dirichlet Allocation (LDA). LDA is a generative probabilistic topic model for discrete data, but unfortunately, solving for the posterior distribution of LDA is intractable, given the numerous latent variables that have cross dependencies. It is widely acknowledged that inference methods such Markov Chain Monte Carlo and Variational Inference are a good way forward to achieve suitable approximate solutions for LDA. In this report, we will explore both these methods to solve the LDA problem on the Wikipedia corpus. We find that better performance can be achieved via preprocessing the data to filter only certain parts-of-speech via lemmatization, and also exclude extremely rare or common words. We improved on the Expectations-Maximization (EM) Algorithm used for variational inference by limiting the number of iterations in the E step even if sub-optimal. This leads to benefit of faster runtimes and better convergences due to fewer iterations and avoidance of local minima. Finally, we explore early stopping runtimes on under-parameterized LDA models to infer the true dimensionality of the Wikipedia vocabulary to solve for topics. While the English language has around a million words, our findings are that it only takes around fifteen thousand words to infer around twenty major topics in the dataset.

# To get it up and running:
## 1) Set up python environment
### Option 1: (Easiest) Pulling our Docker from Dockerhub
- Just run ""`docker run -it --rm daldebak/dsc180b bash`""
- If the docker is not on your machine locally, the command should pull it from docker hub automatically. Here is how to force the action, ""`docker pull daldebak/dsc180b`""
- Run the docker using 'docker run -it --rm daldebak/dsc180b bash '
### Option 2: Rebuilding a Docker
- Build from Dockerfile using the docker CLI command
- Type ""`docker build -t <image-fullname> .`"" and hit \<Enter\>, notice the ""period/dot"" at the end of the command, which denotes the current directory. Docker will then build the image in the current directory's context. The resulting image will be labeled `<image-fullname>`. Monitor the build process for errors.
- For example, a command could be 'docker build -t daldebak/dsc180b .'
- Run the docker using 'docker run -it --rm daldebak/dsc180b bash ', or replace with your own <image-fullname>
### Option 3: If using DSMLP
- SSH to `dsmlp-login.ucsd.edu`. (Note if working outside the school, you would need to first connect via VPN)
- Run ""`launch.sh -i daldebak/dsc180b:latest`""
### For local development:
- Make sure, preferably, you have python3.7+ installed and assuming you have configured the `PATH` and `PATHEXT` variables upon installation:
- `python3.7 -m venv env`
- `source env/bin/activate`
- `pip install -r requirements_pip.txt`

To interact with jupyter notebooks (make sure virtual environment is activated and requirements_pip.txt are installed):
- `cd DSC180B-LDACode`
- `ipython kernel install --user --name=env` (assuming you named your virtual environment `env`)
-  `jupyter notebook`
- When notebook server is running: Navigate to `Kernel` > `Change kernel` > select `env`
  
## 2) Getting the repository from Github
- ""`git clone https://github.com/duha-aldebakel/DSC180B-LDACode.git`""
- ""`cd DSC180B-LDACode`""
- ""`python run.py test-gensim`"" to run gensim on test data
- ""`python run.py gensim`"" to run gensim on production data 
- ""`python run.py test-lda-cgs`"" to run gibbs on test data
- ""`python run.py lda-cgs`"" to run gibbs on production data 
- ""`python run.py onlineldavb`"" to run blei MFVI LDA on production data   
"
109,https://github.com/889884m/DSC180_Project,"{'889884m': 'https://github.com/889884m', 'rdszhao': 'https://github.com/rdszhao'}","{'Jupyter Notebook': 0.56, 'HTML': 0.44, 'Python': 0.01, 'Dockerfile': 0.0}","# Locating Sound with Machine Learning

Site URL: https://889884m.github.io/DSC180_Project/

Most of the project code was actually built using Jupyter Notebooks, so the latest working code would be found in the `Notebooks` folder. Here the latest figures and hyperparameter tuning can be found. Demonstrations of the Neural Net, Support Vector Machine, and Random Forest are here.

The code is run via the command `python run.py test`. This runs the baseline model on the test data, which is simply the normal data but randomized.

Project code with working models can be found in `src` folder. Here the code for data generation and the test data can be found. This is also where the model code can be found which is in the `prediction` folder. Here, the code for the feed-forward Neural Net and the SVM can be found.

To build the `docker build -t <tag_name> .` which gives a local docker container with the libraries scipy, numpy, pandas, pytorch, and sklearn.","# Locating Sound with Machine Learning

Site URL: https://889884m.github.io/DSC180_Project/

Most of the project code was actually built using Jupyter Notebooks, so the latest working code would be found in the `Notebooks` folder. Here the latest figures and hyperparameter tuning can be found. Demonstrations of the Neural Net, Support Vector Machine, and Random Forest are here.

The code is run via the command `python run.py test`. This runs the baseline model on the test data, which is simply the normal data but randomized.

Project code with working models can be found in `src` folder. Here the code for data generation and the test data can be found. This is also where the model code can be found which is in the `prediction` folder. Here, the code for the feed-forward Neural Net and the SVM can be found.

To build the `docker build -t <tag_name> .` which gives a local docker container with the libraries scipy, numpy, pandas, pytorch, and sklearn."
110,https://github.com/sunqiaochen/NeurlPS_2022_DSC180,{'sunqiaochen': 'https://github.com/sunqiaochen'},"{'Jupyter Notebook': 1.0, 'Python': 0.0}","# NeurlPS_2022_DSC180
In this project, dataset we used: https://drive.google.com/drive/u/0/folders/10gwW55-9xQyAvJh7KHR1uJqYSpDBW0iL
https://drive.google.com/file/d/1TmPrp74d2y77FCu3Fv0u_vlOP7HvLxkX/view?usp=sharing

Nowadays, human activities such as wildfires and hunting have become the largest factor that would have serious negative effects on biodiversity. In order to deeply understand how anthropogenic activities deeply affect wildlife populations, field biologists utilize automated image classification driven by neural networks to get relevant biodiversity information from the images. However, for some small animals such as insects or birds, the camera could not work very well because of the small size of these animals. It is extremely hard for cameras to capture the movement and activities of small animals. To effectively solve this problem, passive acoustic monitoring (PAM) has become one of the most popular methods. We could utilize sounds we collect from PAM to train certain machine learning models which could tell us the fluctuation of biodiversity of all these small animals. The goal of the whole program is to test the biodiversity of these small animals (most of them are birds). However, the whole program could be divided into plenty of small parts. I and Jinsong will pay attention to the intermediate step of the program.

The goal of our project is to generate subsets of audio recordings that have higher probability of vocalization of interest, which could help our labeling volunteer to save time and energy. The solutions could help us reduce down the amount of time and resources required to achieve enough training data for species-level classifiers. We perform the same thing with AID_NeurIPS_2021. Only the data is different between these two github. For this github, we use the peru data instead of Coastal_Reserve data.


","# NeurlPS_2022_DSC180
In this project, dataset we used: https://drive.google.com/drive/u/0/folders/10gwW55-9xQyAvJh7KHR1uJqYSpDBW0iL
https://drive.google.com/file/d/1TmPrp74d2y77FCu3Fv0u_vlOP7HvLxkX/view?usp=sharing

Nowadays, human activities such as wildfires and hunting have become the largest factor that would have serious negative effects on biodiversity. In order to deeply understand how anthropogenic activities deeply affect wildlife populations, field biologists utilize automated image classification driven by neural networks to get relevant biodiversity information from the images. However, for some small animals such as insects or birds, the camera could not work very well because of the small size of these animals. It is extremely hard for cameras to capture the movement and activities of small animals. To effectively solve this problem, passive acoustic monitoring (PAM) has become one of the most popular methods. We could utilize sounds we collect from PAM to train certain machine learning models which could tell us the fluctuation of biodiversity of all these small animals. The goal of the whole program is to test the biodiversity of these small animals (most of them are birds). However, the whole program could be divided into plenty of small parts. I and Jinsong will pay attention to the intermediate step of the program.

The goal of our project is to generate subsets of audio recordings that have higher probability of vocalization of interest, which could help our labeling volunteer to save time and energy. The solutions could help us reduce down the amount of time and resources required to achieve enough training data for species-level classifiers. We perform the same thing with AID_NeurIPS_2021. Only the data is different between these two github. For this github, we use the peru data instead of Coastal_Reserve data.


"
111,https://github.com/EdmundoZamora/TweetyNet_CUDA_GPU_Adaptation,"{'EdmundoZamora': 'https://github.com/EdmundoZamora', 'mugen13lue': 'https://github.com/mugen13lue'}","{'Jupyter Notebook': 0.93, 'Python': 0.07, 'Dockerfile': 0.0, 'HTML': 0.0}","### Binary classifies bird vocalizations in a wav files

### Stores wav and csv data in data/raw/ it outputs results in data/out/

### Takes in raw wave files, trains and outputs best weights and performance and data evaluation(labeling). 

### Run entire project with: python run.py data features model evaluate nips  : deletes data directory and recreates each time the above command is ran. To supress future dependent library version warnings : python -W ignore run.py data features model evaluate nips :

### If data is already downloaded, spare your self the wait using : python run.py data skip features model evaluate nips: including skip in the targets skips the data downloading step. To supress future dependent library version warnings : python -W ignore run.py skip features model evaluate nips :


website: [DSC180-A09-Eco-Acoustic-Event-Detection](https://edmundozamora.github.io/DSC180-A09-Eco-Acoustic-Detection)

link to CPU Model Repository: [CPU Model Repo](https://github.com/EdmundoZamora/Q1-Project-Code)
","### Binary classifies bird vocalizations in a wav files

### Stores wav and csv data in data/raw/ it outputs results in data/out/

### Takes in raw wave files, trains and outputs best weights and performance and data evaluation(labeling). 

### Run entire project with: python run.py data features model evaluate nips  : deletes data directory and recreates each time the above command is ran. To supress future dependent library version warnings : python -W ignore run.py data features model evaluate nips :

### If data is already downloaded, spare your self the wait using : python run.py data skip features model evaluate nips: including skip in the targets skips the data downloading step. To supress future dependent library version warnings : python -W ignore run.py skip features model evaluate nips :


website: [DSC180-A09-Eco-Acoustic-Event-Detection](https://edmundozamora.github.io/DSC180-A09-Eco-Acoustic-Detection)

link to CPU Model Repository: [CPU Model Repo](https://github.com/EdmundoZamora/Q1-Project-Code)
"
112,https://github.com/UCSD-E4E/Pyrenote,"{'Sean1572': 'https://github.com/Sean1572', 'nishantbalaji': 'https://github.com/nishantbalaji', 'erikajoun': 'https://github.com/erikajoun', 'victoriacyzhang': 'https://github.com/victoriacyzhang', 'dependabot[bot]': 'https://github.com/apps/dependabot', 'vaibhavtiwari33': 'https://github.com/vaibhavtiwari33', 'Vanessa-Salgado': 'https://github.com/Vanessa-Salgado', 'ntlhui': 'https://github.com/ntlhui', 'WesCodes': 'https://github.com/WesCodes', 'JacobGlennAyers': 'https://github.com/JacobGlennAyers', 'Bipul-Harsh': 'https://github.com/Bipul-Harsh', 'sprestrelski': 'https://github.com/sprestrelski', 'kchen283': 'https://github.com/kchen283', 'mugen13lue': 'https://github.com/mugen13lue'}","{'JavaScript': 0.76, 'Python': 0.2, 'Shell': 0.02, 'Dockerfile': 0.01, 'CSS': 0.0, 'HTML': 0.0, 'Mako': 0.0}","## Pyrenote, The E4E Manual Audio Labeling System

This project, Pyrenote, creates moment to moment or strong labels for audio data. Pyrenote and much of this README are based on heavily on [Audino](https://github.com/midas-research/audino) as well as [Wavesurfer.js](https://github.com/katspaugh/wavesurfer.js). The name is a combination of Py, Lyrebird, and note (such as making a note on a label).

If you want to use Pyrenote, use the following to get started!

**NOTE**: Before making any changes to the code, make sure to create a branch to safely make changes. Never commit directly to main or production branch.
**Read github_procedures.md for more detailed information before contributing to the repo.** 

## Usage

*Note: Before getting the project set up, message project leads for env file. This file should be put in `/audino`. **Make sure the file is never pushed to the github***

Please install the following dependencies to run `Pyrenote` on your system:

1. [git](https://git-scm.com/) *[tested on v2.23.0]*
2. [docker](https://www.docker.com/) *[tested on v19.03.8, build afacb8b]*
3. [docker-compose](https://docs.docker.com/compose/) *[tested on v1.25.5, build 8a1c60f6]*

### Clone the repository

```sh
$ git clone https://github.com/UCSD-E4E/Pyrenote.git
$ cd audino
```

**Note for Windows users**: Please configure git to handle line endings correctly as services might throw an error and not come up. You can do this by cloning the project this way:

```sh
$ git clone https://github.com/UCSD-E4E/Pyrenote.git --config core.autocrlf=input
```

### For Development (Note this is the one we will test on and use)

Similar to `production` setup, you need to use development [configuration](./docker-compose.dev.yml) for working on the project, fixing bugs and making contributions.
**Note**: Before proceeding further, you might need to give docker `sudo` access or run the commands listed below as `sudo`.

**To build the services (do this when you first start it), run:**  
**Note**: Remember to cd into audino before starting
```sh
$ docker-compose -f docker-compose.dev.yml build
```

**To bring up the services, run:**
```sh
$ docker-compose -f docker-compose.dev.yml up
```
Then, in browser, go to [http://localhost:3000/](http://localhost:3000/) to view the application.

**To bring down the services, run:**

```sh
$ docker-compose -f docker-compose.dev.yml down
```
## Troubleshooting for starting docker

1) Docker containers do not even get a chance to start
  - Make sure docker is set up properly
  - Make sure docker itself has started. On Windows, check the system tray and hover over the icon to see the current status. Restart it if necessary
2) Backend crashes
  - For this error, check the top of the log. It should be complaining about /r characters in the run-dev.sh files
  - The backend will crash if the endline characters are set to CRLF rather than LF
  - On VSCode, you can swap this locally via going into the file and changing the CRLF icon in the bottom right to LF
  - Do this for `frontend/scripts/run-dev.sh` and `backend/scripts/run-dev.sh`
3) Database migration issues
  - If the backend complains about compiler issues while the database migration is occurring go into `backend/scripts/run-dev.sh`
  - On line 25, check and make sure that the stamp command is pointing to the right migration for the database
      - Ask for help on this one

## Getting Started

At this point, the docker should have gotten everything set up. After going to [http://localhost:3000/](http://localhost:3000/) you should be able to log into the docker

To access the site, sign in with the username of **admin** and password of **password**. On logging in, navigate to the admin-portal to create your first project. Make sure to make a label group and some labels for the project!

After creating a project, get the API key by returning to the admin portal. You can use the API key to add data to a project. Create a new terminal (while docker is running the severs) and cd into `audino/backend/scripts`. Here use the following command:

```
python upload_mass.py --username admin.test --is_marked_for_review True --audio_file C:\REPLACE\THIS\WITH\FOLDER\PATH\TO\AUDIO\DATA --host localhost --port 5000 --api_key REPLACE_THIS_WITH_API_KEY
```
Make sure to have a folder with the audio data ready to be added. For testing purposes, get a folder with about 20 clips. 

Once that runs, you are ready to start testing!


### For Production (Don't use on windows)

You can either run the project on [default configuration](./docker-compose.prod.yml) or modify them to your need.  
**Note**: Before proceeding further, you might need to give docker `sudo` access or run the commands listed below as `sudo`.  
**Note**: Remember to cd into audino before starting  

**To build the services, run:**

```sh
$ docker-compose -f docker-compose.prod.yml build
```

**To bring up the services, run:**

```sh
$ docker-compose -f docker-compose.prod.yml up
```

Then, in browser, go to [http://0.0.0.0/](http://0.0.0.0/) to view the application.

**To bring down the services, run:**

```sh
$ docker-compose -f docker-compose.prod.yml down
```

### For Dev Team:
Features should be turned on and off by admins for individual projects. When adding a new feature to either a project's data page or
annotation page, make sure to do the following:
1) Go to `.\audino\frontend\src\containers\forms\featureForm.js`
2) Add a new item in the featuresEnabled directory. This will be the name of the `feature_toggle` variable. 
3) Return to the page you are working on. 
  - For example, if you are working on the annotation page, navigate to the `componentDidMount()` method
  - about 20 lines down in the `setState` callback, add to the list `SOME_VAR: response.data.features_list['VARIABLE_NAMED_IN_STEP_2']`.

  ","## Pyrenote, The E4E Manual Audio Labeling System

This project, Pyrenote, creates moment to moment or strong labels for audio data. Pyrenote and much of this README are based on heavily on [Audino](https://github.com/midas-research/audino) as well as [Wavesurfer.js](https://github.com/katspaugh/wavesurfer.js). The name is a combination of Py, Lyrebird, and note (such as making a note on a label).

If you want to use Pyrenote, use the following to get started!

**NOTE**: Before making any changes to the code, make sure to create a branch to safely make changes. Never commit directly to main or production branch.
**Read github_procedures.md for more detailed information before contributing to the repo.** 

## Usage

*Note: Before getting the project set up, message project leads for env file. This file should be put in `/audino`. **Make sure the file is never pushed to the github***

Please install the following dependencies to run `Pyrenote` on your system:

1. [git](https://git-scm.com/) *[tested on v2.23.0]*
2. [docker](https://www.docker.com/) *[tested on v19.03.8, build afacb8b]*
3. [docker-compose](https://docs.docker.com/compose/) *[tested on v1.25.5, build 8a1c60f6]*

### Clone the repository

```sh
$ git clone https://github.com/UCSD-E4E/Pyrenote.git
$ cd audino
```

**Note for Windows users**: Please configure git to handle line endings correctly as services might throw an error and not come up. You can do this by cloning the project this way:

```sh
$ git clone https://github.com/UCSD-E4E/Pyrenote.git --config core.autocrlf=input
```

### For Development (Note this is the one we will test on and use)

Similar to `production` setup, you need to use development [configuration](./docker-compose.dev.yml) for working on the project, fixing bugs and making contributions.
**Note**: Before proceeding further, you might need to give docker `sudo` access or run the commands listed below as `sudo`.

**To build the services (do this when you first start it), run:**  
**Note**: Remember to cd into audino before starting
```sh
$ docker-compose -f docker-compose.dev.yml build
```

**To bring up the services, run:**
```sh
$ docker-compose -f docker-compose.dev.yml up
```
Then, in browser, go to [http://localhost:3000/](http://localhost:3000/) to view the application.

**To bring down the services, run:**

```sh
$ docker-compose -f docker-compose.dev.yml down
```
## Troubleshooting for starting docker

1) Docker containers do not even get a chance to start
  - Make sure docker is set up properly
  - Make sure docker itself has started. On Windows, check the system tray and hover over the icon to see the current status. Restart it if necessary
2) Backend crashes
  - For this error, check the top of the log. It should be complaining about /r characters in the run-dev.sh files
  - The backend will crash if the endline characters are set to CRLF rather than LF
  - On VSCode, you can swap this locally via going into the file and changing the CRLF icon in the bottom right to LF
  - Do this for `frontend/scripts/run-dev.sh` and `backend/scripts/run-dev.sh`
3) Database migration issues
  - If the backend complains about compiler issues while the database migration is occurring go into `backend/scripts/run-dev.sh`
  - On line 25, check and make sure that the stamp command is pointing to the right migration for the database
      - Ask for help on this one

## Getting Started

At this point, the docker should have gotten everything set up. After going to [http://localhost:3000/](http://localhost:3000/) you should be able to log into the docker

To access the site, sign in with the username of **admin** and password of **password**. On logging in, navigate to the admin-portal to create your first project. Make sure to make a label group and some labels for the project!

After creating a project, get the API key by returning to the admin portal. You can use the API key to add data to a project. Create a new terminal (while docker is running the severs) and cd into `audino/backend/scripts`. Here use the following command:

```
python upload_mass.py --username admin.test --is_marked_for_review True --audio_file C:\REPLACE\THIS\WITH\FOLDER\PATH\TO\AUDIO\DATA --host localhost --port 5000 --api_key REPLACE_THIS_WITH_API_KEY
```
Make sure to have a folder with the audio data ready to be added. For testing purposes, get a folder with about 20 clips. 

Once that runs, you are ready to start testing!


### For Production (Don't use on windows)

You can either run the project on [default configuration](./docker-compose.prod.yml) or modify them to your need.  
**Note**: Before proceeding further, you might need to give docker `sudo` access or run the commands listed below as `sudo`.  
**Note**: Remember to cd into audino before starting  

**To build the services, run:**

```sh
$ docker-compose -f docker-compose.prod.yml build
```

**To bring up the services, run:**

```sh
$ docker-compose -f docker-compose.prod.yml up
```

Then, in browser, go to [http://0.0.0.0/](http://0.0.0.0/) to view the application.

**To bring down the services, run:**

```sh
$ docker-compose -f docker-compose.prod.yml down
```

### For Dev Team:
Features should be turned on and off by admins for individual projects. When adding a new feature to either a project's data page or
annotation page, make sure to do the following:
1) Go to `.\audino\frontend\src\containers\forms\featureForm.js`
2) Add a new item in the featuresEnabled directory. This will be the name of the `feature_toggle` variable. 
3) Return to the page you are working on. 
  - For example, if you are working on the annotation page, navigate to the `componentDidMount()` method
  - about 20 lines down in the `setState` callback, add to the list `SOME_VAR: response.data.features_list['VARIABLE_NAMED_IN_STEP_2']`.

  "
113,https://github.com/UCSD-E4E/Pyrenote,"{'Sean1572': 'https://github.com/Sean1572', 'nishantbalaji': 'https://github.com/nishantbalaji', 'erikajoun': 'https://github.com/erikajoun', 'victoriacyzhang': 'https://github.com/victoriacyzhang', 'dependabot[bot]': 'https://github.com/apps/dependabot', 'vaibhavtiwari33': 'https://github.com/vaibhavtiwari33', 'Vanessa-Salgado': 'https://github.com/Vanessa-Salgado', 'ntlhui': 'https://github.com/ntlhui', 'WesCodes': 'https://github.com/WesCodes', 'JacobGlennAyers': 'https://github.com/JacobGlennAyers', 'Bipul-Harsh': 'https://github.com/Bipul-Harsh', 'sprestrelski': 'https://github.com/sprestrelski', 'kchen283': 'https://github.com/kchen283', 'mugen13lue': 'https://github.com/mugen13lue'}","{'JavaScript': 0.76, 'Python': 0.2, 'Shell': 0.02, 'Dockerfile': 0.01, 'CSS': 0.0, 'HTML': 0.0, 'Mako': 0.0}","## Pyrenote, The E4E Manual Audio Labeling System

This project, Pyrenote, creates moment to moment or strong labels for audio data. Pyrenote and much of this README are based on heavily on [Audino](https://github.com/midas-research/audino) as well as [Wavesurfer.js](https://github.com/katspaugh/wavesurfer.js). The name is a combination of Py, Lyrebird, and note (such as making a note on a label).

If you want to use Pyrenote, use the following to get started!

**NOTE**: Before making any changes to the code, make sure to create a branch to safely make changes. Never commit directly to main or production branch.
**Read github_procedures.md for more detailed information before contributing to the repo.** 

## Usage

*Note: Before getting the project set up, message project leads for env file. This file should be put in `/audino`. **Make sure the file is never pushed to the github***

Please install the following dependencies to run `Pyrenote` on your system:

1. [git](https://git-scm.com/) *[tested on v2.23.0]*
2. [docker](https://www.docker.com/) *[tested on v19.03.8, build afacb8b]*
3. [docker-compose](https://docs.docker.com/compose/) *[tested on v1.25.5, build 8a1c60f6]*

### Clone the repository

```sh
$ git clone https://github.com/UCSD-E4E/Pyrenote.git
$ cd audino
```

**Note for Windows users**: Please configure git to handle line endings correctly as services might throw an error and not come up. You can do this by cloning the project this way:

```sh
$ git clone https://github.com/UCSD-E4E/Pyrenote.git --config core.autocrlf=input
```

### For Development (Note this is the one we will test on and use)

Similar to `production` setup, you need to use development [configuration](./docker-compose.dev.yml) for working on the project, fixing bugs and making contributions.
**Note**: Before proceeding further, you might need to give docker `sudo` access or run the commands listed below as `sudo`.

**To build the services (do this when you first start it), run:**  
**Note**: Remember to cd into audino before starting
```sh
$ docker-compose -f docker-compose.dev.yml build
```

**To bring up the services, run:**
```sh
$ docker-compose -f docker-compose.dev.yml up
```
Then, in browser, go to [http://localhost:3000/](http://localhost:3000/) to view the application.

**To bring down the services, run:**

```sh
$ docker-compose -f docker-compose.dev.yml down
```
## Troubleshooting for starting docker

1) Docker containers do not even get a chance to start
  - Make sure docker is set up properly
  - Make sure docker itself has started. On Windows, check the system tray and hover over the icon to see the current status. Restart it if necessary
2) Backend crashes
  - For this error, check the top of the log. It should be complaining about /r characters in the run-dev.sh files
  - The backend will crash if the endline characters are set to CRLF rather than LF
  - On VSCode, you can swap this locally via going into the file and changing the CRLF icon in the bottom right to LF
  - Do this for `frontend/scripts/run-dev.sh` and `backend/scripts/run-dev.sh`
3) Database migration issues
  - If the backend complains about compiler issues while the database migration is occurring go into `backend/scripts/run-dev.sh`
  - On line 25, check and make sure that the stamp command is pointing to the right migration for the database
      - Ask for help on this one

## Getting Started

At this point, the docker should have gotten everything set up. After going to [http://localhost:3000/](http://localhost:3000/) you should be able to log into the docker

To access the site, sign in with the username of **admin** and password of **password**. On logging in, navigate to the admin-portal to create your first project. Make sure to make a label group and some labels for the project!

After creating a project, get the API key by returning to the admin portal. You can use the API key to add data to a project. Create a new terminal (while docker is running the severs) and cd into `audino/backend/scripts`. Here use the following command:

```
python upload_mass.py --username admin.test --is_marked_for_review True --audio_file C:\REPLACE\THIS\WITH\FOLDER\PATH\TO\AUDIO\DATA --host localhost --port 5000 --api_key REPLACE_THIS_WITH_API_KEY
```
Make sure to have a folder with the audio data ready to be added. For testing purposes, get a folder with about 20 clips. 

Once that runs, you are ready to start testing!


### For Production (Don't use on windows)

You can either run the project on [default configuration](./docker-compose.prod.yml) or modify them to your need.  
**Note**: Before proceeding further, you might need to give docker `sudo` access or run the commands listed below as `sudo`.  
**Note**: Remember to cd into audino before starting  

**To build the services, run:**

```sh
$ docker-compose -f docker-compose.prod.yml build
```

**To bring up the services, run:**

```sh
$ docker-compose -f docker-compose.prod.yml up
```

Then, in browser, go to [http://0.0.0.0/](http://0.0.0.0/) to view the application.

**To bring down the services, run:**

```sh
$ docker-compose -f docker-compose.prod.yml down
```

### For Dev Team:
Features should be turned on and off by admins for individual projects. When adding a new feature to either a project's data page or
annotation page, make sure to do the following:
1) Go to `.\audino\frontend\src\containers\forms\featureForm.js`
2) Add a new item in the featuresEnabled directory. This will be the name of the `feature_toggle` variable. 
3) Return to the page you are working on. 
  - For example, if you are working on the annotation page, navigate to the `componentDidMount()` method
  - about 20 lines down in the `setState` callback, add to the list `SOME_VAR: response.data.features_list['VARIABLE_NAMED_IN_STEP_2']`.

  ","## Pyrenote, The E4E Manual Audio Labeling System

This project, Pyrenote, creates moment to moment or strong labels for audio data. Pyrenote and much of this README are based on heavily on [Audino](https://github.com/midas-research/audino) as well as [Wavesurfer.js](https://github.com/katspaugh/wavesurfer.js). The name is a combination of Py, Lyrebird, and note (such as making a note on a label).

If you want to use Pyrenote, use the following to get started!

**NOTE**: Before making any changes to the code, make sure to create a branch to safely make changes. Never commit directly to main or production branch.
**Read github_procedures.md for more detailed information before contributing to the repo.** 

## Usage

*Note: Before getting the project set up, message project leads for env file. This file should be put in `/audino`. **Make sure the file is never pushed to the github***

Please install the following dependencies to run `Pyrenote` on your system:

1. [git](https://git-scm.com/) *[tested on v2.23.0]*
2. [docker](https://www.docker.com/) *[tested on v19.03.8, build afacb8b]*
3. [docker-compose](https://docs.docker.com/compose/) *[tested on v1.25.5, build 8a1c60f6]*

### Clone the repository

```sh
$ git clone https://github.com/UCSD-E4E/Pyrenote.git
$ cd audino
```

**Note for Windows users**: Please configure git to handle line endings correctly as services might throw an error and not come up. You can do this by cloning the project this way:

```sh
$ git clone https://github.com/UCSD-E4E/Pyrenote.git --config core.autocrlf=input
```

### For Development (Note this is the one we will test on and use)

Similar to `production` setup, you need to use development [configuration](./docker-compose.dev.yml) for working on the project, fixing bugs and making contributions.
**Note**: Before proceeding further, you might need to give docker `sudo` access or run the commands listed below as `sudo`.

**To build the services (do this when you first start it), run:**  
**Note**: Remember to cd into audino before starting
```sh
$ docker-compose -f docker-compose.dev.yml build
```

**To bring up the services, run:**
```sh
$ docker-compose -f docker-compose.dev.yml up
```
Then, in browser, go to [http://localhost:3000/](http://localhost:3000/) to view the application.

**To bring down the services, run:**

```sh
$ docker-compose -f docker-compose.dev.yml down
```
## Troubleshooting for starting docker

1) Docker containers do not even get a chance to start
  - Make sure docker is set up properly
  - Make sure docker itself has started. On Windows, check the system tray and hover over the icon to see the current status. Restart it if necessary
2) Backend crashes
  - For this error, check the top of the log. It should be complaining about /r characters in the run-dev.sh files
  - The backend will crash if the endline characters are set to CRLF rather than LF
  - On VSCode, you can swap this locally via going into the file and changing the CRLF icon in the bottom right to LF
  - Do this for `frontend/scripts/run-dev.sh` and `backend/scripts/run-dev.sh`
3) Database migration issues
  - If the backend complains about compiler issues while the database migration is occurring go into `backend/scripts/run-dev.sh`
  - On line 25, check and make sure that the stamp command is pointing to the right migration for the database
      - Ask for help on this one

## Getting Started

At this point, the docker should have gotten everything set up. After going to [http://localhost:3000/](http://localhost:3000/) you should be able to log into the docker

To access the site, sign in with the username of **admin** and password of **password**. On logging in, navigate to the admin-portal to create your first project. Make sure to make a label group and some labels for the project!

After creating a project, get the API key by returning to the admin portal. You can use the API key to add data to a project. Create a new terminal (while docker is running the severs) and cd into `audino/backend/scripts`. Here use the following command:

```
python upload_mass.py --username admin.test --is_marked_for_review True --audio_file C:\REPLACE\THIS\WITH\FOLDER\PATH\TO\AUDIO\DATA --host localhost --port 5000 --api_key REPLACE_THIS_WITH_API_KEY
```
Make sure to have a folder with the audio data ready to be added. For testing purposes, get a folder with about 20 clips. 

Once that runs, you are ready to start testing!


### For Production (Don't use on windows)

You can either run the project on [default configuration](./docker-compose.prod.yml) or modify them to your need.  
**Note**: Before proceeding further, you might need to give docker `sudo` access or run the commands listed below as `sudo`.  
**Note**: Remember to cd into audino before starting  

**To build the services, run:**

```sh
$ docker-compose -f docker-compose.prod.yml build
```

**To bring up the services, run:**

```sh
$ docker-compose -f docker-compose.prod.yml up
```

Then, in browser, go to [http://0.0.0.0/](http://0.0.0.0/) to view the application.

**To bring down the services, run:**

```sh
$ docker-compose -f docker-compose.prod.yml down
```

### For Dev Team:
Features should be turned on and off by admins for individual projects. When adding a new feature to either a project's data page or
annotation page, make sure to do the following:
1) Go to `.\audino\frontend\src\containers\forms\featureForm.js`
2) Add a new item in the featuresEnabled directory. This will be the name of the `feature_toggle` variable. 
3) Return to the page you are working on. 
  - For example, if you are working on the annotation page, navigate to the `componentDidMount()` method
  - about 20 lines down in the `setState` callback, add to the list `SOME_VAR: response.data.features_list['VARIABLE_NAMED_IN_STEP_2']`.

  "
114,https://github.com/RuojiaTao/Covid-Misinformation-Spread-on-Tweets,"{'RuojiaTao': 'https://github.com/RuojiaTao', 'axl401': 'https://github.com/axl401'}",{'Python': 1.0},"# Covid-Misinformation-Spread-on-Tweets
Analyze the misinformation about covid spreading in tweet

Our website: https://ruojiatao.github.io/Covid-Misinformation-Spread-on-Tweets/

Abstract: 
Spread of misinformation over social media posts challenges to daily information intake and exchange. Especially under current covid 19 pandemic, the disperse of misinformation regarding to covid 19 diseases and vaccination posts threats to individuals' wellbeings and general publich health. This project seeks to invertigate the spread of misinformation over social media (Twitter) under covid 19 pademic. The first topic is the effect of bot users on the spread of misinformation, and the second topic is to examine users' attitude towards misinformation. These two topics are analyze under the social structure (connected social graphs) created through user's interactions on Twitter. This project also seeks to invertigate the change in proportion of bot users and users' attitude towards misinformation as it's approaching to the center of the social network. 



 - The `run.py` file run all part of code
 - `Data_Pre` folder contains: 
     - `hydrate_tweets_twarc.py` : hydrating the data
     - `EDA.py`: explore the raw data and try to find interesting points
 - `K_Core` folder contains:
     - `K_Core_Degree.py`: set up a graph and calculate the K-Core degree , assign K Core degree back to all data, and plot out the spread of K-Core.
     - `sample_dataset.py` : sample a small dataset for analzye
     - `Tests_result.py` : run a linear machine learning model to predict the negative sentiment based on different parapmeters. Base on the coefficient of ML model to find the relstionship between parpameters and negative sentiment.
 - `Bot_User` folder contains: 
     - `bot_detection.py`: use Botomerter to predict if a user is bot or not
     - `Bot_merge.py`: merge predictions to K Core results, generating graphs for percentage of bot useres in each K core degrees
 - `NLP` folder contains:
     - `NLP.py`: get the sentiment scores of each tweets, and plot out overall trends
     - `NLP_ab_testing`: run a 2 sample t-test to see if there is an sigificant difference between negative sentiments in different pair of groups that has different K-Core degrees
 - `data` folder contains:
     - `iffy.csv` used for checking misinformation links


","# Covid-Misinformation-Spread-on-Tweets
Analyze the misinformation about covid spreading in tweet

Our website: https://ruojiatao.github.io/Covid-Misinformation-Spread-on-Tweets/

Abstract: 
Spread of misinformation over social media posts challenges to daily information intake and exchange. Especially under current covid 19 pandemic, the disperse of misinformation regarding to covid 19 diseases and vaccination posts threats to individuals' wellbeings and general publich health. This project seeks to invertigate the spread of misinformation over social media (Twitter) under covid 19 pademic. The first topic is the effect of bot users on the spread of misinformation, and the second topic is to examine users' attitude towards misinformation. These two topics are analyze under the social structure (connected social graphs) created through user's interactions on Twitter. This project also seeks to invertigate the change in proportion of bot users and users' attitude towards misinformation as it's approaching to the center of the social network. 



 - The `run.py` file run all part of code
 - `Data_Pre` folder contains: 
     - `hydrate_tweets_twarc.py` : hydrating the data
     - `EDA.py`: explore the raw data and try to find interesting points
 - `K_Core` folder contains:
     - `K_Core_Degree.py`: set up a graph and calculate the K-Core degree , assign K Core degree back to all data, and plot out the spread of K-Core.
     - `sample_dataset.py` : sample a small dataset for analzye
     - `Tests_result.py` : run a linear machine learning model to predict the negative sentiment based on different parapmeters. Base on the coefficient of ML model to find the relstionship between parpameters and negative sentiment.
 - `Bot_User` folder contains: 
     - `bot_detection.py`: use Botomerter to predict if a user is bot or not
     - `Bot_merge.py`: merge predictions to K Core results, generating graphs for percentage of bot useres in each K core degrees
 - `NLP` folder contains:
     - `NLP.py`: get the sentiment scores of each tweets, and plot out overall trends
     - `NLP_ab_testing`: run a 2 sample t-test to see if there is an sigificant difference between negative sentiments in different pair of groups that has different K-Core degrees
 - `data` folder contains:
     - `iffy.csv` used for checking misinformation links


"
115,https://github.com/davamini/DSC180B_Reddit_MisInfo_Capstone,{'davamini': 'https://github.com/davamini'},"{'Jupyter Notebook': 1.0, 'Python': 0.0}","# DSC180B Subreddit Misinformation Capstone Project
Analyzing the spread of misinformation in the Reddit platform.<br>
Website URL: https://davamini.com/dsc180_project.github.io/index.html
<br>
### Build Instructions:
```sh
pip install pandas praw gspread oauth2client
```
> Must create conf.json and google_sheets_creds.json in the <b>config directory</b>.<br>
> * conf.json must contain values for client_secret, and client_id, which are aquired from Reddit after creating an application.<br>
> * google_sheets_creds.json is aquired from Google Cloud after enabling APIs for google sheets.<br>
#### Run either:
```sh
python run.py
```
#### Or:
```sh
python run.py test
```
","# DSC180B Subreddit Misinformation Capstone Project
Analyzing the spread of misinformation in the Reddit platform.<br>
Website URL: https://davamini.com/dsc180_project.github.io/index.html
<br>
### Build Instructions:
```sh
pip install pandas praw gspread oauth2client
```
> Must create conf.json and google_sheets_creds.json in the <b>config directory</b>.<br>
> * conf.json must contain values for client_secret, and client_id, which are aquired from Reddit after creating an application.<br>
> * google_sheets_creds.json is aquired from Google Cloud after enabling APIs for google sheets.<br>
#### Run either:
```sh
python run.py
```
#### Or:
```sh
python run.py test
```
"
116,https://github.com/fieryashes/DSC180B_Misinformation_Project,"{'anaaamika': 'https://github.com/anaaamika', 'fieryashes': 'https://github.com/fieryashes'}","{'Jupyter Notebook': 0.86, 'HTML': 0.11, 'Python': 0.03}","# DSC180B Misinformation Project
Project Website: https://anaaamika.github.io/DSC180B-Misinformation/

## Build Instructions
Ensure that you have twarckeys.py and youtubekeys.py files in your secrets folder. The twarckeys.py file should contain the variables consumer_key, consumer_secret, access_token, access_token_secret populated from the project keys and tokens from [the Developer Portal for the Twitter API](https://developer.twitter.com/en/portal/dashboard). The youtubekeys.py file should contain the variable api_key populated with a API key for [the YouTube Data API v3](https://developers.google.com/youtube/v3). 

To build the project run the following commands: 
`python run.py data` This will populate the *data* folder with tweet datasets, YouTube video ids dataset, and datasets with YouTube metadata. 

Then run `python run.py analysis` to view preliminary analysis on the YouTube captions and metadata.

Finally, run `python run.py model` to build a misinformation detection model based on the YouTube transcripts. 
","# DSC180B Misinformation Project
Project Website: https://anaaamika.github.io/DSC180B-Misinformation/

## Build Instructions
Ensure that you have twarckeys.py and youtubekeys.py files in your secrets folder. The twarckeys.py file should contain the variables consumer_key, consumer_secret, access_token, access_token_secret populated from the project keys and tokens from [the Developer Portal for the Twitter API](https://developer.twitter.com/en/portal/dashboard). The youtubekeys.py file should contain the variable api_key populated with a API key for [the YouTube Data API v3](https://developers.google.com/youtube/v3). 

To build the project run the following commands: 
`python run.py data` This will populate the *data* folder with tweet datasets, YouTube video ids dataset, and datasets with YouTube metadata. 

Then run `python run.py analysis` to view preliminary analysis on the YouTube captions and metadata.

Finally, run `python run.py model` to build a misinformation detection model based on the YouTube transcripts. 
"
117,https://github.com/Bryan-Az/DSC-Capstone-Result-Replication,{'Bryan-Az': 'https://github.com/Bryan-Az'},"{'Jupyter Notebook': 0.98, 'Python': 0.02}","## DSC-Capstone-Result-Replication
### DSC180A Quarter 1 and 2

Website Link: http://www.bambriz.me/wp-content/uploads/2022/03/dsc180b.html

Website code is included in the 'docs' directory of this repo.

To run the GENConv GNN model, create docker pod using jmduarte/capstone-particle-physics-domain:latest and group key to access data folder.

Then clone this repo and cd into root folder.

In the root folder, you can use python run.py [train, test] to produce a dataframe with predictions and other data useful for visualization in the src/analysis/evaluation.ipynb notebook. 

To visualize, you can simply run all cells in the eval. notebook.

Pls. ignore the loading bar in terminal it doesn't mean anything - but training it will take a while. 

There is really no need to run the training since the testing script loads presaved weights (stored in github) to initialize the pre-trained model for testing.  
","## DSC-Capstone-Result-Replication
### DSC180A Quarter 1 and 2

Website Link: http://www.bambriz.me/wp-content/uploads/2022/03/dsc180b.html

Website code is included in the 'docs' directory of this repo.

To run the GENConv GNN model, create docker pod using jmduarte/capstone-particle-physics-domain:latest and group key to access data folder.

Then clone this repo and cd into root folder.

In the root folder, you can use python run.py [train, test] to produce a dataframe with predictions and other data useful for visualization in the src/analysis/evaluation.ipynb notebook. 

To visualize, you can simply run all cells in the eval. notebook.

Pls. ignore the loading bar in terminal it doesn't mean anything - but training it will take a while. 

There is really no need to run the training since the testing script loads presaved weights (stored in github) to initialize the pre-trained model for testing.  
"
118,https://github.com/UCSDJLEE/DSC180B-A11-Project,{'UCSDJLEE': 'https://github.com/UCSDJLEE'},"{'Python': 0.59, 'Jupyter Notebook': 0.39, 'Dockerfile': 0.02, 'Shell': 0.01}","# DSC180B-A11-Project
## Particle Jet Mass Regression

<img src='https://raw.githubusercontent.com/isacmlee/particle-physics-visuals/main/images/cern_atlas.jpeg'>


With jet data collected from proton-proton collision, the contents in this repository aim to predict the mass of particle jet, which is an important feature in classifying the types of those jets.
The model is implemented based on PyTorch Neural Network APIs and gets fitted to training sets available on DSMLP server.

#### Repository Structure:

 - `conf`: directory in which all necessary variables needed for configuration in development are stored
 - `data`: temporary directory that explains the source of our data
 - `notebooks`: directory in which .ipynb notebooks for EDA and model evaluation is stored
 - `src`: source directory for all library codes used for this project
 - `run.py`: main script to run within properly set-up environment using `python3 run.py train` for model training or `python3 run.py test` for model assessment
 - `simplenetwork_best.pt`: PyTorch-based file that stores weights of optimized, or best ""fitting,"" model
","# DSC180B-A11-Project
## Particle Jet Mass Regression

<img src='https://raw.githubusercontent.com/isacmlee/particle-physics-visuals/main/images/cern_atlas.jpeg'>


With jet data collected from proton-proton collision, the contents in this repository aim to predict the mass of particle jet, which is an important feature in classifying the types of those jets.
The model is implemented based on PyTorch Neural Network APIs and gets fitted to training sets available on DSMLP server.

#### Repository Structure:

 - `conf`: directory in which all necessary variables needed for configuration in development are stored
 - `data`: temporary directory that explains the source of our data
 - `notebooks`: directory in which .ipynb notebooks for EDA and model evaluation is stored
 - `src`: source directory for all library codes used for this project
 - `run.py`: main script to run within properly set-up environment using `python3 run.py train` for model training or `python3 run.py test` for model assessment
 - `simplenetwork_best.pt`: PyTorch-based file that stores weights of optimized, or best ""fitting,"" model
"
119,https://github.com/shonepatil/GNN-Spotify-Recommender-Project,"{'Melody-Wang': 'https://github.com/Melody-Wang', 'Benjamin242': 'https://github.com/Benjamin242', 'shonepatil': 'https://github.com/shonepatil'}","{'Jupyter Notebook': 0.95, 'Python': 0.05, 'Dockerfile': 0.0}","# Graph Neural Networks for Song Recommendation on Spotify Playlists

This project tackles the task of creating meaningful and accurate song recommendations to Spotify Playlists by using Graph Neural Networks. The goal is to better capture the characteristics of songs by analyzing co-occurence of song pairs across thousands of playlists in the form of a graph.

To obtain the spotify playlist data, visit https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge and put the files in `data/playlists`. You will have to create these subfolders. Confirm data paths for future files in `config/data-params`. To add song features and create graph from scratch if you only have the playlist data, set `create_graph_from_scratch` to be `True` in data-params. For obtaining song features using the Spotify API, put secret key and secret id in `spotifyAPI_script.py` within `/src/api`. Also within data-params, we suggest setting `playlist-num` to be 1000 such that the Spotify API requesting only takes 15-20 minutes. If you set it to 10000 for a larger dataset, the code may take 4 hours.

To run the GraphSAGE based model on the Spotify Playlist data, first use this command from the root folder: `python run.py data model`. You can leave out the `model` argument if you don't want to train the model from scratch. Next to see and create recommendations open and run through the notebook `Recommender Demonstration.ipynb` from within `src/dgl_graphsage/`.

To customize the model parameters, edit `config/model-params`.
","# Graph Neural Networks for Song Recommendation on Spotify Playlists

This project tackles the task of creating meaningful and accurate song recommendations to Spotify Playlists by using Graph Neural Networks. The goal is to better capture the characteristics of songs by analyzing co-occurence of song pairs across thousands of playlists in the form of a graph.

To obtain the spotify playlist data, visit https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge and put the files in `data/playlists`. You will have to create these subfolders. Confirm data paths for future files in `config/data-params`. To add song features and create graph from scratch if you only have the playlist data, set `create_graph_from_scratch` to be `True` in data-params. For obtaining song features using the Spotify API, put secret key and secret id in `spotifyAPI_script.py` within `/src/api`. Also within data-params, we suggest setting `playlist-num` to be 1000 such that the Spotify API requesting only takes 15-20 minutes. If you set it to 10000 for a larger dataset, the code may take 4 hours.

To run the GraphSAGE based model on the Spotify Playlist data, first use this command from the root folder: `python run.py data model`. You can leave out the `model` argument if you don't want to train the model from scratch. Next to see and create recommendations open and run through the notebook `Recommender Demonstration.ipynb` from within `src/dgl_graphsage/`.

To customize the model parameters, edit `config/model-params`.
"
120,https://github.com/yangshengaa/dynamic_stock_industry_classification,{'yangshengaa': 'https://github.com/yangshengaa'},"{'Python': 1.0, 'Dockerfile': 0.0}","# Dynamic Stock Industrial Classification

Use graph-based analysis to re-classify stocks and experiment different re-classification methodologies to improve Markowitz portfolio optimization performance in the low-frequency quantitative trading context.

Note that for strategy confidentiality, many files are hidden.

To accommodate speedy development, the current code structure simplicity is sacrificed. This will be addressed in later versions.

Project Website: [Dynamic Stock Industrial Classification](https://yangshengaa.github.io/dynamic_stock_industry_classification/)

## Module Breakdown

This project contains the following six modules:

- [data ingestion](src/data_ingestion): address finance data I/O and handle storage of intermediate results;
- [factor generation](src/factor_generation): compute and store factors alpha factors and risk factors for low-frequency trading;
- [backtest](src/backtest): low-frequency backtest framework (both factors and signals). Factors have continuous values on each cross section whereas signals have only -1, 0, and 1 overall;
- [factor combination](src/factor_combination): combine factors using ML models;
- [portfolio optimization](src/portfolio_optimization): Markowitz portfolio optimization, with turnover, industrial exposure, style exposure, and various other constraints.
- [graph cluster](src/graph_cluster): experiment different graph-based clustering on stocks.

## Data

China A-Share stocks, the corresponding major index data (sz50, hs300, zz500, zz1000), and the member stock weights from 20150101 to 20211231, provided by [Shanghai Probability Quantitative Investment](http://www.probquant.cn/).

## Experiment Results

With a fixed predicted ML results, we go through the optimization pipeline to optimize each trained classification.

**Stock Pool**: zz1000 member stocks  
**Benchmark**: zz1000 index  
**Time Period**: 20170701 - 20211231  

| Model | AlphaReturn (cumsum) | AlphaSharpe | AlphaDrawdown | Turnover |
| ----- | :---------------------: | :----------: | :-----------: | :------: |
| LinearRegressor | 71.58 | 1.92 | -19.84 | **1.01** |
| LgbmRegressor | 145.64 | **3.65** | **-11.58** | 1.21 |
| LgbmRegressor-opt | **146.73** | 2.96 | -29.79 | 1.11 |
| .. | .. | .. | .. | .. | .. |
| 40-cluster PMFG Unfiltered Spectral | 154.45 | 3.15 | **-22.69** | 1.11 |
| 10-cluster PMFG Filtered Average Linkage | 160.95 | **3.32** | -26.77 | 1.11 |
| 30-cluster AG Unfiltered Sub2Vec | 160.96 | 3.24 | -23.05 | 1.10 |
| 5-cluster MST Unfiltered Sub2Vec | 163.26 | 3.27 | -27.39 | 1.11 |
| **20-cluster PMFG Filtered Node2Vec** | **164.68** | 3.30 | -27.06 | 1.11 |

Compared to the original optimization result, we observe a 12.23% improvement in excess return and 12.16% improvement in excess Sharpe ratio.

Since factors based on price and volume lost their predictive power staring from 20200701, we also look at the performances before that time.

**Time Period**: 20170701 - 20200701

| Model | AlphaReturn (cumsum) | AlphaSharpe | AlphaDrawdown | Turnover |
| ----- | :---------------------: | :----------: | :-----------: | :------: |
| LgbmRegressor | 150.64 | 6.06 | **-4.59** | 1.23 |
| LgbmRegressor-opt | **170.31** | 5.43 | -6.76 | 1.12 |
| .. | .. | .. | .. | .. | .. |
| 10-cluster PMFG Filtered Sub2Vec | 173.10 | 5.49 | **-5.51** | 1.12 |
| 5-cluster MST Filtered Sub2Vec | 182.89 | 5.78 | -7.14 | 1.12 |
| 10-cluster AG Filtered Sub2Vec | 181.50 | 5.64 | -7.40 | 1.12 |
| **20-cluster PMFG Filtered Node2Vec** | **184.21** | **5.85** | -6.42 | 1.12 |

In this period, we observe a 8.16% improvement in excess return and a 7.73 improvement in excess Sharpe ratio, compared to the original optimization result.

For a complete list of results, check out [summary_20170701_20211231.csv](out/res/signal_test_file_20220305_long_experiment/summary.csv) and [summary_20170701_20200701.csv](out/res/signal_test_file_20220305_short_experiment/summary.csv). And more details are discussed on the project website listed above.

## Environment

To run codes in this project, it is recommended to create an environment listed in the [environment.yml](environment.yml). If conda is installed, run:

```bash
conda env create -f environment.yml
conda activate finance-base
```

Alternatively, one could also pull the corresponding docker image from [yangshengaa/finance-base](https://hub.docker.com/repository/docker/yangshengaa/finance-base) and then activate the finance-base environment using the latter conda command.

## Quick Start

It's very easy to use this platform!

Tips:

- run each module at a time, and run the following command sequentially;
- change config for corresponding module in respective files (file location indicated inside [run.py](run.py));
- detailed running instructions, including a walkthrough of parameters in each modules, are in README of each module.

To run each module, in current directory:

Factor Generation:

- factor generation: `python run.py gen`

Backtest:

- backtest factor: `python run.py backtest_factor`
- backtest signal: `python run.py backtest_signal`

Factor Combination:

- factor combination: `python run.py comb`

Portfolio Optimization:

- generate factor returns: `python run.py opt_fac_ret`
- estimate covariance matrices: `python run.py opt_cov_est`
- adjust weight: `python run.py opt_weight`

Graph Clustering:

- train graph clustering: `python run.py cluster_train`

To run each submodules, in current directory:

- generate pairs factors: `python run.py pairs`
- generate risk factors: `python run.py gen_risk`

Currently risk attribution module is very slow and suboptimal. To be addressed later.

## Acknowledgement

Special thanks to coworkers and my best friends at [Shanghai Probability Quantitative Investment](http://www.probquant.cn/): Beilei Xu, Zhongyuan Wang, Zhenghang Xie, Cong Chen, Yihao Zhou, Weilin Chen, Yuhan Tao, Wan Zheng, and many others. This project would be impossible without their data, insights, and experiences.

## For Developer

Log known issues here:

- signals given by factor test could not give the same alpha returns (slightly less) as in signal test
  - examine output holding stats
- plain risk attribution
","# Dynamic Stock Industrial Classification

Use graph-based analysis to re-classify stocks and experiment different re-classification methodologies to improve Markowitz portfolio optimization performance in the low-frequency quantitative trading context.

Note that for strategy confidentiality, many files are hidden.

To accommodate speedy development, the current code structure simplicity is sacrificed. This will be addressed in later versions.

Project Website: [Dynamic Stock Industrial Classification](https://yangshengaa.github.io/dynamic_stock_industry_classification/)

## Module Breakdown

This project contains the following six modules:

- [data ingestion](src/data_ingestion): address finance data I/O and handle storage of intermediate results;
- [factor generation](src/factor_generation): compute and store factors alpha factors and risk factors for low-frequency trading;
- [backtest](src/backtest): low-frequency backtest framework (both factors and signals). Factors have continuous values on each cross section whereas signals have only -1, 0, and 1 overall;
- [factor combination](src/factor_combination): combine factors using ML models;
- [portfolio optimization](src/portfolio_optimization): Markowitz portfolio optimization, with turnover, industrial exposure, style exposure, and various other constraints.
- [graph cluster](src/graph_cluster): experiment different graph-based clustering on stocks.

## Data

China A-Share stocks, the corresponding major index data (sz50, hs300, zz500, zz1000), and the member stock weights from 20150101 to 20211231, provided by [Shanghai Probability Quantitative Investment](http://www.probquant.cn/).

## Experiment Results

With a fixed predicted ML results, we go through the optimization pipeline to optimize each trained classification.

**Stock Pool**: zz1000 member stocks  
**Benchmark**: zz1000 index  
**Time Period**: 20170701 - 20211231  

| Model | AlphaReturn (cumsum) | AlphaSharpe | AlphaDrawdown | Turnover |
| ----- | :---------------------: | :----------: | :-----------: | :------: |
| LinearRegressor | 71.58 | 1.92 | -19.84 | **1.01** |
| LgbmRegressor | 145.64 | **3.65** | **-11.58** | 1.21 |
| LgbmRegressor-opt | **146.73** | 2.96 | -29.79 | 1.11 |
| .. | .. | .. | .. | .. | .. |
| 40-cluster PMFG Unfiltered Spectral | 154.45 | 3.15 | **-22.69** | 1.11 |
| 10-cluster PMFG Filtered Average Linkage | 160.95 | **3.32** | -26.77 | 1.11 |
| 30-cluster AG Unfiltered Sub2Vec | 160.96 | 3.24 | -23.05 | 1.10 |
| 5-cluster MST Unfiltered Sub2Vec | 163.26 | 3.27 | -27.39 | 1.11 |
| **20-cluster PMFG Filtered Node2Vec** | **164.68** | 3.30 | -27.06 | 1.11 |

Compared to the original optimization result, we observe a 12.23% improvement in excess return and 12.16% improvement in excess Sharpe ratio.

Since factors based on price and volume lost their predictive power staring from 20200701, we also look at the performances before that time.

**Time Period**: 20170701 - 20200701

| Model | AlphaReturn (cumsum) | AlphaSharpe | AlphaDrawdown | Turnover |
| ----- | :---------------------: | :----------: | :-----------: | :------: |
| LgbmRegressor | 150.64 | 6.06 | **-4.59** | 1.23 |
| LgbmRegressor-opt | **170.31** | 5.43 | -6.76 | 1.12 |
| .. | .. | .. | .. | .. | .. |
| 10-cluster PMFG Filtered Sub2Vec | 173.10 | 5.49 | **-5.51** | 1.12 |
| 5-cluster MST Filtered Sub2Vec | 182.89 | 5.78 | -7.14 | 1.12 |
| 10-cluster AG Filtered Sub2Vec | 181.50 | 5.64 | -7.40 | 1.12 |
| **20-cluster PMFG Filtered Node2Vec** | **184.21** | **5.85** | -6.42 | 1.12 |

In this period, we observe a 8.16% improvement in excess return and a 7.73 improvement in excess Sharpe ratio, compared to the original optimization result.

For a complete list of results, check out [summary_20170701_20211231.csv](out/res/signal_test_file_20220305_long_experiment/summary.csv) and [summary_20170701_20200701.csv](out/res/signal_test_file_20220305_short_experiment/summary.csv). And more details are discussed on the project website listed above.

## Environment

To run codes in this project, it is recommended to create an environment listed in the [environment.yml](environment.yml). If conda is installed, run:

```bash
conda env create -f environment.yml
conda activate finance-base
```

Alternatively, one could also pull the corresponding docker image from [yangshengaa/finance-base](https://hub.docker.com/repository/docker/yangshengaa/finance-base) and then activate the finance-base environment using the latter conda command.

## Quick Start

It's very easy to use this platform!

Tips:

- run each module at a time, and run the following command sequentially;
- change config for corresponding module in respective files (file location indicated inside [run.py](run.py));
- detailed running instructions, including a walkthrough of parameters in each modules, are in README of each module.

To run each module, in current directory:

Factor Generation:

- factor generation: `python run.py gen`

Backtest:

- backtest factor: `python run.py backtest_factor`
- backtest signal: `python run.py backtest_signal`

Factor Combination:

- factor combination: `python run.py comb`

Portfolio Optimization:

- generate factor returns: `python run.py opt_fac_ret`
- estimate covariance matrices: `python run.py opt_cov_est`
- adjust weight: `python run.py opt_weight`

Graph Clustering:

- train graph clustering: `python run.py cluster_train`

To run each submodules, in current directory:

- generate pairs factors: `python run.py pairs`
- generate risk factors: `python run.py gen_risk`

Currently risk attribution module is very slow and suboptimal. To be addressed later.

## Acknowledgement

Special thanks to coworkers and my best friends at [Shanghai Probability Quantitative Investment](http://www.probquant.cn/): Beilei Xu, Zhongyuan Wang, Zhenghang Xie, Cong Chen, Yihao Zhou, Weilin Chen, Yuhan Tao, Wan Zheng, and many others. This project would be impossible without their data, insights, and experiences.

## For Developer

Log known issues here:

- signals given by factor test could not give the same alpha returns (slightly less) as in signal test
  - examine output holding stats
- plain risk attribution
"
121,https://github.com/MarthaY01/hdsi_faculty_tool/tree/main,"{'srpatel2000': 'https://github.com/srpatel2000', 'xd00099': 'https://github.com/xd00099', 'MarthaY01': 'https://github.com/MarthaY01', 'brianrqian': 'https://github.com/brianrqian'}","{'Jupyter Notebook': 0.89, 'Python': 0.11, 'HTML': 0.0}","# HDSI Faculty Exploration Tool

This repository contains project code for experimenting with LDA for Faculty Information Retrieval System.

This tool is now deployed @ https://hdsi-faulty-tool.herokuapp.com/

## Running the Project
* All of the below lines should be run within a terminal:

* Before running any of the below commands, launch the docker image by running `launch.sh -i duxiang/dsc180a:latest`

* To get the preprocessed data file, run `python run.py process_data`
* To get the fitted sklearn.lda model, run `python run.py model`
* To prepare/update the dashboard, run `python run.py prepare_sankey`
* To run the live dashboard, run `python run.py run_dashboard`

## Using the Dashboard
* When executing `run_dashboard`, it will launch dash with a locally hosted port.
* It would require port-forwarding on a remote server.

# Web Link
Website: https://marthay01.github.io/hdsi_faculty_tool/
","# HDSI Faculty Exploration Tool

This repository contains project code for experimenting with LDA for Faculty Information Retrieval System.

This tool is now deployed @ https://hdsi-faulty-tool.herokuapp.com/

## Running the Project
* All of the below lines should be run within a terminal:

* Before running any of the below commands, launch the docker image by running `launch.sh -i duxiang/dsc180a:latest`

* To get the preprocessed data file, run `python run.py process_data`
* To get the fitted sklearn.lda model, run `python run.py model`
* To prepare/update the dashboard, run `python run.py prepare_sankey`
* To run the live dashboard, run `python run.py run_dashboard`

## Using the Dashboard
* When executing `run_dashboard`, it will launch dash with a locally hosted port.
* It would require port-forwarding on a remote server.

# Web Link
Website: https://marthay01.github.io/hdsi_faculty_tool/
"
122,https://github.com/MarthaY01/hdsi_faculty_tool/tree/main,"{'srpatel2000': 'https://github.com/srpatel2000', 'xd00099': 'https://github.com/xd00099', 'MarthaY01': 'https://github.com/MarthaY01', 'brianrqian': 'https://github.com/brianrqian'}","{'Jupyter Notebook': 0.89, 'Python': 0.11, 'HTML': 0.0}","# HDSI Faculty Exploration Tool

This repository contains project code for experimenting with LDA for Faculty Information Retrieval System.

This tool is now deployed @ https://hdsi-faulty-tool.herokuapp.com/

## Running the Project
* All of the below lines should be run within a terminal:

* Before running any of the below commands, launch the docker image by running `launch.sh -i duxiang/dsc180a:latest`

* To get the preprocessed data file, run `python run.py process_data`
* To get the fitted sklearn.lda model, run `python run.py model`
* To prepare/update the dashboard, run `python run.py prepare_sankey`
* To run the live dashboard, run `python run.py run_dashboard`

## Using the Dashboard
* When executing `run_dashboard`, it will launch dash with a locally hosted port.
* It would require port-forwarding on a remote server.

# Web Link
Website: https://marthay01.github.io/hdsi_faculty_tool/
","# HDSI Faculty Exploration Tool

This repository contains project code for experimenting with LDA for Faculty Information Retrieval System.

This tool is now deployed @ https://hdsi-faulty-tool.herokuapp.com/

## Running the Project
* All of the below lines should be run within a terminal:

* Before running any of the below commands, launch the docker image by running `launch.sh -i duxiang/dsc180a:latest`

* To get the preprocessed data file, run `python run.py process_data`
* To get the fitted sklearn.lda model, run `python run.py model`
* To prepare/update the dashboard, run `python run.py prepare_sankey`
* To run the live dashboard, run `python run.py run_dashboard`

## Using the Dashboard
* When executing `run_dashboard`, it will launch dash with a locally hosted port.
* It would require port-forwarding on a remote server.

# Web Link
Website: https://marthay01.github.io/hdsi_faculty_tool/
"
123,https://github.com/amuamushu/adv_avod_ssn,"{'amuamushu': 'https://github.com/amuamushu', 'ayushmore': 'https://github.com/ayushmore'}","{'Python': 0.7, 'Jupyter Notebook': 0.2, 'C++': 0.1, 'Shell': 0.01, 'Dockerfile': 0.0, 'Makefile': 0.0}","# AVOD for Single Source Robustness Against Adversarial Attacks.
This project is worked on by Amy Nguyen ([@amuamushu](https://github.com/amuamushu)) and Ayush More ([@ayushmore](https://github.com/ayushmore)) over the course of 20 weeks under the mentorship of Lily Weng. 

Visual Presentation: https://ayushmore.github.io/2022-03-07-improving-robustness-via-adversarial-training/

Oral Presentation Slides: https://docs.google.com/presentation/d/1DSoFa1vUQBcjghLfds6ce4sIG0qkBvtL1c2ixlEs7qc/edit?usp=sharing

## Setup
### Container Setup
To mimic our environment, please build a docker container using the image `amytn/avod-adv:latest`. To prevent memory errors, please ensure your container has **at least 16 GB of RAM** available. For faster runtime, including a GPU may be useful.

### Cloning the repository
Since this reposity contains a submodule, cloning would require an additional commandline argument. 

Make sure to clone the reposity in your home directory so the Python paths in the Docker image match up. If the repository is cloned elsewhere, please set up the python path yourself (see [Setting up Necessary Python Paths](#setting-up-necessary-python-paths)).

```
git clone --recurse-submodules https://github.com/amuamushu/adv_avod_ssn.git
```

### The dataset
The dataset we will be using is the [KITTI dataset](http://www.cvlibs.net/datasets/kitti/). For the dataset and mini-batch setup, please follow the download steps listed in the [AVOD repository](https://github.com/kujason/avod#dataset).

For the dataset, we will be follow a slightly different setup to the one on the AVOD repository.

In your home directory, the layout should look like this:

```
home
..\avod_data
....\Kitti
......\object
........\testing
........\training
..........\calib # camera calibration (can ignore)
..........\image_2 # the 2D images
..........\label_2 # true labels and bounding boxes
..........\planes 
..........\velodyne # the lidar point clouds
....... train.txt # list of sample names to use for training
....... val.txt # list of sample names to use for validation
..\adv_avod_ssn # this repository!
  
```
More information about the true labels can be found here: https://github.com/kujason/avod/wiki/Data-Formats

## Run
```
python3 run.py [clean] [test] [clean-model] [adv-model] [ssn-model]
```

These targets can be run one-by-one in the order provided.

### `clean` target: 
Deletes all files in the `outputs` folder.

### `test` target: 
Runs training and inference on test data found under `test/testdata` and writes the predictions and AP scores to the `outputs/<checkpoint_name>` directory. For this target, the checkpoint name is `test_data`.
- Note: If you plan on only using the test data and not downloading the full dataset, in `scripts/offline_eval/kitti_native/eval/run_eval.sh`, please update `$prev/avod_data/Kitti/object/training/label_2/` to be `$repo/test/testdata/Kitti/object/training/label_2/`. `run_eval.sh` is used for computing the AP Scores after running inference. 

### `clean-model` target:
Runs training for a clean model and adversarial inference on the full dataset found at `home/avod_data` and writes the predictions and AP scores to the `outputs/<checkpoint_name>` directory. For this target, the checkpoint name is `pyramid_cars_with_aug_simple`.

### `adv-model` target:
Runs training for an adversarial model and all three types of inference (clean, adversarial, SSN) on the full dataset found at `home/avod_data` and writes the predictions and AP scores to the `outputs/<checkpoint_name>` directory. For this target, the checkpoint name is `test_adv`.
- **NOTE**: Since the adversarial model only fine-tunes the clean model, running this target requires the clean model to be completely trained.

### `ssn-model` target:
Runs training for a clean model and adversarial inference on the full dataset found at `home/avod_data` and writes the predictions and AP scores to the `outputs/<checkpoint_name>` directory. For this target, the checkpoint name is `trainsin_pyramid_cars_with_aug_simple_rand_5`.
- **NOTE**: Since the single-source-noise model only fine-tunes the clean model, running this target requires the clean model to be completely trained.

Note: `pyramid_cars_with_aug_simple` and `trainsin_pyramid_cars_with_aug_simple_rand_5` are the same checkpoint names that our previous work author Taewan Kim had so we kept it for ease of comparison.

## The Shell Script
Each shell script is responsible for running the entire experiment for one model. Details about the specific experiments are covered in the paper.

**Arguments for training, inference, and evaluation:**

`pipeline_config`: Specifies the path to the experiment configurations (batch size, number of steps, learning rate, number of iterations, dataset path, etc).

`data_split`: Can be `train`, `val`, or `test`. Whichever keyword is specified determines which samples are used.

`output_dir`: Path to where the results and AP scores are written to.

`ckpt_indices`: This value specifies which model checkpoint to use for inference. 
  - Every couple of steps during training, the current trained model is saved to a checkpoint. 

#### Configuration Files: 


## Experimental Configurations
Each shell script references multiple `.config` files: one for training the model and one or more for inference. These are found under `./avod/configs`.

### `model_config`:
Contains configurations for the object detection model. Notable configurations are:
- `model_name`: Should be `avod_model` for all the experiments we run.
- `checkpoint_name`: The name of the experiment checkpoint. This determines what folder the outputs are saved to.
- `is_adversarial`: A boolean value defaulted to False. Train the model adversarially and runs adversarial inference if True, otherwise train the model normally.
- `adv_epsilon`: The epsilon to use for our perturbation. Only used if the model is being trained or running inference adversarially.

### `train_config`: 
- `pretrained_ckpt`: What checkpoint to continue training from. This would be used and should be updated as the model is being fine-tuned.

### `eval_config`:
- `dataset_dir`: The dataset used for training and infernece. 
- `pretrained_chkpt`: What trained model checkpoint to use for inference. This is useful if the inference `.config` file is different from the training `.config` file. 


### Viewing Results
#### AP Scores: 
The AP scores can be found under `outputs/<checkpoint_name>/offline_eval/results` where the 3 values provided for each test corresponds to easy, medium, and hard respectively.

**Here is an exmaple of what the AP Scores look like**:
```
car_detection AP: 89.973267 87.620293 80.301704
car_detection_BEV AP: 89.292168 86.383148 79.538277
car_heading_BEV AP: 89.177887 85.891479 78.938744
car_detection_3D AP: 77.332970 67.945251 66.929985
car_heading_3D AP: 77.288345 67.749474 66.543098
```

After inference, if the AP scores are not saved properly, they can be manually calculated and saved again using this command:
```
bash scripts/offline_eval/kitti_native_eval/run_eval.sh ./outputs/<checkpoint_name>/<prediction_type>/kitti_native_eval/ 0.1_val <training_step> <checkpoint_name>
```
- `checkpoint_name`: name of the checkpoint as specified in the `.config` file
- `prediction_type`: would be ...
   - `prediction` if inference results on clean data is wanted
   - `prediction_adv` if inference results on adversarial data is wanted
   - `predictions_sin_rand_5.0_5` if inference results on SSN data is wanted


#### Visualization with bounding boxes, IoU scores, and confidence level: 
Run the below code to generate bounding boxes on top of the images used during inference. Images will be saved to `outputs/<checkpoint_name>/predictions/images_2d`

```
python3 demos/show_predictions_2d.py <checkpoint_name>
```

**Here is an example of a generated image**:
![000152](https://user-images.githubusercontent.com/35519361/152208158-833ca90f-911a-4ab5-a846-e167cfc2e1a3.png)



## References
This project builds off of Kim, Taewan and Ghosh, Joydeep's work on ""Single Source Robustness in Deep Fusion Models."" In their GitHub repository (https://github.com/twankim/avod_ssn), they incorporate single source noise into the inputs of the AVOD 3D object detection model. We expand on their work and code by incorporating adversarial noise, rather than Gaussian noise, into the input images.
```
@inproceedings{kim2019single,
  title={On Single Source Robustness in Deep Fusion Models},
  author={Kim, Taewan and Ghosh, Joydeep},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4815--4826},
  year={2019}
}
```

We also relied on the documentation of the original AVOD code (https://github.com/kujason/avod) for setting up the model and data as well as understanding our results.
```
@article{ku2018joint, 
  title={Joint 3D Proposal Generation and Object Detection from View Aggregation}, 
  author={Ku, Jason and Mozifian, Melissa and Lee, Jungwook and Harakeh, Ali and Waslander, Steven}, 
  journal={IROS}, 
  year={2018}
}
```

We referred to the KITTI dataset website (http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d) and its related papers to better understand our dataset.
```
A. Geiger, P. Lenz and R. Urtasun, ""Are we ready for autonomous driving? The KITTI vision benchmark suite,"" 
2012 IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 3354-3361, doi: 10.1109/CVPR.2012.6248074. 
http://www.cvlibs.net/publications/Geiger2012CVPR.pdf
```


## Appendix
### Setting Up Necessary Python Paths
Run these two commands to set the Python paths for `avod_ssn` and `wavedata`:
```
export PYTHONPATH=$PYTHONPATH:'<path to avod>'
export PYTHONPATH=$PYTHONPATH:'<path to wavedata>'
```

### Pretrained Models:
Since training takes many hours, we included our pretrained clean model here:
https://drive.google.com/drive/folders/18U7t-4gU4sXvAD33GEuVnwSwUSItHdPX?usp=sharing
","# AVOD for Single Source Robustness Against Adversarial Attacks.
This project is worked on by Amy Nguyen ([@amuamushu](https://github.com/amuamushu)) and Ayush More ([@ayushmore](https://github.com/ayushmore)) over the course of 20 weeks under the mentorship of Lily Weng. 

Visual Presentation: https://ayushmore.github.io/2022-03-07-improving-robustness-via-adversarial-training/

Oral Presentation Slides: https://docs.google.com/presentation/d/1DSoFa1vUQBcjghLfds6ce4sIG0qkBvtL1c2ixlEs7qc/edit?usp=sharing

## Setup
### Container Setup
To mimic our environment, please build a docker container using the image `amytn/avod-adv:latest`. To prevent memory errors, please ensure your container has **at least 16 GB of RAM** available. For faster runtime, including a GPU may be useful.

### Cloning the repository
Since this reposity contains a submodule, cloning would require an additional commandline argument. 

Make sure to clone the reposity in your home directory so the Python paths in the Docker image match up. If the repository is cloned elsewhere, please set up the python path yourself (see [Setting up Necessary Python Paths](#setting-up-necessary-python-paths)).

```
git clone --recurse-submodules https://github.com/amuamushu/adv_avod_ssn.git
```

### The dataset
The dataset we will be using is the [KITTI dataset](http://www.cvlibs.net/datasets/kitti/). For the dataset and mini-batch setup, please follow the download steps listed in the [AVOD repository](https://github.com/kujason/avod#dataset).

For the dataset, we will be follow a slightly different setup to the one on the AVOD repository.

In your home directory, the layout should look like this:

```
home
..\avod_data
....\Kitti
......\object
........\testing
........\training
..........\calib # camera calibration (can ignore)
..........\image_2 # the 2D images
..........\label_2 # true labels and bounding boxes
..........\planes 
..........\velodyne # the lidar point clouds
....... train.txt # list of sample names to use for training
....... val.txt # list of sample names to use for validation
..\adv_avod_ssn # this repository!
  
```
More information about the true labels can be found here: https://github.com/kujason/avod/wiki/Data-Formats

## Run
```
python3 run.py [clean] [test] [clean-model] [adv-model] [ssn-model]
```

These targets can be run one-by-one in the order provided.

### `clean` target: 
Deletes all files in the `outputs` folder.

### `test` target: 
Runs training and inference on test data found under `test/testdata` and writes the predictions and AP scores to the `outputs/<checkpoint_name>` directory. For this target, the checkpoint name is `test_data`.
- Note: If you plan on only using the test data and not downloading the full dataset, in `scripts/offline_eval/kitti_native/eval/run_eval.sh`, please update `$prev/avod_data/Kitti/object/training/label_2/` to be `$repo/test/testdata/Kitti/object/training/label_2/`. `run_eval.sh` is used for computing the AP Scores after running inference. 

### `clean-model` target:
Runs training for a clean model and adversarial inference on the full dataset found at `home/avod_data` and writes the predictions and AP scores to the `outputs/<checkpoint_name>` directory. For this target, the checkpoint name is `pyramid_cars_with_aug_simple`.

### `adv-model` target:
Runs training for an adversarial model and all three types of inference (clean, adversarial, SSN) on the full dataset found at `home/avod_data` and writes the predictions and AP scores to the `outputs/<checkpoint_name>` directory. For this target, the checkpoint name is `test_adv`.
- **NOTE**: Since the adversarial model only fine-tunes the clean model, running this target requires the clean model to be completely trained.

### `ssn-model` target:
Runs training for a clean model and adversarial inference on the full dataset found at `home/avod_data` and writes the predictions and AP scores to the `outputs/<checkpoint_name>` directory. For this target, the checkpoint name is `trainsin_pyramid_cars_with_aug_simple_rand_5`.
- **NOTE**: Since the single-source-noise model only fine-tunes the clean model, running this target requires the clean model to be completely trained.

Note: `pyramid_cars_with_aug_simple` and `trainsin_pyramid_cars_with_aug_simple_rand_5` are the same checkpoint names that our previous work author Taewan Kim had so we kept it for ease of comparison.

## The Shell Script
Each shell script is responsible for running the entire experiment for one model. Details about the specific experiments are covered in the paper.

**Arguments for training, inference, and evaluation:**

`pipeline_config`: Specifies the path to the experiment configurations (batch size, number of steps, learning rate, number of iterations, dataset path, etc).

`data_split`: Can be `train`, `val`, or `test`. Whichever keyword is specified determines which samples are used.

`output_dir`: Path to where the results and AP scores are written to.

`ckpt_indices`: This value specifies which model checkpoint to use for inference. 
  - Every couple of steps during training, the current trained model is saved to a checkpoint. 

#### Configuration Files: 


## Experimental Configurations
Each shell script references multiple `.config` files: one for training the model and one or more for inference. These are found under `./avod/configs`.

### `model_config`:
Contains configurations for the object detection model. Notable configurations are:
- `model_name`: Should be `avod_model` for all the experiments we run.
- `checkpoint_name`: The name of the experiment checkpoint. This determines what folder the outputs are saved to.
- `is_adversarial`: A boolean value defaulted to False. Train the model adversarially and runs adversarial inference if True, otherwise train the model normally.
- `adv_epsilon`: The epsilon to use for our perturbation. Only used if the model is being trained or running inference adversarially.

### `train_config`: 
- `pretrained_ckpt`: What checkpoint to continue training from. This would be used and should be updated as the model is being fine-tuned.

### `eval_config`:
- `dataset_dir`: The dataset used for training and infernece. 
- `pretrained_chkpt`: What trained model checkpoint to use for inference. This is useful if the inference `.config` file is different from the training `.config` file. 


### Viewing Results
#### AP Scores: 
The AP scores can be found under `outputs/<checkpoint_name>/offline_eval/results` where the 3 values provided for each test corresponds to easy, medium, and hard respectively.

**Here is an exmaple of what the AP Scores look like**:
```
car_detection AP: 89.973267 87.620293 80.301704
car_detection_BEV AP: 89.292168 86.383148 79.538277
car_heading_BEV AP: 89.177887 85.891479 78.938744
car_detection_3D AP: 77.332970 67.945251 66.929985
car_heading_3D AP: 77.288345 67.749474 66.543098
```

After inference, if the AP scores are not saved properly, they can be manually calculated and saved again using this command:
```
bash scripts/offline_eval/kitti_native_eval/run_eval.sh ./outputs/<checkpoint_name>/<prediction_type>/kitti_native_eval/ 0.1_val <training_step> <checkpoint_name>
```
- `checkpoint_name`: name of the checkpoint as specified in the `.config` file
- `prediction_type`: would be ...
   - `prediction` if inference results on clean data is wanted
   - `prediction_adv` if inference results on adversarial data is wanted
   - `predictions_sin_rand_5.0_5` if inference results on SSN data is wanted


#### Visualization with bounding boxes, IoU scores, and confidence level: 
Run the below code to generate bounding boxes on top of the images used during inference. Images will be saved to `outputs/<checkpoint_name>/predictions/images_2d`

```
python3 demos/show_predictions_2d.py <checkpoint_name>
```

**Here is an example of a generated image**:
![000152](https://user-images.githubusercontent.com/35519361/152208158-833ca90f-911a-4ab5-a846-e167cfc2e1a3.png)



## References
This project builds off of Kim, Taewan and Ghosh, Joydeep's work on ""Single Source Robustness in Deep Fusion Models."" In their GitHub repository (https://github.com/twankim/avod_ssn), they incorporate single source noise into the inputs of the AVOD 3D object detection model. We expand on their work and code by incorporating adversarial noise, rather than Gaussian noise, into the input images.
```
@inproceedings{kim2019single,
  title={On Single Source Robustness in Deep Fusion Models},
  author={Kim, Taewan and Ghosh, Joydeep},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4815--4826},
  year={2019}
}
```

We also relied on the documentation of the original AVOD code (https://github.com/kujason/avod) for setting up the model and data as well as understanding our results.
```
@article{ku2018joint, 
  title={Joint 3D Proposal Generation and Object Detection from View Aggregation}, 
  author={Ku, Jason and Mozifian, Melissa and Lee, Jungwook and Harakeh, Ali and Waslander, Steven}, 
  journal={IROS}, 
  year={2018}
}
```

We referred to the KITTI dataset website (http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d) and its related papers to better understand our dataset.
```
A. Geiger, P. Lenz and R. Urtasun, ""Are we ready for autonomous driving? The KITTI vision benchmark suite,"" 
2012 IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 3354-3361, doi: 10.1109/CVPR.2012.6248074. 
http://www.cvlibs.net/publications/Geiger2012CVPR.pdf
```


## Appendix
### Setting Up Necessary Python Paths
Run these two commands to set the Python paths for `avod_ssn` and `wavedata`:
```
export PYTHONPATH=$PYTHONPATH:'<path to avod>'
export PYTHONPATH=$PYTHONPATH:'<path to wavedata>'
```

### Pretrained Models:
Since training takes many hours, we included our pretrained clean model here:
https://drive.google.com/drive/folders/18U7t-4gU4sXvAD33GEuVnwSwUSItHdPX?usp=sharing
"
124,https://github.com/Maderlime/DSC180_Q1_Code,"{'Maderlime': 'https://github.com/Maderlime', 'rakeshsenthil': 'https://github.com/rakeshsenthil'}","{'Jupyter Notebook': 0.98, 'Python': 0.01, 'Dockerfile': 0.0}","# STEPS TO RUN CODE ON ADVERSARIAL ROBUST TRAINING (QUARTER 2 - DSC 180B)

## How to SSH into the DSMLP server
ssh [user]@dsmlp-login.ucsd.edu

## How to build the Docker file
docker build -t test .
docker run -it --rm test /bin/bash
  
## Deploy a pod with GPU support
launch-scipy-ml-gpu.sh

# LOADING IN DATA

Load in the data from the following source: https://www.dropbox.com/sh/tg6xij9hhfzgio9/AADqu6BMq3Rko7U7-q6vwmMFa?dl=0

We will use the following files for each dataset:
- val_test_x_preprocess.npy
- val_test_y.npy

Make a folder in for the test and train data for each dataset. Within each of these folders, create two subfolders titled as '0' and '1'. 

From here, go to the file at DSC180_Q1_Code/patch_attacks/data/cxr/make_fast_adversarial_documents.ipynb and run the code in these cells. This will load in the images as Numpy files and partition them into training and test sets. Set the output writing dirctories to the folders you created above. Use a 70/30 split in the ranges in the code based on the size of the dataset. 

# MAKING ADJUSTMENTS TO THE CODE

You can edit hyperparameters for the FGSM training model in the train_fgsm.py file in src/test. You can edit attack parameters in the evaluate_pgd method on the utils.py file in src/model. You can select the datasets you want to load in from the ones you created above. 


# Command line prompt to run the code
python run.py test

# STEPS TO RUN CODE ON ADVERSARIAL ATTACKS (QUARTER 1 - DSC 180A)

# Build Overview
### Command line prompt to run the code
#### python run.py test
### Run the larger test/analysis code with
#### python run.py analysis


## How to SSH into the DSMLP server
ssh <user>@dsmlp-login.ucsd.edu

## How to build the Docker file
docker build -t test .
docker run -it --rm test /bin/bash

<!-- docker run -it --rm mjtjoa/dsc180a_quarter1_code bash -->


docker tag test mjtjoa/dsc180a_quarter1_code
docker push mjtjoa/dsc180a_quarter1_code:latest

# Cleaning docker
docker system prune
docker system prune -a
sudo rm -rf /var/lib/docker

### Launching the Docker File  
launch-scipy-ml-gpu.sh -i mjtjoa/dsc180a_quarter1_code:latest
  (if this doesn't work, add -P Always)
### Relevant Links
  Project Report: https://docs.google.com/document/d/1iQ0lZ_wpxqQXRwwjwKANrwNn6FRDwvasISHf9vpNIHw/edit?usp=sharing
  
  Project Proposal for Q2: https://docs.google.com/document/d/1d4Z4yS0aSyCMxht0NaaEf_mMxH5KFPg2B8b3rKXyRwk/edit?usp=sharing
  
  Source Report: https://arxiv.org/pdf/1804.05296.pdf
  
  Source Repository: https://github.com/sgfin/adversarial-medicine
","# STEPS TO RUN CODE ON ADVERSARIAL ROBUST TRAINING (QUARTER 2 - DSC 180B)

## How to SSH into the DSMLP server
ssh [user]@dsmlp-login.ucsd.edu

## How to build the Docker file
docker build -t test .
docker run -it --rm test /bin/bash
  
## Deploy a pod with GPU support
launch-scipy-ml-gpu.sh

# LOADING IN DATA

Load in the data from the following source: https://www.dropbox.com/sh/tg6xij9hhfzgio9/AADqu6BMq3Rko7U7-q6vwmMFa?dl=0

We will use the following files for each dataset:
- val_test_x_preprocess.npy
- val_test_y.npy

Make a folder in for the test and train data for each dataset. Within each of these folders, create two subfolders titled as '0' and '1'. 

From here, go to the file at DSC180_Q1_Code/patch_attacks/data/cxr/make_fast_adversarial_documents.ipynb and run the code in these cells. This will load in the images as Numpy files and partition them into training and test sets. Set the output writing dirctories to the folders you created above. Use a 70/30 split in the ranges in the code based on the size of the dataset. 

# MAKING ADJUSTMENTS TO THE CODE

You can edit hyperparameters for the FGSM training model in the train_fgsm.py file in src/test. You can edit attack parameters in the evaluate_pgd method on the utils.py file in src/model. You can select the datasets you want to load in from the ones you created above. 


# Command line prompt to run the code
python run.py test

# STEPS TO RUN CODE ON ADVERSARIAL ATTACKS (QUARTER 1 - DSC 180A)

# Build Overview
### Command line prompt to run the code
#### python run.py test
### Run the larger test/analysis code with
#### python run.py analysis


## How to SSH into the DSMLP server
ssh <user>@dsmlp-login.ucsd.edu

## How to build the Docker file
docker build -t test .
docker run -it --rm test /bin/bash

<!-- docker run -it --rm mjtjoa/dsc180a_quarter1_code bash -->


docker tag test mjtjoa/dsc180a_quarter1_code
docker push mjtjoa/dsc180a_quarter1_code:latest

# Cleaning docker
docker system prune
docker system prune -a
sudo rm -rf /var/lib/docker

### Launching the Docker File  
launch-scipy-ml-gpu.sh -i mjtjoa/dsc180a_quarter1_code:latest
  (if this doesn't work, add -P Always)
### Relevant Links
  Project Report: https://docs.google.com/document/d/1iQ0lZ_wpxqQXRwwjwKANrwNn6FRDwvasISHf9vpNIHw/edit?usp=sharing
  
  Project Proposal for Q2: https://docs.google.com/document/d/1d4Z4yS0aSyCMxht0NaaEf_mMxH5KFPg2B8b3rKXyRwk/edit?usp=sharing
  
  Source Report: https://arxiv.org/pdf/1804.05296.pdf
  
  Source Repository: https://github.com/sgfin/adversarial-medicine
"
125,https://github.com/Actionable-Recourse/recourse-api,{'yabutaka': 'https://github.com/yabutaka'},"{'Python': 0.93, 'Dockerfile': 0.07, 'Procfile': 0.01}","# Recourse API

https://recourse-api.herokuapp.com/recourse

From the link above, you can check a list of actions a person can take to be accepted by a Credit Scoring algorithm.

## How to set up the environment

```pip install virtualenv```
```virtualenv venv``` to create your new environment
```. venv/bin/activate```
```pip install -r requirements.txt``` to install all the required packages

## How to run

Activate the python environment.
```
. venv/bin/activate
```

```
python app.py
```

**To run this in WSL, you might need to run ```wsl --shutdown``` in Powershell. Otherwise, localhost will not be accessible.**","# Recourse API

https://recourse-api.herokuapp.com/recourse

From the link above, you can check a list of actions a person can take to be accepted by a Credit Scoring algorithm.

## How to set up the environment

```pip install virtualenv```
```virtualenv venv``` to create your new environment
```. venv/bin/activate```
```pip install -r requirements.txt``` to install all the required packages

## How to run

Activate the python environment.
```
. venv/bin/activate
```

```
python app.py
```

**To run this in WSL, you might need to run ```wsl --shutdown``` in Powershell. Otherwise, localhost will not be accessible.**"
126,https://github.com/freebreadstix/capstone_B02,"{'freebreadstix': 'https://github.com/freebreadstix', 'micmiccitymax': 'https://github.com/micmiccitymax'}","{'Jupyter Notebook': 1.0, 'Python': 0.0, 'Makefile': 0.0, 'Dockerfile': 0.0}","# Group B02 Capstone project repo

To run:


If running from DSMLP cluster:
```
ssh user@dsmlp-login.ucsd.edu
launch-scipy-ml.sh -i freebreadstix/q1-replication
```
Else be sure to run in container: https://hub.docker.com/repository/docker/freebreadstix/q1-replication

Then:
```
git clone https://github.com/freebreadstix/capstone_B02.git
cd capstone_B02
```
If not merged to main, make sure to switch to branch with run.py
```
git checkout lucas-runpy
```

Configure config yaml with appropriate parameters. You can make your own .yml using config.yml as reference, just pass it as the argument on CLI

Run run.py w/ config yaml corresponding to configuration you are running. For testing this is test_config.yml
```
python3 run.py test_config.yml
```
Link to Presentation Website
```
https://micmiccitymax.github.io/dsc180b02-site/
```

Explainations of Config.yml output options
```
num_words: how many words are in the ""important words"" for the models
save_predictions: saves output of predictions to a file
print_results: prints results of evaluations to terminal
print words: Prints important words of each model in terminal
intersections: computes the important words similarity of all combinations of model and topics, USE ONLY WHEN YOU HAVE ALL MODELS MADE
decision_tree_model: outputs a plotting of decision tree to a figure
wordcloud: outputs an important word wordcloud to a figure in the figures folder
```

**Note**: if you are using intersections, decision_tree_model, or wordcloud options, make sure data is saved as 'data/processed/general.csv' and has columns 'Original Article Text' as the document text, 'Verdict' as 'TRUE' or 'FALSE', 'Category' as category, or change code within old_utils.py

Project Organization
------------

    ├── LICENSE
    ├── Makefile           <- Makefile with commands like `make data` or `make train`
    ├── README.md          <- The top-level README for developers using this project.
    ├── data
    │   ├── external       <- Data from third party sources.
    │   ├── interim        <- Intermediate data that has been transformed.
    │   ├── processed      <- The final, canonical data sets for modeling.
    │   └── raw            <- The original, immutable data dump.
    │
    ├── docs               <- A default Sphinx project; see sphinx-doc.org for details
    │
    ├── models             <- Trained and serialized models, model predictions, or model summaries
    │
    ├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    │                         the creator's initials, and a short `-` delimited description, e.g.
    │                         `1.0-jqp-initial-data-exploration`.
    │
    ├── references         <- Data dictionaries, manuals, and all other explanatory materials.
    │
    ├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    │   └── figures        <- Generated graphics and figures to be used in reporting
    │
    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    │                         generated with `pip freeze > requirements.txt`
    │
    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ├── src                <- Source code for use in this project.
    │   ├── __init__.py    <- Makes src a Python module
    │   │
    │   ├── data           <- Scripts to download or generate data
    │   │   └── make_dataset.py
    │   │
    │   ├── features       <- Scripts to turn raw data into features for modeling
    │   │   └── build_features.py
    │   │
    │   ├── models         <- Scripts to train models and then use trained models to make
    │   │   │                 predictions
    │   │   ├── predict_model.py
    │   │   └── train_model.py
    │   │
    │   └── visualization  <- Scripts to create exploratory and results oriented visualizations
    │       └── visualize.py
    │
    └── tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io


--------

<p><small>Project based on the <a target=""_blank"" href=""https://drivendata.github.io/cookiecutter-data-science/"">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
","# Group B02 Capstone project repo

To run:


If running from DSMLP cluster:
```
ssh user@dsmlp-login.ucsd.edu
launch-scipy-ml.sh -i freebreadstix/q1-replication
```
Else be sure to run in container: https://hub.docker.com/repository/docker/freebreadstix/q1-replication

Then:
```
git clone https://github.com/freebreadstix/capstone_B02.git
cd capstone_B02
```
If not merged to main, make sure to switch to branch with run.py
```
git checkout lucas-runpy
```

Configure config yaml with appropriate parameters. You can make your own .yml using config.yml as reference, just pass it as the argument on CLI

Run run.py w/ config yaml corresponding to configuration you are running. For testing this is test_config.yml
```
python3 run.py test_config.yml
```
Link to Presentation Website
```
https://micmiccitymax.github.io/dsc180b02-site/
```

Explainations of Config.yml output options
```
num_words: how many words are in the ""important words"" for the models
save_predictions: saves output of predictions to a file
print_results: prints results of evaluations to terminal
print words: Prints important words of each model in terminal
intersections: computes the important words similarity of all combinations of model and topics, USE ONLY WHEN YOU HAVE ALL MODELS MADE
decision_tree_model: outputs a plotting of decision tree to a figure
wordcloud: outputs an important word wordcloud to a figure in the figures folder
```

**Note**: if you are using intersections, decision_tree_model, or wordcloud options, make sure data is saved as 'data/processed/general.csv' and has columns 'Original Article Text' as the document text, 'Verdict' as 'TRUE' or 'FALSE', 'Category' as category, or change code within old_utils.py

Project Organization
------------

    ├── LICENSE
    ├── Makefile           <- Makefile with commands like `make data` or `make train`
    ├── README.md          <- The top-level README for developers using this project.
    ├── data
    │   ├── external       <- Data from third party sources.
    │   ├── interim        <- Intermediate data that has been transformed.
    │   ├── processed      <- The final, canonical data sets for modeling.
    │   └── raw            <- The original, immutable data dump.
    │
    ├── docs               <- A default Sphinx project; see sphinx-doc.org for details
    │
    ├── models             <- Trained and serialized models, model predictions, or model summaries
    │
    ├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    │                         the creator's initials, and a short `-` delimited description, e.g.
    │                         `1.0-jqp-initial-data-exploration`.
    │
    ├── references         <- Data dictionaries, manuals, and all other explanatory materials.
    │
    ├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    │   └── figures        <- Generated graphics and figures to be used in reporting
    │
    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    │                         generated with `pip freeze > requirements.txt`
    │
    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ├── src                <- Source code for use in this project.
    │   ├── __init__.py    <- Makes src a Python module
    │   │
    │   ├── data           <- Scripts to download or generate data
    │   │   └── make_dataset.py
    │   │
    │   ├── features       <- Scripts to turn raw data into features for modeling
    │   │   └── build_features.py
    │   │
    │   ├── models         <- Scripts to train models and then use trained models to make
    │   │   │                 predictions
    │   │   ├── predict_model.py
    │   │   └── train_model.py
    │   │
    │   └── visualization  <- Scripts to create exploratory and results oriented visualizations
    │       └── visualize.py
    │
    └── tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io


--------

<p><small>Project based on the <a target=""_blank"" href=""https://drivendata.github.io/cookiecutter-data-science/"">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
"
127,https://github.com/aavelasq/dsc180-Q2sentiment,"{'aavelasq': 'https://github.com/aavelasq', 'nvgopal': 'https://github.com/nvgopal', 'peter-wu1': 'https://github.com/peter-wu1'}",{'Python': 1.0},"# The Effect of Cancel Culture on Sentiment Over Time

This project analyzes the change in public sentiment towards musicians
who were cancelled on English-speaking Twitter due to socially unacceptable behavior.
Specifically, we looked at how the 
type of issue, the background of the artist, and the strength of their
parasocial relationship with their fans affected sentiment towards them over time. 
For our analysis, we chose to focus on music artists from three different genres: 
K-Pop, Hip-Hop, and Western Pop. 
To measure sentiment, we utilized the 
[Google Perspective API](https://www.perspectiveapi.com/). 

## Running the Project
- Install dependencies using `pip install -r requirements.txt`

- To run the project using test data: run `python run.py test`

- To scrape Twitter data: run `python getTweets.py`
    - In order to run this script, must obtain valid Twitter API keys and save to a 
    file named `twitterkeys.py`
    - To change artist and timeframe of tweets, change the query attribute in the `query_params` variable
    - Saves scraped Twitter data with columns `id, text, author_id, created_at` to `data\raw`

- To run the project using real data: run `python run.py data`
    - This calls `etl.py` and retrieves data stored in `data\temp` folders. The directory where data is stored can be changed in `data-params.json`

- The different sentiment API and library scripts are found in `run.py`.
    - To run Google Perpsective API script on Twitter data: run 
    `python run.py data toxicity`
        - Before runnning, must obtain Google Developer API keys to run API script.
        - Outputs a dataframe containing toxicity, severe toxicity, insult, 
        and profanity probability scores and saves to `data\temp`
    - To run TextBlob library script on Twitter data: run 
    `python run.py data polarity`
        - Outputs a dataframe with sentiment polarity values and saves to `data\out`
    - To run Vader library script on Twitter data: run 
    `python run.py data vader`
        - Outputs a dataframe with sentiment polarity values and saves to `data\out`

- To calculate some exploratory statistics and visualizations: run
    `python run.py data eda`
    - Runs on each individual artist, both canceled and control
        - Saves a dataframe containing the number of tweets collected per day 
        to `data\out`
        - Saves visualizations of user activity, toxicity, and polarity over time to `data\out`

- To smooth out short-term trends and compute rolling average of sentiment data:
    run `python run.py data preprocessing`
    - Saves rolling average dataframe to `data\temp`

- To generate results for first sub-question (type of issue): 
    - run `python run.py data typefOfIssue`
        - Saves dataframes used later to generate visuals to `data\temp\rq1_type`
    - run `python run.py visuals_ti`
        - Saves visualizations to `data\out\rq1_type`

- To generate results for second sub-question (background of artist): run `python run.py data background`
    - Saves dataframes used later to generate visuals to `data\temp\rq_bg2`
    - Saves visualizations to `data\out\rq_bg2`

- To generate results for third sub-question (parasocial relationships): 
    - run `python run.py data parasocial`
        - Saves dataframes used later to generate visuals to `data\temp\rq3_ps`
    - run `python run.py ps_visuals`
        - Saves visualizations to `data\out\rq3_ps`","# The Effect of Cancel Culture on Sentiment Over Time

This project analyzes the change in public sentiment towards musicians
who were cancelled on English-speaking Twitter due to socially unacceptable behavior.
Specifically, we looked at how the 
type of issue, the background of the artist, and the strength of their
parasocial relationship with their fans affected sentiment towards them over time. 
For our analysis, we chose to focus on music artists from three different genres: 
K-Pop, Hip-Hop, and Western Pop. 
To measure sentiment, we utilized the 
[Google Perspective API](https://www.perspectiveapi.com/). 

## Running the Project
- Install dependencies using `pip install -r requirements.txt`

- To run the project using test data: run `python run.py test`

- To scrape Twitter data: run `python getTweets.py`
    - In order to run this script, must obtain valid Twitter API keys and save to a 
    file named `twitterkeys.py`
    - To change artist and timeframe of tweets, change the query attribute in the `query_params` variable
    - Saves scraped Twitter data with columns `id, text, author_id, created_at` to `data\raw`

- To run the project using real data: run `python run.py data`
    - This calls `etl.py` and retrieves data stored in `data\temp` folders. The directory where data is stored can be changed in `data-params.json`

- The different sentiment API and library scripts are found in `run.py`.
    - To run Google Perpsective API script on Twitter data: run 
    `python run.py data toxicity`
        - Before runnning, must obtain Google Developer API keys to run API script.
        - Outputs a dataframe containing toxicity, severe toxicity, insult, 
        and profanity probability scores and saves to `data\temp`
    - To run TextBlob library script on Twitter data: run 
    `python run.py data polarity`
        - Outputs a dataframe with sentiment polarity values and saves to `data\out`
    - To run Vader library script on Twitter data: run 
    `python run.py data vader`
        - Outputs a dataframe with sentiment polarity values and saves to `data\out`

- To calculate some exploratory statistics and visualizations: run
    `python run.py data eda`
    - Runs on each individual artist, both canceled and control
        - Saves a dataframe containing the number of tweets collected per day 
        to `data\out`
        - Saves visualizations of user activity, toxicity, and polarity over time to `data\out`

- To smooth out short-term trends and compute rolling average of sentiment data:
    run `python run.py data preprocessing`
    - Saves rolling average dataframe to `data\temp`

- To generate results for first sub-question (type of issue): 
    - run `python run.py data typefOfIssue`
        - Saves dataframes used later to generate visuals to `data\temp\rq1_type`
    - run `python run.py visuals_ti`
        - Saves visualizations to `data\out\rq1_type`

- To generate results for second sub-question (background of artist): run `python run.py data background`
    - Saves dataframes used later to generate visuals to `data\temp\rq_bg2`
    - Saves visualizations to `data\out\rq_bg2`

- To generate results for third sub-question (parasocial relationships): 
    - run `python run.py data parasocial`
        - Saves dataframes used later to generate visuals to `data\temp\rq3_ps`
    - run `python run.py ps_visuals`
        - Saves visualizations to `data\out\rq3_ps`"
128,https://github.com/18anguyen9/Single_Cell_Coupled_Autoencoders,"{'brianvi-98': 'https://github.com/brianvi-98', 'brianvi98': 'https://github.com/brianvi98', '18anguyen9': 'https://github.com/18anguyen9'}",{'Python': 1.0},"# Single-Cell-Coupled-Autoencoders

&emsp; &emsp; In this project, we implement a coupled autoencoder for working with single-cell data, which includes data sets on DNA, mRNA and protein data.

## About

&emsp; &emsp; Historically, analysis on single-cell data has been difficult to perform, due to data collection methods often resulting in the destruction of the cell in the process of collecting information. However, an ongoing endeavor of biological data science has recently been to analyze different modalities, or forms, of the genetic information within a cell. Doing so will allow modern medicine a greater understanding of cellular functions and how cells work in the context of illnesses. The information collected on the three modalities of DNA, RNA, and protein can be done safely and because it is known that they are same information in different forms, analysis done on them can be extrapolated understand the cell as a whole. Previous research has been conducted by Gala, R., Budzillo, A., Baftizadeh, F. et al. to capture gene expression in neuron cells with a neural network called a coupled autoencoder. This autoencoder framework is able to reconstruct the inputs, allowing the prediction of one input to another, as well as align the multiple inputs in the same low dimensional representation. In our paper, we build upon this coupled autoencoder on a data set of cells taken from several sites of the human body, predicting from RNA information to protein. We find that the autoencoder is able to adequately cluster the cell types in its lower dimensional representation, as well as perform decently at the prediction task. We show that the autoencoder is a powerful tool for analyzing single-cell data analysis and may prove to be a valuable asset in single-cell data analysis.

## How to run this project

1. Clone this repository onto your local machine with `git clone https://github.com/18anguyen9/Single_Cell_Coupled_Autoencoders.git` and change into the directory.

2. Launch the Docker image for the project with the following line: `launch.sh -i alandnin/method3:latest`

3. The code is ran with command line arguments:

    * `test`: (`python3 run.py test`) Due to the size of the full data set, this will perform a test run on a much smaller subset of our data sets to simulate the output of our project. 
    
    *  `test-full`: (`python3 run.py test-full`) This a full run of training the coupled autoencoder with the entire data set. This will take much longer (expect 20-30 minutes) than `test`, but will contain meaningful outputs compared to the simulated `test`. However, you will first need to download the entire data set using the following command line argument `aws s3 sync s3://openproblems-bio/public/ $HOME/data/ --no-sign-request` and moving the `cite_gex_processed_training.h5ad` and `cite_adt_processed_training.h5ad` files into the `/data` file of this directory.
    
    *  `clear-cache`: The training computation is memory expensive. `python3 run.py clear-cache` can be run in the case that your machine runs out of memory. This will only happen when `test-full` is ran.

## Related

We based our project off of this NeurIPS competition: https://openproblems.bio/neurips_docs/about/about/

Our website: https://18anguyen9.github.io/DSC_180_website/
    
    
    
","# Single-Cell-Coupled-Autoencoders

&emsp; &emsp; In this project, we implement a coupled autoencoder for working with single-cell data, which includes data sets on DNA, mRNA and protein data.

## About

&emsp; &emsp; Historically, analysis on single-cell data has been difficult to perform, due to data collection methods often resulting in the destruction of the cell in the process of collecting information. However, an ongoing endeavor of biological data science has recently been to analyze different modalities, or forms, of the genetic information within a cell. Doing so will allow modern medicine a greater understanding of cellular functions and how cells work in the context of illnesses. The information collected on the three modalities of DNA, RNA, and protein can be done safely and because it is known that they are same information in different forms, analysis done on them can be extrapolated understand the cell as a whole. Previous research has been conducted by Gala, R., Budzillo, A., Baftizadeh, F. et al. to capture gene expression in neuron cells with a neural network called a coupled autoencoder. This autoencoder framework is able to reconstruct the inputs, allowing the prediction of one input to another, as well as align the multiple inputs in the same low dimensional representation. In our paper, we build upon this coupled autoencoder on a data set of cells taken from several sites of the human body, predicting from RNA information to protein. We find that the autoencoder is able to adequately cluster the cell types in its lower dimensional representation, as well as perform decently at the prediction task. We show that the autoencoder is a powerful tool for analyzing single-cell data analysis and may prove to be a valuable asset in single-cell data analysis.

## How to run this project

1. Clone this repository onto your local machine with `git clone https://github.com/18anguyen9/Single_Cell_Coupled_Autoencoders.git` and change into the directory.

2. Launch the Docker image for the project with the following line: `launch.sh -i alandnin/method3:latest`

3. The code is ran with command line arguments:

    * `test`: (`python3 run.py test`) Due to the size of the full data set, this will perform a test run on a much smaller subset of our data sets to simulate the output of our project. 
    
    *  `test-full`: (`python3 run.py test-full`) This a full run of training the coupled autoencoder with the entire data set. This will take much longer (expect 20-30 minutes) than `test`, but will contain meaningful outputs compared to the simulated `test`. However, you will first need to download the entire data set using the following command line argument `aws s3 sync s3://openproblems-bio/public/ $HOME/data/ --no-sign-request` and moving the `cite_gex_processed_training.h5ad` and `cite_adt_processed_training.h5ad` files into the `/data` file of this directory.
    
    *  `clear-cache`: The training computation is memory expensive. `python3 run.py clear-cache` can be run in the case that your machine runs out of memory. This will only happen when `test-full` is ran.

## Related

We based our project off of this NeurIPS competition: https://openproblems.bio/neurips_docs/about/about/

Our website: https://18anguyen9.github.io/DSC_180_website/
    
    
    
"
129,https://github.com/zwcolin/T5_SQuAD_Prompt_Tuning,"{'zwcolin': 'https://github.com/zwcolin', 'wanglec': 'https://github.com/wanglec', 'rachelluoyt': 'https://github.com/rachelluoyt'}","{'Python': 0.97, 'Jupyter Notebook': 0.03, 'Makefile': 0.0, 'Shell': 0.0, 'Dockerfile': 0.0}","# On Evaluating the Robustness of Language Models with Tuning
Authors: Colin Wang, Lechuan Wang, Yutong Luo

Website: https://rachelluoyt.github.io/T5_SQuAD_Prompt_Tuning/
## Pipeline for DSC 180B (not normally used by us, but for DSC 180B which asserts a certain format)
Build a container using `zwcolin/180_method5:latest` docker. Clone the repo, then at the root folder, run `python run.py test`. Warning: lots of time may be spent on downloading the data, pretrained model, tokenizer, and preparation. The testing itself may take ~30 minutes (not including downloading and building the dataset) to output evaluation metrics (we've modified the script for you so it just measures the first 10 examples, which may take around 30 seconds to intialize and process). If you do want to see some results, you may want to wait for quite a bit. Alternatively, some existing train/testing logging has been provided inside the the `prompt_tuning` folder. You can take a look at that instead of actually running the code.

## Internal Pipeline
### Manipulating Model in `run.py`
#### Training & Testing
- Simply do `bash experiment.sh` and modify any experiment meta info as well as hyperparameter as necessary. The pipeline has been built to suit single/multi GPU configurations under a single server instance.

## Deployment
A `Dockerfile` has been provided in the root folder to set up a docker environment. Note that this dockerfile has only been experimented at UCSD's DataHub. Use it with caution.

## DSC 180B Specific Instructions
We don't strictly follow the structure of the given suggestions, with a `test` folder and a `testdata` folder inside it. It's too rigid. Instead, all the training and test data will be store inside the `data` folder and `experiment.sh` contains all the necessary code to build the model or to test the model for running the model. We don't like the way that you need to run `test.py` with some arguments in the command line because it's obviously not suitable for a deep learning project where there are way many possible arguments (i.e. you will likely have to type `test.py -xx -xx -xx -xx`, repeating `-` for dozens of times).

## Reference
The script is based on the following paper:
@misc{lester2021power,
      title={The Power of Scale for Parameter-Efficient Prompt Tuning}, 
      author={Brian Lester and Rami Al-Rfou and Noah Constant},
      year={2021},
      eprint={2104.08691},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
","# On Evaluating the Robustness of Language Models with Tuning
Authors: Colin Wang, Lechuan Wang, Yutong Luo

Website: https://rachelluoyt.github.io/T5_SQuAD_Prompt_Tuning/
## Pipeline for DSC 180B (not normally used by us, but for DSC 180B which asserts a certain format)
Build a container using `zwcolin/180_method5:latest` docker. Clone the repo, then at the root folder, run `python run.py test`. Warning: lots of time may be spent on downloading the data, pretrained model, tokenizer, and preparation. The testing itself may take ~30 minutes (not including downloading and building the dataset) to output evaluation metrics (we've modified the script for you so it just measures the first 10 examples, which may take around 30 seconds to intialize and process). If you do want to see some results, you may want to wait for quite a bit. Alternatively, some existing train/testing logging has been provided inside the the `prompt_tuning` folder. You can take a look at that instead of actually running the code.

## Internal Pipeline
### Manipulating Model in `run.py`
#### Training & Testing
- Simply do `bash experiment.sh` and modify any experiment meta info as well as hyperparameter as necessary. The pipeline has been built to suit single/multi GPU configurations under a single server instance.

## Deployment
A `Dockerfile` has been provided in the root folder to set up a docker environment. Note that this dockerfile has only been experimented at UCSD's DataHub. Use it with caution.

## DSC 180B Specific Instructions
We don't strictly follow the structure of the given suggestions, with a `test` folder and a `testdata` folder inside it. It's too rigid. Instead, all the training and test data will be store inside the `data` folder and `experiment.sh` contains all the necessary code to build the model or to test the model for running the model. We don't like the way that you need to run `test.py` with some arguments in the command line because it's obviously not suitable for a deep learning project where there are way many possible arguments (i.e. you will likely have to type `test.py -xx -xx -xx -xx`, repeating `-` for dozens of times).

## Reference
The script is based on the following paper:
@misc{lester2021power,
      title={The Power of Scale for Parameter-Efficient Prompt Tuning}, 
      author={Brian Lester and Rami Al-Rfou and Noah Constant},
      year={2021},
      eprint={2104.08691},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
"
131,https://github.com/zhw005/DSC180B-Project,"{'yujiezhang0914': 'https://github.com/yujiezhang0914', 'apochira': 'https://github.com/apochira', 'JerryYC': 'https://github.com/JerryYC', 'zhw005': 'https://github.com/zhw005'}","{'Jupyter Notebook': 0.98, 'Python': 0.02, 'Dockerfile': 0.0}","# DSC180B: Explainable AI
This is a repository that contains code for DSC180B section B06's Q2 Project: Explainable AI.

""build-script"": ""zhw005/dsc180b-project""

## Authors
- [Jerry (Yung-Chieh) Chan](https://github.com/JerryYC)
- [Apoorv Pochiraju](https://github.com/apochira)
- [Zhendong Wang](https://github.com/zhw005)
- [Yujie Zhang](https://github.com/yujiezhang0914)

## Introduction
In our project, we will be focusing on using different techniques from causal inferences and explainable AI to interpret various machine learning models across various domains. In particular, we are interested in three domains - healthcare, banking, and the housing market. Within each domain, we are going to train several machine learning models first:XGBoost, LightGBM, TabNet, and SVM. And we have four goals in general: 
1) Explaining black-box models both globally and locally with various XAI methods; 
2) Assessing the fairness of each learning algorithm with regard to different sensitive attributes; 
3) Explaining False Negative and False Positive predictions using Causal Inference;
4) Generating recourse for individuals - a set of minimal actions to change the prediction of those black-box models.

## Running the project

 target | config | experiment |
| :---: | :---: | :---: |
| airbnb_features | 'config/FeatureEng-params-airbnb.json' | Do feature engineering for airbnb dataset |
| loan_features | 'config/FeatureEng-params-loan.json' | Do feature engineering for loan dataset |
| diabetes_features | 'config/FeatureEng-params-diabetes.json' | Do feature engineering for diabetes dataset |
| fairness | 'config/Fairness-example.json' | Do fairness evaluation |
| FN_FP | 'config/FN_FP-example.json' | Do False Negative and False Positive explanation |
| model_explanations | 'config/Model_Explanations_Example_loan.json'| Do model explanations - loan data example|
| recourse | 'config/Recourse-example.json'| Generate recourse explanation - loan data example|
","# DSC180B: Explainable AI
This is a repository that contains code for DSC180B section B06's Q2 Project: Explainable AI.

""build-script"": ""zhw005/dsc180b-project""

## Authors
- [Jerry (Yung-Chieh) Chan](https://github.com/JerryYC)
- [Apoorv Pochiraju](https://github.com/apochira)
- [Zhendong Wang](https://github.com/zhw005)
- [Yujie Zhang](https://github.com/yujiezhang0914)

## Introduction
In our project, we will be focusing on using different techniques from causal inferences and explainable AI to interpret various machine learning models across various domains. In particular, we are interested in three domains - healthcare, banking, and the housing market. Within each domain, we are going to train several machine learning models first:XGBoost, LightGBM, TabNet, and SVM. And we have four goals in general: 
1) Explaining black-box models both globally and locally with various XAI methods; 
2) Assessing the fairness of each learning algorithm with regard to different sensitive attributes; 
3) Explaining False Negative and False Positive predictions using Causal Inference;
4) Generating recourse for individuals - a set of minimal actions to change the prediction of those black-box models.

## Running the project

 target | config | experiment |
| :---: | :---: | :---: |
| airbnb_features | 'config/FeatureEng-params-airbnb.json' | Do feature engineering for airbnb dataset |
| loan_features | 'config/FeatureEng-params-loan.json' | Do feature engineering for loan dataset |
| diabetes_features | 'config/FeatureEng-params-diabetes.json' | Do feature engineering for diabetes dataset |
| fairness | 'config/Fairness-example.json' | Do fairness evaluation |
| FN_FP | 'config/FN_FP-example.json' | Do False Negative and False Positive explanation |
| model_explanations | 'config/Model_Explanations_Example_loan.json'| Do model explanations - loan data example|
| recourse | 'config/Recourse-example.json'| Generate recourse explanation - loan data example|
"
132,https://github.com/TanveerMittal/Feature_Type_Inference_Capstone,"{'TanveerMittal': 'https://github.com/TanveerMittal', 'Shen-Andrew': 'https://github.com/Shen-Andrew'}","{'Jupyter Notebook': 0.96, 'Python': 0.03, 'TeX': 0.01, 'Dockerfile': 0.0}","# Feature Type Inference Capstone

### Team Members: Tanveer Mittal & Andrew Shen
### Mentor: Arun Kumar

## Resources:
- [Torch Hub Release of Pretrained Models](https://github.com/TanveerMittal/BERT-Feature-Type-Inference)
    - Allows anyone to load our models in a single line of code using the th PyTorch Hub API
- [Tech Report](https://tanveermittal.github.io/capstone/)
    - Provides detailed methodology and results of our experiments
- [ML Data Prep Zoo](https://github.com/pvn25/ML-Data-Prep-Zoo)
    - Provides benchmark data and pretrained models for Feature Type Inference
- [Project Sortinghat](https://adalabucsd.github.io/sortinghat.html)

## Overview:

One of the first steps in automated data prepration in AutoML platforms is to identify the feature types of individual columns in input data. This information then allows the software to understand the data and then preprocess it to allow machine learning algorithms to run on it. Project Sortinghat frames this task of Feature Type Inference as a machine learning multiclass classification problem. As an extension of Project SortingHat, we worked on applying Bidirectional Encoding Representation Transformer(BERT) models to this task and did further investigations on the effects of adjusting the feature set for input with a random forest model. Our BERT CNN models currently outperform all existing tools currently benchmarked against SortingHat's ML Data Prep Zoo.

This repository includes code for architecture and feature experiments for the transformer models. The results of our 2 released models can be seen in the tables below:

- BERT CNN with Descriptive Statistics:
    - 9 Class Test Accuracy: **0.934**

| Data Type | numeric | categorical | datetime | sentence | url   | embedded-number | list  | not-generalizable | context-specific |
|-----------|---------|-------------|----------|----------|-------|-----------------|-------|-------------------|------------------|
| **Accuracy**  |   0.983 |       0.972 |        1 |    0.986 | 0.999 |           0.997 | 0.994 |             0.968 |            0.967 |
| **Precision** |   0.959 |       0.935 |        1 |    0.849 | 0.969 |           0.989 |  0.96 |             0.848 |             0.87 |
| **Recall**    |   0.996 |       0.943 |        1 |    0.859 | 0.969 |           0.949 | 0.842 |             0.856 |            0.762 |

- BERT CNN without Descriptive Statistics:
    - 9 Class Test Accuracy: **0.929**

| Data Type | numeric | categorical | datetime | sentence | url   | embedded-number | list  | not-generalizable | context-specific |
|-----------|---------|-------------|----------|----------|-------|-----------------|-------|-------------------|------------------|
| Accuracy  |   0.981 |       0.967 |    0.999 |    0.987 | 0.999 |           0.997 | 0.994 |             0.966 |            0.968 |
| Precision |   0.958 |       0.917 |    0.993 |    0.853 | 0.969 |            0.99 | 0.959 |             0.869 |            0.854 |
| Recall    |   0.992 |       0.941 |        1 |     0.88 | 0.969 |            0.96 | 0.825 |             0.805 |            0.789 |
","# Feature Type Inference Capstone

### Team Members: Tanveer Mittal & Andrew Shen
### Mentor: Arun Kumar

## Resources:
- [Torch Hub Release of Pretrained Models](https://github.com/TanveerMittal/BERT-Feature-Type-Inference)
    - Allows anyone to load our models in a single line of code using the th PyTorch Hub API
- [Tech Report](https://tanveermittal.github.io/capstone/)
    - Provides detailed methodology and results of our experiments
- [ML Data Prep Zoo](https://github.com/pvn25/ML-Data-Prep-Zoo)
    - Provides benchmark data and pretrained models for Feature Type Inference
- [Project Sortinghat](https://adalabucsd.github.io/sortinghat.html)

## Overview:

One of the first steps in automated data prepration in AutoML platforms is to identify the feature types of individual columns in input data. This information then allows the software to understand the data and then preprocess it to allow machine learning algorithms to run on it. Project Sortinghat frames this task of Feature Type Inference as a machine learning multiclass classification problem. As an extension of Project SortingHat, we worked on applying Bidirectional Encoding Representation Transformer(BERT) models to this task and did further investigations on the effects of adjusting the feature set for input with a random forest model. Our BERT CNN models currently outperform all existing tools currently benchmarked against SortingHat's ML Data Prep Zoo.

This repository includes code for architecture and feature experiments for the transformer models. The results of our 2 released models can be seen in the tables below:

- BERT CNN with Descriptive Statistics:
    - 9 Class Test Accuracy: **0.934**

| Data Type | numeric | categorical | datetime | sentence | url   | embedded-number | list  | not-generalizable | context-specific |
|-----------|---------|-------------|----------|----------|-------|-----------------|-------|-------------------|------------------|
| **Accuracy**  |   0.983 |       0.972 |        1 |    0.986 | 0.999 |           0.997 | 0.994 |             0.968 |            0.967 |
| **Precision** |   0.959 |       0.935 |        1 |    0.849 | 0.969 |           0.989 |  0.96 |             0.848 |             0.87 |
| **Recall**    |   0.996 |       0.943 |        1 |    0.859 | 0.969 |           0.949 | 0.842 |             0.856 |            0.762 |

- BERT CNN without Descriptive Statistics:
    - 9 Class Test Accuracy: **0.929**

| Data Type | numeric | categorical | datetime | sentence | url   | embedded-number | list  | not-generalizable | context-specific |
|-----------|---------|-------------|----------|----------|-------|-----------------|-------|-------------------|------------------|
| Accuracy  |   0.981 |       0.967 |    0.999 |    0.987 | 0.999 |           0.997 | 0.994 |             0.966 |            0.968 |
| Precision |   0.958 |       0.917 |    0.993 |    0.853 | 0.969 |            0.99 | 0.959 |             0.869 |            0.854 |
| Recall    |   0.992 |       0.941 |        1 |     0.88 | 0.969 |            0.96 | 0.825 |             0.805 |            0.789 |
"
133,https://github.com/amelia-kawasaki/dsc_capstone,"{'amelia-kawasaki': 'https://github.com/amelia-kawasaki', 'rdunnUCSD': 'https://github.com/rdunnUCSD', 'cheolmin711': 'https://github.com/cheolmin711'}","{'Python': 0.96, 'Jupyter Notebook': 0.04}","# DSC Capstone: Group B08
## Exploring Noise in Data: Applications to ML Models
### Models Supported:
Kernel machines, Random Forests, k-Nearest Neighbor Classification
### Building the Project:
Please build the project using the Docker container located at the DockerHub repo in submission.json
### Running the Project:
To run on all data:
> python3 run.py

To run on all data with a custom config file:
> python3 run.py all [json config file]

To run the code on test section of data:
> python3 run.py test

To clean all output files:
> python3 run.py clean

Website for an introduction to our project:
https://amelia-kawasaki.github.io/dsc_capstone/
","# DSC Capstone: Group B08
## Exploring Noise in Data: Applications to ML Models
### Models Supported:
Kernel machines, Random Forests, k-Nearest Neighbor Classification
### Building the Project:
Please build the project using the Docker container located at the DockerHub repo in submission.json
### Running the Project:
To run on all data:
> python3 run.py

To run on all data with a custom config file:
> python3 run.py all [json config file]

To run the code on test section of data:
> python3 run.py test

To clean all output files:
> python3 run.py clean

Website for an introduction to our project:
https://amelia-kawasaki.github.io/dsc_capstone/
"
134,https://github.com/edinhluo/DSC180-Capstone-Project,"{'edinhluo': 'https://github.com/edinhluo', 'j2chu': 'https://github.com/j2chu', 'MengfanChen27': 'https://github.com/MengfanChen27'}","{'Python': 0.99, 'Dockerfile': 0.01}","# COVID-19 Group Testing Strategies

## Abstract

The COVID-19 pandemic that has persisted for more than two years has been combated by efficient testing strategies that reliably identifies positive individuals to slow the spread of the pandemic. Opposed to other pooling strategies within the domain, the methods described in this paper prioritize true negative samples over overall accuracy. In the Monte Carlo simulations, both nonadaptive and adaptive testing strategies with random pool sampling resulted in high accuracy approaching at least 95% with varying pooling sizes and population sizes to decrease the number of tests given. A split tensor rank 2 method attempts to identify all infected samples within 961 samples, converging the number of tests to 99 as the prevalence of infection converges to 1%.
","# COVID-19 Group Testing Strategies

## Abstract

The COVID-19 pandemic that has persisted for more than two years has been combated by efficient testing strategies that reliably identifies positive individuals to slow the spread of the pandemic. Opposed to other pooling strategies within the domain, the methods described in this paper prioritize true negative samples over overall accuracy. In the Monte Carlo simulations, both nonadaptive and adaptive testing strategies with random pool sampling resulted in high accuracy approaching at least 95% with varying pooling sizes and population sizes to decrease the number of tests given. A split tensor rank 2 method attempts to identify all infected samples within 961 samples, converging the number of tests to 99 as the prevalence of infection converges to 1%.
"
135,https://github.com/pnair7/ml-fairness,"{'pnair7': 'https://github.com/pnair7', 'danieltong000': 'https://github.com/danieltong000', 'annemxu': 'https://github.com/annemxu'}","{'Jupyter Notebook': 0.97, 'Python': 0.03, 'Dockerfile': 0.0, 'HTML': 0.0}","# Patterns of Fairness in Machine Learning

[Website Link](https://annemxu.github.io/ml-fairness/)

[Report Link](https://raw.githubusercontent.com/pnair7/artifact-directory-template/main/report.pdf)

An empirical analysis of machine learning fairness using a variety of metrics, models, and datasets.

## Instructions
`python run.py` for full output matrix

`python run.py test` to run on just test data

### Adding your own data
Raw datasets sit in the `rawDatasets/` folder, and can be processed by a script in the `preprocessing/` folder. The output should add a folder to `cleanedDatasets/` containing two elements: a JSON config file (see folder for example format) and a CSV file of the cleaned dataset. Input columns should all be numerical, and output columns should be 0/1 for binary classification. (If your data is already cleaned to these specifications, it can be placed directly in the `cleanedDatasets` folder, no need to upload the raw dataset or use a preprocessing script.)

#### Example Config File

```
{
    ""y_col"": ""refer"",                                    # which column are you predicting?
    ""X_cols"": [                                          # which columns are the predictor variables?
        ""dem_age_band_18-24_tm1"",
        ""dem_age_band_25-34_tm1"",
        ""dem_age_band_35-44_tm1"",
        ...
        ""trig_max-high_tm1"",
        ""trig_max-normal_tm1"",
        ""gagne_sum_tm1""
    ],
    ""group_cols"": [                                      # which column is the protected attribute? is a list for consistency, but just one element
        ""race""
    ],
    ""prediction_type"": ""binary"",                         # only binary is implemented (not used)
    ""dataset_name"": ""Obermeyer Health Dataset"",          # display name for dataset
    ""data_path"": ""rawDatasets/obermeyer_data.csv"",       # path to raw dataset (not used)
    ""data_script"": ""preprocessing/obermeyer.py""          # script to preprocess raw data (not used)
}
```

### Adding your own models or metrics
Adding your own models or metrics is a little more involved, but still quite simple. 

Currently, models are contained in `models/sklearn_models.py`. First, you must write a model function that takes in the same parameters as the other models, which can be located in the same file, or in a different file in the `models` directory. Next, in the `utils/utils.py` file, you must import the model, and add the model to the `run_models` function, assigning it a string name for the model to map to the function. Finally, you must add the string name of the model to the list of models at the top of `run.py`.

The process is nearly identical for metrics. Create a metric function that takes in the same parameters as the other metric functions, add the metric to the `apply_metric` function in `utils/utils.py` with its own string name, and add that string name to the list of metrics in `run.py`.
","# Patterns of Fairness in Machine Learning

[Website Link](https://annemxu.github.io/ml-fairness/)

[Report Link](https://raw.githubusercontent.com/pnair7/artifact-directory-template/main/report.pdf)

An empirical analysis of machine learning fairness using a variety of metrics, models, and datasets.

## Instructions
`python run.py` for full output matrix

`python run.py test` to run on just test data

### Adding your own data
Raw datasets sit in the `rawDatasets/` folder, and can be processed by a script in the `preprocessing/` folder. The output should add a folder to `cleanedDatasets/` containing two elements: a JSON config file (see folder for example format) and a CSV file of the cleaned dataset. Input columns should all be numerical, and output columns should be 0/1 for binary classification. (If your data is already cleaned to these specifications, it can be placed directly in the `cleanedDatasets` folder, no need to upload the raw dataset or use a preprocessing script.)

#### Example Config File

```
{
    ""y_col"": ""refer"",                                    # which column are you predicting?
    ""X_cols"": [                                          # which columns are the predictor variables?
        ""dem_age_band_18-24_tm1"",
        ""dem_age_band_25-34_tm1"",
        ""dem_age_band_35-44_tm1"",
        ...
        ""trig_max-high_tm1"",
        ""trig_max-normal_tm1"",
        ""gagne_sum_tm1""
    ],
    ""group_cols"": [                                      # which column is the protected attribute? is a list for consistency, but just one element
        ""race""
    ],
    ""prediction_type"": ""binary"",                         # only binary is implemented (not used)
    ""dataset_name"": ""Obermeyer Health Dataset"",          # display name for dataset
    ""data_path"": ""rawDatasets/obermeyer_data.csv"",       # path to raw dataset (not used)
    ""data_script"": ""preprocessing/obermeyer.py""          # script to preprocess raw data (not used)
}
```

### Adding your own models or metrics
Adding your own models or metrics is a little more involved, but still quite simple. 

Currently, models are contained in `models/sklearn_models.py`. First, you must write a model function that takes in the same parameters as the other models, which can be located in the same file, or in a different file in the `models` directory. Next, in the `utils/utils.py` file, you must import the model, and add the model to the `run_models` function, assigning it a string name for the model to map to the function. Finally, you must add the string name of the model to the list of models at the top of `run.py`.

The process is nearly identical for metrics. Create a metric function that takes in the same parameters as the other metric functions, add the metric to the `apply_metric` function in `utils/utils.py` with its own string name, and add that string name to the list of metrics in `run.py`.
"
136,https://github.com/mglevitt/Medical-Disparity-Causal-Analysis,"{'mglevitt': 'https://github.com/mglevitt', 'AdamKreitz': 'https://github.com/AdamKreitz'}","{'Jupyter Notebook': 0.87, 'Python': 0.13, 'Dockerfile': 0.0}","## Quality of Life Causal Analysis Project

In this project, we aim to establish causality between various socioeconomic variables and life expectancy outcomes in  roughly 166 different countries, noting the strongest connections between economic and political factors with the length of life expectancy. 

### How to Use

To run our project just run the following two lines of code in a Unix shell, we utilized Unbuntu 20.04 LTS but it should work for others.
```
docker pull mglevitt/world_happiness_project:run_project
docker run mglevitt/world_happiness_project:run_project
```
The pull should not take too long, but to run the code may take upwards of half an hour as a result of the many computations being made with PC. The end output should be a dictionary of the relations we found that will be printed to your terminal and all the graphs of the relations we found will pop up on your device as the code is run. Feel free to consult us if you run into any difficulties with getting our code to run properly.

#### Run PC on Your Own Data

The pipeline we developed is flexible to work with any data in the correct format to find causal relations present in the data. You can follow the step by step instructions below to run PC on your own datasets. 

1. In a local terminal, navigate to where you would like to place the repository of our code
2. Clone this repository on to your local machine in the destination of your choice and navigate into the repository of our code with: 
```
git clone https://github.com/mglevitt/Medical-Disparity-Causal-Analysis.git
cd .\Medical-Disparity-Causal-Analysis\
```
3. Install pipenv on your local machine and use it to to install the dependencies needed to run our code with: 
```
pip install pipenv
pipenv install
```
4. Save the data you would like to reformat to src/data as a csv or xlsx file. 
5. When adding in your own datasets there are a few prior cleaning steps that may have to be done. For any of our provided datasets that you want to use, this step can be skipped. First, the data must be global time series data identified by country names or country name and year. All datasets must have a column with country names that has ""country"" in the column name. The country names also must follow the same naming conventions as other data that you are merging your data with. If you wish to have an included column with years, then ""year"" must be in the column's name. When combining datasets with a year column, the pipeline will only include years that are in both datasets, so make sure the years are overlapping in their span. 
6. Open the code of src/scripts/download_data.py with whatever method works from you local terminal or file exploror. Edit the 2 variables after the line ""# Add your own data here"" towards the bottom of the script to have the correct names for your file names in the list for variable name ""new_file_names"", including the extension .csv or .xlsx, and the name for your output file before .csv. Once these variable are edited, delete the ""#"" from before the last line of code then to save the changes to this file.
7. Run the code to have your new data file added to src/final_data with your inputted file name with: 
```
pipenv run .\src\scripts\download_data.py
```
8. After all these steps, a table of relations with the most common relations at the top should be outputted in your terminal. 
9. This step is not neccesary, but for further exploration into the causal relations present you can adjust the signifigance level of PC. Higher signifigance will lead to more relations being present and vice versa. The default signifigance is .2 and this value must be between 0 and 1. The signifigance can be adjusted by editing the value of alpha in the last line of code. 
","## Quality of Life Causal Analysis Project

In this project, we aim to establish causality between various socioeconomic variables and life expectancy outcomes in  roughly 166 different countries, noting the strongest connections between economic and political factors with the length of life expectancy. 

### How to Use

To run our project just run the following two lines of code in a Unix shell, we utilized Unbuntu 20.04 LTS but it should work for others.
```
docker pull mglevitt/world_happiness_project:run_project
docker run mglevitt/world_happiness_project:run_project
```
The pull should not take too long, but to run the code may take upwards of half an hour as a result of the many computations being made with PC. The end output should be a dictionary of the relations we found that will be printed to your terminal and all the graphs of the relations we found will pop up on your device as the code is run. Feel free to consult us if you run into any difficulties with getting our code to run properly.

#### Run PC on Your Own Data

The pipeline we developed is flexible to work with any data in the correct format to find causal relations present in the data. You can follow the step by step instructions below to run PC on your own datasets. 

1. In a local terminal, navigate to where you would like to place the repository of our code
2. Clone this repository on to your local machine in the destination of your choice and navigate into the repository of our code with: 
```
git clone https://github.com/mglevitt/Medical-Disparity-Causal-Analysis.git
cd .\Medical-Disparity-Causal-Analysis\
```
3. Install pipenv on your local machine and use it to to install the dependencies needed to run our code with: 
```
pip install pipenv
pipenv install
```
4. Save the data you would like to reformat to src/data as a csv or xlsx file. 
5. When adding in your own datasets there are a few prior cleaning steps that may have to be done. For any of our provided datasets that you want to use, this step can be skipped. First, the data must be global time series data identified by country names or country name and year. All datasets must have a column with country names that has ""country"" in the column name. The country names also must follow the same naming conventions as other data that you are merging your data with. If you wish to have an included column with years, then ""year"" must be in the column's name. When combining datasets with a year column, the pipeline will only include years that are in both datasets, so make sure the years are overlapping in their span. 
6. Open the code of src/scripts/download_data.py with whatever method works from you local terminal or file exploror. Edit the 2 variables after the line ""# Add your own data here"" towards the bottom of the script to have the correct names for your file names in the list for variable name ""new_file_names"", including the extension .csv or .xlsx, and the name for your output file before .csv. Once these variable are edited, delete the ""#"" from before the last line of code then to save the changes to this file.
7. Run the code to have your new data file added to src/final_data with your inputted file name with: 
```
pipenv run .\src\scripts\download_data.py
```
8. After all these steps, a table of relations with the most common relations at the top should be outputted in your terminal. 
9. This step is not neccesary, but for further exploration into the causal relations present you can adjust the signifigance level of PC. Higher signifigance will lead to more relations being present and vice versa. The default signifigance is .2 and this value must be between 0 and 1. The signifigance can be adjusted by editing the value of alpha in the last line of code. 
"
137,https://github.com/GogoHYX/DSC180_sleep_apnea,"{'ShubhamKaushal15': 'https://github.com/ShubhamKaushal15', 'GogoHYX': 'https://github.com/GogoHYX', 'alxjareyliu': 'https://github.com/alxjareyliu', 'chinkevin': 'https://github.com/chinkevin'}","{'Jupyter Notebook': 0.9, 'Python': 0.1}","# DSC180 Capstone Project
## [Project Website](https://gogohyx.github.io/DSC180_sleep_apnea/)
### Build instructions

In the home directory, running `python run.py --targets` builds the requisite files.

#### Targets:

1. `features`: reads raw data files, builds features, joins the files, and stores the columns of resulting dataframe in different files under `data/out`.
2. `model`: trains the voting models and saves them under `results/` directory. **RUNNING THIS TAKES A WHILE FOR THE FIRST TIME BECAUSE OF THE IMPORT STATEMENT. AFTER THAT IT'S INSTANTANEOUS.**
3. `predict`: loads the saved models, makes predictions, stores them under `results/` directory, and also stores the model recall values in a `.txt` file.
4. `test_rnn` test the rnn model on a subset of test files and save the output under `data/out`.
5. `test`: provides the functionality of `1` `2` `3` and `4` targets combined, on the test raw data.
6. `all`: provides the functionality of `1` `2` `3` and `4` targets combined, on the real raw data

### Directory Map

1. `config/`: contains the configuration `.json` files.

    a. `create-test-data-params.json`: config file to build test-data.
    
    b. `data-params.json`: config file for cleaning data in the future.
    
    c. `eda-params.json`: config file for any EDA figures that will be generated.
    
    d. `features-params.json`: config file for building features from raw data.

    e. `test-features-params.json`: config file for building features from test raw data.
    
    f. `model-params.json`: config file for model parameters.

    g. `test-model-params.json`: config file for test model parameters.
    
    h. `test-rnn-params.json`: config file for testing rnn model.

2. `data/`: contains the raw data files and data files after feature engineering. 

    a. `raw/`: contains the raw data files downloaded from source. _Does not include anything in the repo because raw data is confidential_. 
    
    b. `out/`: contains the data after feature generation.
    
3. `notebooks/`: notebooks with some EDA and experimentation.
4. `references/`: this contains acknowledgement for any models or results that we use to build our project off of.
5. `results/`: running the build script populates this directory with the trained models (`.pkl`) and a `.txt` file which outlines the model performance. The `actual/` directory has the results of our actual model. The 'test/' directory will be created on running the `test` target and will contain results of our model on the test data.
6. `src/`: contains all the script `.py` files.
    
    a. `features/`: `build_features.py` performs feature engineering on clean data and populates `data/out` with files that can be used to train and test the models.
    
    b. `models/`: `train_model.py` trains the voting models and saves them in a `.pkl` file ; `test_model.py` makes predictions using the saved models and saves the performance metric in a `.txt` file. Both files are stored in `results/` directory.
    
    c. `helper_functions.py`: library of functions that are used to perform common tasks.
    
7. `test/`: contains `testdata/` which has the artificially generated test data.
8. `dockerfile`: creates a container with the necessary libraries and packages to run all the scripts.
9. `run.py`: running this script builds the requisite files.
10. `submission.json`: contains the dockerhub-id for building the container and build-script command to build the targets.
","# DSC180 Capstone Project
## [Project Website](https://gogohyx.github.io/DSC180_sleep_apnea/)
### Build instructions

In the home directory, running `python run.py --targets` builds the requisite files.

#### Targets:

1. `features`: reads raw data files, builds features, joins the files, and stores the columns of resulting dataframe in different files under `data/out`.
2. `model`: trains the voting models and saves them under `results/` directory. **RUNNING THIS TAKES A WHILE FOR THE FIRST TIME BECAUSE OF THE IMPORT STATEMENT. AFTER THAT IT'S INSTANTANEOUS.**
3. `predict`: loads the saved models, makes predictions, stores them under `results/` directory, and also stores the model recall values in a `.txt` file.
4. `test_rnn` test the rnn model on a subset of test files and save the output under `data/out`.
5. `test`: provides the functionality of `1` `2` `3` and `4` targets combined, on the test raw data.
6. `all`: provides the functionality of `1` `2` `3` and `4` targets combined, on the real raw data

### Directory Map

1. `config/`: contains the configuration `.json` files.

    a. `create-test-data-params.json`: config file to build test-data.
    
    b. `data-params.json`: config file for cleaning data in the future.
    
    c. `eda-params.json`: config file for any EDA figures that will be generated.
    
    d. `features-params.json`: config file for building features from raw data.

    e. `test-features-params.json`: config file for building features from test raw data.
    
    f. `model-params.json`: config file for model parameters.

    g. `test-model-params.json`: config file for test model parameters.
    
    h. `test-rnn-params.json`: config file for testing rnn model.

2. `data/`: contains the raw data files and data files after feature engineering. 

    a. `raw/`: contains the raw data files downloaded from source. _Does not include anything in the repo because raw data is confidential_. 
    
    b. `out/`: contains the data after feature generation.
    
3. `notebooks/`: notebooks with some EDA and experimentation.
4. `references/`: this contains acknowledgement for any models or results that we use to build our project off of.
5. `results/`: running the build script populates this directory with the trained models (`.pkl`) and a `.txt` file which outlines the model performance. The `actual/` directory has the results of our actual model. The 'test/' directory will be created on running the `test` target and will contain results of our model on the test data.
6. `src/`: contains all the script `.py` files.
    
    a. `features/`: `build_features.py` performs feature engineering on clean data and populates `data/out` with files that can be used to train and test the models.
    
    b. `models/`: `train_model.py` trains the voting models and saves them in a `.pkl` file ; `test_model.py` makes predictions using the saved models and saves the performance metric in a `.txt` file. Both files are stored in `results/` directory.
    
    c. `helper_functions.py`: library of functions that are used to perform common tasks.
    
7. `test/`: contains `testdata/` which has the artificially generated test data.
8. `dockerfile`: creates a container with the necessary libraries and packages to run all the scripts.
9. `run.py`: running this script builds the requisite files.
10. `submission.json`: contains the dockerhub-id for building the container and build-script command to build the targets.
"
138,https://github.com/chinkevin/DSC180_sleep_apnea,"{'chinkevin': 'https://github.com/chinkevin', 'shaheendane': 'https://github.com/shaheendane', 'YilanG08': 'https://github.com/YilanG08', 'alxjareyliu': 'https://github.com/alxjareyliu'}","{'Jupyter Notebook': 0.99, 'Python': 0.01}","# DSC180 Capstone Project

Sleep apnea is a sleep disorder where breathing starts and stops intermittenly. It can cause many issues while sleeping and even increases the risk of strokes and heart attacks. Traditionally, sleep research relies on human visual scoring. However with the advancement of machine learning, sleep research can be become a highly automated process. The purpose of this respoitory is to automatically classify sleep stages specifically for people with sleep apnea. Using signals from polysomnography data, such as EEG, EMG, EOG, and ECG, we can score sleep records using a Light Gradient Boosted Machine classifier into five stages: wake state, REM, N1, N2, and N3.

### Building the project stages using `run.py`

* To get the data, from the project root dir, run `python run.py data features`
  - This fetches the data, then creates features (defined in
    `src/features.py`) and saves them in the location specified in
    `features-params.json`.
* To include ECG features, run 'python run.py data features_ecg'
  - This builds the same features as before with additional ECG features.
* To build a model, from the project root dir, run `python run.py data
  features model`
  - This fetches the data, creates the features, then trains a lgbm classifier
    (with parameters specified in `config`).
* To predict and validate a model, from the project root dir, run `python run.py predict validate`
  - This runs the model on validation data, analyzes its performance, and creates visualizations.
","# DSC180 Capstone Project

Sleep apnea is a sleep disorder where breathing starts and stops intermittenly. It can cause many issues while sleeping and even increases the risk of strokes and heart attacks. Traditionally, sleep research relies on human visual scoring. However with the advancement of machine learning, sleep research can be become a highly automated process. The purpose of this respoitory is to automatically classify sleep stages specifically for people with sleep apnea. Using signals from polysomnography data, such as EEG, EMG, EOG, and ECG, we can score sleep records using a Light Gradient Boosted Machine classifier into five stages: wake state, REM, N1, N2, and N3.

### Building the project stages using `run.py`

* To get the data, from the project root dir, run `python run.py data features`
  - This fetches the data, then creates features (defined in
    `src/features.py`) and saves them in the location specified in
    `features-params.json`.
* To include ECG features, run 'python run.py data features_ecg'
  - This builds the same features as before with additional ECG features.
* To build a model, from the project root dir, run `python run.py data
  features model`
  - This fetches the data, creates the features, then trains a lgbm classifier
    (with parameters specified in `config`).
* To predict and validate a model, from the project root dir, run `python run.py predict validate`
  - This runs the model on validation data, analyzes its performance, and creates visualizations.
"
139,https://github.com/a2lu/CAPSTONE_WILDFIRE,"{'a2lu': 'https://github.com/a2lu', 'ant-chi': 'https://github.com/ant-chi', 'ojimenez2517': 'https://github.com/ojimenez2517', 'Jsingh-23': 'https://github.com/Jsingh-23'}","{'Jupyter Notebook': 0.97, 'Python': 0.03, 'Dockerfile': 0.0}","# CAPSTONE_WILDFIRE

## Usage
```
git clone https://github.com/a2lu/CAPSTONE_WILDFIRE.git
cd CAPSTONE_WILDFIRE
python run.py test
```","# CAPSTONE_WILDFIRE

## Usage
```
git clone https://github.com/a2lu/CAPSTONE_WILDFIRE.git
cd CAPSTONE_WILDFIRE
python run.py test
```"
140,https://github.com/LauraDiao/Anomaly_Detectives,"{'Ben243': 'https://github.com/Ben243', 'LauraDiao': 'https://github.com/LauraDiao', 'jenna-my': 'https://github.com/jenna-my'}","{'Jupyter Notebook': 1.0, 'Python': 0.0, 'Dockerfile': 0.0}","# Anomaly Detectives
An in depth approach to detecting significant real-time shifts in network performance indicating network degradation. Building on the data generation process behind [DANE](https://github.com/dane-tool/dane) and Viasat's [network stats](https://github.com/Viasat/network-stats), we build a classification system that determines if there are substantial changes to packet loss rate and degree of latency. Please visit [our webpage](https://lauradiao.github.io/Anomaly_Detectives) for a more comprehensive view of this project.

<br>

# Quick Links
- [Modified DANE](https://github.com/jenna-my/modified_dane)
- [network-stats](https://github.com/Viasat/network-stats)

<br>

## To generate data for this project:

1. Generate data using our [modified fork of DANE](https://github.com/jenna-my/modified_dane)
    - ```make```, ```docker.io```, and ```docker-compose``` are required on your machine to run modified_dane properly.
    - a recursive flag is required to properly install modified_dane: <br>```git clone https://github.com/jenna-my/modified_dane --recursive```

2. Clone this branch of the repository
   ```
   git clone https://github.com/LauraDiao/Anomaly_Detectives
   ```

3. Place all raw DANE csv files within the directory ```data/raw``` of this repository. If the directory has not been created, run the command ```run.py``` once to generate all relevant directories.

<br>

## To use this repository: 
Each of these targets implements a core feature of the repository within ```run.py```. All code can be executed with the run.py according to various targets specified below. <br>
Example call: ```python run.py data inference```
### Target List:
- ```data```: generates features from unseen and seen data
- ```eda```: Generates visualizations used in exploring which features to use for the model
- ```train```: prints results of model performance tested on training (""seen"") data with four different models with varying architectures: decision tree, random forest, extra trees, and gradient boost
- ```inference```: (deprecated) prints results of model performance tested on testing (""unseen"") data with the same exact models.
- ```clean```: Removes files generated by targets in commonly used output directories
- ```test```: Verifies target functionality by running the targets ```data```,```eda```, ```train```, and ```inference``` with a subset of the original model training data.
- ```all```: runs all targets except ```test```

<br><br>

Our modified version of DANE creates csv files with a naming scheme in the following format: 
> *datevalue*_*latency*-*loss*-*deterministic*-*laterlatency*-*laterloss*-iperf.csv

e.g. ```20220117T015822_200-100-true-200-10000-iperf.csv```

this format is crucial for the model to train on the proper labels.

## Configuration Files
### eda.json

- `lst`: [1, 2], # list of runs to compare side by side made by plottogether() inside of eda.py
- `filen1`: ""combined_subset_latency.csv"", - subset of the processed data to make eda
- `filen2`: ""combined_t_latency.csv"", - features generated from processed data
- `filen3`: ""combined_all_latency.csv"" - all processed - 

### model.json

- `n_jobs`: -1 - number of cores the model training is done on
- `train_window`: 20 - number of seconds that the model will aggregate on for training window size
- `pca_components`: 4 - number of components for PCA, we determined 4 was optimal for our model
- `test_size`: 0.005 - model validation set size (train _test_ split)
- `threshold`: -0.15 - threshold for loss anomaly detection
- `emplosswindow`: 25 - rolling window aggregation of empirical loss, set at 25 seconds
- `pct_change_window`: 2 - how many seconds the anomaly detection system looks back for determining change.
- `verbose`: ""True"" - whether terminal output should be verbose or not. For debugging purposes.
 

","# Anomaly Detectives
An in depth approach to detecting significant real-time shifts in network performance indicating network degradation. Building on the data generation process behind [DANE](https://github.com/dane-tool/dane) and Viasat's [network stats](https://github.com/Viasat/network-stats), we build a classification system that determines if there are substantial changes to packet loss rate and degree of latency. Please visit [our webpage](https://lauradiao.github.io/Anomaly_Detectives) for a more comprehensive view of this project.

<br>

# Quick Links
- [Modified DANE](https://github.com/jenna-my/modified_dane)
- [network-stats](https://github.com/Viasat/network-stats)

<br>

## To generate data for this project:

1. Generate data using our [modified fork of DANE](https://github.com/jenna-my/modified_dane)
    - ```make```, ```docker.io```, and ```docker-compose``` are required on your machine to run modified_dane properly.
    - a recursive flag is required to properly install modified_dane: <br>```git clone https://github.com/jenna-my/modified_dane --recursive```

2. Clone this branch of the repository
   ```
   git clone https://github.com/LauraDiao/Anomaly_Detectives
   ```

3. Place all raw DANE csv files within the directory ```data/raw``` of this repository. If the directory has not been created, run the command ```run.py``` once to generate all relevant directories.

<br>

## To use this repository: 
Each of these targets implements a core feature of the repository within ```run.py```. All code can be executed with the run.py according to various targets specified below. <br>
Example call: ```python run.py data inference```
### Target List:
- ```data```: generates features from unseen and seen data
- ```eda```: Generates visualizations used in exploring which features to use for the model
- ```train```: prints results of model performance tested on training (""seen"") data with four different models with varying architectures: decision tree, random forest, extra trees, and gradient boost
- ```inference```: (deprecated) prints results of model performance tested on testing (""unseen"") data with the same exact models.
- ```clean```: Removes files generated by targets in commonly used output directories
- ```test```: Verifies target functionality by running the targets ```data```,```eda```, ```train```, and ```inference``` with a subset of the original model training data.
- ```all```: runs all targets except ```test```

<br><br>

Our modified version of DANE creates csv files with a naming scheme in the following format: 
> *datevalue*_*latency*-*loss*-*deterministic*-*laterlatency*-*laterloss*-iperf.csv

e.g. ```20220117T015822_200-100-true-200-10000-iperf.csv```

this format is crucial for the model to train on the proper labels.

## Configuration Files
### eda.json

- `lst`: [1, 2], # list of runs to compare side by side made by plottogether() inside of eda.py
- `filen1`: ""combined_subset_latency.csv"", - subset of the processed data to make eda
- `filen2`: ""combined_t_latency.csv"", - features generated from processed data
- `filen3`: ""combined_all_latency.csv"" - all processed - 

### model.json

- `n_jobs`: -1 - number of cores the model training is done on
- `train_window`: 20 - number of seconds that the model will aggregate on for training window size
- `pca_components`: 4 - number of components for PCA, we determined 4 was optimal for our model
- `test_size`: 0.005 - model validation set size (train _test_ split)
- `threshold`: -0.15 - threshold for loss anomaly detection
- `emplosswindow`: 25 - rolling window aggregation of empirical loss, set at 25 seconds
- `pct_change_window`: 2 - how many seconds the anomaly detection system looks back for determining change.
- `verbose`: ""True"" - whether terminal output should be verbose or not. For debugging purposes.
 

"
141,https://github.com/tatummaston/anomaly_network_detection,"{'jjharsono1': 'https://github.com/jjharsono1', 'tatummaston': 'https://github.com/tatummaston', 'ctran0615': 'https://github.com/ctran0615'}","{'Python': 0.98, 'HTML': 0.01, 'Dockerfile': 0.01, 'Jupyter Notebook': 0.0}",# anomoly_network_detection,# anomoly_network_detection
142,https://github.com/arjunsawhney1/intel-capstone-project,"{'arjunsawhney1': 'https://github.com/arjunsawhney1', 'AndrewChinGitHub': 'https://github.com/AndrewChinGitHub', 'SrikarPrayaga06': 'https://github.com/SrikarPrayaga06'}","{'Jupyter Notebook': 0.98, 'C': 0.01, 'Python': 0.0, 'Batchfile': 0.0, 'Dockerfile': 0.0}","# Intel Telemetry: Data Collection & Time-Series Prediction of App Usage
## Abstract
Despite advancements in hardware technology, PC users continue to face frustrating app launch times, especially on lower end Windows machines. The desktop experience differs vastly from the instantaneous app launches and optimized experience we have come to expect even from low end smartphones. We propose a solution to preemptively run Windows apps in the background based on the app usage patterns of the user. 

Our solution is two-step. First, we built telemetry collector modules in C/C++ to collect real-world app usage data from two of our personal Windows 10 devices. Next, we developed neural network models, trained on the collected data, to predict app usage times and corresponding launch sequences in python. We achieved impressive results on selected evaluation metrics across different user profiles. 

## Usage
Due to the nature of our project, we have two distinct predictive tasks.

The project pipeline for our HMM model may be run as follows:
```
launch-scipy-ml.sh -i arjunsawhney1/intel-telemetry:latest
git clone git@github.com:arjunsawhney1/intel-capstone-project.git
cd intel-capstone-project/src/models/HMM
python run.py
```

The project pipeline for our LSTM model may be run as follows:
```
launch-scipy-ml.sh -i arjunsawhney1/intel-telemetry:latest
git clone git@github.com:arjunsawhney1/intel-capstone-project.git
cd intel-capstone-project/src/models/LSTM
python run.py
```

Outputs for both models can be located in the outputs folder.

## Project Website
https://arjunsawhney1.github.io/intel-capstone-project/

## Docker Image
arjunsawhney1/intel-telemetry:latest
","# Intel Telemetry: Data Collection & Time-Series Prediction of App Usage
## Abstract
Despite advancements in hardware technology, PC users continue to face frustrating app launch times, especially on lower end Windows machines. The desktop experience differs vastly from the instantaneous app launches and optimized experience we have come to expect even from low end smartphones. We propose a solution to preemptively run Windows apps in the background based on the app usage patterns of the user. 

Our solution is two-step. First, we built telemetry collector modules in C/C++ to collect real-world app usage data from two of our personal Windows 10 devices. Next, we developed neural network models, trained on the collected data, to predict app usage times and corresponding launch sequences in python. We achieved impressive results on selected evaluation metrics across different user profiles. 

## Usage
Due to the nature of our project, we have two distinct predictive tasks.

The project pipeline for our HMM model may be run as follows:
```
launch-scipy-ml.sh -i arjunsawhney1/intel-telemetry:latest
git clone git@github.com:arjunsawhney1/intel-capstone-project.git
cd intel-capstone-project/src/models/HMM
python run.py
```

The project pipeline for our LSTM model may be run as follows:
```
launch-scipy-ml.sh -i arjunsawhney1/intel-telemetry:latest
git clone git@github.com:arjunsawhney1/intel-capstone-project.git
cd intel-capstone-project/src/models/LSTM
python run.py
```

Outputs for both models can be located in the outputs folder.

## Project Website
https://arjunsawhney1.github.io/intel-capstone-project/

## Docker Image
arjunsawhney1/intel-telemetry:latest
"
143,https://github.com/andydo1998/dsc180-data-analysis,"{'andydo1998': 'https://github.com/andydo1998', 'SasamiScott': 'https://github.com/SasamiScott'}","{'Jupyter Notebook': 1.0, 'Python': 0.0}","# DSC 180 Section B14 Project

## Project Introduction
In an effort to reduce app wait time, the time it takes for an application to launch, we collected data on application use and app wait time for a single user over several weeks. With this data we plan to build a series of models to predict which application a user will open with an emphasis on when and for how long. The focus is currently on our foreground app data which shows us which app is in the foreground of the user’s computer with timestamp. Using this data, we created a single chain Hidden Markov Model (HMM) to predict the next app the user opens based on their current one. With not enough amount of layers, we implemented a Long Short-Term Memory (LSTM) model to predict the amount of time a user will use an application.

## Overview
We collected foreground windows with our data collection library for 2 months on a Windows laptop. The Jupyter Notebook contains the code to read in and combine each database file, data cleaning/preprocessing, the HMM model, and the LSTM model. The run.py file is a streamline version of our notebook that reads in the files, creates a Hidden Markov Model, and outputs prediction on every unique application that was present in our data collection.

## How to Use
1. Pull the repo to obtain all necessary files to run test
2. With a terminal, navigate onto overarching folder (dsc180-data-analysis)
3. Run the command: 
```
python run.py test
```
4. When finished, the terminal should report an accuracy of the model and outputs all possible predictions onto outputs/outputs.txt

## More Information
For more information, please read our report, the pdf file on the repository, for a more in depth explanation of the process.

A visual presentation can also be viewed here: https://www.youtube.com/watch?v=2h5k6alz3WU

(note that to retrieve the most amount of information, please view the report, as the video only provides a summary of our process)
","# DSC 180 Section B14 Project

## Project Introduction
In an effort to reduce app wait time, the time it takes for an application to launch, we collected data on application use and app wait time for a single user over several weeks. With this data we plan to build a series of models to predict which application a user will open with an emphasis on when and for how long. The focus is currently on our foreground app data which shows us which app is in the foreground of the user’s computer with timestamp. Using this data, we created a single chain Hidden Markov Model (HMM) to predict the next app the user opens based on their current one. With not enough amount of layers, we implemented a Long Short-Term Memory (LSTM) model to predict the amount of time a user will use an application.

## Overview
We collected foreground windows with our data collection library for 2 months on a Windows laptop. The Jupyter Notebook contains the code to read in and combine each database file, data cleaning/preprocessing, the HMM model, and the LSTM model. The run.py file is a streamline version of our notebook that reads in the files, creates a Hidden Markov Model, and outputs prediction on every unique application that was present in our data collection.

## How to Use
1. Pull the repo to obtain all necessary files to run test
2. With a terminal, navigate onto overarching folder (dsc180-data-analysis)
3. Run the command: 
```
python run.py test
```
4. When finished, the terminal should report an accuracy of the model and outputs all possible predictions onto outputs/outputs.txt

## More Information
For more information, please read our report, the pdf file on the repository, for a more in depth explanation of the process.

A visual presentation can also be viewed here: https://www.youtube.com/watch?v=2h5k6alz3WU

(note that to retrieve the most amount of information, please view the report, as the video only provides a summary of our process)
"
144,https://github.com/cgorlla/intel-sur,"{'cgorlla': 'https://github.com/cgorlla', 'jared8thach': 'https://github.com/jared8thach'}","{'Jupyter Notebook': 0.94, 'Python': 0.06, 'Dockerfile': 0.0}","# INTELli*next*: A Fully Integrated LSTM and HMM-Based Solution for Next-App Prediction With Intel SUR SDK Data Collection
# Intel DCA x HDSI UCSD System Usage Reporting Research

Cyril Gorlla, Jared Thach, Hiroki Hoshida. [INTELli*next*: A Fully Integrated LSTM and HMM-Based Solution for Next-App Prediction With Intel SUR SDK Data Collection.](https://github.com/cgorlla/intel-capstone-submission/blob/main/report.pdf) *Halıcıoğlu Data Science Institute Capstone Showcase, March 11, 2022*

As the power of modern computing devices increases, so too do user expectations for them. Despite advancements in technology, computer users are often faced with the dreaded spinning icon waiting for an application to load. Building upon our previous work developing data collectors with the Intel System Usage Reporting (SUR) SDK, we introduce INTELli*next*, a comprehensive solution for next-app prediction for application preload to improve perceived system fluidity. We develop a Hidden Markov Model (HMM) for prediction of the k most likely next apps, achieving an accuracy of 70% when k = 3. We then implement a long short-term memory (LSTM) model to predict the total duration that applications will be used. After hyperparameter optimization leading to an optimal lookback value of 5 previous applications, we are able to predict the usage time of a given application with a mean absolute error of ~45 seconds. Our work constitutes a promising comprehensive application preload solution with data collection based on the Intel SUR SDK and prediction with machine learning.


This repository contains the code for our research at UCSD on predicting PC user behavior in collaboration with Intel Corporation.

You can read about the development of the Input Libraries that collected the data used to predict in the paper below.

Cyril Gorlla, Jared Thach, Hiroki Hoshida. Development of Input Libraries With Intel XLSDK to Capture Data for App Start Prediction. 2022. ⟨[hal-03527679](https://hal.archives-ouvertes.fr/hal-03527679)⟩

## Repository Overview
- `config\`: contains configuration files for various scripts, such as data and output locations
- `notebooks\`: contains EDA with visualizations and other helpful Jupyter Notebooks to better understand the data
- `src\`: contains the main data loading, analysis, and model building scripts
- `main.py`: Python script to execute data parsing, data cleaning, training, and testing

## `run.py`
This Python file contains the necessary code to parse and clean data from the Input Libraries detailed in the above paper, as well as to build the models in the project. These include:
- First Order Hidden Markov Model for Next-App Prediction
- LSTM for Next-App Prediction
- LSTM for App Duration Prediction

### Building `run.py`
To run: `python run.py {data} {analysis} {model}`

To just build the model: `python run.py data model`

To test: `python run.py test` 

This will load in test data in `test\testdata` and build the HMM and LSTM prediction models off of it. The predictions of the test model will be stored in `data\out\test_{model}.csv`, which you may verify against the provided files named `prov_{model}.csv` to ensure the model is functioning as expected. Note that due to the inherently probabilistic nature of the models your outputs may not be the same as the provided files, but they provide a sanity check.

## `src\model\model.py`

This file contains the:

- First order Hidden Markov model classfor predicting future foreground applications. After splitting the data and fitting the training set to a `first_order_HMM` instance using `fit`, the model keeps track of the prior and posterior probabilities of the training set's foreground applications. When inputting an observation, `X`, to `predict`, the function returns a list of foregrounds, (of size `n_foregrounds`, with default value of 1) with the highest conditional probability given `X`'s inputted foreground application and the trained model's posterior probabilities. `accuracy` returns the accuracy of the `y_test` on `y_pred` by taking each true foreground application in `y_test` and checking whether or not it appears in its respective list of foregrounds in `y_pred`.

- The next-app prediction LSTM model using a “look-back” value of one previous foreground application in order to predict one future foreground application, where a “look-back” is defined as the number of previous events a single input will use in order to generate the next output

- The duration prediction LSTM using a look-back value of five. In other words, the model uses the previous five data points to predict the next. 

Both LSTM models' architecture is similar, with the four layers in the same order. 



## Docker
A dockerfile is included and will create a Docker environment that allows for the successful execution of all code in this repository.
","# INTELli*next*: A Fully Integrated LSTM and HMM-Based Solution for Next-App Prediction With Intel SUR SDK Data Collection
# Intel DCA x HDSI UCSD System Usage Reporting Research

Cyril Gorlla, Jared Thach, Hiroki Hoshida. [INTELli*next*: A Fully Integrated LSTM and HMM-Based Solution for Next-App Prediction With Intel SUR SDK Data Collection.](https://github.com/cgorlla/intel-capstone-submission/blob/main/report.pdf) *Halıcıoğlu Data Science Institute Capstone Showcase, March 11, 2022*

As the power of modern computing devices increases, so too do user expectations for them. Despite advancements in technology, computer users are often faced with the dreaded spinning icon waiting for an application to load. Building upon our previous work developing data collectors with the Intel System Usage Reporting (SUR) SDK, we introduce INTELli*next*, a comprehensive solution for next-app prediction for application preload to improve perceived system fluidity. We develop a Hidden Markov Model (HMM) for prediction of the k most likely next apps, achieving an accuracy of 70% when k = 3. We then implement a long short-term memory (LSTM) model to predict the total duration that applications will be used. After hyperparameter optimization leading to an optimal lookback value of 5 previous applications, we are able to predict the usage time of a given application with a mean absolute error of ~45 seconds. Our work constitutes a promising comprehensive application preload solution with data collection based on the Intel SUR SDK and prediction with machine learning.


This repository contains the code for our research at UCSD on predicting PC user behavior in collaboration with Intel Corporation.

You can read about the development of the Input Libraries that collected the data used to predict in the paper below.

Cyril Gorlla, Jared Thach, Hiroki Hoshida. Development of Input Libraries With Intel XLSDK to Capture Data for App Start Prediction. 2022. ⟨[hal-03527679](https://hal.archives-ouvertes.fr/hal-03527679)⟩

## Repository Overview
- `config\`: contains configuration files for various scripts, such as data and output locations
- `notebooks\`: contains EDA with visualizations and other helpful Jupyter Notebooks to better understand the data
- `src\`: contains the main data loading, analysis, and model building scripts
- `main.py`: Python script to execute data parsing, data cleaning, training, and testing

## `run.py`
This Python file contains the necessary code to parse and clean data from the Input Libraries detailed in the above paper, as well as to build the models in the project. These include:
- First Order Hidden Markov Model for Next-App Prediction
- LSTM for Next-App Prediction
- LSTM for App Duration Prediction

### Building `run.py`
To run: `python run.py {data} {analysis} {model}`

To just build the model: `python run.py data model`

To test: `python run.py test` 

This will load in test data in `test\testdata` and build the HMM and LSTM prediction models off of it. The predictions of the test model will be stored in `data\out\test_{model}.csv`, which you may verify against the provided files named `prov_{model}.csv` to ensure the model is functioning as expected. Note that due to the inherently probabilistic nature of the models your outputs may not be the same as the provided files, but they provide a sanity check.

## `src\model\model.py`

This file contains the:

- First order Hidden Markov model classfor predicting future foreground applications. After splitting the data and fitting the training set to a `first_order_HMM` instance using `fit`, the model keeps track of the prior and posterior probabilities of the training set's foreground applications. When inputting an observation, `X`, to `predict`, the function returns a list of foregrounds, (of size `n_foregrounds`, with default value of 1) with the highest conditional probability given `X`'s inputted foreground application and the trained model's posterior probabilities. `accuracy` returns the accuracy of the `y_test` on `y_pred` by taking each true foreground application in `y_test` and checking whether or not it appears in its respective list of foregrounds in `y_pred`.

- The next-app prediction LSTM model using a “look-back” value of one previous foreground application in order to predict one future foreground application, where a “look-back” is defined as the number of previous events a single input will use in order to generate the next output

- The duration prediction LSTM using a look-back value of five. In other words, the model uses the previous five data points to predict the next. 

Both LSTM models' architecture is similar, with the four layers in the same order. 



## Docker
A dockerfile is included and will create a Docker environment that allows for the successful execution of all code in this repository.
"
145,https://github.com/wolftossH/DSC--180AB-escrow,"{'wolftossH': 'https://github.com/wolftossH', 'aliriasa': 'https://github.com/aliriasa'}","{'HTML': 0.66, 'JavaScript': 0.21, 'Solidity': 0.08, 'CSS': 0.05}","# DSC--180AB-escrow
<!-- Improved compatibility of back to top link: See: https://github.com/othneildrew/Best-README-Template/pull/73 -->
<a name=""readme-top""></a>
<!--
*** Thanks for checking out the Best-README-Template. If you have a suggestion
*** that would make this better, please fork the repo and create a pull request
*** or simply open an issue with the tag ""enhancement"".
*** Don't forget to give the project a star!
*** Thanks again! Now go create something AMAZING! :D
-->



<!-- PROJECT SHIELDS -->
<!--
*** I'm using markdown ""reference style"" links for readability.
*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).
*** See the bottom of this document for the declaration of the reference variables
*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.
*** https://www.markdownguide.org/basic-syntax/#reference-style-links
-->
[![Contributors][contributors-shield]][contributors-url]
[![MIT License][license-shield]][license-url]
<!-- [![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url] -->



<!-- PROJECT LOGO -->
<br />
<div align=""center"">
  <a href=""https://github.com/othneildrew/Best-README-Template"">
    <img src=""new_client/images/final_logo.png"" alt=""Logo"" width=""200"" height=""200"">
  </a>

  <h3 align=""center"">Best-README-Template</h3>

  <p align=""center"">
    An awesome README template to jumpstart your projects!
    <br />
    <a href=""https://github.com/othneildrew/Best-README-Template""><strong>Explore the docs »</strong></a>
    <br />
    <br />
    <a href=""https://escryptow.net/"">View Webiste</a>

  </p>
</div>



<!-- TABLE OF CONTENTS -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href=""#about-the-project"">About The Project</a>
      <ul>
        <li><a href=""#built-with"">Built With</a></li>
      </ul>
    </li>
    <li>
      <a href=""#getting-started"">Getting Started</a>
      <ul>
        <li><a href=""#prerequisites"">Prerequisites</a></li>
        <li><a href=""#installation"">Installation</a></li>
      </ul>
    </li>
    <li><a href=""#usage"">Usage</a></li>
    <li><a href=""#roadmap"">Roadmap</a></li>
    <li><a href=""#contributors-and-contact"">Contributors and Contact</a></li>
    <li><a href=""#acknowledgments"">Acknowledgments</a></li>
  </ol>
</details>



<!-- ABOUT THE PROJECT -->
## About The Project

[![Product Name Screen Shot][product-screenshot]](https://example.com)



<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



### Built With
<p>

[![React][React.js]][React-url] \
[![Node][Node.js]][Node-url] \
[![Solidity][solidity]][solidity-url] \
[![Vite][vite]][vite-url] \
[![Tailwind][tailwind]][tailwind-url] \
[![Hardhat][hardhat]][hardhat-url] \
![HTML][html] \
![CSS][css]
<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>

<!-- GETTING STARTED -->
## Getting Started

This is an example of how you can use the website

### Prerequisites

This is an example of how to list things you need to use the software and how to install them.


<!-- USAGE EXAMPLES -->
## Usage

Escrow shop for all users
<div align=""center"">
  <a href=""https://github.com/othneildrew/Best-README-Template"">
    <img src=""images/cart.png"" alt=""Logo"" width=""500"" height=""300"">
  </a>
</div>



<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



<!-- ROADMAP -->
## Roadmap

- [x] Design logo
- [x] Added ipfs


<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



<!-- LICENSE -->
## License

Distributed under the ....... License. See `LICENSE.md` for more information.

<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



<!-- CONTACT -->
## Contributors and Contact

Huy Trinh - [![LinkedIn][linkedin-shield]][linkedin-url-huy]

Antoni Liria-Sala - [![LinkedIn][linkedin-shield]][linkedin-url-antoni]

William Li - [![LinkedIn][linkedin-shield]][linkedin-url-william] 

Guangyu Yang - [![LinkedIn][linkedin-shield]][linkedin-url-irvin] 



<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



<!-- ACKNOWLEDGMENTS -->
## Acknowledgments

Use this space to list resources you find helpful and would like to give credit to. I've included a few of my favorites to kick things off!

* [Choose an Open Source License](https://choosealicense.com)
* [Img Shields](https://shields.io)
* [Font Awesome](https://fontawesome.com)
* [React Icons](https://react-icons.github.io/react-icons/search)

<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



<!-- MARKDOWN LINKS & IMAGES -->
<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->
[contributors-shield]: https://img.shields.io/github/contributors/wolftossH/DSC--180AB-escrow.svg?style=for-the-badge
[contributors-url]: https://github.com/wolftossH/DSC--180AB-escrow/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/othneildrew/Best-README-Template.svg?style=for-the-badge
[forks-url]: https://github.com/othneildrew/Best-README-Template/network/members
[stars-shield]: https://img.shields.io/github/stars/othneildrew/Best-README-Template.svg?style=for-the-badge
[stars-url]: https://github.com/othneildrew/Best-README-Template/stargazers
[issues-shield]: https://img.shields.io/github/issues/othneildrew/Best-README-Template.svg?style=for-the-badge
[issues-url]: https://github.com/othneildrew/Best-README-Template/issues
[license-shield]: https://img.shields.io/github/license/othneildrew/Best-README-Template.svg?style=for-the-badge
[license-url]: https://github.com/othneildrew/Best-README-Template/blob/master/LICENSE.txt
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555

[linkedin-url]: https://www.linkedin.com/feed/
[linkedin-url-huy]: https://www.linkedin.com/in/huy-trinh-9868ba194
[linkedin-url-antoni]: https://www.linkedin.com/in/antoniliriasala/
[linkedin-url-william]: https://www.linkedin.com/in/tianyangwillli/
[linkedin-url-irvin]: https://www.linkedin.com/in/irvinyang/

[product-screenshot]: images/website_main_pic.jpg
[Next.js]: https://img.shields.io/badge/next.js-000000?style=for-the-badge&logo=nextdotjs&logoColor=white
[Next-url]: https://nextjs.org/
[React.js]: https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB
[React-url]: https://reactjs.org/
[Vue.js]: https://img.shields.io/badge/Vue.js-35495E?style=for-the-badge&logo=vuedotjs&logoColor=4FC08D
[Vue-url]: https://vuejs.org/
[Angular.io]: https://img.shields.io/badge/Angular-DD0031?style=for-the-badge&logo=angular&logoColor=white
[Angular-url]: https://angular.io/
[Svelte.dev]: https://img.shields.io/badge/Svelte-4A4A55?style=for-the-badge&logo=svelte&logoColor=FF3E00
[Svelte-url]: https://svelte.dev/
[Laravel.com]: https://img.shields.io/badge/Laravel-FF2D20?style=for-the-badge&logo=laravel&logoColor=white
[Laravel-url]: https://laravel.com
[Bootstrap.com]: https://img.shields.io/badge/Bootstrap-563D7C?style=for-the-badge&logo=bootstrap&logoColor=white
[Bootstrap-url]: https://getbootstrap.com
[JQuery.com]: https://img.shields.io/badge/jQuery-0769AD?style=for-the-badge&logo=jquery&logoColor=white
[JQuery-url]: https://jquery.com 
[html]: 	https://img.shields.io/badge/HTML5-E34F26?style=for-the-badge&logo=html5&logoColor=white
[css]: https://img.shields.io/badge/CSS3-1572B6?style=for-the-badge&logo=css3&logoColor=white
[solidity]: https://img.shields.io/badge/Solidity-e6e6e6?style=for-the-badge&logo=solidity&logoColor=black
[solidity-url]: https://soliditylang.org/
[vite]: https://img.shields.io/badge/Vite-B73BFE?style=for-the-badge&logo=vite&logoColor=FFD62E
[vite-url]: https://vitejs.dev/

[tailwind]: https://img.shields.io/badge/Tailwind_CSS-38B2AC?style=for-the-badge&logo=tailwind-css&logoColor=white
[tailwind-url]: https://tailwindcss.com/

[Node.js]: https://img.shields.io/badge/Node.js-339933?style=for-the-badge&logo=nodedotjs&logoColor=white
[node-url]: https://nodejs.org/en/

[hardhat]: https://hardhat.org/_next/static/media/hardhat-logo.5c5f687b.svg
[hardhat-url]: https://hardhat.org/
","# DSC--180AB-escrow
<!-- Improved compatibility of back to top link: See: https://github.com/othneildrew/Best-README-Template/pull/73 -->
<a name=""readme-top""></a>
<!--
*** Thanks for checking out the Best-README-Template. If you have a suggestion
*** that would make this better, please fork the repo and create a pull request
*** or simply open an issue with the tag ""enhancement"".
*** Don't forget to give the project a star!
*** Thanks again! Now go create something AMAZING! :D
-->



<!-- PROJECT SHIELDS -->
<!--
*** I'm using markdown ""reference style"" links for readability.
*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).
*** See the bottom of this document for the declaration of the reference variables
*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.
*** https://www.markdownguide.org/basic-syntax/#reference-style-links
-->
[![Contributors][contributors-shield]][contributors-url]
[![MIT License][license-shield]][license-url]
<!-- [![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url] -->



<!-- PROJECT LOGO -->
<br />
<div align=""center"">
  <a href=""https://github.com/othneildrew/Best-README-Template"">
    <img src=""new_client/images/final_logo.png"" alt=""Logo"" width=""200"" height=""200"">
  </a>

  <h3 align=""center"">Best-README-Template</h3>

  <p align=""center"">
    An awesome README template to jumpstart your projects!
    <br />
    <a href=""https://github.com/othneildrew/Best-README-Template""><strong>Explore the docs »</strong></a>
    <br />
    <br />
    <a href=""https://escryptow.net/"">View Webiste</a>

  </p>
</div>



<!-- TABLE OF CONTENTS -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href=""#about-the-project"">About The Project</a>
      <ul>
        <li><a href=""#built-with"">Built With</a></li>
      </ul>
    </li>
    <li>
      <a href=""#getting-started"">Getting Started</a>
      <ul>
        <li><a href=""#prerequisites"">Prerequisites</a></li>
        <li><a href=""#installation"">Installation</a></li>
      </ul>
    </li>
    <li><a href=""#usage"">Usage</a></li>
    <li><a href=""#roadmap"">Roadmap</a></li>
    <li><a href=""#contributors-and-contact"">Contributors and Contact</a></li>
    <li><a href=""#acknowledgments"">Acknowledgments</a></li>
  </ol>
</details>



<!-- ABOUT THE PROJECT -->
## About The Project

[![Product Name Screen Shot][product-screenshot]](https://example.com)



<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



### Built With
<p>

[![React][React.js]][React-url] \
[![Node][Node.js]][Node-url] \
[![Solidity][solidity]][solidity-url] \
[![Vite][vite]][vite-url] \
[![Tailwind][tailwind]][tailwind-url] \
[![Hardhat][hardhat]][hardhat-url] \
![HTML][html] \
![CSS][css]
<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>

<!-- GETTING STARTED -->
## Getting Started

This is an example of how you can use the website

### Prerequisites

This is an example of how to list things you need to use the software and how to install them.


<!-- USAGE EXAMPLES -->
## Usage

Escrow shop for all users
<div align=""center"">
  <a href=""https://github.com/othneildrew/Best-README-Template"">
    <img src=""images/cart.png"" alt=""Logo"" width=""500"" height=""300"">
  </a>
</div>



<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



<!-- ROADMAP -->
## Roadmap

- [x] Design logo
- [x] Added ipfs


<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



<!-- LICENSE -->
## License

Distributed under the ....... License. See `LICENSE.md` for more information.

<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



<!-- CONTACT -->
## Contributors and Contact

Huy Trinh - [![LinkedIn][linkedin-shield]][linkedin-url-huy]

Antoni Liria-Sala - [![LinkedIn][linkedin-shield]][linkedin-url-antoni]

William Li - [![LinkedIn][linkedin-shield]][linkedin-url-william] 

Guangyu Yang - [![LinkedIn][linkedin-shield]][linkedin-url-irvin] 



<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



<!-- ACKNOWLEDGMENTS -->
## Acknowledgments

Use this space to list resources you find helpful and would like to give credit to. I've included a few of my favorites to kick things off!

* [Choose an Open Source License](https://choosealicense.com)
* [Img Shields](https://shields.io)
* [Font Awesome](https://fontawesome.com)
* [React Icons](https://react-icons.github.io/react-icons/search)

<p align=""right"">(<a href=""#readme-top"">back to top</a>)</p>



<!-- MARKDOWN LINKS & IMAGES -->
<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->
[contributors-shield]: https://img.shields.io/github/contributors/wolftossH/DSC--180AB-escrow.svg?style=for-the-badge
[contributors-url]: https://github.com/wolftossH/DSC--180AB-escrow/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/othneildrew/Best-README-Template.svg?style=for-the-badge
[forks-url]: https://github.com/othneildrew/Best-README-Template/network/members
[stars-shield]: https://img.shields.io/github/stars/othneildrew/Best-README-Template.svg?style=for-the-badge
[stars-url]: https://github.com/othneildrew/Best-README-Template/stargazers
[issues-shield]: https://img.shields.io/github/issues/othneildrew/Best-README-Template.svg?style=for-the-badge
[issues-url]: https://github.com/othneildrew/Best-README-Template/issues
[license-shield]: https://img.shields.io/github/license/othneildrew/Best-README-Template.svg?style=for-the-badge
[license-url]: https://github.com/othneildrew/Best-README-Template/blob/master/LICENSE.txt
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555

[linkedin-url]: https://www.linkedin.com/feed/
[linkedin-url-huy]: https://www.linkedin.com/in/huy-trinh-9868ba194
[linkedin-url-antoni]: https://www.linkedin.com/in/antoniliriasala/
[linkedin-url-william]: https://www.linkedin.com/in/tianyangwillli/
[linkedin-url-irvin]: https://www.linkedin.com/in/irvinyang/

[product-screenshot]: images/website_main_pic.jpg
[Next.js]: https://img.shields.io/badge/next.js-000000?style=for-the-badge&logo=nextdotjs&logoColor=white
[Next-url]: https://nextjs.org/
[React.js]: https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB
[React-url]: https://reactjs.org/
[Vue.js]: https://img.shields.io/badge/Vue.js-35495E?style=for-the-badge&logo=vuedotjs&logoColor=4FC08D
[Vue-url]: https://vuejs.org/
[Angular.io]: https://img.shields.io/badge/Angular-DD0031?style=for-the-badge&logo=angular&logoColor=white
[Angular-url]: https://angular.io/
[Svelte.dev]: https://img.shields.io/badge/Svelte-4A4A55?style=for-the-badge&logo=svelte&logoColor=FF3E00
[Svelte-url]: https://svelte.dev/
[Laravel.com]: https://img.shields.io/badge/Laravel-FF2D20?style=for-the-badge&logo=laravel&logoColor=white
[Laravel-url]: https://laravel.com
[Bootstrap.com]: https://img.shields.io/badge/Bootstrap-563D7C?style=for-the-badge&logo=bootstrap&logoColor=white
[Bootstrap-url]: https://getbootstrap.com
[JQuery.com]: https://img.shields.io/badge/jQuery-0769AD?style=for-the-badge&logo=jquery&logoColor=white
[JQuery-url]: https://jquery.com 
[html]: 	https://img.shields.io/badge/HTML5-E34F26?style=for-the-badge&logo=html5&logoColor=white
[css]: https://img.shields.io/badge/CSS3-1572B6?style=for-the-badge&logo=css3&logoColor=white
[solidity]: https://img.shields.io/badge/Solidity-e6e6e6?style=for-the-badge&logo=solidity&logoColor=black
[solidity-url]: https://soliditylang.org/
[vite]: https://img.shields.io/badge/Vite-B73BFE?style=for-the-badge&logo=vite&logoColor=FFD62E
[vite-url]: https://vitejs.dev/

[tailwind]: https://img.shields.io/badge/Tailwind_CSS-38B2AC?style=for-the-badge&logo=tailwind-css&logoColor=white
[tailwind-url]: https://tailwindcss.com/

[Node.js]: https://img.shields.io/badge/Node.js-339933?style=for-the-badge&logo=nodedotjs&logoColor=white
[node-url]: https://nodejs.org/en/

[hardhat]: https://hardhat.org/_next/static/media/hardhat-logo.5c5f687b.svg
[hardhat-url]: https://hardhat.org/
"
146,https://github.com/matin-g/Q2-DSC180B-A02,"{'YuHuang0525': 'https://github.com/YuHuang0525', 'matin-g': 'https://github.com/matin-g', 'WenyuanChen1326': 'https://github.com/WenyuanChen1326'}","{'HTML': 0.34, 'CSS': 0.33, 'SCSS': 0.29, 'JavaScript': 0.03, 'Solidity': 0.01}","# Most up to date web app code in this folder --> front-end-webApp
# Most up to date contract code is final_purchase.sol

<br>

# Q2 capstone project 
A decentralized exchange via ethereum smart contracts intergrated within a website (Dapp) in order to create a decentralized peer-to-peer ecommerce platform. Please refer to our [report](https://github.com/matin-g/DSC180a-Q1-final-code/blob/main/report.pdf) (note: the future work section explains what we are doing now in this repository)

# Please see Q1 code and report here: 
https://github.com/matin-g/DSC180a-Q1-final-code


# Instruction to run on local machine
After pulling all the codes within front-end folder to your local machine ---

Make sure you have node.js (npm) installed (https://nodejs.org/en/download/)
install express package: open up terminal, input ""npm install express --save""
cd into the front-end folder, and input ""node server.js"", and you will see ""port is open on 8082""
open chrome - ""http://127.0.0.1:8082/""
","# Most up to date web app code in this folder --> front-end-webApp
# Most up to date contract code is final_purchase.sol

<br>

# Q2 capstone project 
A decentralized exchange via ethereum smart contracts intergrated within a website (Dapp) in order to create a decentralized peer-to-peer ecommerce platform. Please refer to our [report](https://github.com/matin-g/DSC180a-Q1-final-code/blob/main/report.pdf) (note: the future work section explains what we are doing now in this repository)

# Please see Q1 code and report here: 
https://github.com/matin-g/DSC180a-Q1-final-code


# Instruction to run on local machine
After pulling all the codes within front-end folder to your local machine ---

Make sure you have node.js (npm) installed (https://nodejs.org/en/download/)
install express package: open up terminal, input ""npm install express --save""
cd into the front-end folder, and input ""node server.js"", and you will see ""port is open on 8082""
open chrome - ""http://127.0.0.1:8082/""
"
147,https://github.com/crvander/capstoneproj2023,"{'crvander': 'https://github.com/crvander', 'TIMHX': 'https://github.com/TIMHX'}","{'Python': 0.83, 'TeX': 0.16, 'Dockerfile': 0.01}","<h1 align=""center"">
  Transformers for Sentiment Analysis on Financial Text
</h1>

<h4 align=""center"">
  Fine-tuned Models based on pretrained Hugging Face Transformers
</h4>

## Usage

### From Command Line
```bash
# this will install necessary packages
pip install -r requirements.txt

# this will run the whole pipeline consists of downloading full dataset, generate data, 
# downloading our finetuned models from google drive, unzip model folders,
# predict sentiments on testing dataset

python run.py generate_data download_models test

# trainning process based on pretrained models from HuggingFace
python run.py generate_data train test

# for predict based on tweets data from twitter API
python run.py download_models predict

# for default testing run (submission for Quater1), 
# test run will download, unzip our finetuned models from google drive,
# predict on dummy testdata(3 samples) and output predicted labels

python run.py testing
```

### File structure and configuration
All configuration will be read from config folder, **data-params.yml** will be read when generating data, **model_config.yml** will be read when downloading, unzipping finetuned models, **train-params.yml** will be read as training hyperparameters as well as io path, **test-params.yml** consists io for test dataset and testrun dummy dataset.

Raw dataset will be scraped from data sources and saved in data/raw, processed data will be saved in data/temp predictions will be saved in data/out, all datafiles will be saved in .csv file.

Finetuned models, no matter directly output from training process, or download from google shared drive, all will be saved in results folder **train.py** will download pretrained models from Hugging Face hub, and read data from data/temp, finally save finetuned model to result folder.

**test.py** will take two argument, test_target and test_lines. test_target can be specified as testing, which will generate prediction on testing data
or default as test to predict on testrun dummy data. All prediction will be saved in data/out.

```
FinTech Project                      //
├─ config                            //
│  ├─ config.json                    //
│  ├─ data-params.yml                //
│  ├─ model-config.yml               //
│  ├─ test-params.yml                //
│  └─ train-params.yml               //
├─ data                                                            //
│  ├─ kaggle.json                                                  //
│  ├─ out                                                          //
│  │  ├─ model.joblib                                              //
│  │  └─ preds.csv                                                 //
│  ├─ raw                                                          //
│  ├─ temp                                                         //
│  │  ├─ test.csv                                                  //
│  │  └─ train.csv                                                 //
├─ myapp.log                                                       //
├─ README.md                                                       //
├─ run.py                                                          //
├─ spacy                                                           //
│  ├─ .DS_Store                                                    //
│  └─ create_model.py                                              //
├─ reports                           //
│  ├─ abstract.md                    //
│  ├─ demo.md                        //
│  ├─ discussion.md                  //
│  ├─ figures                        //
│  │  └─ logo_png.png                //
│  ├─ intro.md                       //
│  ├─ introduction.md                //
│  ├─ methods.md                     //
│  ├─ requirements_jb.txt            //
│  ├─ results.md                     //
│  ├─ _config.yml                    //
│  └─ _toc.yml                       //
├─ src                                                             //
│  ├─ data                                                         //
│  │  └─ make_dataset.py                                           //
│  ├─ test.py                                                      //
│  ├─ train.py                                                     //
│  ├─ utils                                                        //
│  │     └─ download_models.py                                     //
├─ submission.json                                                 //
├─ test                                                            //
│  └─ testdata                                                     //
│     └─ test.csv                                                  //
├─ twitter                                                         //
│  ├─ pull_tweets.py                                               //
│  └─ twitter_credentials.py                                       //
├─ _requirements.txt                                               //
└─ _run.py                                                         //
```

","<h1 align=""center"">
  Transformers for Sentiment Analysis on Financial Text
</h1>

<h4 align=""center"">
  Fine-tuned Models based on pretrained Hugging Face Transformers
</h4>

## Usage

### From Command Line
```bash
# this will install necessary packages
pip install -r requirements.txt

# this will run the whole pipeline consists of downloading full dataset, generate data, 
# downloading our finetuned models from google drive, unzip model folders,
# predict sentiments on testing dataset

python run.py generate_data download_models test

# trainning process based on pretrained models from HuggingFace
python run.py generate_data train test

# for predict based on tweets data from twitter API
python run.py download_models predict

# for default testing run (submission for Quater1), 
# test run will download, unzip our finetuned models from google drive,
# predict on dummy testdata(3 samples) and output predicted labels

python run.py testing
```

### File structure and configuration
All configuration will be read from config folder, **data-params.yml** will be read when generating data, **model_config.yml** will be read when downloading, unzipping finetuned models, **train-params.yml** will be read as training hyperparameters as well as io path, **test-params.yml** consists io for test dataset and testrun dummy dataset.

Raw dataset will be scraped from data sources and saved in data/raw, processed data will be saved in data/temp predictions will be saved in data/out, all datafiles will be saved in .csv file.

Finetuned models, no matter directly output from training process, or download from google shared drive, all will be saved in results folder **train.py** will download pretrained models from Hugging Face hub, and read data from data/temp, finally save finetuned model to result folder.

**test.py** will take two argument, test_target and test_lines. test_target can be specified as testing, which will generate prediction on testing data
or default as test to predict on testrun dummy data. All prediction will be saved in data/out.

```
FinTech Project                      //
├─ config                            //
│  ├─ config.json                    //
│  ├─ data-params.yml                //
│  ├─ model-config.yml               //
│  ├─ test-params.yml                //
│  └─ train-params.yml               //
├─ data                                                            //
│  ├─ kaggle.json                                                  //
│  ├─ out                                                          //
│  │  ├─ model.joblib                                              //
│  │  └─ preds.csv                                                 //
│  ├─ raw                                                          //
│  ├─ temp                                                         //
│  │  ├─ test.csv                                                  //
│  │  └─ train.csv                                                 //
├─ myapp.log                                                       //
├─ README.md                                                       //
├─ run.py                                                          //
├─ spacy                                                           //
│  ├─ .DS_Store                                                    //
│  └─ create_model.py                                              //
├─ reports                           //
│  ├─ abstract.md                    //
│  ├─ demo.md                        //
│  ├─ discussion.md                  //
│  ├─ figures                        //
│  │  └─ logo_png.png                //
│  ├─ intro.md                       //
│  ├─ introduction.md                //
│  ├─ methods.md                     //
│  ├─ requirements_jb.txt            //
│  ├─ results.md                     //
│  ├─ _config.yml                    //
│  └─ _toc.yml                       //
├─ src                                                             //
│  ├─ data                                                         //
│  │  └─ make_dataset.py                                           //
│  ├─ test.py                                                      //
│  ├─ train.py                                                     //
│  ├─ utils                                                        //
│  │     └─ download_models.py                                     //
├─ submission.json                                                 //
├─ test                                                            //
│  └─ testdata                                                     //
│     └─ test.csv                                                  //
├─ twitter                                                         //
│  ├─ pull_tweets.py                                               //
│  └─ twitter_credentials.py                                       //
├─ _requirements.txt                                               //
└─ _run.py                                                         //
```

"
148,https://github.com/nathansng/fintech_library,"{'nathansng': 'https://github.com/nathansng', 'rrichardtang': 'https://github.com/rrichardtang', 'DavidMo113': 'https://github.com/DavidMo113'}","{'Jupyter Notebook': 0.87, 'Python': 0.13}","# FinDL - Financial Deep Learning Library

## DSC 180 Project

The goal of this project is to create a deep learning and machine learning library that allows users to easily create and deploy machine learning models for finance related tasks, such as future stock forecasting. This repo contains a data loader, data preprocessing functions, time series forecasting models, and loss visualization functions to provide an end-to-end machine learning and visualization pipeline. This project is made in parallel with the finance library's NLP group. 

#### Project Website
Link to FinDL's project website: [https://nathansng.github.io/fintech_library](https://nathansng.github.io/fintech_library/)

The project website code can be found in the finDL_website branch of this repository.

Link to FinDL's documentation: [https://fintech-library.readthedocs.io/en/latest/code/overview.html](https://fintech-library.readthedocs.io/en/latest/code/overview.html)

The project documentation website code can be found in the docs folder of the main branch. 

### Downloading Data

Our data is downloaded from Kaggle.com. The dataset that we used for our experiment is the Stock Exchange Data created by Cody in 2018. The dataset is available at: https://www.kaggle.com/datasets/mattiuzc/stock-exchange-data. The dataset we used from this Kaggle dataset is ""indexProcessed.csv"". Save the csv file in the path `./data/raw/`. 

### Models 

The following models have been implemented in the current implementation of the library. 

- CNN
- LSTM
- GRU
- TreNet

The Convolutional Neural Network (CNN) takes raw data points as input, extracts and learns the local feature information, and outputs the predicted local feature. The tuneable parameters of CNN are as follows:

- number of layers
- convolutional layer size 
- filter size
- dropout
- output size

The Long Short Term Memory takes the trends' slope and duration, which are extracted by using linear approximation approach on the raw data points, learns the trend dependencies, and outputs the predicted slope and duration of the next trend. The tunable parameters of LSTM are as follows:

- input size
- hidden size
- number of layers
- output size

The Gated Recurrent Network works similar to LSTM and uses an update and reset gate. It is also used to extract information from the trends' slope and duration. The tunable parameters of GRU are as follows: 

- input size
- hidden size
- number of layers
- output size

TreNet takes the predicted results from both CNN and LSTM, and combines them using a fully connected layer to generate the predicted output. The tunable parameters of TreNet are as follows: 

- Hyperparameters of LSTM 
- Hyperparameters of CNN
- size of feature fusion layer
- output size 


### Running Code

*Note*: Running the code on a gpu will make the program run significantly faster than only cpu. The code also benefits with more RAM, as too low of memory will kill the process. 

To run the code, run `python run.py [target]` to run the corresponding target. The available targets and their description are listed below: 

- `all`: Runs all targets using actual data, data path can be specified in `./config/data_params.json` file

- `test`: Runs all targets using test data, test data can be found in `./test/testdata/testdata.csv`, test path can be specified in `./config/test_data_params.json` file

- `data`: Runs the data and feature loading code, which opens a dataset and converts the dataset into trend durations and slopes
  - Configure parameters in `./config/data_params.json`

- `features`: Runs the preprocessing code for the trends and stock data, normalizes all data, and splits data into training and testing sets for machine learning model use 
  - Configure parameters in `./config/feature_params.json`

- `model`: Runs the machine learning model training code 
  - Configure model parameters in `./config/model_params.json`
  - Configure training parameters in `./config/training_params.json`

- `visual`: Runs the loss plot visualization code which stores a line plot of the loss per epoch to a path 
  - Configure parameters in `./config/visual_params.json`

You can also specify the model to run by specifying the model name in addition to any targets listed above by using `python run.py [targets] [model]`. The default model used if no model is specified is TreNet. The available models and their description are listed below: 

- `trenet`: Runs the TreNet model. Processes 1 dimensional time series data into a sequence of linear regressions encoded by the regressions slope and duration. Uses trend slope and duration data to predict future trends. 

- `lstm`: Runs an LSTM model. Trains on 1 dimensional time series data to predict future time series data. 

- `cnn`: Runs the CNN stack from the TreNet model. Trains on 1 dimensional time series data to predict future time series data. 


","# FinDL - Financial Deep Learning Library

## DSC 180 Project

The goal of this project is to create a deep learning and machine learning library that allows users to easily create and deploy machine learning models for finance related tasks, such as future stock forecasting. This repo contains a data loader, data preprocessing functions, time series forecasting models, and loss visualization functions to provide an end-to-end machine learning and visualization pipeline. This project is made in parallel with the finance library's NLP group. 

#### Project Website
Link to FinDL's project website: [https://nathansng.github.io/fintech_library](https://nathansng.github.io/fintech_library/)

The project website code can be found in the finDL_website branch of this repository.

Link to FinDL's documentation: [https://fintech-library.readthedocs.io/en/latest/code/overview.html](https://fintech-library.readthedocs.io/en/latest/code/overview.html)

The project documentation website code can be found in the docs folder of the main branch. 

### Downloading Data

Our data is downloaded from Kaggle.com. The dataset that we used for our experiment is the Stock Exchange Data created by Cody in 2018. The dataset is available at: https://www.kaggle.com/datasets/mattiuzc/stock-exchange-data. The dataset we used from this Kaggle dataset is ""indexProcessed.csv"". Save the csv file in the path `./data/raw/`. 

### Models 

The following models have been implemented in the current implementation of the library. 

- CNN
- LSTM
- GRU
- TreNet

The Convolutional Neural Network (CNN) takes raw data points as input, extracts and learns the local feature information, and outputs the predicted local feature. The tuneable parameters of CNN are as follows:

- number of layers
- convolutional layer size 
- filter size
- dropout
- output size

The Long Short Term Memory takes the trends' slope and duration, which are extracted by using linear approximation approach on the raw data points, learns the trend dependencies, and outputs the predicted slope and duration of the next trend. The tunable parameters of LSTM are as follows:

- input size
- hidden size
- number of layers
- output size

The Gated Recurrent Network works similar to LSTM and uses an update and reset gate. It is also used to extract information from the trends' slope and duration. The tunable parameters of GRU are as follows: 

- input size
- hidden size
- number of layers
- output size

TreNet takes the predicted results from both CNN and LSTM, and combines them using a fully connected layer to generate the predicted output. The tunable parameters of TreNet are as follows: 

- Hyperparameters of LSTM 
- Hyperparameters of CNN
- size of feature fusion layer
- output size 


### Running Code

*Note*: Running the code on a gpu will make the program run significantly faster than only cpu. The code also benefits with more RAM, as too low of memory will kill the process. 

To run the code, run `python run.py [target]` to run the corresponding target. The available targets and their description are listed below: 

- `all`: Runs all targets using actual data, data path can be specified in `./config/data_params.json` file

- `test`: Runs all targets using test data, test data can be found in `./test/testdata/testdata.csv`, test path can be specified in `./config/test_data_params.json` file

- `data`: Runs the data and feature loading code, which opens a dataset and converts the dataset into trend durations and slopes
  - Configure parameters in `./config/data_params.json`

- `features`: Runs the preprocessing code for the trends and stock data, normalizes all data, and splits data into training and testing sets for machine learning model use 
  - Configure parameters in `./config/feature_params.json`

- `model`: Runs the machine learning model training code 
  - Configure model parameters in `./config/model_params.json`
  - Configure training parameters in `./config/training_params.json`

- `visual`: Runs the loss plot visualization code which stores a line plot of the loss per epoch to a path 
  - Configure parameters in `./config/visual_params.json`

You can also specify the model to run by specifying the model name in addition to any targets listed above by using `python run.py [targets] [model]`. The default model used if no model is specified is TreNet. The available models and their description are listed below: 

- `trenet`: Runs the TreNet model. Processes 1 dimensional time series data into a sequence of linear regressions encoded by the regressions slope and duration. Uses trend slope and duration data to predict future trends. 

- `lstm`: Runs an LSTM model. Trains on 1 dimensional time series data to predict future time series data. 

- `cnn`: Runs the CNN stack from the TreNet model. Trains on 1 dimensional time series data to predict future time series data. 


"
149,https://github.com/vineettalla/Servicechain.io,"{'jxmauricio': 'https://github.com/jxmauricio', 'vineettalla': 'https://github.com/vineettalla', 'anyachandorkar': 'https://github.com/anyachandorkar'}","{'JavaScript': 0.86, 'CSS': 0.08, 'Solidity': 0.06}","This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).

## Getting Started
First, Make sure to install the dependencies in the package.json file by running:

```bash
npm install
```


Next(get it?), run the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

You can start editing the page by modifying `pages/index.js`. The page auto-updates as you edit the file.

The `pages/api` directory is mapped to `/api/*`. Files in this directory are treated as [API routes](https://nextjs.org/docs/api-routes/introduction) instead of React pages.

You now should be able to work with our current codebase!

### Editing Smart Contract 
If you wish to make changes to the smart contract 
```bash
cd ./ethereum/contracts
```
In here you will see the solidity file that has the smart contract logic.<br>
Once you have made changes you will need to compile and deploy the factory contract once again in order to get the ABI and Byte Code<br>
We can do this by running:
```bash
node compile.js
node deploy.js
```
After the deploy command you will be give an address in the terminal window similar to the image below:

![deployment](https://user-images.githubusercontent.com/80795080/225056996-8a1e5df9-6f87-4a60-aa08-c4a0bf53ee1d.png)
<br>Copy the contract address and paste it into the factory.js file located at 
```bash
cd ./ethereum/factory.js
```
You will replace the preexisting ""addressOfDeployedFactory"" with the one you pasted at the location shown below:
![image](https://user-images.githubusercontent.com/80795080/225057653-dcd111f9-0691-4b3a-9a37-d2b95f58ea92.png)
<br>
Now you have a fresh new smart contract that is connected to the frontend of ServiceChain.io! 

### Changing Backend
If you are editing the smart contract you must also change the configuration of the firebase backend to connect to your own! In order to do so go to:
```bash
cd ./config/firebase.js
```
All you have to do is login to [firebase](https://firebase.google.com/), create a project, enable authentication and firestore, then just change the firebase config variable with your own! 

## Deployed Version 
You can also checkout the actual deployed webpage [here](https://servicechain-io.vercel.app/).

## Using the App

Currently our app only works if you have a metamask account since this is a way to interact with the ethereum blockchain with little to no work.
1. Download [Metamask](https://metamask.io/download/) 
2. Create an Account
3. You should be prompted to a screen like this.
<br><img src =""https://user-images.githubusercontent.com/80795080/225046985-9b79bf0b-86fd-4da8-9023-0908b620ea22.png"" width ='200' height ='300'><br> 
**Make sure to switch your network to the Goerli Test Network**
<br>
5. Optionally if you wish to use actions in our app like sending ratings, tips, etc. You must load your account with test ether. In order to do so go to to a [faucet](https://goerlifaucet.com/) and paste your public address from metamask which is the highlighted value in the image below.
<br> <br><img src =""https://user-images.githubusercontent.com/80795080/225049758-e570310c-452a-4a9b-98ce-92e9aa570ba1.png""><br> 
6. Head to our [website](https://servicechain-io.vercel.app/) and click signup to create your own account and use the app! 

## Test Accounts
If you wish to just browse the site, we have an account for each user type. Note that functionalities like tipping **will not work** unless you have completed step 5.
<br>
<br>
User Type: Employee<br>
Username: je@gmail.com<br>
Password: password<br>
<br>
User Type: Manager<br>
Username: jm@gmail.com  <br>
Password: password<br>
<br>
User Type: Customer<br>
Username: sr@gmail.com  <br>
Password: password<br>
<br>
## Extras
I hope you enjoy playing around and improving our dapp. Feel free to point out any flaws or inefficiencies that can be improved upon. After all the beauty of a DAPP is that its all open source!



","This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).

## Getting Started
First, Make sure to install the dependencies in the package.json file by running:

```bash
npm install
```


Next(get it?), run the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

You can start editing the page by modifying `pages/index.js`. The page auto-updates as you edit the file.

The `pages/api` directory is mapped to `/api/*`. Files in this directory are treated as [API routes](https://nextjs.org/docs/api-routes/introduction) instead of React pages.

You now should be able to work with our current codebase!

### Editing Smart Contract 
If you wish to make changes to the smart contract 
```bash
cd ./ethereum/contracts
```
In here you will see the solidity file that has the smart contract logic.<br>
Once you have made changes you will need to compile and deploy the factory contract once again in order to get the ABI and Byte Code<br>
We can do this by running:
```bash
node compile.js
node deploy.js
```
After the deploy command you will be give an address in the terminal window similar to the image below:

![deployment](https://user-images.githubusercontent.com/80795080/225056996-8a1e5df9-6f87-4a60-aa08-c4a0bf53ee1d.png)
<br>Copy the contract address and paste it into the factory.js file located at 
```bash
cd ./ethereum/factory.js
```
You will replace the preexisting ""addressOfDeployedFactory"" with the one you pasted at the location shown below:
![image](https://user-images.githubusercontent.com/80795080/225057653-dcd111f9-0691-4b3a-9a37-d2b95f58ea92.png)
<br>
Now you have a fresh new smart contract that is connected to the frontend of ServiceChain.io! 

### Changing Backend
If you are editing the smart contract you must also change the configuration of the firebase backend to connect to your own! In order to do so go to:
```bash
cd ./config/firebase.js
```
All you have to do is login to [firebase](https://firebase.google.com/), create a project, enable authentication and firestore, then just change the firebase config variable with your own! 

## Deployed Version 
You can also checkout the actual deployed webpage [here](https://servicechain-io.vercel.app/).

## Using the App

Currently our app only works if you have a metamask account since this is a way to interact with the ethereum blockchain with little to no work.
1. Download [Metamask](https://metamask.io/download/) 
2. Create an Account
3. You should be prompted to a screen like this.
<br><img src =""https://user-images.githubusercontent.com/80795080/225046985-9b79bf0b-86fd-4da8-9023-0908b620ea22.png"" width ='200' height ='300'><br> 
**Make sure to switch your network to the Goerli Test Network**
<br>
5. Optionally if you wish to use actions in our app like sending ratings, tips, etc. You must load your account with test ether. In order to do so go to to a [faucet](https://goerlifaucet.com/) and paste your public address from metamask which is the highlighted value in the image below.
<br> <br><img src =""https://user-images.githubusercontent.com/80795080/225049758-e570310c-452a-4a9b-98ce-92e9aa570ba1.png""><br> 
6. Head to our [website](https://servicechain-io.vercel.app/) and click signup to create your own account and use the app! 

## Test Accounts
If you wish to just browse the site, we have an account for each user type. Note that functionalities like tipping **will not work** unless you have completed step 5.
<br>
<br>
User Type: Employee<br>
Username: je@gmail.com<br>
Password: password<br>
<br>
User Type: Manager<br>
Username: jm@gmail.com  <br>
Password: password<br>
<br>
User Type: Customer<br>
Username: sr@gmail.com  <br>
Password: password<br>
<br>
## Extras
I hope you enjoy playing around and improving our dapp. Feel free to point out any flaws or inefficiencies that can be improved upon. After all the beauty of a DAPP is that its all open source!



"
150,https://github.com/scottasut/dsc180b-project,"{'PravarBhandari': 'https://github.com/PravarBhandari', 'scottasut': 'https://github.com/scottasut', 'ryand0n': 'https://github.com/ryand0n', 'felicia-chan': 'https://github.com/felicia-chan'}","{'Jupyter Notebook': 0.83, 'Python': 0.17, 'Dockerfile': 0.0}","<h1 align=""center"">
<img src=""https://upload.wikimedia.org/wikipedia/commons/1/18/UCSD_Seal.png"", width=150, height=150>
<img src=""https://avatars.githubusercontent.com/u/71526309?s=280&v=4"", width=150, height=150>
<img src=""https://logodownload.org/wp-content/uploads/2018/02/reddit-logo-16.png"", width=150, height=150>

Interaction Graph-Based Community Recommendation on Reddit
</h1>

#### Group Members

- Scott Sutherland (sasuther@ucsd.edu)
- Ryan Don (rdon@ucsd.edu)
- Felicia Chan (f4chan@ucsd.edu)
- Pravar Bhandari (psbhanda@ucsd.edu)

## Overview:
<br/>
<div align=""center"">
<img src=""https://user-images.githubusercontent.com/55766484/224842781-a9657aef-54d5-4305-8a09-fce7112693a1.png""  width=""600"" height=""300"">
</div><br/>

This capstone project focuses on graph-based recommender systems for the social media platform Reddit. Users can choose to comment, subscribe, or otherwise interact in different online communities within Reddit called subreddits. Utilizing the graph database and analytics software TigerGraph, we create a recommendation model that recommends subreddits to users based on a variety of different interaction-related features.

The source code for the project is broken up as follows:
- `src/dataset`: files which handle data downloading and parsing it into a heterogeneous graph representation.
- `src/features`: files which handles the non-graph feature generation process for our graph data (users/subreddits).
- `src/models`: our baseline and final models which actually make recommendations for users as well as an evaluation handler class. `src/models/baselines.py` contains the non-graph baseline models while `src/models/models.py` contains the graph-based final models. `src/models/evaluator.py` handles evaluation of recommendations via precision@k calculation given a testing interaction set.

The website associated to this project can be found [here](https://scottasut.github.io/dsc180b-project/).

Due to this project's reliance on TigerGraph's tools, our models cannot be run via a test target without access to a cluster. For a quick demo that our code which makes recommendations for a user, please refer to [this video](https://www.youtube.com/watch?v=fD63_7fDCcM). Additionally, you may refer to `notebooks/model_testing.ipynb` to see the evaluation of the models.

## Project Structure:
```
dsc180b-project/
├─ docs/
│  ├─ css/
│  ├─ images/
│  ├─ _config.yml
│  ├─ index.html
├─ notebooks/
│  ├─ eda.ipynb
│  ├─ model_testing.ipynb
│  ├─ network_stats.ipynb
│  ├─ test.ipynb
├─ src/
│  ├─ dataset/
│  │  ├─ create_dataset.py
│  │  ├─ generate_dataset.py
│  │  ├─ make_dataset.py
│  ├─ features/
│  │  ├─ build_features.py
│  ├─ models/
│  │  ├─ baselines.py
│  │  ├─ evaluator.py
│  │  ├─ model.py
│  │  ├─ models.py
│  ├─ util/
│  │  ├─ logger_util.py
│  │  ├─ tigergraph_util.py
├─ .gitignore
├─ Dockerfile
├─ README.md
├─ poster.pdf
├─ report.pdf
├─ run.py
├─ submission.json
```

## Prerequisites:

Beyond the packages outlines in `requirements.txt`, there are a few tools needed for this project. Namely:
- [wget](https://www.gnu.org/software/wget/): In order to download the data, you will need wget installed on your system.
  - If you do not meet this requirement, [here](https://www.jcchouinard.com/wget/) is a useful guide on how you can get it.
- [TigerGraph](https://www.tigergraph.com/): To get this setup for this project there are quite a few steps. Let's walk through them:

### Working with TigerGraph:<a name=""workingwithtigergraph""></a>

#### Setting up a TGCloud account
In order to leverage graph-based machine learning techniques and TigerGraph's suite of tools in particular, we need to set up a TGCloud instance to work from:

1. Got to https://tgcloud.io/.
2. Select 'sign up'.
3. Fill in the requested information on the sign up page. The organization name can be anything you like, but you will need it to log in.
4. Log in using the information you just provided.
5. Select 'Clusters' on the left hand side menu bar, and then select 'Create Cluster' in the upper right of the interface.
6. From here, you can choose how to configure your cluster. We were able to achieve all the goals of this project using a free cluster with the following specifications: Version: `3.8`, Instance type: `4 vCPU, 7.5GB Memory`, Storage: `50GB`, Number of nodes: `1 Node, Partition Factor 1, Replication Factor 1`.

#### Defining a Graph and Graph Schema
Next, within the GraphStudio tool, we need to create a graph schema to hold our data:

1. Nagivate to 'Tools' > 'GraphStudio' and select the cluster you created.
2. Follow [these](https://youtu.be/Z48cjYuJXX4) steps to create the required schema:


#### What is a graph schema?

A graph schema is a kind of blueprint that defines the types of nodes and edges in the graph data structure, as well as the relationships and constraints between them. TigerGraph has a graphical user interface called GraphStudio that can be used to set up the initial schema and data mapping/loading. [Here](https://docs.tigergraph.com/gsql-ref/current/ddl-and-loading/defining-a-graph-schema#:~:text=A%20graph%20schema%20is%20a,(properties)%20associated%20with%20it) is a useful link that goes more in depth in terms of defining and loading a graph using TigerGraph. [This](https://www.youtube.com/watch?v=Q0JUkiU0lbs) is another short video demonstration showing how to create a schema in GraphStudio.

#### Loading our Data:
*This step requires that the data has been downloaded and processed, please refer to the [Usage](#usage) section*

Within GraphStudio, you can follow [these](https://www.youtube.com/watch?v=7sg6Cw7BuWw) steps

Once you have TigerGraph up and running, you need to be able to authenticate yourself when using it. In this project, you can do so by creating `configs/tigergraph_config.json` in this directory which contains the following: 
```
{
    ""host"": ""<The Host for your TigerGraph Cluster>"",
    ""graphname"": ""<The Name of Your Graph>"",
    ""username"": ""<Your TigerGraph Username>"",
    ""password"": ""<Your TigerGraph Password>"",
    ""gsqlSecret"": ""<Your Secret Key>"",
    ""certPath"": ""<The location of your my-cert.txt>""
}
``` 
While we worked in TigerGraph, we needed to have a file `my-cert.txt` located in our local machine's root directory `~`. Please refer to [this](https://dev.tigergraph.com/forum/t/tigergraph-python-connection-issue/2776) thread for information on how to get that file.

## Usage:<a name=""usage""></a>
In order to run the different components of the project, you will interact with the `run.py` file. There are two main 'targets' or arguments you can pass to the script when running it to work with the project: `data`, `features`. Due to the nature of the project and reliance on running TGCloud instance, testing targets are not available out of the box.

- `data`: downloads the raw data and parses it into a heterogeneous graph format
- `features`: generates necessary features for final model from raw data. Depends on the `data` target.

Targets can be called as follows `python run.py data features`.


#### Important Usage Notes:
- Your TigerGraph cluster must be on when calling any of the functions here which use `pyTigerGraph` otherwise a connection will not be able to be established. If you are experiencing connection errors, ensure that the cluster you are using is indeed turned on.
- The `data` and `feature` target processes can be configured in a couple of ways via a mandatory file `configs/setup.json` which contains the following where `year`, `month`, `test_year`, `test_month` specify the years and months which training and testing data should be pulled from Reddit respectively and `keywords` specifies the number of keywords we save from a user's comment history (and by extension the size of their keyword embeddings). *Note that more recent data within Reddit is larger and will increase the computational needs for almost every aspect of the project. To see where the data is pulled from and see the file sizes, please refer [here](https://files.pushshift.io/reddit/comments/)*.
```
{
    ""year"": ""2010"",
    ""month"": ""12"",
    ""test_year"": ""2011"",
    ""test_month"": ""03"",
    ""keywords"": 25
}
```

## Resources:
- [Course Site](https://dsc-capstone.github.io/)
- [Project Specifications](https://dsc-capstone.github.io/assignments/projects/q2/)
- [TigerGraph](https://www.tigergraph.com/)
- [TigerGraph Cloud](https://tgcloud.io/)
- [Reddit Comment Datasets](https://files.pushshift.io/reddit/comments/)
- [TigerGraph Community ML Algos](https://docs.tigergraph.com/graph-ml/current/community-algorithms/)
","<h1 align=""center"">
<img src=""https://upload.wikimedia.org/wikipedia/commons/1/18/UCSD_Seal.png"", width=150, height=150>
<img src=""https://avatars.githubusercontent.com/u/71526309?s=280&v=4"", width=150, height=150>
<img src=""https://logodownload.org/wp-content/uploads/2018/02/reddit-logo-16.png"", width=150, height=150>

Interaction Graph-Based Community Recommendation on Reddit
</h1>

#### Group Members

- Scott Sutherland (sasuther@ucsd.edu)
- Ryan Don (rdon@ucsd.edu)
- Felicia Chan (f4chan@ucsd.edu)
- Pravar Bhandari (psbhanda@ucsd.edu)

## Overview:
<br/>
<div align=""center"">
<img src=""https://user-images.githubusercontent.com/55766484/224842781-a9657aef-54d5-4305-8a09-fce7112693a1.png""  width=""600"" height=""300"">
</div><br/>

This capstone project focuses on graph-based recommender systems for the social media platform Reddit. Users can choose to comment, subscribe, or otherwise interact in different online communities within Reddit called subreddits. Utilizing the graph database and analytics software TigerGraph, we create a recommendation model that recommends subreddits to users based on a variety of different interaction-related features.

The source code for the project is broken up as follows:
- `src/dataset`: files which handle data downloading and parsing it into a heterogeneous graph representation.
- `src/features`: files which handles the non-graph feature generation process for our graph data (users/subreddits).
- `src/models`: our baseline and final models which actually make recommendations for users as well as an evaluation handler class. `src/models/baselines.py` contains the non-graph baseline models while `src/models/models.py` contains the graph-based final models. `src/models/evaluator.py` handles evaluation of recommendations via precision@k calculation given a testing interaction set.

The website associated to this project can be found [here](https://scottasut.github.io/dsc180b-project/).

Due to this project's reliance on TigerGraph's tools, our models cannot be run via a test target without access to a cluster. For a quick demo that our code which makes recommendations for a user, please refer to [this video](https://www.youtube.com/watch?v=fD63_7fDCcM). Additionally, you may refer to `notebooks/model_testing.ipynb` to see the evaluation of the models.

## Project Structure:
```
dsc180b-project/
├─ docs/
│  ├─ css/
│  ├─ images/
│  ├─ _config.yml
│  ├─ index.html
├─ notebooks/
│  ├─ eda.ipynb
│  ├─ model_testing.ipynb
│  ├─ network_stats.ipynb
│  ├─ test.ipynb
├─ src/
│  ├─ dataset/
│  │  ├─ create_dataset.py
│  │  ├─ generate_dataset.py
│  │  ├─ make_dataset.py
│  ├─ features/
│  │  ├─ build_features.py
│  ├─ models/
│  │  ├─ baselines.py
│  │  ├─ evaluator.py
│  │  ├─ model.py
│  │  ├─ models.py
│  ├─ util/
│  │  ├─ logger_util.py
│  │  ├─ tigergraph_util.py
├─ .gitignore
├─ Dockerfile
├─ README.md
├─ poster.pdf
├─ report.pdf
├─ run.py
├─ submission.json
```

## Prerequisites:

Beyond the packages outlines in `requirements.txt`, there are a few tools needed for this project. Namely:
- [wget](https://www.gnu.org/software/wget/): In order to download the data, you will need wget installed on your system.
  - If you do not meet this requirement, [here](https://www.jcchouinard.com/wget/) is a useful guide on how you can get it.
- [TigerGraph](https://www.tigergraph.com/): To get this setup for this project there are quite a few steps. Let's walk through them:

### Working with TigerGraph:<a name=""workingwithtigergraph""></a>

#### Setting up a TGCloud account
In order to leverage graph-based machine learning techniques and TigerGraph's suite of tools in particular, we need to set up a TGCloud instance to work from:

1. Got to https://tgcloud.io/.
2. Select 'sign up'.
3. Fill in the requested information on the sign up page. The organization name can be anything you like, but you will need it to log in.
4. Log in using the information you just provided.
5. Select 'Clusters' on the left hand side menu bar, and then select 'Create Cluster' in the upper right of the interface.
6. From here, you can choose how to configure your cluster. We were able to achieve all the goals of this project using a free cluster with the following specifications: Version: `3.8`, Instance type: `4 vCPU, 7.5GB Memory`, Storage: `50GB`, Number of nodes: `1 Node, Partition Factor 1, Replication Factor 1`.

#### Defining a Graph and Graph Schema
Next, within the GraphStudio tool, we need to create a graph schema to hold our data:

1. Nagivate to 'Tools' > 'GraphStudio' and select the cluster you created.
2. Follow [these](https://youtu.be/Z48cjYuJXX4) steps to create the required schema:


#### What is a graph schema?

A graph schema is a kind of blueprint that defines the types of nodes and edges in the graph data structure, as well as the relationships and constraints between them. TigerGraph has a graphical user interface called GraphStudio that can be used to set up the initial schema and data mapping/loading. [Here](https://docs.tigergraph.com/gsql-ref/current/ddl-and-loading/defining-a-graph-schema#:~:text=A%20graph%20schema%20is%20a,(properties)%20associated%20with%20it) is a useful link that goes more in depth in terms of defining and loading a graph using TigerGraph. [This](https://www.youtube.com/watch?v=Q0JUkiU0lbs) is another short video demonstration showing how to create a schema in GraphStudio.

#### Loading our Data:
*This step requires that the data has been downloaded and processed, please refer to the [Usage](#usage) section*

Within GraphStudio, you can follow [these](https://www.youtube.com/watch?v=7sg6Cw7BuWw) steps

Once you have TigerGraph up and running, you need to be able to authenticate yourself when using it. In this project, you can do so by creating `configs/tigergraph_config.json` in this directory which contains the following: 
```
{
    ""host"": ""<The Host for your TigerGraph Cluster>"",
    ""graphname"": ""<The Name of Your Graph>"",
    ""username"": ""<Your TigerGraph Username>"",
    ""password"": ""<Your TigerGraph Password>"",
    ""gsqlSecret"": ""<Your Secret Key>"",
    ""certPath"": ""<The location of your my-cert.txt>""
}
``` 
While we worked in TigerGraph, we needed to have a file `my-cert.txt` located in our local machine's root directory `~`. Please refer to [this](https://dev.tigergraph.com/forum/t/tigergraph-python-connection-issue/2776) thread for information on how to get that file.

## Usage:<a name=""usage""></a>
In order to run the different components of the project, you will interact with the `run.py` file. There are two main 'targets' or arguments you can pass to the script when running it to work with the project: `data`, `features`. Due to the nature of the project and reliance on running TGCloud instance, testing targets are not available out of the box.

- `data`: downloads the raw data and parses it into a heterogeneous graph format
- `features`: generates necessary features for final model from raw data. Depends on the `data` target.

Targets can be called as follows `python run.py data features`.


#### Important Usage Notes:
- Your TigerGraph cluster must be on when calling any of the functions here which use `pyTigerGraph` otherwise a connection will not be able to be established. If you are experiencing connection errors, ensure that the cluster you are using is indeed turned on.
- The `data` and `feature` target processes can be configured in a couple of ways via a mandatory file `configs/setup.json` which contains the following where `year`, `month`, `test_year`, `test_month` specify the years and months which training and testing data should be pulled from Reddit respectively and `keywords` specifies the number of keywords we save from a user's comment history (and by extension the size of their keyword embeddings). *Note that more recent data within Reddit is larger and will increase the computational needs for almost every aspect of the project. To see where the data is pulled from and see the file sizes, please refer [here](https://files.pushshift.io/reddit/comments/)*.
```
{
    ""year"": ""2010"",
    ""month"": ""12"",
    ""test_year"": ""2011"",
    ""test_month"": ""03"",
    ""keywords"": 25
}
```

## Resources:
- [Course Site](https://dsc-capstone.github.io/)
- [Project Specifications](https://dsc-capstone.github.io/assignments/projects/q2/)
- [TigerGraph](https://www.tigergraph.com/)
- [TigerGraph Cloud](https://tgcloud.io/)
- [Reddit Comment Datasets](https://files.pushshift.io/reddit/comments/)
- [TigerGraph Community ML Algos](https://docs.tigergraph.com/graph-ml/current/community-algorithms/)
"
151,https://github.com/KazumaYamamoto2023/DSC180B-Q2-Project,"{'srgelinas': 'https://github.com/srgelinas', 'KazumaYamamoto2023': 'https://github.com/KazumaYamamoto2023', 'ethannhzhouu': 'https://github.com/ethannhzhouu'}","{'Jupyter Notebook': 0.72, 'TeX': 0.18, 'Python': 0.1, 'Dockerfile': 0.0}","# Graph-Based Deep Learning for Fraud Detection in Ethereum Transaction Networks

This project aims to compare graph-based to non-graph based algorithms for fraud detection in Ethereum transaction networks. We will predict whether a given Ethereum wallet in the transaction graph is fraudulent or non-fraudulent, given the wallet's transaction history in the network.

Graph exploration, analysis, and model building will be conducted using [TigerGraph](https://tgcloud.io/), an enterprise-scale graph data platform for advanced analytics and machine learning. 

Model performance was determined by taking the average classification accuracy on the testing set over 10 model runs. The resulting classifier performance for this prediction task are as follows:

* Support Vector Machine (~60.5%)
* K-Nearest Neighbors (~74.6%)
* XGBoost (~81.6%)
* Graph Convolutional Network (~79.6%)
* Graph Attention Network (~78.5%)
* GraphSAGE (~81.9%)
* Node2Vec (~76.6%)
* Topology Adaptive Graph Convolutional Network (~82.2%)

## Getting Started
1. Launch a docker container with the following command
```bash
docker run -it srgelinas/dsc180b_eth_fraud:latest
```
2. Clone the repository and `cd` to the project directory:
```bash
git clone https://github.com/KazumaYamamoto2023/DSC180B-Q2-Project.git
```

3. Create a [TigerGraph](https://tgcloud.io/) account and launch an ""ML Bundle"" database cluster. Save the cluster's domain name in `config/tigergraph.json`.

4. Open [GraphStudio](https://tgcloud.io/app/tools/GraphStudio/) and create a new graph named 'Ethereum'

5. Open [AdminPortal](https://tgcloud.io/app/tools/Admin%20Portal/) and navigate to the ""Management"" tab and select ""Users."" Generate a secret alias and secret value, and save the secret value in `config/tigergraph.json`.

6. Run the following command to connect to the TigerGraph database instance, build the graph schema, load the dataset, and evaluate the models
    * This process is detailed in `notebooks/tg_data_loading.ipynb`
```bash
python run.py eth
```

## Project Structure 
```bash
├── config
│   └── tigergraph.json
├── data
│   └── visuals 
│       ├── tagcn_feat_imp.png
│       └── tagcn_subgraph.png
│   ├── edges.csv
│   └── nodes_train_test_split.csv
├── gsql
│   ├── build_schema.gsql
│   ├── get_degrees.gsql
│   ├── load_data.gsql
│   └── summarize_ammounts.gsql
├── notebooks
│   ├── tagcn_model_validation.ipynb
│   └── tg_data_loading.ipynb
├── src
│   ├── baseline.py
│   ├── connect.py
│   ├── gnn_models.py
│   ├── node2vec.py
│   ├── ta_gcn.py
│   └── visualize.py
├── Dockerfile
├── README.md
└── run.py
```

## File Descriptions

`root`
* `run.py:` Python file with main method to run the project code
* `Dockerfile:` Dockerfile with dependencies to bulid docker image to deploy containerized environment

`gsql`
* `build_schema.gsql:` GSQL query to create transaction network graph schema in TigerGraph
* `get_degrees.gsql:` GSQL query to add indegree/outdegree as node features
* `load_data.gsql:` GSQL query to load dataset into TigerGraph database
* `summarize_amounts.gsql:` GSQL query to add summary statistics of sent/received ETH as node features

`notebooks`
* `tagcn_model_validation.ipynb:` Notebook comparing models, visualizing node feature importance and fraudulent wallet subgraphs
* `tg_data_loading.ipynb:` Notebook documenting graph schema design and data upload to TigerGraph

`src`
* `baseline.py:` Python file containing baseline models: Support Vector Machine, K-Nearest Neighbors, XGBoost
* `connect.py:` Python file to connect to TigerGraph database, add node features, upload and retreive data
* `gnn_models.py:` Python file containing Graph Neural Network models (GCN, GAT, GraphSAGE)
* `node2vec.py:` Python file containing Node2Vec model
* `ta_gcn.py:` Python file containing TAGCN model
* `visualize.py:` Python file to generate visualizations for node feature importance and fraudulent wallet subgraphs


## Data Source
This dataset contains transaction records of 445 phishing accounts and 445 non-phishing accounts of Ethereum. We obtain 445 phishing accounts labeled by [Etherscan](etherscan.io) and the same number of randomly selected unlabeled accounts as our objective nodes. The dataset can be used to conduct node classification of financial transaction networks. 

We collect the transaction records based on an assumption that for a typical money transfer flow centered on a phishing node, the previous node of the phishing node may be a victim, and the next one to three nodes may be the bridge nodes with money laundering behaviors, as figure shows. Therefore, we collect subgraphs by [K-order sampling](https://ieeexplore.ieee.org/document/8964468) with K-in = 1, K-out = 3 for each of the 890 objective nodes and then splice them into a large-scale network with 86,623 nodes and 106,083 edges. 

![A schematic illustration of a directed K-order subgraph for phishing node classification.](https://s1.ax1x.com/2020/03/27/GCZGmd.md.jpg)

[XBlock](http://xblock.pro/#/dataset/6) collects the current mainstream blockchain data and is one of the blockchain data platforms with the largest amount of data and the widest coverage in the academic community.
```
@article{ wu2019tedge,
  author = ""Jiajing Wu and Dan Lin and Qi Yuan and Zibin Zheng"",
  title = ""T-EDGE: Temporal WEighted MultiDiGraph Embedding for Ethereum Transaction Network Analysis"",
  journal = ""arXiv preprint arXiv:1905.08038"",
  year = ""2019"",
  URL = ""https://arxiv.org/abs/1905.08038""
}
```
---
[Project Website](https://srgelinas.github.io/dsc180b_eth_fraud/)

[Demo Video](https://youtu.be/WStx_VLHuNk)","# Graph-Based Deep Learning for Fraud Detection in Ethereum Transaction Networks

This project aims to compare graph-based to non-graph based algorithms for fraud detection in Ethereum transaction networks. We will predict whether a given Ethereum wallet in the transaction graph is fraudulent or non-fraudulent, given the wallet's transaction history in the network.

Graph exploration, analysis, and model building will be conducted using [TigerGraph](https://tgcloud.io/), an enterprise-scale graph data platform for advanced analytics and machine learning. 

Model performance was determined by taking the average classification accuracy on the testing set over 10 model runs. The resulting classifier performance for this prediction task are as follows:

* Support Vector Machine (~60.5%)
* K-Nearest Neighbors (~74.6%)
* XGBoost (~81.6%)
* Graph Convolutional Network (~79.6%)
* Graph Attention Network (~78.5%)
* GraphSAGE (~81.9%)
* Node2Vec (~76.6%)
* Topology Adaptive Graph Convolutional Network (~82.2%)

## Getting Started
1. Launch a docker container with the following command
```bash
docker run -it srgelinas/dsc180b_eth_fraud:latest
```
2. Clone the repository and `cd` to the project directory:
```bash
git clone https://github.com/KazumaYamamoto2023/DSC180B-Q2-Project.git
```

3. Create a [TigerGraph](https://tgcloud.io/) account and launch an ""ML Bundle"" database cluster. Save the cluster's domain name in `config/tigergraph.json`.

4. Open [GraphStudio](https://tgcloud.io/app/tools/GraphStudio/) and create a new graph named 'Ethereum'

5. Open [AdminPortal](https://tgcloud.io/app/tools/Admin%20Portal/) and navigate to the ""Management"" tab and select ""Users."" Generate a secret alias and secret value, and save the secret value in `config/tigergraph.json`.

6. Run the following command to connect to the TigerGraph database instance, build the graph schema, load the dataset, and evaluate the models
    * This process is detailed in `notebooks/tg_data_loading.ipynb`
```bash
python run.py eth
```

## Project Structure 
```bash
├── config
│   └── tigergraph.json
├── data
│   └── visuals 
│       ├── tagcn_feat_imp.png
│       └── tagcn_subgraph.png
│   ├── edges.csv
│   └── nodes_train_test_split.csv
├── gsql
│   ├── build_schema.gsql
│   ├── get_degrees.gsql
│   ├── load_data.gsql
│   └── summarize_ammounts.gsql
├── notebooks
│   ├── tagcn_model_validation.ipynb
│   └── tg_data_loading.ipynb
├── src
│   ├── baseline.py
│   ├── connect.py
│   ├── gnn_models.py
│   ├── node2vec.py
│   ├── ta_gcn.py
│   └── visualize.py
├── Dockerfile
├── README.md
└── run.py
```

## File Descriptions

`root`
* `run.py:` Python file with main method to run the project code
* `Dockerfile:` Dockerfile with dependencies to bulid docker image to deploy containerized environment

`gsql`
* `build_schema.gsql:` GSQL query to create transaction network graph schema in TigerGraph
* `get_degrees.gsql:` GSQL query to add indegree/outdegree as node features
* `load_data.gsql:` GSQL query to load dataset into TigerGraph database
* `summarize_amounts.gsql:` GSQL query to add summary statistics of sent/received ETH as node features

`notebooks`
* `tagcn_model_validation.ipynb:` Notebook comparing models, visualizing node feature importance and fraudulent wallet subgraphs
* `tg_data_loading.ipynb:` Notebook documenting graph schema design and data upload to TigerGraph

`src`
* `baseline.py:` Python file containing baseline models: Support Vector Machine, K-Nearest Neighbors, XGBoost
* `connect.py:` Python file to connect to TigerGraph database, add node features, upload and retreive data
* `gnn_models.py:` Python file containing Graph Neural Network models (GCN, GAT, GraphSAGE)
* `node2vec.py:` Python file containing Node2Vec model
* `ta_gcn.py:` Python file containing TAGCN model
* `visualize.py:` Python file to generate visualizations for node feature importance and fraudulent wallet subgraphs


## Data Source
This dataset contains transaction records of 445 phishing accounts and 445 non-phishing accounts of Ethereum. We obtain 445 phishing accounts labeled by [Etherscan](etherscan.io) and the same number of randomly selected unlabeled accounts as our objective nodes. The dataset can be used to conduct node classification of financial transaction networks. 

We collect the transaction records based on an assumption that for a typical money transfer flow centered on a phishing node, the previous node of the phishing node may be a victim, and the next one to three nodes may be the bridge nodes with money laundering behaviors, as figure shows. Therefore, we collect subgraphs by [K-order sampling](https://ieeexplore.ieee.org/document/8964468) with K-in = 1, K-out = 3 for each of the 890 objective nodes and then splice them into a large-scale network with 86,623 nodes and 106,083 edges. 

![A schematic illustration of a directed K-order subgraph for phishing node classification.](https://s1.ax1x.com/2020/03/27/GCZGmd.md.jpg)

[XBlock](http://xblock.pro/#/dataset/6) collects the current mainstream blockchain data and is one of the blockchain data platforms with the largest amount of data and the widest coverage in the academic community.
```
@article{ wu2019tedge,
  author = ""Jiajing Wu and Dan Lin and Qi Yuan and Zibin Zheng"",
  title = ""T-EDGE: Temporal WEighted MultiDiGraph Embedding for Ethereum Transaction Network Analysis"",
  journal = ""arXiv preprint arXiv:1905.08038"",
  year = ""2019"",
  URL = ""https://arxiv.org/abs/1905.08038""
}
```
---
[Project Website](https://srgelinas.github.io/dsc180b_eth_fraud/)

[Demo Video](https://youtu.be/WStx_VLHuNk)"
152,https://github.com/nickthegroot/recipe-recommendation,{'nickthegroot': 'https://github.com/nickthegroot'},"{'Jupyter Notebook': 0.9, 'TeX': 0.07, 'Python': 0.02, 'JavaScript': 0.0, 'TypeScript': 0.0, 'Makefile': 0.0, 'CSS': 0.0, 'Dockerfile': 0.0, 'Shell': 0.0}","<h1 align=""center"">
   <img src=""reports/badges/ucsdseal.png"" width=20% />
   <img src=""reports/badges/tigergraph.png"" width=20% />

Personalized Recipe Recommendation Using Heterogeneous Graphs

</h1>

**Authors**:

- Nicholas DeGroot (Halıcıoğlu Data Science Institute, UC San Diego)

## Description

This project was created for UCSD's DSC 180: Data Science Capstone. According to the university, the course:

> Span(s) the entire lifecycle, including assessing the problem, learning domain knowledge, collecting/cleaning data, creating a model, addressing ethical issues, designing the system, analyzing the output, and presenting the results.
>
> https://catalog.ucsd.edu/courses/DSC.html#dsc180b

## Getting Started

This project is configured with `devcontainer` support. This automatically creates a fully isolated environment with all required dependencies installed.

The easiest way to get started with `devcontainers` is through [GitHub Codespaces](https://github.com/features/codespaces).

1. Click [here](https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=571806935) to create a new codespace on this repository.
   - Alternatively, this can be done through the `gh` CLI.
2. Configure the codespace to your liking. We recommend the 8-core machine.
3. Start the codespace and connect. It might take a minute to install all the dependencies. Grab a :coffee:!
4. Connect to the codespace through your preferred method (browser / VS Code).

## Testing

This project is setup with an array of tests using `pytest` to ensure things are working. With a working environment, run the following command.

```
make test
```

### Testing on DSLMP

For UCSD students & staff, we've ensured that everything works on the Data Science Machine Learning Platform servers.

The (auto!) published Docker image contains everything you need to test the project. Under the hood, it's running the same container that any `devcontainer` is.

In DSMLP: log in with your credentials, then run the following:

```
launch.sh -s -i ghcr.io/nickthegroot/recipe-recommendation:main
cd /app
make test
```

This will begin a full run of every test in the project. Currently, this includes a full pipeline test and a smaller data processing test.

## Downloading/Preparing the Data

1. Download the data by creating an Kaggle account and downloading the [`shuyangli94/food-com-recipes-and-user-interactions`](https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions) dataset.
2. Unzip the data into `data/raw`.
   - You should see a number of files, including `data/raw/RAW_interactions.csv` and `data/raw/RAW_recipes.csv`
3. Run `make data` to clean the data into its cleaned form.

## Running

All models can be trained using `python src/cli/train.py`.

- Run `python src/cli/train.py --help` for all configuration options
- In general, all models can be trained via `python src/cli/train.py --model {model}`
  - For example, `LightGCN` is trained with `python src/cli/train.py --model LightGCN`
","<h1 align=""center"">
   <img src=""reports/badges/ucsdseal.png"" width=20% />
   <img src=""reports/badges/tigergraph.png"" width=20% />

Personalized Recipe Recommendation Using Heterogeneous Graphs

</h1>

**Authors**:

- Nicholas DeGroot (Halıcıoğlu Data Science Institute, UC San Diego)

## Description

This project was created for UCSD's DSC 180: Data Science Capstone. According to the university, the course:

> Span(s) the entire lifecycle, including assessing the problem, learning domain knowledge, collecting/cleaning data, creating a model, addressing ethical issues, designing the system, analyzing the output, and presenting the results.
>
> https://catalog.ucsd.edu/courses/DSC.html#dsc180b

## Getting Started

This project is configured with `devcontainer` support. This automatically creates a fully isolated environment with all required dependencies installed.

The easiest way to get started with `devcontainers` is through [GitHub Codespaces](https://github.com/features/codespaces).

1. Click [here](https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=571806935) to create a new codespace on this repository.
   - Alternatively, this can be done through the `gh` CLI.
2. Configure the codespace to your liking. We recommend the 8-core machine.
3. Start the codespace and connect. It might take a minute to install all the dependencies. Grab a :coffee:!
4. Connect to the codespace through your preferred method (browser / VS Code).

## Testing

This project is setup with an array of tests using `pytest` to ensure things are working. With a working environment, run the following command.

```
make test
```

### Testing on DSLMP

For UCSD students & staff, we've ensured that everything works on the Data Science Machine Learning Platform servers.

The (auto!) published Docker image contains everything you need to test the project. Under the hood, it's running the same container that any `devcontainer` is.

In DSMLP: log in with your credentials, then run the following:

```
launch.sh -s -i ghcr.io/nickthegroot/recipe-recommendation:main
cd /app
make test
```

This will begin a full run of every test in the project. Currently, this includes a full pipeline test and a smaller data processing test.

## Downloading/Preparing the Data

1. Download the data by creating an Kaggle account and downloading the [`shuyangli94/food-com-recipes-and-user-interactions`](https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions) dataset.
2. Unzip the data into `data/raw`.
   - You should see a number of files, including `data/raw/RAW_interactions.csv` and `data/raw/RAW_recipes.csv`
3. Run `make data` to clean the data into its cleaned form.

## Running

All models can be trained using `python src/cli/train.py`.

- Run `python src/cli/train.py --help` for all configuration options
- In general, all models can be trained via `python src/cli/train.py --model {model}`
  - For example, `LightGCN` is trained with `python src/cli/train.py --model LightGCN`
"
153,https://github.com/mjw49/DSC180B-Quarter-2-Project,"{'mjw49': 'https://github.com/mjw49', 'dylanknlee': 'https://github.com/dylanknlee'}","{'Jupyter Notebook': 0.81, 'Python': 0.19, 'Dockerfile': 0.0}","# Running the Project
For building, please run the commands below in this order 

- `launch-scipy-ml.sh -i mjw49/q1project`
- `git clone https://github.com/mjw49/DSC180B-Quarter-2-Project.git`
- `python run.py test`
- `python run.py sampling_city_single (takes around 10 minutes to run)`

# Obtaining the Data Locally

The raw data for this project is obtainable from the website for Stanford's Network Analysis Project (SNAP): http://snap.stanford.edu/higher-order/data.html

Once downloaded, extract the compressed zip file and drop the file into the `data/raw` folder.

","# Running the Project
For building, please run the commands below in this order 

- `launch-scipy-ml.sh -i mjw49/q1project`
- `git clone https://github.com/mjw49/DSC180B-Quarter-2-Project.git`
- `python run.py test`
- `python run.py sampling_city_single (takes around 10 minutes to run)`

# Obtaining the Data Locally

The raw data for this project is obtainable from the website for Stanford's Network Analysis Project (SNAP): http://snap.stanford.edu/higher-order/data.html

Once downloaded, extract the compressed zip file and drop the file into the `data/raw` folder.

"
154,https://github.com/camille-004/Graph-HSCN,"{'camille-004': 'https://github.com/camille-004', 'DylanTao': 'https://github.com/DylanTao'}","{'Python': 0.56, 'Jupyter Notebook': 0.43, 'Makefile': 0.01, 'Dockerfile': 0.0}","<h1 align=""center"">
GraphHSCN: Heterogenized Spectral Cluster Network for Long Range Representation Learning</h1>
<div align=""center"">

  <a href=""https://camille-004.github.io/"">Camille Dunning</a>, <a href=""https://www.linkedin.com/in/zhishang-luo-a51a8120b/"">Zhishang Luo</a>, <a href=""https://dylantao.github.io/"">Sirui Tao</a>
  <p><a href=""https://datascience.ucsd.edu/"">Halıcıoğlu Data Science Institute</a>, UC San Diego, La Jolla, CA</p>
</div>

<p align=""center"">
  <a href=""https://drive.google.com/file/d/1kODg7Qw4hAj1e2Ct91R_tvom8MHdeGln/view"" alt=""Paper"">
        <img src=""https://img.shields.io/badge/Project-Paper-%238affca?style=plastic"" /></a>
        
  <a href=""https://graphhscn.github.io//"" alt=""Website"">
        <img src=""https://img.shields.io/badge/Project-Website-%238affca?style=plastic"" /></a>
        
  <a href=""https://github.com/camille-004/Graph-HSCN/actions/workflows/build-and-push.yml"" alt=""Build"">
        <img src=""https://github.com/camille-004/Graph-HSCN/actions/workflows/build-and-push.yml/badge.svg"" /></a>

</p>
<hr/>


<!-- [![Paper (First Draft)](https://img.shields.io/badge/Project-Paper-9cf)](https://drive.google.com/file/d/1kODg7Qw4hAj1e2Ct91R_tvom8MHdeGln/view) -->

## :rocket: Highlights and Contributions

TODO: Flowchart figure

>**<p align=""justify""> Abstract:** *Graph Neural Networks (GNNs) have gained tremendous popularity for their potential to effectively learn from graph-structured data, commonly encountered in real-world applications. However, most of these models, based on the message-passing paradigm (interactions within a neighborhood of a few nodes), can only handle local interactions within a graph. When we enforce the models to use information from far away nodes, we will encounter two major issues: oversmoothing & oversquashing. Architectures such as the transformer and diffusion models are introduced to solve this; although transformers are powerful, they require significant computational resources for both training and inference, thereby limiting their scalability, particularly for graphs with long-term dependencies. Hence, this paper proposes GraphHSCN—a Heterogenized Spectral Cluster Network, a message-passing-based approach specifically designed for capturing long-range interaction. On our first iteration of ablation studies, we observe reduced time complexities compared to SAN, the most popular graph transformer model, yet comparable performance in graph-level prediction tasks.*

### Main Contributions
1. **Graph coarsening via spectral clustering**: We propose a scheme to coarsen graph representation via spectral clustering with the relaxed formulation of the MinCUT problem, as presented in the [paper](https://arxiv.org/abs/1907.00481) from Bianchi et. al. We observe the structural patterns uncovered by SC reveal which long-range virtual connections should be made.
2. **New connections learned by a heterogeneous network**: We create an intra-cluster connection with a virtual node, and learn the new relationship as a graph indepdenent of the original graph. A heterogeneous convolutional network is trained on these separate relations, further coarsening the representations. On our set of ablation studies, and after hyperparameter tuning, Graph-HSCN out-performs the traditional message-passing architectures by up to 10 percent, achieving metrics similar to those of SAN while reducing the time complexity.

## Getting Started

### Prerequisites
To set up the environment and install all dependencies, run `make env`. The `logs` and `datasets` directories will be created automatically at the project level.
  
### `.devcontainers` Support
TODO
### Running with CLI
TODO
### Running in Prefect UI
TODO

<hr/>

## Hyperparameter Tuning & Results
TODO

<hr/>

## Contact
Feel free to open an issue on this repository or e-mail adunning@ucsd.edu.
  
## Acknowledgements
The code in this project is heavily adapted and modified from the following repositories:
1. [Long Range Graph Benchmark](https://github.com/vijaydwivedi75/lrgb)
2. [torch_geometric GraphGym](https://github.com/pyg-team/pytorch_geometric/tree/master/graphgym)
3. [Hierarchical Graph Net](https://github.com/rampasek/HGNet)
","<h1 align=""center"">
GraphHSCN: Heterogenized Spectral Cluster Network for Long Range Representation Learning</h1>
<div align=""center"">

  <a href=""https://camille-004.github.io/"">Camille Dunning</a>, <a href=""https://www.linkedin.com/in/zhishang-luo-a51a8120b/"">Zhishang Luo</a>, <a href=""https://dylantao.github.io/"">Sirui Tao</a>
  <p><a href=""https://datascience.ucsd.edu/"">Halıcıoğlu Data Science Institute</a>, UC San Diego, La Jolla, CA</p>
</div>

<p align=""center"">
  <a href=""https://drive.google.com/file/d/1kODg7Qw4hAj1e2Ct91R_tvom8MHdeGln/view"" alt=""Paper"">
        <img src=""https://img.shields.io/badge/Project-Paper-%238affca?style=plastic"" /></a>
        
  <a href=""https://graphhscn.github.io//"" alt=""Website"">
        <img src=""https://img.shields.io/badge/Project-Website-%238affca?style=plastic"" /></a>
        
  <a href=""https://github.com/camille-004/Graph-HSCN/actions/workflows/build-and-push.yml"" alt=""Build"">
        <img src=""https://github.com/camille-004/Graph-HSCN/actions/workflows/build-and-push.yml/badge.svg"" /></a>

</p>
<hr/>


<!-- [![Paper (First Draft)](https://img.shields.io/badge/Project-Paper-9cf)](https://drive.google.com/file/d/1kODg7Qw4hAj1e2Ct91R_tvom8MHdeGln/view) -->

## :rocket: Highlights and Contributions

TODO: Flowchart figure

>**<p align=""justify""> Abstract:** *Graph Neural Networks (GNNs) have gained tremendous popularity for their potential to effectively learn from graph-structured data, commonly encountered in real-world applications. However, most of these models, based on the message-passing paradigm (interactions within a neighborhood of a few nodes), can only handle local interactions within a graph. When we enforce the models to use information from far away nodes, we will encounter two major issues: oversmoothing & oversquashing. Architectures such as the transformer and diffusion models are introduced to solve this; although transformers are powerful, they require significant computational resources for both training and inference, thereby limiting their scalability, particularly for graphs with long-term dependencies. Hence, this paper proposes GraphHSCN—a Heterogenized Spectral Cluster Network, a message-passing-based approach specifically designed for capturing long-range interaction. On our first iteration of ablation studies, we observe reduced time complexities compared to SAN, the most popular graph transformer model, yet comparable performance in graph-level prediction tasks.*

### Main Contributions
1. **Graph coarsening via spectral clustering**: We propose a scheme to coarsen graph representation via spectral clustering with the relaxed formulation of the MinCUT problem, as presented in the [paper](https://arxiv.org/abs/1907.00481) from Bianchi et. al. We observe the structural patterns uncovered by SC reveal which long-range virtual connections should be made.
2. **New connections learned by a heterogeneous network**: We create an intra-cluster connection with a virtual node, and learn the new relationship as a graph indepdenent of the original graph. A heterogeneous convolutional network is trained on these separate relations, further coarsening the representations. On our set of ablation studies, and after hyperparameter tuning, Graph-HSCN out-performs the traditional message-passing architectures by up to 10 percent, achieving metrics similar to those of SAN while reducing the time complexity.

## Getting Started

### Prerequisites
To set up the environment and install all dependencies, run `make env`. The `logs` and `datasets` directories will be created automatically at the project level.
  
### `.devcontainers` Support
TODO
### Running with CLI
TODO
### Running in Prefect UI
TODO

<hr/>

## Hyperparameter Tuning & Results
TODO

<hr/>

## Contact
Feel free to open an issue on this repository or e-mail adunning@ucsd.edu.
  
## Acknowledgements
The code in this project is heavily adapted and modified from the following repositories:
1. [Long Range Graph Benchmark](https://github.com/vijaydwivedi75/lrgb)
2. [torch_geometric GraphGym](https://github.com/pyg-team/pytorch_geometric/tree/master/graphgym)
3. [Hierarchical Graph Net](https://github.com/rampasek/HGNet)
"
155,https://github.com/bliu8923/dsc180b-project,"{'bliu8923': 'https://github.com/bliu8923', 'R1chZhang': 'https://github.com/R1chZhang'}","{'Python': 0.7, 'Jupyter Notebook': 0.16, 'Shell': 0.13, 'Dockerfile': 0.01}","## GNN Performance on Long Range Node Classification and Graph Classification

This repository holds the code to test 4 different neural network architectures
on 2 different long range datasets. 

Network architectures can be found under src/models, and test data (Cora) can
be found under the test directory.

To test the model's performance on a small dataset, use the docker repo b6liu/dsc180b (cpu or gpu for tag) and run:
```azure
python run.py --test True --bz (number)
```
IF ON DSMLP: Run a smaller bz if GAN errors, defaults to 32 for test. Also, we recommend using
16+ GB of ram as the networks tend to have large numbers of parameters (especially for GAN/SAN).

We would also recommend a GPU for running these tests/benchmarks, in this case, you should pull
the GPU docker image that has Cuda 11.7 support. (b6liu/dsc180b:gpu)

Different parameters can be run on the file as well.

```--datatype```: Where to extract dataset (LRGB for pascal and peptides, 3D for PSB)

```--dataset```: Dataset to run, currently only support all LRGB datasets. Defaults to PascalVOC-SP

```--model```: Model to run, currently GNN, GatedGCN, GIN, GAT, SAN

```--bz```: Batch size, defaults to 32

```--epoch```: Number of epochs to run the model

```--criterion```: Loss function, defaults to cross entropy 

```--optimizer```: Optimizer to use, defaults to adam

```--lr```: Learning rate, defaults to 0.0005

```--momentum```: Momentum term, defaults to 0.9

```--weight-decay```: Weight decay term, defaults to 5e-6

```--task```: task for network, defaults to node level

```--metric```: Accuracy metric to perform, defaults to macro f1, support for AP

```--gamma```: (SAN only) sparcity of attention, 0 indicates sparse attention while 1 indicates no bias

```--hidden```: Hidden parameters, made after linearly encoding data

```--scheduler```: Enable or disable scheduling on plateau

Shortcut methods have been added:

```--add_edges```: ratio of edges to be created (fake, random connections between nodes)

```--encode```: Positional encoding, we support ""lap"" for laplacian encoding or ""walk"" for RWSE

```--encode_k```: number of features to be added by encoding

And for partial (GAT supported, distance weighting):

```--partial```: Number of distance weighted layers, should be 1

```--space```: Spacial representation of data (2 for 2d, 3 for 3d, etc)

```--k```: for KNN in distance weighting

These are the recommended commands to run all datasets on the best models:

```python run.py --model san --dataset PascalVOC-SP --metric macrof1  --add_edges 1 --encode lap --encode_k 10```

```python run.py --model san --dataset peptides-func --task graph --metric ap  --encode lap --encode_k 10```

```python run.py --model san --datatype 3d --dataset psb --metric ap --encode walk --encode_k 10 --add_edges 1```

You can find the results in the results folder, under the model, timestamped.

### Citations
Thank you to Long Range Graph Benchmarks for the SAN implementation and datasets.
```
@article{dwivedi2022LRGB,
  title={Long Range Graph Benchmark}, 
  author={Dwivedi, Vijay Prakash and Rampášek, Ladislav and Galkin, Mikhail and Parviz, Ali and Wolf, Guy and Luu, Anh Tuan and Beaini, Dominique},
  journal={arXiv:2206.08164},
  year={2022}
}
```
And to GraphGPS, for many loss functions, SAN, and encoders:
```
@article{rampasek2022GPS,
  title={{Recipe for a General, Powerful, Scalable Graph Transformer}}, 
  author={Ladislav Ramp\'{a}\v{s}ek and Mikhail Galkin and Vijay Prakash Dwivedi and Anh Tuan Luu and Guy Wolf and Dominique Beaini},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  year={2022}
}
```","## GNN Performance on Long Range Node Classification and Graph Classification

This repository holds the code to test 4 different neural network architectures
on 2 different long range datasets. 

Network architectures can be found under src/models, and test data (Cora) can
be found under the test directory.

To test the model's performance on a small dataset, use the docker repo b6liu/dsc180b (cpu or gpu for tag) and run:
```azure
python run.py --test True --bz (number)
```
IF ON DSMLP: Run a smaller bz if GAN errors, defaults to 32 for test. Also, we recommend using
16+ GB of ram as the networks tend to have large numbers of parameters (especially for GAN/SAN).

We would also recommend a GPU for running these tests/benchmarks, in this case, you should pull
the GPU docker image that has Cuda 11.7 support. (b6liu/dsc180b:gpu)

Different parameters can be run on the file as well.

```--datatype```: Where to extract dataset (LRGB for pascal and peptides, 3D for PSB)

```--dataset```: Dataset to run, currently only support all LRGB datasets. Defaults to PascalVOC-SP

```--model```: Model to run, currently GNN, GatedGCN, GIN, GAT, SAN

```--bz```: Batch size, defaults to 32

```--epoch```: Number of epochs to run the model

```--criterion```: Loss function, defaults to cross entropy 

```--optimizer```: Optimizer to use, defaults to adam

```--lr```: Learning rate, defaults to 0.0005

```--momentum```: Momentum term, defaults to 0.9

```--weight-decay```: Weight decay term, defaults to 5e-6

```--task```: task for network, defaults to node level

```--metric```: Accuracy metric to perform, defaults to macro f1, support for AP

```--gamma```: (SAN only) sparcity of attention, 0 indicates sparse attention while 1 indicates no bias

```--hidden```: Hidden parameters, made after linearly encoding data

```--scheduler```: Enable or disable scheduling on plateau

Shortcut methods have been added:

```--add_edges```: ratio of edges to be created (fake, random connections between nodes)

```--encode```: Positional encoding, we support ""lap"" for laplacian encoding or ""walk"" for RWSE

```--encode_k```: number of features to be added by encoding

And for partial (GAT supported, distance weighting):

```--partial```: Number of distance weighted layers, should be 1

```--space```: Spacial representation of data (2 for 2d, 3 for 3d, etc)

```--k```: for KNN in distance weighting

These are the recommended commands to run all datasets on the best models:

```python run.py --model san --dataset PascalVOC-SP --metric macrof1  --add_edges 1 --encode lap --encode_k 10```

```python run.py --model san --dataset peptides-func --task graph --metric ap  --encode lap --encode_k 10```

```python run.py --model san --datatype 3d --dataset psb --metric ap --encode walk --encode_k 10 --add_edges 1```

You can find the results in the results folder, under the model, timestamped.

### Citations
Thank you to Long Range Graph Benchmarks for the SAN implementation and datasets.
```
@article{dwivedi2022LRGB,
  title={Long Range Graph Benchmark}, 
  author={Dwivedi, Vijay Prakash and Rampášek, Ladislav and Galkin, Mikhail and Parviz, Ali and Wolf, Guy and Luu, Anh Tuan and Beaini, Dominique},
  journal={arXiv:2206.08164},
  year={2022}
}
```
And to GraphGPS, for many loss functions, SAN, and encoders:
```
@article{rampasek2022GPS,
  title={{Recipe for a General, Powerful, Scalable Graph Transformer}}, 
  author={Ladislav Ramp\'{a}\v{s}ek and Mikhail Galkin and Vijay Prakash Dwivedi and Anh Tuan Luu and Guy Wolf and Dominique Beaini},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  year={2022}
}
```"
156,https://github.com/Barry0121/graph-neural-net-benchmark,"{'Barry0121': 'https://github.com/Barry0121', 'winston-yu': 'https://github.com/winston-yu', 'jgeng99': 'https://github.com/jgeng99'}","{'Jupyter Notebook': 0.94, 'Python': 0.06, 'Dockerfile': 0.0}","# DSC180 - Graph Neural Network

Author: Barry Xue

This repository contains the materials and codes from the exploration of the topic of Graph Neural Network.

# Option 1: Train WGAN network with GraphRNN, Discriminator, and Inverter with `main.py`
`main.py` will call the training function, and then it will call the visualization function to generate the graphs.
* All user defined parameters can be modified in `src/models/args.py` file.

# Option 2: Test GCN and GCN-AE with `run.py`
`run.py` can run node classification task and edge prediction task with GCN Models (Multi-layer GCN for node classification, and GCN-AE for edge prediction).
* Use the `--task` flag to choose between the two tasks.
    - Example: python run.py --name 'expt_test' --dataset 'Cora' --task 'Edge Prediction'

There are also two dataset to run either task upon: Cora, CiteSeer, and PubMed.
* Use the `--dataset` flag to choose between the two datasets.
    - Example: python run.py --name 'expt_test' --dataset 'CiteSeer' --task 'Node Classification'

Other options:
1. `--epochs`: change the number of training epochs.
2. `--hidden_size`: change the first layer hidden layer size.
3. `--encode_size`: change the encoding size/final hidden layer size.
4. `--train`/`--validation`/`--test`: specify number of training sample per classes, validation size, and testing data size.

## Note on installing `pytorch_geometric`
* Normally, the command: `conda install -c pyg pyg` will work on MacOS, Windows, and any Linux distro with anaconda/miniconda installed.
* There are two known cases where this wouldn't work:
1. If the pytorch isn't installed, or the installed version in the environment is <1.12.x, y9ou might need to look into alternative installation method (ex: building from source, use pip, etc).
2. The newest MacOS Ventura (v13.0.1) has installation issue, due to the lack of support for M1 Macbooks (pytorch scatter doesn't support 'mps' device yet). One way to get around this issue is to follow this post: https://github.com/rusty1s/pytorch_scatter/issues/241.

### Note on our codebase 
* If you are interested in the ""Discovering Continuous Latent Space Representation of Graph"", everything is in the `src` directory. 
* If you are interested in simple GNNs, everything is in `legacy_code/model` and `legacy_code/features`. 
","# DSC180 - Graph Neural Network

Author: Barry Xue

This repository contains the materials and codes from the exploration of the topic of Graph Neural Network.

# Option 1: Train WGAN network with GraphRNN, Discriminator, and Inverter with `main.py`
`main.py` will call the training function, and then it will call the visualization function to generate the graphs.
* All user defined parameters can be modified in `src/models/args.py` file.

# Option 2: Test GCN and GCN-AE with `run.py`
`run.py` can run node classification task and edge prediction task with GCN Models (Multi-layer GCN for node classification, and GCN-AE for edge prediction).
* Use the `--task` flag to choose between the two tasks.
    - Example: python run.py --name 'expt_test' --dataset 'Cora' --task 'Edge Prediction'

There are also two dataset to run either task upon: Cora, CiteSeer, and PubMed.
* Use the `--dataset` flag to choose between the two datasets.
    - Example: python run.py --name 'expt_test' --dataset 'CiteSeer' --task 'Node Classification'

Other options:
1. `--epochs`: change the number of training epochs.
2. `--hidden_size`: change the first layer hidden layer size.
3. `--encode_size`: change the encoding size/final hidden layer size.
4. `--train`/`--validation`/`--test`: specify number of training sample per classes, validation size, and testing data size.

## Note on installing `pytorch_geometric`
* Normally, the command: `conda install -c pyg pyg` will work on MacOS, Windows, and any Linux distro with anaconda/miniconda installed.
* There are two known cases where this wouldn't work:
1. If the pytorch isn't installed, or the installed version in the environment is <1.12.x, y9ou might need to look into alternative installation method (ex: building from source, use pip, etc).
2. The newest MacOS Ventura (v13.0.1) has installation issue, due to the lack of support for M1 Macbooks (pytorch scatter doesn't support 'mps' device yet). One way to get around this issue is to follow this post: https://github.com/rusty1s/pytorch_scatter/issues/241.

### Note on our codebase 
* If you are interested in the ""Discovering Continuous Latent Space Representation of Graph"", everything is in the `src` directory. 
* If you are interested in simple GNNs, everything is in `legacy_code/model` and `legacy_code/features`. 
"
157,https://github.com/hblyx/CommunityDetection,"{'hblyx': 'https://github.com/hblyx', 'Leffania': 'https://github.com/Leffania'}","{'Jupyter Notebook': 0.99, 'Python': 0.01}","# DSC180 Project 2 - Performance Evaluation of Community Detection on Neural Networks
#### Yaoxin Li, Justin Nguyen, Vivek Rayalu

### Introduction
As our previous work, we explored the traditional solutions of community detection, including Louvain, Girvan-Newman, and etc, we are also explring the solutions with neural networks. For this project, we explore the community detection solutions with machine learning, deep learning in particular. We specifically explore the performance of neural networks on task of community detection by implementing and training different neural networks, including Multiple Layers Perceptrons and Graph Neural Networks, to test whether neural networks can be a solution for community detection.

This repository contains all code for all findings and attempt related to this project.

### Files
* `checkpoints` contains best models' stats in format of PyTorch's `.pt`.
* `config` contains parameters used for models.
* `notebooks` contains notebooks which illustrates data analysis, the result,  and training process.
* `outputs` contains the training plots including loss and score plots of models.
* `references` contains all reference.
* `src` contains all source code used for data analysis, models, and training. 
    * `data` contains source code used for generate random and test data, data analysis, loading data for models, and some code for read specific format of data.
    * `features` contains code for feature engineering.
    * `models` contains the code for models, algorithms, and training.
* `test` contains the test data for `run.py`. Specifically, the test data are stored in `/test/testdata/`.
* `requiremetnts.txt` speficy the requirements of running this project.
* `run.py` can run a simple test for this project.
* `submission.json` contains information of Docker image for this project.

### Requirements
`submission.json` contains the Docker image which have all packages needed. Meanwhile, the specific requirements are in `requirements.txt`. The Docker image contains all needed environment exclude `torch` and `torch_geometric`. Specifically, since `torch` and `torch_geometric` requires specific version according to the device, CUDA version, we choose to leave them. Therefore, to reproduce our results, `torch` and `torch_geometric` needed to be installed correctly according to CUDA version or CPU only version. The details of installing `torch` and `torch_geometric` can be found in https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html and https://pytorch.org/get-started/locally/ .

### Run
To run the project, we left the test with `run.py`. However, since this project is  more about to explore the solutions for community detection, the content is relatively mass. It is really difficult to re-run all analysis, model training, and algorithms implemented in a short period of time. Therefore, in `test` of `run.py`, it will run our naive traditional community detection algorithm which depends on the number of common neighbors on the test dataset. It will just make sure the graph/network enviroment has been set correctly with test data. In addition, since the entire environment of the running models of Graph Neural Network depends on the hardware environment. Specifically, since the training process requires to specify the GPU/CPU, it must corporate with the correct version of PyTorch, `torch`, and `torch_geometric` which need to specify whether use CPU only or specific CUDA version. Since the device run these code might have different hardware environment, we leave this part free to change. However, all training results and process are reproducible in the notebooks and code.

Instead of presenting our results and code in `run.py`, we choose notebooks to show the results and process. Specifically `/notebooks` folder contains all attempts, models, and results we did, built, and ran. 

### Website

The project [website](https://hblyx.github.io/CommunityDetection/) and its source code is under the [gh-pages branch](https://github.com/hblyx/CommunityDetection/tree/gh-pages) of the same repository.
","# DSC180 Project 2 - Performance Evaluation of Community Detection on Neural Networks
#### Yaoxin Li, Justin Nguyen, Vivek Rayalu

### Introduction
As our previous work, we explored the traditional solutions of community detection, including Louvain, Girvan-Newman, and etc, we are also explring the solutions with neural networks. For this project, we explore the community detection solutions with machine learning, deep learning in particular. We specifically explore the performance of neural networks on task of community detection by implementing and training different neural networks, including Multiple Layers Perceptrons and Graph Neural Networks, to test whether neural networks can be a solution for community detection.

This repository contains all code for all findings and attempt related to this project.

### Files
* `checkpoints` contains best models' stats in format of PyTorch's `.pt`.
* `config` contains parameters used for models.
* `notebooks` contains notebooks which illustrates data analysis, the result,  and training process.
* `outputs` contains the training plots including loss and score plots of models.
* `references` contains all reference.
* `src` contains all source code used for data analysis, models, and training. 
    * `data` contains source code used for generate random and test data, data analysis, loading data for models, and some code for read specific format of data.
    * `features` contains code for feature engineering.
    * `models` contains the code for models, algorithms, and training.
* `test` contains the test data for `run.py`. Specifically, the test data are stored in `/test/testdata/`.
* `requiremetnts.txt` speficy the requirements of running this project.
* `run.py` can run a simple test for this project.
* `submission.json` contains information of Docker image for this project.

### Requirements
`submission.json` contains the Docker image which have all packages needed. Meanwhile, the specific requirements are in `requirements.txt`. The Docker image contains all needed environment exclude `torch` and `torch_geometric`. Specifically, since `torch` and `torch_geometric` requires specific version according to the device, CUDA version, we choose to leave them. Therefore, to reproduce our results, `torch` and `torch_geometric` needed to be installed correctly according to CUDA version or CPU only version. The details of installing `torch` and `torch_geometric` can be found in https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html and https://pytorch.org/get-started/locally/ .

### Run
To run the project, we left the test with `run.py`. However, since this project is  more about to explore the solutions for community detection, the content is relatively mass. It is really difficult to re-run all analysis, model training, and algorithms implemented in a short period of time. Therefore, in `test` of `run.py`, it will run our naive traditional community detection algorithm which depends on the number of common neighbors on the test dataset. It will just make sure the graph/network enviroment has been set correctly with test data. In addition, since the entire environment of the running models of Graph Neural Network depends on the hardware environment. Specifically, since the training process requires to specify the GPU/CPU, it must corporate with the correct version of PyTorch, `torch`, and `torch_geometric` which need to specify whether use CPU only or specific CUDA version. Since the device run these code might have different hardware environment, we leave this part free to change. However, all training results and process are reproducible in the notebooks and code.

Instead of presenting our results and code in `run.py`, we choose notebooks to show the results and process. Specifically `/notebooks` folder contains all attempts, models, and results we did, built, and ran. 

### Website

The project [website](https://hblyx.github.io/CommunityDetection/) and its source code is under the [gh-pages branch](https://github.com/hblyx/CommunityDetection/tree/gh-pages) of the same repository.
"
158,https://github.com/stassinopoulosari/dsc180b-a15-q2project,"{'stassinopoulosari': 'https://github.com/stassinopoulosari', 'Tuz2': 'https://github.com/Tuz2'}",{'Python': 1.0},"# DSC 180B WI23-A15-2: Community Detection on Twitter

## Abstract

Recent work examined the vast unfolding of communities in large networks, in which it was shown that the Louvain Algorithm was the most effective at identifying and dividing communities into clusters. The growth of social media networks in the modern world nurtures the growth and identification of similarities between groups of people. These similarities between groups can be identified more formally as communities. While the number and types of communities grow, the identification and classification of these communities becomes more challenging. To define the scope of the project, we will be utilizing public data from the social media network Twitter. Specifically, we will look at the followers and followings of users throughout twitter. In this paper, we utilize the Louvain Algorithm to explore communities within the social media platform twitter. The results of the study allowed us to uncover and analyze distinct communities based on our seed account of a modern day rap musician named Dessa (@DessaDarling). Further research is needed to determine the potential applications of these algorithms in the field of community detection in social media since we only used one platform’s data.

## Running our code

We recommend using [this docker repository](https://hub.docker.com/repository/docker/stassinopoulosari/dsc180b-wi23-a15-2/general). To start the code, use the following entry point command:

`python run.py data test`","# DSC 180B WI23-A15-2: Community Detection on Twitter

## Abstract

Recent work examined the vast unfolding of communities in large networks, in which it was shown that the Louvain Algorithm was the most effective at identifying and dividing communities into clusters. The growth of social media networks in the modern world nurtures the growth and identification of similarities between groups of people. These similarities between groups can be identified more formally as communities. While the number and types of communities grow, the identification and classification of these communities becomes more challenging. To define the scope of the project, we will be utilizing public data from the social media network Twitter. Specifically, we will look at the followers and followings of users throughout twitter. In this paper, we utilize the Louvain Algorithm to explore communities within the social media platform twitter. The results of the study allowed us to uncover and analyze distinct communities based on our seed account of a modern day rap musician named Dessa (@DessaDarling). Further research is needed to determine the potential applications of these algorithms in the field of community detection in social media since we only used one platform’s data.

## Running our code

We recommend using [this docker repository](https://hub.docker.com/repository/docker/stassinopoulosari/dsc180b-wi23-a15-2/general). To start the code, use the following entry point command:

`python run.py data test`"
159,https://github.com/darehunt/DSC180B-Project2,"{'darehunt': 'https://github.com/darehunt', 'anmokhta': 'https://github.com/anmokhta', 'Btran206': 'https://github.com/Btran206', 'jul016': 'https://github.com/jul016'}","{'Jupyter Notebook': 1.0, 'Python': 0.0}","# DSC 180B [WI 23] Project 2:<br> Community Detection of Music Genres
*** Classifying Spotify Artists through Community Detection and Clustering ***

<!--This site was built using [GitHub Pages](https://pages.github.com/).-->

## Data Background
<!-- TODO -->
Our primary source of data is a dataset of Spotify playlists collected by Andrew Maranhão, which is freely available on Kaggle [(link)](https://www.kaggle.com/datasets/andrewmvd/spotify-playlists). This dataset was collected using a subset of users who published their #nowplaying tweets via Spotify. This tabular dataset lists a row for each song that was tweeted out, containing the name of the song, the artist of the song, and the playlist that song was playing from. While the data also contained the user IDs of each person who tweeted the track they were listening to, our model does not take personal information as input.

## Deployment

Clone the project

```bash
  git clone https://github.com/darehunt/DSC180B-Project2
```

After running Docker and logging into your account, pull and launch the docker image: (note that this requires more RAM than the usual to run)
```bash
  launch.sh -i anmokhta/DSC180B-proj2:latest -m 32
```

From there, copy in the directory, change directories into the project and run commands from bash using run.py:

```bash
  cp DSC180B-Project2 .
  cd DSC180B-Project2
  python run.py all
```
## Commands

The build script can be run directly from bash `python run.py`

| Command | Description |
| --- | --- |
| `clean`  | Clears reminents of previous networks or community detection  |
| `data`  | Downloads, extracts, and prepares data network from Kaggle dataset  |
| `model`  | Runs community detection to attempt to group artists by genre |
| `all`  | equivalent of running `data model`  |

## Authors

- [@darehunt](https://www.github.com/darehunt)
- [@anmokhta](https://www.github.com/anmokhta)
- [@Btran206](https://www.github.com/Btran206)
- [@jul016](https://www.github.com/jul016)

","# DSC 180B [WI 23] Project 2:<br> Community Detection of Music Genres
*** Classifying Spotify Artists through Community Detection and Clustering ***

<!--This site was built using [GitHub Pages](https://pages.github.com/).-->

## Data Background
<!-- TODO -->
Our primary source of data is a dataset of Spotify playlists collected by Andrew Maranhão, which is freely available on Kaggle [(link)](https://www.kaggle.com/datasets/andrewmvd/spotify-playlists). This dataset was collected using a subset of users who published their #nowplaying tweets via Spotify. This tabular dataset lists a row for each song that was tweeted out, containing the name of the song, the artist of the song, and the playlist that song was playing from. While the data also contained the user IDs of each person who tweeted the track they were listening to, our model does not take personal information as input.

## Deployment

Clone the project

```bash
  git clone https://github.com/darehunt/DSC180B-Project2
```

After running Docker and logging into your account, pull and launch the docker image: (note that this requires more RAM than the usual to run)
```bash
  launch.sh -i anmokhta/DSC180B-proj2:latest -m 32
```

From there, copy in the directory, change directories into the project and run commands from bash using run.py:

```bash
  cp DSC180B-Project2 .
  cd DSC180B-Project2
  python run.py all
```
## Commands

The build script can be run directly from bash `python run.py`

| Command | Description |
| --- | --- |
| `clean`  | Clears reminents of previous networks or community detection  |
| `data`  | Downloads, extracts, and prepares data network from Kaggle dataset  |
| `model`  | Runs community detection to attempt to group artists by genre |
| `all`  | equivalent of running `data model`  |

## Authors

- [@darehunt](https://www.github.com/darehunt)
- [@anmokhta](https://www.github.com/anmokhta)
- [@Btran206](https://www.github.com/Btran206)
- [@jul016](https://www.github.com/jul016)

"
160,https://github.com/gordonhu608/Revisit_CLIP,"{'gordonhu608': 'https://github.com/gordonhu608', 'jawkneeLoo': 'https://github.com/jawkneeLoo'}","{'Python': 0.98, 'Shell': 0.02}","# Revist CLIP: Multi-perspective improvements on Vision-Language Model


> [Wenbo Hu](https://gordonhu608.github.io/), [Johnny Liu](https://github.com/jawkneeLoo)

[![Website](https://img.shields.io/badge/Project-Website-87CEEB)](https://gordonhu608.github.io/Revisit_CLIP/)
[![Temporary paper](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://gordonhu608.github.io/files/revisitclip.pdf)


<hr />

# :rocket: Highlights

![main figure](docs/main_figure.png)
> **<p align=""justify""> Abstract:** *Large-scale contrastive vision-language pre-training
> has shown significant progress in visual representation
> learning. Unlike traditional visual systems trained by a
> fixed set of discrete labels, a new paradigm was introduced
> in CLIP to directly learn to align images with raw texts in
> an open-vocabulary setting. On downstream tasks, a carefully designed text prompt is employed to make zero-shot
> predictions. To avoid non-trivial prompt engineering, context optimization has been proposed to learn continuous vectors
> as task-specific prompts with few-shot training  examples. Instead of learning the input prompt token,
> an orthogonal way is learning the weight distributions of
> prompt, which is also very effective. An alternative
> path is fine-tuning with a light-weight feature adapter
> on the visual branch The most recent work introduces multimodal prompt learning, which uses a synergy function
> to simultaneously adapt language and vision branches for
> improved generalization. In our work, we revisit recent improvements in CLIP from different perspectives and propose
> an optimal way of combining the model’s architecture. We
> demonstrate that Data Augmentation (DA) and Test-Time
> Augmentation (TTA) are important for few-shot learning
> (FSL). We propose an end-to-end few-shot learning pipeline
> (DA + MaPLe + Adapters + TTA) that can be referenced for
> all downstream tasks. Compared with the state-of-the-art
> method ProDA  in FSL, our model achieves an absolute
> gain of 6.33% on the 1-shot learning setting and 4.43% on
> the 16-shot setting, averaged over 10 diverse image recognition datasets.* </p>

## Main Contributions

1) **Standard workflow for few-shot learning:** We combined current state-of-the-art models from different perspectives and achieved better performance.
We gained an average of 5.28% absolute improvement for [1,2,4,8,16] shots learning over 10 diverse image
recognition datasets than the best baseline model.
2) **Data and Test Time Augmentation:** We employed optimal Data Augmentation and Test Time Augmentation (TTA) and demonstrates TTA’s
importance in few-shot learning and thus should be used as a convention in future few-shot learning tasks


## :ballot_box_with_check: Supported Methods

| Method                    | Paper                                         |                             Configs                             |          Training Scripts          |
|---------------------------|:----------------------------------------------|:---------------------------------------------------------------:|:----------------------------------:|
| MaPLe                     | [arXiv](https://arxiv.org/abs/2210.03117)    | [link](configs/trainers/MaPLe/vit_b16_c2_ep5_batch4_2ctx.yaml)  |       [link](scripts/maple)        |
| CoOp                      | [IJCV 2022](https://arxiv.org/abs/2109.01134) |                  [link](configs/trainers/CoOp)                  |        [link](scripts/coop)        |
| Co-CoOp                   | [CVPR 2022](https://arxiv.org/abs/2203.05557) |                 [link](configs/trainers/CoCoOp)                 |       [link](scripts/cocoop)       |
| Ours                    | [arXiv](https://gordonhu608.github.io/files/revisitclip.pdf) |                  [link](configs/trainers/Ours)                  |        [link](scripts/ours)        |
<hr />

## Results
### Our method in comparison with existing methods
Results reported below show accuracy for few shot training on [1,2,4,8,16] for across 10 recognition datasets averaged over 3 seeds.

| Name                                                      | 1-shot | 2-shot |   4-shot     | 8-shot |  16-shot | 
|-----------------------------------------------------------|:---------:|:----------:|:---------:|:------:|:------:|
| [CLIP](https://arxiv.org/abs/2103.00020)                  |   36.13   |   47.83    |   58.52  |   66.24   |   72.03    |   
| [CoOp](https://arxiv.org/abs/2109.01134)                  | 59.95 |   63.74    |    67.18  |  70.52  |  74.03  | 
| [CLIP-Adapter](https://arxiv.org/abs/2110.04544)                |   62.13   |   65.64    |   69.07   |   72.55  |   76.  |  
| [ProDA](https://arxiv.org/abs/2205.03340)                |   65.19   |   68.59    |   71.4   |   74.21   |   76.78   | 
| [Ours](https://gordonhu608.github.io/files/revisitclip.pdf)          |   **71.94** | **74.12**  | **78.48** |   **80.2**    |   **82.64**   | 

## Installation 
For installation and other package requirements, please follow the instructions detailed in [INSTALL.md](docs/INSTALL.md). 

## Data preparation
Please follow the instructions at [DATASETS.md](docs/DATASETS.md) to prepare all datasets.

## Training and Evaluation
Please refer to the [RUN.md](docs/RUN.md) for detailed instructions on training, evaluating and reproducing the results using our pre-trained models.


<hr />

## Contact
If you have any questions, please create an issue on this repository or contact at w1hu@ucsd.edu


## Acknowledgements

Our code is based on [Co-CoOp, CoOp](https://github.com/KaiyangZhou/CoOp) and [MaPLe](https://github.com/muzairkhattak/multimodal-prompt-learning) repository. We thank the authors for releasing their code. If you use our model and code, please consider citing these works as well.

","# Revist CLIP: Multi-perspective improvements on Vision-Language Model


> [Wenbo Hu](https://gordonhu608.github.io/), [Johnny Liu](https://github.com/jawkneeLoo)

[![Website](https://img.shields.io/badge/Project-Website-87CEEB)](https://gordonhu608.github.io/Revisit_CLIP/)
[![Temporary paper](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://gordonhu608.github.io/files/revisitclip.pdf)


<hr />

# :rocket: Highlights

![main figure](docs/main_figure.png)
> **<p align=""justify""> Abstract:** *Large-scale contrastive vision-language pre-training
> has shown significant progress in visual representation
> learning. Unlike traditional visual systems trained by a
> fixed set of discrete labels, a new paradigm was introduced
> in CLIP to directly learn to align images with raw texts in
> an open-vocabulary setting. On downstream tasks, a carefully designed text prompt is employed to make zero-shot
> predictions. To avoid non-trivial prompt engineering, context optimization has been proposed to learn continuous vectors
> as task-specific prompts with few-shot training  examples. Instead of learning the input prompt token,
> an orthogonal way is learning the weight distributions of
> prompt, which is also very effective. An alternative
> path is fine-tuning with a light-weight feature adapter
> on the visual branch The most recent work introduces multimodal prompt learning, which uses a synergy function
> to simultaneously adapt language and vision branches for
> improved generalization. In our work, we revisit recent improvements in CLIP from different perspectives and propose
> an optimal way of combining the model’s architecture. We
> demonstrate that Data Augmentation (DA) and Test-Time
> Augmentation (TTA) are important for few-shot learning
> (FSL). We propose an end-to-end few-shot learning pipeline
> (DA + MaPLe + Adapters + TTA) that can be referenced for
> all downstream tasks. Compared with the state-of-the-art
> method ProDA  in FSL, our model achieves an absolute
> gain of 6.33% on the 1-shot learning setting and 4.43% on
> the 16-shot setting, averaged over 10 diverse image recognition datasets.* </p>

## Main Contributions

1) **Standard workflow for few-shot learning:** We combined current state-of-the-art models from different perspectives and achieved better performance.
We gained an average of 5.28% absolute improvement for [1,2,4,8,16] shots learning over 10 diverse image
recognition datasets than the best baseline model.
2) **Data and Test Time Augmentation:** We employed optimal Data Augmentation and Test Time Augmentation (TTA) and demonstrates TTA’s
importance in few-shot learning and thus should be used as a convention in future few-shot learning tasks


## :ballot_box_with_check: Supported Methods

| Method                    | Paper                                         |                             Configs                             |          Training Scripts          |
|---------------------------|:----------------------------------------------|:---------------------------------------------------------------:|:----------------------------------:|
| MaPLe                     | [arXiv](https://arxiv.org/abs/2210.03117)    | [link](configs/trainers/MaPLe/vit_b16_c2_ep5_batch4_2ctx.yaml)  |       [link](scripts/maple)        |
| CoOp                      | [IJCV 2022](https://arxiv.org/abs/2109.01134) |                  [link](configs/trainers/CoOp)                  |        [link](scripts/coop)        |
| Co-CoOp                   | [CVPR 2022](https://arxiv.org/abs/2203.05557) |                 [link](configs/trainers/CoCoOp)                 |       [link](scripts/cocoop)       |
| Ours                    | [arXiv](https://gordonhu608.github.io/files/revisitclip.pdf) |                  [link](configs/trainers/Ours)                  |        [link](scripts/ours)        |
<hr />

## Results
### Our method in comparison with existing methods
Results reported below show accuracy for few shot training on [1,2,4,8,16] for across 10 recognition datasets averaged over 3 seeds.

| Name                                                      | 1-shot | 2-shot |   4-shot     | 8-shot |  16-shot | 
|-----------------------------------------------------------|:---------:|:----------:|:---------:|:------:|:------:|
| [CLIP](https://arxiv.org/abs/2103.00020)                  |   36.13   |   47.83    |   58.52  |   66.24   |   72.03    |   
| [CoOp](https://arxiv.org/abs/2109.01134)                  | 59.95 |   63.74    |    67.18  |  70.52  |  74.03  | 
| [CLIP-Adapter](https://arxiv.org/abs/2110.04544)                |   62.13   |   65.64    |   69.07   |   72.55  |   76.  |  
| [ProDA](https://arxiv.org/abs/2205.03340)                |   65.19   |   68.59    |   71.4   |   74.21   |   76.78   | 
| [Ours](https://gordonhu608.github.io/files/revisitclip.pdf)          |   **71.94** | **74.12**  | **78.48** |   **80.2**    |   **82.64**   | 

## Installation 
For installation and other package requirements, please follow the instructions detailed in [INSTALL.md](docs/INSTALL.md). 

## Data preparation
Please follow the instructions at [DATASETS.md](docs/DATASETS.md) to prepare all datasets.

## Training and Evaluation
Please refer to the [RUN.md](docs/RUN.md) for detailed instructions on training, evaluating and reproducing the results using our pre-trained models.


<hr />

## Contact
If you have any questions, please create an issue on this repository or contact at w1hu@ucsd.edu


## Acknowledgements

Our code is based on [Co-CoOp, CoOp](https://github.com/KaiyangZhou/CoOp) and [MaPLe](https://github.com/muzairkhattak/multimodal-prompt-learning) repository. We thank the authors for releasing their code. If you use our model and code, please consider citing these works as well.

"
161,https://github.com/GSam789/DSC180B-Capstone-Network-Dissection,"{'kent831': 'https://github.com/kent831', 'jyulkm': 'https://github.com/jyulkm', 'GSam789': 'https://github.com/GSam789'}","{'Python': 0.88, 'Jupyter Notebook': 0.07, 'Cuda': 0.04, 'C': 0.01}","# Improving Network Accuracy through Network Manipulation


## To run our code:
Simply run ```python3 test.py``` to find the accuracies of our 3 trained VGG16 models on the first 100 test data points of CIFAR-100.

## Project Background & Context
Deep learning has been growing rapidly over the past couple decades due to its ability in solving extremely complex problems. However, this machine learning
method is often considered as a ""black box"" since it is unclear how the neurons of a deep learning model work together to arrive at the final output. A recently found
method called Network Dissection has solved this interpretability issue by coming up with a visual that shows what each neuron looks for and why. Given that we have 
information regarding neuron activities, we want to investigate if we can use this information to further improve our network's performance.

Thus, in this project, we explored 3 methods:
1. Network Dissection Intervention
2. FocusedDropout
3. Input Gradient Regularization

We implemented these network dissection intervention on VGG16 that was pre-trained on Places365.
We implemented FocusedDropout and Input Gradient Regularization separately on VGG16 on CIFAR-100 dataset. This was done due to our limited resources since training on 
Places365 would take 50 days.

## Rundown of Folders & Files
In this Github repository, you will find the following folders and files:

- Folders:
  - models: Contains all the state dictionaries of the trained models
  - utils: Contains all the model skeletons and FocusedDropout implementation

- Files:
  - dataloader.py: Loads the CIFAR-100 dataset into a DataLoader
  - plotting.py: Plots the resulting accuracies and losses over epochs
  - test_data.pt: The first 100 test data points of CIFAR-100
  - test.py: Runs our 3 VGG16 on CIFAR-100 models (Baseline, VGG16 with FocusedDropout, VGG16 with Input Gradient Regularization) on test_data.pt 
  - train_baseline_focuseddropout.py: When run, trains either baseline model (Plain VGG16) or VGG16 with FocusedDropout from scratch based on selected model in file and 
saves final model's state dictionary into the models/ folder
  - train_input_grad.py: When run, trains VGG16 with Input Gradient Regularization and saves final model's state dictionary into the models/ folder

","# Improving Network Accuracy through Network Manipulation


## To run our code:
Simply run ```python3 test.py``` to find the accuracies of our 3 trained VGG16 models on the first 100 test data points of CIFAR-100.

## Project Background & Context
Deep learning has been growing rapidly over the past couple decades due to its ability in solving extremely complex problems. However, this machine learning
method is often considered as a ""black box"" since it is unclear how the neurons of a deep learning model work together to arrive at the final output. A recently found
method called Network Dissection has solved this interpretability issue by coming up with a visual that shows what each neuron looks for and why. Given that we have 
information regarding neuron activities, we want to investigate if we can use this information to further improve our network's performance.

Thus, in this project, we explored 3 methods:
1. Network Dissection Intervention
2. FocusedDropout
3. Input Gradient Regularization

We implemented these network dissection intervention on VGG16 that was pre-trained on Places365.
We implemented FocusedDropout and Input Gradient Regularization separately on VGG16 on CIFAR-100 dataset. This was done due to our limited resources since training on 
Places365 would take 50 days.

## Rundown of Folders & Files
In this Github repository, you will find the following folders and files:

- Folders:
  - models: Contains all the state dictionaries of the trained models
  - utils: Contains all the model skeletons and FocusedDropout implementation

- Files:
  - dataloader.py: Loads the CIFAR-100 dataset into a DataLoader
  - plotting.py: Plots the resulting accuracies and losses over epochs
  - test_data.pt: The first 100 test data points of CIFAR-100
  - test.py: Runs our 3 VGG16 on CIFAR-100 models (Baseline, VGG16 with FocusedDropout, VGG16 with Input Gradient Regularization) on test_data.pt 
  - train_baseline_focuseddropout.py: When run, trains either baseline model (Plain VGG16) or VGG16 with FocusedDropout from scratch based on selected model in file and 
saves final model's state dictionary into the models/ folder
  - train_input_grad.py: When run, trains VGG16 with Input Gradient Regularization and saves final model's state dictionary into the models/ folder

"
162,https://github.com/agupta01/ml-theory-capstone,"{'agupta01': 'https://github.com/agupta01', 'ro-mish': 'https://github.com/ro-mish', 'wjluu': 'https://github.com/wjluu', 'mbouassa': 'https://github.com/mbouassa'}","{'Jupyter Notebook': 1.0, 'Python': 0.0, 'Dockerfile': 0.0}","# Benchmarking Kernel Machines on Text Datasets
Data Science Capstone Project advised by Mikhail Belkin.

## Scaling Tests
If you would like to run the expriments used in the report ""On Feature Scaling of Recursive Feature Machines"", follow the steps below:

1. Have a GPU and a GPU-enabled Pytorch v1.13 environment (feel free to use the environment.yml file in the repo, although this contains a lot of other stuff).
2. Navigate to `src` within the project root (`cd src`)
3. Run the following:
```shell
python scaling.py --name=<PROVIDE A NAME HERE> \
	--noise=<specify noise to add to dataset here> \
	--N_runs=<set to 100 for 100 trials> \
	--N=<number of examples in dataset> \
	--target_fn=<cubic for the default function, randmat for the random matrix function> \
	--baseline=<True to run baseline (Laplacian) kernel, False to train full RFM)
```
4. After the experiment is run (100 trials takes about 30 minutes when N=1000 on a RTX 2060), you can find result files in `<project root>/results/arrays/scaling_results`. Every run will generate two numpy arrays, named `train_MSEs_<name>.npy` and `test_MSEs_<name>.npy`, appended with ""\_baseline"" if a baseline run was used. Each array has shape (N_runs, len(d_range)), where d_range are the feature sizes attempted ([5, 6, 7, ..., 99] + [100, 110, 120, ..., 2000] in the base experiment in the original paper).

## How to use
```shell
usage: run.py [-h] [--verbose] {test,test-data,mnist,cifar10,fashionmnist}

positional arguments:
  {test,test-data,mnist,cifar10,fashionmnist}
                        task to run

optional arguments:
  -h, --help            show this help message and exit
  --verbose             Set logging to DEBUG
```

Example:

```shell
$ python run.py mnist --verbose
Dec 04 2022 11:38PM [DEBUG] 	 Logging set to DEBUG
Dec 04 2022 11:38PM [DEBUG] 	 Using 1259 training samples and 210 test samples
Dec 04 2022 11:38PM [DEBUG] 	 x_train shape: (1259, 784)
Dec 04 2022 11:38PM [DEBUG] 	 y_train shape: (1259,)
Dec 04 2022 11:39PM [INFO] 	 TRAIN MSE: 0.000 | Accuracy: 1.000
 TEST MSE: 0.019 | Accuracy: 0.995 | Precision: 0.995 | Recall: 0.992
```

The corresponding config files for each dataset can be found in `config/<dataset>.json`. Below is an explanation of the options:

```json
{
    ""kernel_type"": ""laplace"", # can be either laplace or gaussian
    ""print_result"": true, # to print result to log
    ""data"": {
        ""dataset"": ""mnist"", # dataset to use, can be ""mnist"", ""cifar10"", or ""fashionmnist""
        ""subset"": 0.1, # subset of dataset to use. See note below.
        ""pos_class"": 1, # class label for positive class (1)
        ""neg_class"": 8 # class label for negative class (-1)
    },
    ""model"": {
        ""gamma"": 0.00128, # kernel bandwidth, see src/utils.py for further details
        ""return_metrics"": true # return metrics rather than predictions after training kernel
    }
}
```

## A note on not breaking your computer
This code produces pairwise distance kernels for use in kernel machines for binary classification. If your dataset
has $n$ examples, your kernel will be $n \times n$! My 2018 Macbook Pro with 16GB RAM can only handle $n \approx 1000$ 
before it starts to freeze up on itself. Use the `subset` parameter in the config file, do the math, and you may 
avoid bricking your laptop for 5 minutes.

If you're using DSMLP, subset = 0.01 should work for most datasets.
","# Benchmarking Kernel Machines on Text Datasets
Data Science Capstone Project advised by Mikhail Belkin.

## Scaling Tests
If you would like to run the expriments used in the report ""On Feature Scaling of Recursive Feature Machines"", follow the steps below:

1. Have a GPU and a GPU-enabled Pytorch v1.13 environment (feel free to use the environment.yml file in the repo, although this contains a lot of other stuff).
2. Navigate to `src` within the project root (`cd src`)
3. Run the following:
```shell
python scaling.py --name=<PROVIDE A NAME HERE> \
	--noise=<specify noise to add to dataset here> \
	--N_runs=<set to 100 for 100 trials> \
	--N=<number of examples in dataset> \
	--target_fn=<cubic for the default function, randmat for the random matrix function> \
	--baseline=<True to run baseline (Laplacian) kernel, False to train full RFM)
```
4. After the experiment is run (100 trials takes about 30 minutes when N=1000 on a RTX 2060), you can find result files in `<project root>/results/arrays/scaling_results`. Every run will generate two numpy arrays, named `train_MSEs_<name>.npy` and `test_MSEs_<name>.npy`, appended with ""\_baseline"" if a baseline run was used. Each array has shape (N_runs, len(d_range)), where d_range are the feature sizes attempted ([5, 6, 7, ..., 99] + [100, 110, 120, ..., 2000] in the base experiment in the original paper).

## How to use
```shell
usage: run.py [-h] [--verbose] {test,test-data,mnist,cifar10,fashionmnist}

positional arguments:
  {test,test-data,mnist,cifar10,fashionmnist}
                        task to run

optional arguments:
  -h, --help            show this help message and exit
  --verbose             Set logging to DEBUG
```

Example:

```shell
$ python run.py mnist --verbose
Dec 04 2022 11:38PM [DEBUG] 	 Logging set to DEBUG
Dec 04 2022 11:38PM [DEBUG] 	 Using 1259 training samples and 210 test samples
Dec 04 2022 11:38PM [DEBUG] 	 x_train shape: (1259, 784)
Dec 04 2022 11:38PM [DEBUG] 	 y_train shape: (1259,)
Dec 04 2022 11:39PM [INFO] 	 TRAIN MSE: 0.000 | Accuracy: 1.000
 TEST MSE: 0.019 | Accuracy: 0.995 | Precision: 0.995 | Recall: 0.992
```

The corresponding config files for each dataset can be found in `config/<dataset>.json`. Below is an explanation of the options:

```json
{
    ""kernel_type"": ""laplace"", # can be either laplace or gaussian
    ""print_result"": true, # to print result to log
    ""data"": {
        ""dataset"": ""mnist"", # dataset to use, can be ""mnist"", ""cifar10"", or ""fashionmnist""
        ""subset"": 0.1, # subset of dataset to use. See note below.
        ""pos_class"": 1, # class label for positive class (1)
        ""neg_class"": 8 # class label for negative class (-1)
    },
    ""model"": {
        ""gamma"": 0.00128, # kernel bandwidth, see src/utils.py for further details
        ""return_metrics"": true # return metrics rather than predictions after training kernel
    }
}
```

## A note on not breaking your computer
This code produces pairwise distance kernels for use in kernel machines for binary classification. If your dataset
has $n$ examples, your kernel will be $n \times n$! My 2018 Macbook Pro with 16GB RAM can only handle $n \approx 1000$ 
before it starts to freeze up on itself. Use the `subset` parameter in the config file, do the math, and you may 
avoid bricking your laptop for 5 minutes.

If you're using DSMLP, subset = 0.01 should work for most datasets.
"
163,https://github.com/jimzers/DSC180B-A08,"{'jimzers': 'https://github.com/jimzers', 'scott-yj-yang': 'https://github.com/scott-yj-yang', 'danielcson': 'https://github.com/danielcson', 'a-murali': 'https://github.com/a-murali'}","{'Jupyter Notebook': 0.96, 'Python': 0.03, 'Shell': 0.0}","# DSC180B A08: Imitating Behavior to Understand the Brain

Scott Yang, Daniel Son, Akshay Murali, Adam Lee, Eric Leonardis, Talmo Pereira

#### Repos combined:

- https://github.com/danielcson/dsc_capstone_q1
- https://github.com/scott-yj-yang/180A-codebase
- https://github.com/akdec00/DSC-180A
- https://github.com/jimzers/dsc180-ma5

#### DSMLP Spawning Script

To train the model using UC San Diego's Data Science & Machine Learning Platform (DSMLP), you can setup a training
environment in the following commands

```bash
# on dsmlp's bash
IDENTITY_PROXY_PORTS=1 launch-scipy-ml.sh -b -j -g 1 -m 32 -c 10 -i scottyang17/dm:latest
```

Explanation: `IDENTITY_PROXY_PORTS=1` allow DSMLP's proxy port forwarding another empty port for the sake of record
keeping interfaces such as `tensorboard`. `-b` means run the pod in background mode, `-j` means launch Jupyter notebook
server within container (default), `-i` means custom docker image name.

#### Training Procedures

Entrypoint: Train expert SAC agent with `train_cheetah.py`

```bash
python train_cheetah.py --automatic_entropy_tuning=True
```

Extract activations

```bash
python src/run/extract_activations.py --model_path data/models/sac_checkpoint_cheetah_123456_10000 --env_name HalfCheetah-v4 --num_episodes 1000 --save_path data/activations/cheetah_123456_10000
```

Collect expert data

```bash
python src/run/collect_expert.py --model_path data/models/sac_checkpoint_cheetah_123456_10000 --env_name HalfCheetah-v4 --num_episodes 15 --save_path data/rollouts/cheetah_123456_10000
```

Train behavioral cloning agent

```bash
python src/run/train_bc.py --rollout_path data/rollouts/cheetah_123456_10000/rollouts.pkl --save_path data/bc_model/cheetah_123456_10000 --epochs 10 --batch_size 32 --lr 3e-4
```

TODO: Run analysis

```bash
python run_analysis --policy=path/to/policy --analysis_path=path/to/analysis 
```


### Fire and forget bash scripts

```bash
bash scripts/collect_expert.sh
bash scripts/train_bc.sh
bash scripts/collect_activations.sh
bash scripts/collect_activations_bc.sh
```
","# DSC180B A08: Imitating Behavior to Understand the Brain

Scott Yang, Daniel Son, Akshay Murali, Adam Lee, Eric Leonardis, Talmo Pereira

#### Repos combined:

- https://github.com/danielcson/dsc_capstone_q1
- https://github.com/scott-yj-yang/180A-codebase
- https://github.com/akdec00/DSC-180A
- https://github.com/jimzers/dsc180-ma5

#### DSMLP Spawning Script

To train the model using UC San Diego's Data Science & Machine Learning Platform (DSMLP), you can setup a training
environment in the following commands

```bash
# on dsmlp's bash
IDENTITY_PROXY_PORTS=1 launch-scipy-ml.sh -b -j -g 1 -m 32 -c 10 -i scottyang17/dm:latest
```

Explanation: `IDENTITY_PROXY_PORTS=1` allow DSMLP's proxy port forwarding another empty port for the sake of record
keeping interfaces such as `tensorboard`. `-b` means run the pod in background mode, `-j` means launch Jupyter notebook
server within container (default), `-i` means custom docker image name.

#### Training Procedures

Entrypoint: Train expert SAC agent with `train_cheetah.py`

```bash
python train_cheetah.py --automatic_entropy_tuning=True
```

Extract activations

```bash
python src/run/extract_activations.py --model_path data/models/sac_checkpoint_cheetah_123456_10000 --env_name HalfCheetah-v4 --num_episodes 1000 --save_path data/activations/cheetah_123456_10000
```

Collect expert data

```bash
python src/run/collect_expert.py --model_path data/models/sac_checkpoint_cheetah_123456_10000 --env_name HalfCheetah-v4 --num_episodes 15 --save_path data/rollouts/cheetah_123456_10000
```

Train behavioral cloning agent

```bash
python src/run/train_bc.py --rollout_path data/rollouts/cheetah_123456_10000/rollouts.pkl --save_path data/bc_model/cheetah_123456_10000 --epochs 10 --batch_size 32 --lr 3e-4
```

TODO: Run analysis

```bash
python run_analysis --policy=path/to/policy --analysis_path=path/to/analysis 
```


### Fire and forget bash scripts

```bash
bash scripts/collect_expert.sh
bash scripts/train_bc.sh
bash scripts/collect_activations.sh
bash scripts/collect_activations_bc.sh
```
"
164,https://github.com/jmryan19/DSC180,"{'jmryan19': 'https://github.com/jmryan19', 'ryanjake19': 'https://github.com/ryanjake19'}","{'Jupyter Notebook': 1.0, 'Python': 0.0}","# DSC180
Helpers contatins python files of helper code written to make experimentation faster
Most exploring is done in 1-3_NAME.ipynb.

Unfortunately, github cannot hold the weights of the final model, as the file is too large. 

In order to run:
Must be run with a GPU!
python run.py [test or train]

Train will go through the training process of the four omodels utilized in the paper.
Test will run all statistics on models used on paper and regenerate figures.

The website corresponding to this project is located at https://jmryan19.github.io/DSC180.io/
","# DSC180
Helpers contatins python files of helper code written to make experimentation faster
Most exploring is done in 1-3_NAME.ipynb.

Unfortunately, github cannot hold the weights of the final model, as the file is too large. 

In order to run:
Must be run with a GPU!
python run.py [test or train]

Train will go through the training process of the four omodels utilized in the paper.
Test will run all statistics on models used on paper and regenerate figures.

The website corresponding to this project is located at https://jmryan19.github.io/DSC180.io/
"
165,https://github.com/ddav118/DSC-180B,"{'ddav118': 'https://github.com/ddav118', 'MarcoM310': 'https://github.com/MarcoM310', 'YashPotdar': 'https://github.com/YashPotdar'}","{'Jupyter Notebook': 1.0, 'Python': 0.0}","# UC San Diego, DSC-180B, Winter 2023 <br>
Predicting Pulmonary Edema Using Deep Learning and Image Segmentation <br>
Team Members: David Davila-Garcia, Marco Morocho, Yash Potdar

Note: All data was deidentified but is not publicly available

├── README.md          <- The top-level README for developers using this project.<br>
├── Final_Report.pdf<br>
├── Final_Poster.pdf<br>
├── models             <- Contains the outputs from trained models: Losses and Test Set Predictions<br>
│   ├── Losses         <- Training and Validation MAE Losses by Epoch.<br>
│   ├── Test Set Preds <- NT-proBNP predictions on test set using the best model (minimize MAE valid loss).<br>
├── 1 - Preprocessing.ipynb                               <- Cleaning the original x-rays + clinical data, excluding rows with missing data/no image available<br>
├── 2 - Transfer Learning Training & Evaluation.ipynb     <- (Not used in project) Provided by UCSD AIDA Lab, shows training of U-Net segmentation model.<br>
├── 3 - Predicting Unannotated.ipynb                      <- Used the U-Net segmentation model from the UCSD AIDA Lab to create binary masks (lungs, heart, clavicles, spinal column) for each radiograph in our dataset. Saved the segmentations to an hdf5 file. <br>
├── 4 - Creating Masks.ipynb                              <- Uses the binary masks created in '3 - Predicting Unannotated.ipynb' to produce the segmentation inputs for our model <br>
├── 5 - CNN Models.ipynb                                  <- Contains all code for training and testing models. <br>
├── model.py           <- Contains modified ResNet152 architectures, extends the Pytorch ResNet152 implementation <br>
├── train.py           <- Contains model training and testing functions; different inputs called for different architectures<br>


Acknowledgements: Thank you to our incredible mentor Albert Hsiao, MD, PhD for his guidance, and Amin Mahmoodi for providing the U-Net segmentation network.
","# UC San Diego, DSC-180B, Winter 2023 <br>
Predicting Pulmonary Edema Using Deep Learning and Image Segmentation <br>
Team Members: David Davila-Garcia, Marco Morocho, Yash Potdar

Note: All data was deidentified but is not publicly available

├── README.md          <- The top-level README for developers using this project.<br>
├── Final_Report.pdf<br>
├── Final_Poster.pdf<br>
├── models             <- Contains the outputs from trained models: Losses and Test Set Predictions<br>
│   ├── Losses         <- Training and Validation MAE Losses by Epoch.<br>
│   ├── Test Set Preds <- NT-proBNP predictions on test set using the best model (minimize MAE valid loss).<br>
├── 1 - Preprocessing.ipynb                               <- Cleaning the original x-rays + clinical data, excluding rows with missing data/no image available<br>
├── 2 - Transfer Learning Training & Evaluation.ipynb     <- (Not used in project) Provided by UCSD AIDA Lab, shows training of U-Net segmentation model.<br>
├── 3 - Predicting Unannotated.ipynb                      <- Used the U-Net segmentation model from the UCSD AIDA Lab to create binary masks (lungs, heart, clavicles, spinal column) for each radiograph in our dataset. Saved the segmentations to an hdf5 file. <br>
├── 4 - Creating Masks.ipynb                              <- Uses the binary masks created in '3 - Predicting Unannotated.ipynb' to produce the segmentation inputs for our model <br>
├── 5 - CNN Models.ipynb                                  <- Contains all code for training and testing models. <br>
├── model.py           <- Contains modified ResNet152 architectures, extends the Pytorch ResNet152 implementation <br>
├── train.py           <- Contains model training and testing functions; different inputs called for different architectures<br>


Acknowledgements: Thank you to our incredible mentor Albert Hsiao, MD, PhD for his guidance, and Amin Mahmoodi for providing the U-Net segmentation network.
"
166,https://github.com/shivsakthivel/CNN-Multilabel-Classification,{'shivsakthivel': 'https://github.com/shivsakthivel'},"{'Jupyter Notebook': 1.0, 'Python': 0.0}","# Exploring the viability of Convolutional Neural Networks (CNNs) on a multi-label classification task to detect radiographic outliers

## Task
An implementation of a Convolutional Neural Network (CNN) multi-label classifier that takes in chest radiograph images as and outputs their corresponding predicted labels for detecting pulmonary edema and pleural effusion.

## Retrieving the Data for this project
The data available for this project came in the form of DICOM files stored on a Google Cloud instance (credentialed access only), with the entire database being of size 4 TB. The required credentialing can be obtained [here](https://physionet.org/content/mimic-cxr/2.0.0/). For the purposes of this project and accessing the DSMLP resources, the source radiograph images had to be manually downloaded in batches and transferred onto the teams drive on DSMLP. 

The scripts associated with this repository, therefore, assume that the user has the required access to the data files, with the required filepaths relative to the directory in which they were developed. However, this GitHub repository contains exploratory notebooks, walking through the data access and model training process, and covers examples of the obtained results. Specifically, the notebook `Single-Var-Model-Edema.ipynb` is a comprehensive exploration of one of the single label binary classifiers developed for this project. The model build and evaluation techniques for the other models developed in this project largely follow a similar process.

## Build and Run
- To run the single label Pulmonary Edema classifier run `python main.py edema`.
- To run the single label Pleural Effusion classifier run `python main.py effusion`.
- To run the multi-label classifier run `python main.py multilabel`.
- To run the multi-class classifier run `python main.py multiclass`.

## Requirements
The dependencies required for this project can be installed by running `pip install -r requirements.txt`.

## Other Notes
If running the code on this repository, the DSMLP instance should be launched with GPU to ensure that the files run efficiently (The project was developed using tensorflow).
","# Exploring the viability of Convolutional Neural Networks (CNNs) on a multi-label classification task to detect radiographic outliers

## Task
An implementation of a Convolutional Neural Network (CNN) multi-label classifier that takes in chest radiograph images as and outputs their corresponding predicted labels for detecting pulmonary edema and pleural effusion.

## Retrieving the Data for this project
The data available for this project came in the form of DICOM files stored on a Google Cloud instance (credentialed access only), with the entire database being of size 4 TB. The required credentialing can be obtained [here](https://physionet.org/content/mimic-cxr/2.0.0/). For the purposes of this project and accessing the DSMLP resources, the source radiograph images had to be manually downloaded in batches and transferred onto the teams drive on DSMLP. 

The scripts associated with this repository, therefore, assume that the user has the required access to the data files, with the required filepaths relative to the directory in which they were developed. However, this GitHub repository contains exploratory notebooks, walking through the data access and model training process, and covers examples of the obtained results. Specifically, the notebook `Single-Var-Model-Edema.ipynb` is a comprehensive exploration of one of the single label binary classifiers developed for this project. The model build and evaluation techniques for the other models developed in this project largely follow a similar process.

## Build and Run
- To run the single label Pulmonary Edema classifier run `python main.py edema`.
- To run the single label Pleural Effusion classifier run `python main.py effusion`.
- To run the multi-label classifier run `python main.py multilabel`.
- To run the multi-class classifier run `python main.py multiclass`.

## Requirements
The dependencies required for this project can be installed by running `pip install -r requirements.txt`.

## Other Notes
If running the code on this repository, the DSMLP instance should be launched with GPU to ensure that the files run efficiently (The project was developed using tensorflow).
"
167,https://github.com/Angela-Wang111/Pneumothorax_classification,"{'cecilialmw': 'https://github.com/cecilialmw', 'Angela-Wang111': 'https://github.com/Angela-Wang111'}","{'Jupyter Notebook': 1.0, 'Python': 0.0}","# DSC-180B-Project
**Project Topic:** classification of penumothorax dataset [CANDID-PTX](https://pubs.rsna.org/doi/10.1148/ryai.2021210136) using classification models, segmentation models, and cascade models.

GROUP NAME: AC/DS :metal: (Angela + Cecilia -> AC, Data Science -> DS, AC/DC -> AC/DS) :fist_right::fist_left:

**Brave Angela Not Afraid of Error :partying_face:**

**Be Calm and Write Code Cecilia :innocent:**

## Goal :pray:
- [x] Birthday (Angela) :birthday:
- [x] Classification Final Results
- [x] Segmentation Final Results
- [x] Cascade Final Results
- [x] Poster [03/09 DDL]
- [x] Website/Report/Code [03/14 DDL]
- [ ] Presentation/Birthday (Cecilia) :birthday: [03/15 DDL]

## Website
If you just want to have an idea of what this project is about without seeing all these codes (which I understand :stuck_out_tongue_winking_eye:), click here :point_right: https://angela-wang111.github.io/Pneumothorax_classification/

## Documentation
The useful files for checkpoint testing phase are: run.py, submission.json, config.json, src(folder), test(folder), and outpout(folder). 

:heavy_exclamation_mark:Execution instruction (in terminal):
1. `ssh <username>@dsmlp-login.ucsd.edu`
2. `launch.sh -i angela010101/pneumothorax:latest -c 8 -m 64 -g 1` at least 1 GPU and 64 GB memory is needed
3. `git clone https://github.com/Angela-Wang111/Pneumothorax_classification` (first time execution only)
4. `cd Pneumothorax_classification`
5. `python run.py <model type>` model type has to be one of ""classification"", ""segmentation"", or ""cascade""
6. :crossed_fingers:
### submission.json
Contains the URLs for this Github Repository and the DockerHub Repository used for building a docker image for this pipeline.
### config.json
Contains the hyperparameters used for model training.
### run.py
This is the main .py file for executing the whole pipeline from data preprocessing to training and test classification models, segmentation models, cascade models. To run it, just run `python run.py <model type>` in the terminal (and hope everything goes fine :crossed_fingers: ). Model type has to be in the following three formats: ""classification"", ""segmentation"", ""cascade"" (all in lowercase). Example of the full terminal command: `python run.py classification`.
### src
#### data_preprocessing.py
This file contains functions to decode the RLE encoded pixels from the source ""test/testdata/Pneumothorax_reports_small.csv"" file and to save both positive and negative masks into test/testdata/masks, so they could be used for the segmentation model training/testing.
#### generate_train_val_test_csv.py
This file contains functions to generate ""test/testdata/train.csv"", ""test/testdata/train_pos.csv"", ""test/testdata/train_neg.csv"", ""test/testdata/validation.csv"", and ""test/testdata/test.csv"" for model training/validation/test.
#### create_dataloader.py
This file contains functions to create dataframe from .csv file like ""test/testdata/validation.csv"", create custermized Dataset, and create DataLoader. The function to create customized Dataset is modified to read .png formatted original images. The full version code is written to read DICOM format images stored in the team group folder.
#### build_model.py
This file contains functions to build customized pytorch pretrained ResNet 34 model and pretrained EfficientNet-B3 model, and train/validate classification models & segmentation models & cascade models.
#### evaluate_test.py
This file contains fucntions to plot, print, and save metrics based on the test set to evaluate all models.
#### save_model_imgs.py
This file contains functions to save predicted masks from pre-trained segmentation models. Mainly used for preparing images to be input to the classification models during the cascade model training.
#### run_model.py
This file contains functions to execute the entire pipeline to run classification, segmentation, and cascade models. It automatically run all models within the same structure.
- We currently disabled save_model() function to avoid saving large files on github. To enable it, please uncomment the lines `save_model(cla_model, file_name)` in the run_class() function and `save_model(seg_model, file_name)` in the run_seg() function.

### test
All the data here are just a small portion of CANDID-PTX (100/19237) for pipeline testing purpose only since the original data size is ~30GB.
*All empty folders currently store the outputs after test trials.*
#### testdata
- *Pneumothorax_reports_small.csv*: source test .csv file. Includes 100 penumothorax samples (15 positive, 85 negative) with **SOPInstanceUID** to identify each sample, and **EncodedPixels** to specify the penumothorax region (in RLE encoded format if positive, -1 if negative).
- *images*: folder contains the original X-Ray images (1024x1024). Named in the format ""\<SOPInstanceUID>.png"". The original images are in DICOM format, but are changed to .png format for the pipeline testing purpose. 
- *masks*: empty folder to store the decoded binary masks.
- *intermediate_data*: empty folder to store the intermediate images generated by the segmentation models. These images will be used as input for classification models in the cascade structure.

After runing the pipeline, the following files would be created :point_down:
- *masks*: folder contains the decoded binary masks (1024x1024). Named in the format ""\<SOPInstanceUID>.png"" if positive, ""negative_mask.png"" if negative.
- *train.csv*: training set with 80 samples (12 postive, 68 negative).
- *train_pos.csv*: all positive samples in the training set
- *train_neg.csv*: all negative samples in the training set
- *validation.csv*: validation set with 10 samples (2 positive, 8 negative).
- *test.csv*: test set with 10 samples (1 positive, 9 negative).

#### saved_model
This is the folder where saved models will be located if the function is enabled.

### output
This should be an empty folder before executing the pipeline. After executing the pipeline, the following metrics plots will be created :point_down:
- auc-roc plot inside *auc-roc* folder.
- train/val losses inside *both_loss* folder.
- confusion matrix inside *confusion_matrix* folder.
","# DSC-180B-Project
**Project Topic:** classification of penumothorax dataset [CANDID-PTX](https://pubs.rsna.org/doi/10.1148/ryai.2021210136) using classification models, segmentation models, and cascade models.

GROUP NAME: AC/DS :metal: (Angela + Cecilia -> AC, Data Science -> DS, AC/DC -> AC/DS) :fist_right::fist_left:

**Brave Angela Not Afraid of Error :partying_face:**

**Be Calm and Write Code Cecilia :innocent:**

## Goal :pray:
- [x] Birthday (Angela) :birthday:
- [x] Classification Final Results
- [x] Segmentation Final Results
- [x] Cascade Final Results
- [x] Poster [03/09 DDL]
- [x] Website/Report/Code [03/14 DDL]
- [ ] Presentation/Birthday (Cecilia) :birthday: [03/15 DDL]

## Website
If you just want to have an idea of what this project is about without seeing all these codes (which I understand :stuck_out_tongue_winking_eye:), click here :point_right: https://angela-wang111.github.io/Pneumothorax_classification/

## Documentation
The useful files for checkpoint testing phase are: run.py, submission.json, config.json, src(folder), test(folder), and outpout(folder). 

:heavy_exclamation_mark:Execution instruction (in terminal):
1. `ssh <username>@dsmlp-login.ucsd.edu`
2. `launch.sh -i angela010101/pneumothorax:latest -c 8 -m 64 -g 1` at least 1 GPU and 64 GB memory is needed
3. `git clone https://github.com/Angela-Wang111/Pneumothorax_classification` (first time execution only)
4. `cd Pneumothorax_classification`
5. `python run.py <model type>` model type has to be one of ""classification"", ""segmentation"", or ""cascade""
6. :crossed_fingers:
### submission.json
Contains the URLs for this Github Repository and the DockerHub Repository used for building a docker image for this pipeline.
### config.json
Contains the hyperparameters used for model training.
### run.py
This is the main .py file for executing the whole pipeline from data preprocessing to training and test classification models, segmentation models, cascade models. To run it, just run `python run.py <model type>` in the terminal (and hope everything goes fine :crossed_fingers: ). Model type has to be in the following three formats: ""classification"", ""segmentation"", ""cascade"" (all in lowercase). Example of the full terminal command: `python run.py classification`.
### src
#### data_preprocessing.py
This file contains functions to decode the RLE encoded pixels from the source ""test/testdata/Pneumothorax_reports_small.csv"" file and to save both positive and negative masks into test/testdata/masks, so they could be used for the segmentation model training/testing.
#### generate_train_val_test_csv.py
This file contains functions to generate ""test/testdata/train.csv"", ""test/testdata/train_pos.csv"", ""test/testdata/train_neg.csv"", ""test/testdata/validation.csv"", and ""test/testdata/test.csv"" for model training/validation/test.
#### create_dataloader.py
This file contains functions to create dataframe from .csv file like ""test/testdata/validation.csv"", create custermized Dataset, and create DataLoader. The function to create customized Dataset is modified to read .png formatted original images. The full version code is written to read DICOM format images stored in the team group folder.
#### build_model.py
This file contains functions to build customized pytorch pretrained ResNet 34 model and pretrained EfficientNet-B3 model, and train/validate classification models & segmentation models & cascade models.
#### evaluate_test.py
This file contains fucntions to plot, print, and save metrics based on the test set to evaluate all models.
#### save_model_imgs.py
This file contains functions to save predicted masks from pre-trained segmentation models. Mainly used for preparing images to be input to the classification models during the cascade model training.
#### run_model.py
This file contains functions to execute the entire pipeline to run classification, segmentation, and cascade models. It automatically run all models within the same structure.
- We currently disabled save_model() function to avoid saving large files on github. To enable it, please uncomment the lines `save_model(cla_model, file_name)` in the run_class() function and `save_model(seg_model, file_name)` in the run_seg() function.

### test
All the data here are just a small portion of CANDID-PTX (100/19237) for pipeline testing purpose only since the original data size is ~30GB.
*All empty folders currently store the outputs after test trials.*
#### testdata
- *Pneumothorax_reports_small.csv*: source test .csv file. Includes 100 penumothorax samples (15 positive, 85 negative) with **SOPInstanceUID** to identify each sample, and **EncodedPixels** to specify the penumothorax region (in RLE encoded format if positive, -1 if negative).
- *images*: folder contains the original X-Ray images (1024x1024). Named in the format ""\<SOPInstanceUID>.png"". The original images are in DICOM format, but are changed to .png format for the pipeline testing purpose. 
- *masks*: empty folder to store the decoded binary masks.
- *intermediate_data*: empty folder to store the intermediate images generated by the segmentation models. These images will be used as input for classification models in the cascade structure.

After runing the pipeline, the following files would be created :point_down:
- *masks*: folder contains the decoded binary masks (1024x1024). Named in the format ""\<SOPInstanceUID>.png"" if positive, ""negative_mask.png"" if negative.
- *train.csv*: training set with 80 samples (12 postive, 68 negative).
- *train_pos.csv*: all positive samples in the training set
- *train_neg.csv*: all negative samples in the training set
- *validation.csv*: validation set with 10 samples (2 positive, 8 negative).
- *test.csv*: test set with 10 samples (1 positive, 9 negative).

#### saved_model
This is the folder where saved models will be located if the function is enabled.

### output
This should be an empty folder before executing the pipeline. After executing the pipeline, the following metrics plots will be created :point_down:
- auc-roc plot inside *auc-roc* folder.
- train/val losses inside *both_loss* folder.
- confusion matrix inside *confusion_matrix* folder.
"
168,https://github.com/styyxofficial/DSC180B-Quarter-2-Project,"{'styyxofficial': 'https://github.com/styyxofficial', 'rishbob13': 'https://github.com/rishbob13', 'Saketarora13': 'https://github.com/Saketarora13'}","{'HTML': 0.59, 'Jupyter Notebook': 0.41, 'Python': 0.0, 'Dockerfile': 0.0, 'CSS': 0.0}","# Processing Electrophysiology Data to Extract Neural Trajectories

Raw electrophysiology data is very high dimensional and contains a lot of noisy, spiky, activity. Due to this, it must be heavily processed before the accurate neural trajectories can be extracted.
We utilized Variational Latent Gaussian Process in our study to reduce its dimensions and smooth our data.

Using this dimensionality reduced smooth data, we created a classifier to predict mouse behavior.

## Variational Latent Gaussian Process

In a variational latent Gaussian process (VLGP), the observed data, y, is modeled as a Gaussian process, with mean function, f(x), and covariance function, k(x, x'). The underlying structure in the data is captured by latent variables, z, which are treated as random variables. The prior distribution over the latent variables is modeled as a Gaussian distribution.

The goal of the VLGP is to infer the posterior distribution, q(z|x), over the latent variables given the observed data. This is done using variational inference by minimizing the objective function, also known as the evidence lower bound (ELBO), given by:
$ELBO = -D_{KL}(q(z|x) || p(z)) + E_{q(z|x)}[log(p(y|z,x))]$
where $D_{KL}$ is the Kullback-Leibler divergence, which measures the difference between two distributions, and E is the expected value. The first term in the ELBO encourages the approximate posterior, q(z|x), to be close to the prior, p(z), while the second term represents the negative log-likelihood of the data given the latent variables. [1]

The optimization problem can be solved using gradient-based optimization algorithms, such as gradient descent or conjugate gradient. The solution provides estimates of the latent variables, which can be used to reconstruct the hidden patterns in the data. For the purposes of our project, vLGP is used to extract neural trajectories, which are the underlying patterns in neural activity that reflect how the brain processes information.
# To Run
`python run.py <config_name>.json`

Config files are .json files stored in ""config/"". They contain the hyperparameters of the model, as well as the PID, EID, and probe of the mouse, which determines what data will be analyzed. You can go to [the IBL website](https://viz.internationalbrainlab.org/app) to get different data. If the data you want is not already in ""data/raw/ONE/"", then in run.py remove the `mode='local'` flag when instantiating ONE.

Outputs of run.py will be stored in ""output/<exp_name>/""
","# Processing Electrophysiology Data to Extract Neural Trajectories

Raw electrophysiology data is very high dimensional and contains a lot of noisy, spiky, activity. Due to this, it must be heavily processed before the accurate neural trajectories can be extracted.
We utilized Variational Latent Gaussian Process in our study to reduce its dimensions and smooth our data.

Using this dimensionality reduced smooth data, we created a classifier to predict mouse behavior.

## Variational Latent Gaussian Process

In a variational latent Gaussian process (VLGP), the observed data, y, is modeled as a Gaussian process, with mean function, f(x), and covariance function, k(x, x'). The underlying structure in the data is captured by latent variables, z, which are treated as random variables. The prior distribution over the latent variables is modeled as a Gaussian distribution.

The goal of the VLGP is to infer the posterior distribution, q(z|x), over the latent variables given the observed data. This is done using variational inference by minimizing the objective function, also known as the evidence lower bound (ELBO), given by:
$ELBO = -D_{KL}(q(z|x) || p(z)) + E_{q(z|x)}[log(p(y|z,x))]$
where $D_{KL}$ is the Kullback-Leibler divergence, which measures the difference between two distributions, and E is the expected value. The first term in the ELBO encourages the approximate posterior, q(z|x), to be close to the prior, p(z), while the second term represents the negative log-likelihood of the data given the latent variables. [1]

The optimization problem can be solved using gradient-based optimization algorithms, such as gradient descent or conjugate gradient. The solution provides estimates of the latent variables, which can be used to reconstruct the hidden patterns in the data. For the purposes of our project, vLGP is used to extract neural trajectories, which are the underlying patterns in neural activity that reflect how the brain processes information.
# To Run
`python run.py <config_name>.json`

Config files are .json files stored in ""config/"". They contain the hyperparameters of the model, as well as the PID, EID, and probe of the mouse, which determines what data will be analyzed. You can go to [the IBL website](https://viz.internationalbrainlab.org/app) to get different data. If the data you want is not already in ""data/raw/ONE/"", then in run.py remove the `mode='local'` flag when instantiating ONE.

Outputs of run.py will be stored in ""output/<exp_name>/""
"
169,https://github.com/somet3000/1kgp-coverage-analysis,{'somet3000': 'https://github.com/somet3000'},"{'R': 0.81, 'Python': 0.11, 'Shell': 0.06, 'Dockerfile': 0.02}","# 1KGP Coverage Analysis
Expression quantitative trait loci (eQTL) and fine-mapping analysis of a cohort from the 1000 Genomes Project using both lower coverage and higher coverage (30x) data.

The raw VCF data can be obtained from the 1000 Genomes Project: https://www.internationalgenome.org/data

The gene expression data can be obtained from the BioStudies website for the RNA-sequencing 1KGP paper: https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-GEUV-1/sdrf?full=true

The analysis is a little bit different when running on at-scale versus test data, leading to different files between them. To run this code on the test data: ```python run.py test```. The test code will produce the QQ-plots and fine-mapping plots from the test data in the repository directory. The QQ plot will be in ```output_qqplot_test.pdf``` and the fine-mapping plots will be in the ```Rplots.pdf``` file. To run this code on the real data: ```python run.py all```

This repository uses plink (https://www.cog-genomics.org/plink/1.9/), Matrix eQTL (http://www.bios.unc.edu/research/genomic_software/Matrix_eQTL/), susieR (https://github.com/stephenslab/susieR), and UCSC LiftOver (https://genome.ucsc.edu/cgi-bin/hgLiftOver). Check them out! 

Thanks for stopping by this repository! :)

","# 1KGP Coverage Analysis
Expression quantitative trait loci (eQTL) and fine-mapping analysis of a cohort from the 1000 Genomes Project using both lower coverage and higher coverage (30x) data.

The raw VCF data can be obtained from the 1000 Genomes Project: https://www.internationalgenome.org/data

The gene expression data can be obtained from the BioStudies website for the RNA-sequencing 1KGP paper: https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-GEUV-1/sdrf?full=true

The analysis is a little bit different when running on at-scale versus test data, leading to different files between them. To run this code on the test data: ```python run.py test```. The test code will produce the QQ-plots and fine-mapping plots from the test data in the repository directory. The QQ plot will be in ```output_qqplot_test.pdf``` and the fine-mapping plots will be in the ```Rplots.pdf``` file. To run this code on the real data: ```python run.py all```

This repository uses plink (https://www.cog-genomics.org/plink/1.9/), Matrix eQTL (http://www.bios.unc.edu/research/genomic_software/Matrix_eQTL/), susieR (https://github.com/stephenslab/susieR), and UCSC LiftOver (https://genome.ucsc.edu/cgi-bin/hgLiftOver). Check them out! 

Thanks for stopping by this repository! :)

"
170,https://github.com/jacquelinekclee/twas-dsc180-a17,"{'notsamzhou': 'https://github.com/notsamzhou', 'jacquelinekclee': 'https://github.com/jacquelinekclee'}",{'Python': 1.0},"# Application of Transcriptome-Wide Association Studies for Identifying Genes Associated with Inflammatory Bowel Disease

Find the capston project website, including the full report and summary of the project's background, analysis, and findings, [here](https://notsamzhou.github.io/twas/).

## Running the analysis

This repository provides a pipeline to perform a TWAS analysis.

This analysis requires the DockerHub repository at `notsamzhou/twas:latest`

To run the analysis, run `python run.py all`

If running another analysis with the same gene expression and variant data but different GWAS summary statistics, we do not need to recompute weights for each gene. Just run  `python run.py assoc` with an updated data-params.json

To run the respository on test data, run `python run.py test`

## Obtaining raw data

The primary vcfs used in the analysis can be downloaded from [here](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20110521/ALL.chr22.phase1_release_v3.20101123.snps_indels_svs.genotypes.vcf.gz) and [here](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20110521/ALL.chr22.phase1_release_v3.20101123.snps_indels_svs.genotypes.vcf.gz.tbi). This analysis used the Chromosome 22 vcfs from the 1000 Genomes Project.

The gene expression data can downloaded from [here](https://www.ebi.ac.uk/biostudies/files/E-GEUV-1/E-GEUV-1/analysis_results/GD462.GeneQuantRPKM.50FN.samplename.resk10.txt.gz).

The population data can be downloaded from [here](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20110521/phase1_integrated_calls.20101123.ALL.panel).

Various summary statistic files can be downloaded from [here](https://github.com/TiffanyAmariuta/TCSC/tree/main/sumstats) based on a disease of interest.

[This file](https://drive.google.com/uc?export=download&id=1gd6FP4qlteo1dBoAH8zGkXzbZvs2PPt4), which provides IDs and locations for various genes, is also required for plotting purposes.

All of these files should be placed directly in data/raw
","# Application of Transcriptome-Wide Association Studies for Identifying Genes Associated with Inflammatory Bowel Disease

Find the capston project website, including the full report and summary of the project's background, analysis, and findings, [here](https://notsamzhou.github.io/twas/).

## Running the analysis

This repository provides a pipeline to perform a TWAS analysis.

This analysis requires the DockerHub repository at `notsamzhou/twas:latest`

To run the analysis, run `python run.py all`

If running another analysis with the same gene expression and variant data but different GWAS summary statistics, we do not need to recompute weights for each gene. Just run  `python run.py assoc` with an updated data-params.json

To run the respository on test data, run `python run.py test`

## Obtaining raw data

The primary vcfs used in the analysis can be downloaded from [here](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20110521/ALL.chr22.phase1_release_v3.20101123.snps_indels_svs.genotypes.vcf.gz) and [here](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20110521/ALL.chr22.phase1_release_v3.20101123.snps_indels_svs.genotypes.vcf.gz.tbi). This analysis used the Chromosome 22 vcfs from the 1000 Genomes Project.

The gene expression data can downloaded from [here](https://www.ebi.ac.uk/biostudies/files/E-GEUV-1/E-GEUV-1/analysis_results/GD462.GeneQuantRPKM.50FN.samplename.resk10.txt.gz).

The population data can be downloaded from [here](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20110521/phase1_integrated_calls.20101123.ALL.panel).

Various summary statistic files can be downloaded from [here](https://github.com/TiffanyAmariuta/TCSC/tree/main/sumstats) based on a disease of interest.

[This file](https://drive.google.com/uc?export=download&id=1gd6FP4qlteo1dBoAH8zGkXzbZvs2PPt4), which provides IDs and locations for various genes, is also required for plotting purposes.

All of these files should be placed directly in data/raw
"
171,https://github.com/Med-Dash/Med-Dash.github.io,"{'nbrye': 'https://github.com/nbrye', 'ProfessorMomo': 'https://github.com/ProfessorMomo', 'anjsri878': 'https://github.com/anjsri878', 'Kamen-R': 'https://github.com/Kamen-R', 'xxwwyytt1': 'https://github.com/xxwwyytt1'}","{'JavaScript': 0.91, 'Python': 0.06, 'HTML': 0.02, 'Dockerfile': 0.0}","# Medical Dashboarding - Quarter 2 Project
","# Medical Dashboarding - Quarter 2 Project
"
172,https://github.com/Med-Dash/Med-Dash.github.io,"{'nbrye': 'https://github.com/nbrye', 'ProfessorMomo': 'https://github.com/ProfessorMomo', 'anjsri878': 'https://github.com/anjsri878', 'Kamen-R': 'https://github.com/Kamen-R', 'xxwwyytt1': 'https://github.com/xxwwyytt1'}","{'JavaScript': 0.91, 'Python': 0.06, 'HTML': 0.02, 'Dockerfile': 0.0}","# Medical Dashboarding - Quarter 2 Project
","# Medical Dashboarding - Quarter 2 Project
"
173,https://github.com/BradPowell23/First-and-Second-Level-Brain-Analysis,"{'BradPowell23': 'https://github.com/BradPowell23', 'jeremyn644': 'https://github.com/jeremyn644'}","{'Jupyter Notebook': 1.0, 'Python': 0.0}","Dataset Link:
https://openneuro.org/datasets/ds003338/versions/1.1.0

Link to download the pre-processed data needed for analysis:
https://app.globus.org/file-manager?origin_id=dc43f461-0ca7-4203-848c-33a9fc00a464&origin_path=%2Fr8b8-k094%2F

`notebooks`: code and analysis in the form of Jupyter notebooks stored
<br>
`references`: nilearn tutorials
","Dataset Link:
https://openneuro.org/datasets/ds003338/versions/1.1.0

Link to download the pre-processed data needed for analysis:
https://app.globus.org/file-manager?origin_id=dc43f461-0ca7-4203-848c-33a9fc00a464&origin_path=%2Fr8b8-k094%2F

`notebooks`: code and analysis in the form of Jupyter notebooks stored
<br>
`references`: nilearn tutorials
"
174,https://github.com/mzh4ng/DSC180B_Q2_Project,"{'mzh4ng': 'https://github.com/mzh4ng', 'benjaminsacks': 'https://github.com/benjaminsacks', 'echan0': 'https://github.com/echan0'}","{'Python': 0.64, 'Jupyter Notebook': 0.33, 'Dockerfile': 0.03}","# Evaluating Fungal Feature Importance in Predicting Life Expectancy for Cancer Patients
This is the repository for DSC180B Section B18-1's Project consisting of Benjamin Sacks, Ethan Chan, and Mark Zheng.
This project is an extension of a study on the classification of cancer types using fungal mycobiome counts which can
be found here: https://www.cell.com/cell/fulltext/S0092-8674(22)01127-8.

This project consists of two main machine learning models based upon the data presented in the previously mentioned
study as well as additional metadata collected about each sample that was not used in prior models. The first is a 
regression model to predict the ""days to death"" continuous metadata variable measuring when the patient died in days
after their sample was taken. The second is a classification model which aims to distinguish between different cancer
stages(I-IV) as opposed to cancer types in the original study.


INSTRUCTIONS:

To run these models, run the run.py file with 1 argument, the name of the config file for the desired model. 
Ex. ""run.py default-cancer-stage.json"".
Additionally, there is a notebook in the path notebooks/run.ipynb that can be used to run this program in 
Jupyter Notebook if desired.

Different models can be selected and run using the config files. Config files are json files in the ""config"" directory. 
They can be edited to change the parameters of the experiment as well as the type of experiment run. Each experiment
only has 1 config file that it uses to increase the customization of experiments without flooding the folder with 
too many config files.

In each config file, there are 3 subcategories: dataset, preprocessing, and model.
<br /> Dataset specifies information about the raw feature tables including which column is the target variable.
<br /> Preprocessing specifies the parameters of the preprocessing including what transformations to apply to each column. 
    Preprocessing can also be turned off if data is already preprocessed with ""do_preprocessing"".
<br /> Model specifies the parameters of the model as well as cross validation. These are model specific and will vary
    based upon which type of model is being used.

Additionally, these are some important keys in the config file:
<br /> experiment_name: Specifies the unique id of the experiment. This is important for separating plots in figures.
<br /> experiment_title: Title of the experiment that will be displayed on the graphs
<br /> experiment_type: internal parameter telling the pipeline which class of model to use (classification or regression)","# Evaluating Fungal Feature Importance in Predicting Life Expectancy for Cancer Patients
This is the repository for DSC180B Section B18-1's Project consisting of Benjamin Sacks, Ethan Chan, and Mark Zheng.
This project is an extension of a study on the classification of cancer types using fungal mycobiome counts which can
be found here: https://www.cell.com/cell/fulltext/S0092-8674(22)01127-8.

This project consists of two main machine learning models based upon the data presented in the previously mentioned
study as well as additional metadata collected about each sample that was not used in prior models. The first is a 
regression model to predict the ""days to death"" continuous metadata variable measuring when the patient died in days
after their sample was taken. The second is a classification model which aims to distinguish between different cancer
stages(I-IV) as opposed to cancer types in the original study.


INSTRUCTIONS:

To run these models, run the run.py file with 1 argument, the name of the config file for the desired model. 
Ex. ""run.py default-cancer-stage.json"".
Additionally, there is a notebook in the path notebooks/run.ipynb that can be used to run this program in 
Jupyter Notebook if desired.

Different models can be selected and run using the config files. Config files are json files in the ""config"" directory. 
They can be edited to change the parameters of the experiment as well as the type of experiment run. Each experiment
only has 1 config file that it uses to increase the customization of experiments without flooding the folder with 
too many config files.

In each config file, there are 3 subcategories: dataset, preprocessing, and model.
<br /> Dataset specifies information about the raw feature tables including which column is the target variable.
<br /> Preprocessing specifies the parameters of the preprocessing including what transformations to apply to each column. 
    Preprocessing can also be turned off if data is already preprocessed with ""do_preprocessing"".
<br /> Model specifies the parameters of the model as well as cross validation. These are model specific and will vary
    based upon which type of model is being used.

Additionally, these are some important keys in the config file:
<br /> experiment_name: Specifies the unique id of the experiment. This is important for separating plots in figures.
<br /> experiment_title: Title of the experiment that will be displayed on the graphs
<br /> experiment_type: internal parameter telling the pipeline which class of model to use (classification or regression)"
175,https://github.com/Amandoj/DSC180-Q2-Project,"{'Amandoj': 'https://github.com/Amandoj', 'renaldyh27': 'https://github.com/renaldyh27', 'emersonchao': 'https://github.com/emersonchao'}","{'Jupyter Notebook': 0.94, 'Python': 0.06, 'Dockerfile': 0.0}","# MULTI-LABEL DISEASE PREDICTION BASED ON GUT MICROBIOME
Abstract: In this study, we will be exploring the gut microbiome of Latin American immigrants to determine what factors of their gut microbiome affect metabolic diseases. The goal of our project is to determine what metabolic diseases/disorders an individual has based on their gut microbiome and other supporting information on the individual. To achieve our goal, we will be exploring machine learning and data analysis techniques to summarize the key points of the data and understand the patterns and relationships in the data.


## Retrieving the data locally:
(1) Download the data files from the following Google Drive: https://drive.google.com/drive/folders/1cpUvpXbh3YEHHaW4jmeKhL8DYfE7tE5V?usp=sharing

(2) Place files in `data/raw` directory

## Activating Qiime2
After launching container, open terminal and type in the following command before running `run.py`:

`conda activate qiime2-2022.11`

To use within jupyter notebook also type in the following commands: 

`pip install -–user ipykernel`

`python -m ipykernel install -–user -–name=qiime2-2022.11`

then refresh jupyter hub

and select the qiime2 kernel

## Running the Project:
* To revert to a clean repository, from the project root dir, run `python run.py clean`
  * This deletes all built files
* To run the entire project on test data, from the project root dir, run `python run.py test`
  * This fetches the test data, creates features, cleans the data, performs permanova tests, creates pcoa plots, creates machine learning model and model performance graphs
  for given disease types
* To run the entire project on the real data, from the project root dir, run `python run.py all`
  * This fetches the original data, creates features, cleans the data, performs permanova tests, creates pcoa plots, performs UMAP, creates machine learning model and model performance graphs
  for given disease types
  
## Model Performance
To view model performance graphs, permanova tests, and pcoa plots after running `run.py`, download `.qzv` files from `data/out` and upload to https://view.qiime2.org/

Collaborator: Amando Jimenez, Emerson Chao, Renaldy Herlim

For more information visit: https://renaldyh27.github.io/Capstone-Website/
","# MULTI-LABEL DISEASE PREDICTION BASED ON GUT MICROBIOME
Abstract: In this study, we will be exploring the gut microbiome of Latin American immigrants to determine what factors of their gut microbiome affect metabolic diseases. The goal of our project is to determine what metabolic diseases/disorders an individual has based on their gut microbiome and other supporting information on the individual. To achieve our goal, we will be exploring machine learning and data analysis techniques to summarize the key points of the data and understand the patterns and relationships in the data.


## Retrieving the data locally:
(1) Download the data files from the following Google Drive: https://drive.google.com/drive/folders/1cpUvpXbh3YEHHaW4jmeKhL8DYfE7tE5V?usp=sharing

(2) Place files in `data/raw` directory

## Activating Qiime2
After launching container, open terminal and type in the following command before running `run.py`:

`conda activate qiime2-2022.11`

To use within jupyter notebook also type in the following commands: 

`pip install -–user ipykernel`

`python -m ipykernel install -–user -–name=qiime2-2022.11`

then refresh jupyter hub

and select the qiime2 kernel

## Running the Project:
* To revert to a clean repository, from the project root dir, run `python run.py clean`
  * This deletes all built files
* To run the entire project on test data, from the project root dir, run `python run.py test`
  * This fetches the test data, creates features, cleans the data, performs permanova tests, creates pcoa plots, creates machine learning model and model performance graphs
  for given disease types
* To run the entire project on the real data, from the project root dir, run `python run.py all`
  * This fetches the original data, creates features, cleans the data, performs permanova tests, creates pcoa plots, performs UMAP, creates machine learning model and model performance graphs
  for given disease types
  
## Model Performance
To view model performance graphs, permanova tests, and pcoa plots after running `run.py`, download `.qzv` files from `data/out` and upload to https://view.qiime2.org/

Collaborator: Amando Jimenez, Emerson Chao, Renaldy Herlim

For more information visit: https://renaldyh27.github.io/Capstone-Website/
"
176,https://github.com/ZixinMa27/DSC180-Aerosol-Flow-Modeling-and-Simulation-in-a-Classroom-with-Mobile-Sensors,"{'ZixinMa27': 'https://github.com/ZixinMa27', 'DSC-Qian': 'https://github.com/DSC-Qian', 'cathyw36': 'https://github.com/cathyw36'}","{'HTML': 0.98, 'Jupyter Notebook': 0.02, 'Python': 0.0, 'Dockerfile': 0.0}","## Modeling and Simulation of Aerosol Flow in a Classroom Environment with Mobile Sensors
#### Team Members: Zixin Ma, Jiali Qian, Yidan Wang 
#### Mentors: Professor Tauhidur Rahman, PhD Tanjid Hasan Tonmoy

### Overview:
In light of the significant impact of COVID-19, it is crucial for individuals to assess the safety of indoor environments effectively and accurately. Although there are existing apps that monitor factors such as air quality and temperature, they fail to consider the concentrations of respiratory aerosols or other contaminants. To address this issue, we aim to develop a mobile application that utilizes built-in sensor data  and machine learning models to simulate aerosol flow and forecast the safety of indoor environments. Our app will not only serve as a tool for assessing COVID-19 safety, but also for other illnesses and purposes. 

**This project is also related to an ongoing doctorate research project, thus some of the repos are kept private per professor's request.**
**Based on the condition above, this repo is created for the purpose of showing our current progress, so the commits are a little compact. Further proof can be provided to show that our effort on the project is consistent.**
### Resources:
1. Data Collection APP https://github.com/tanjidt/hdsi-capstone-project [private]
2. https://github.com/tanjidt/aerosol-models [private]
3. Thermal Image Analysis: https://github.com/kavetinaveen/Thermal_Image_Processing 

### Build instruction:
To avoid path conflicts, run.py is not located in the root directory.
Please run `python src/models/compartment_model/run.py test` instead.
","## Modeling and Simulation of Aerosol Flow in a Classroom Environment with Mobile Sensors
#### Team Members: Zixin Ma, Jiali Qian, Yidan Wang 
#### Mentors: Professor Tauhidur Rahman, PhD Tanjid Hasan Tonmoy

### Overview:
In light of the significant impact of COVID-19, it is crucial for individuals to assess the safety of indoor environments effectively and accurately. Although there are existing apps that monitor factors such as air quality and temperature, they fail to consider the concentrations of respiratory aerosols or other contaminants. To address this issue, we aim to develop a mobile application that utilizes built-in sensor data  and machine learning models to simulate aerosol flow and forecast the safety of indoor environments. Our app will not only serve as a tool for assessing COVID-19 safety, but also for other illnesses and purposes. 

**This project is also related to an ongoing doctorate research project, thus some of the repos are kept private per professor's request.**
**Based on the condition above, this repo is created for the purpose of showing our current progress, so the commits are a little compact. Further proof can be provided to show that our effort on the project is consistent.**
### Resources:
1. Data Collection APP https://github.com/tanjidt/hdsi-capstone-project [private]
2. https://github.com/tanjidt/aerosol-models [private]
3. Thermal Image Analysis: https://github.com/kavetinaveen/Thermal_Image_Processing 

### Build instruction:
To avoid path conflicts, run.py is not located in the root directory.
Please run `python src/models/compartment_model/run.py test` instead.
"
177,https://github.com/Brian96086/STNP_RL,{'Brian96086': 'https://github.com/Brian96086'},"{'Python': 0.99, 'Dockerfile': 0.01}","## Accelerating STNP with Reinforcement Learning 

Overview: This repository implements the improvement of brute-force parameter search with DeepQ Network(DQN). In the repository, there will 


### Notes to DSC180A TA's

### Instructions - Conda Virtual Environment
In this section, you'll execute the code with the below steps:
1. Create a conda environment with python version 3.9 `conda create --name placeholder_name python=3.9`. Note the ""placeholder_name"" is the environment name that you desire
2. Activate the conda environment `conda activate placeholder_name`. 
3. Within the environment, install the python packages by running `pip install -r requirements.txt`
4. By this stage, the conda environment should contain all of the required packages. To execute the code, run `python main.py` (or `python3 main.py`)

## Repository Structure
- The repository currently contains config folder, models folders, and utils folder. 
- The config folder will store all of the hardcoded constants and allows one to tune and perform the experiments/hyperparameters. In particular, make sure to change the snapshot parameter to the location you want output files to be stored. 
- The models folder contain the original source code from the STNP model. It is seperated into two files - seir(the actual simulation) and dcrnn (the surrogate model)
- The utils folder contains the core components that assist the STNP model to select parameters, namely the reinforcement-related files. It is further categorized into agents, env, exploration_strategies, trainer, which are standard RL modules/abstractions.
- engine.py provides helper methods that run the training procedure and wraps complicated logic into each method 
- main.py performs the execution of the code. Therefore, you'll be compiling on main.py

README update date: March 14th, 2023","## Accelerating STNP with Reinforcement Learning 

Overview: This repository implements the improvement of brute-force parameter search with DeepQ Network(DQN). In the repository, there will 


### Notes to DSC180A TA's

### Instructions - Conda Virtual Environment
In this section, you'll execute the code with the below steps:
1. Create a conda environment with python version 3.9 `conda create --name placeholder_name python=3.9`. Note the ""placeholder_name"" is the environment name that you desire
2. Activate the conda environment `conda activate placeholder_name`. 
3. Within the environment, install the python packages by running `pip install -r requirements.txt`
4. By this stage, the conda environment should contain all of the required packages. To execute the code, run `python main.py` (or `python3 main.py`)

## Repository Structure
- The repository currently contains config folder, models folders, and utils folder. 
- The config folder will store all of the hardcoded constants and allows one to tune and perform the experiments/hyperparameters. In particular, make sure to change the snapshot parameter to the location you want output files to be stored. 
- The models folder contain the original source code from the STNP model. It is seperated into two files - seir(the actual simulation) and dcrnn (the surrogate model)
- The utils folder contains the core components that assist the STNP model to select parameters, namely the reinforcement-related files. It is further categorized into agents, env, exploration_strategies, trainer, which are standard RL modules/abstractions.
- engine.py provides helper methods that run the training procedure and wraps complicated logic into each method 
- main.py performs the execution of the code. Therefore, you'll be compiling on main.py

README update date: March 14th, 2023"
178,https://github.com/3XiangyiKong3/DSC180AB_code,{'3XiangyiKong3': 'https://github.com/3XiangyiKong3'},"{'Jupyter Notebook': 0.99, 'Python': 0.01, 'Shell': 0.0, 'Dockerfile': 0.0}","# Optimization of DeepGLEAM on Flu Forecasting Time-Series Data
The current COVID-19 pandemic and common flu highlight the importance of time-sensitive information in biomedical institutions, politics, and economics. The application of data science in creating real-time predictive models is crucial to help researchers and world leaders better understand disease spread and take preventative measures.

## GLEAM Prediction before and after Interpolation
![GLEAM Before Interpolation](./references/beforeinterp.png)
![GLEAM After Interpolation](./references/afterinterp.png)

## ARIMA and ETS
![ARIMA](./references/arima_validation_plot1.png)
![ETS](./references/ets_validation_plot1.png)
## Prediction
Four weeks ahead Flu prediction residual between groundtruth and prediction for 10 states
![uncertainty_quantification_flu_residual_washingtion](./references/10_states_4_weeks_prediction.png)

## Result Comparison 
![MAE result](./references/Combied_result.png)

## Setup, Model training and Model Testing
 
1. Requirements
```bash
>>> pip install -r requirements.txt
```
2. Train models and make prediction (Model already trainned in the submission)
```bash
>>> python3 run.py --config_filename=data/model/dcrnn_cov.yaml
```
3. For Test, run the following command
```bash
>>> ./test.sh
```
- Visualization 

  - After running the command for test, a new folder named plot_weeknumber_result will appear containing [0.025, 0.5, 0.975] residual predictions the .npz files 
  - Select the one with lowest MAE score 
  - Run the flu_forecast_result_plot notebook

## Docker

```bash
>>> docker build -f ./Dockerfile -t Dockerfile .
>>> docker run --rm -it Dockerfile /bin/bash
>>> launch.sh -i xiangyikong/dsc180a:latest #Use this command below to launch the image in DSMLP
```
","# Optimization of DeepGLEAM on Flu Forecasting Time-Series Data
The current COVID-19 pandemic and common flu highlight the importance of time-sensitive information in biomedical institutions, politics, and economics. The application of data science in creating real-time predictive models is crucial to help researchers and world leaders better understand disease spread and take preventative measures.

## GLEAM Prediction before and after Interpolation
![GLEAM Before Interpolation](./references/beforeinterp.png)
![GLEAM After Interpolation](./references/afterinterp.png)

## ARIMA and ETS
![ARIMA](./references/arima_validation_plot1.png)
![ETS](./references/ets_validation_plot1.png)
## Prediction
Four weeks ahead Flu prediction residual between groundtruth and prediction for 10 states
![uncertainty_quantification_flu_residual_washingtion](./references/10_states_4_weeks_prediction.png)

## Result Comparison 
![MAE result](./references/Combied_result.png)

## Setup, Model training and Model Testing
 
1. Requirements
```bash
>>> pip install -r requirements.txt
```
2. Train models and make prediction (Model already trainned in the submission)
```bash
>>> python3 run.py --config_filename=data/model/dcrnn_cov.yaml
```
3. For Test, run the following command
```bash
>>> ./test.sh
```
- Visualization 

  - After running the command for test, a new folder named plot_weeknumber_result will appear containing [0.025, 0.5, 0.975] residual predictions the .npz files 
  - Select the one with lowest MAE score 
  - Run the flu_forecast_result_plot notebook

## Docker

```bash
>>> docker build -f ./Dockerfile -t Dockerfile .
>>> docker run --rm -it Dockerfile /bin/bash
>>> launch.sh -i xiangyikong/dsc180a:latest #Use this command below to launch the image in DSMLP
```
"
179,https://github.com/apatankar22/hier-neural-proc,{'apatankar22': 'https://github.com/apatankar22'},"{'Jupyter Notebook': 0.68, 'Python': 0.32, 'Dockerfile': 0.0}","
# Capstone Project (DSC 180B): Active Learning with Neural Processes for Epidemiology Modeling
## [Report](https://drive.google.com/file/d/1Mk2uujYlSpMKpOzAgYlZWoz1AOed6XPl/view), [Poster](https://drive.google.com/file/d/1m3Gy5ldjGqiTkYX6XV3meAU44MSHP9dL/view), [Website](http://apatankar22.github.io/hier-neural-proc/) <br>
Authors: Amogh Patankar <br>
Mentors: Rose Yu, Yian Ma

## [SIR Neural Process and Gaussian Process Data](https://drive.google.com/drive/folders/1osXBkuDuzSmB8__2r3lLoOLHIXqju3G2)
SIR_GP: Save <code>nargp_data</code> and <code>sfgp_data</code> on the same level as <code>src</code>. 

SIR_NP: Save <code>mfnp_nested, nested, and nonnested</code> on the same level as <code>src/sir_np/</code>. 

## Packages
Installed packages through pip. To replicate, run <code> pip install -r requirements.txt.</code> <br>
Alternatively install along as you go, the Gaussian processes don't require all the same packages as the neural processes. 

## How to Run
For Gaussian Processes, simply change to <code>src/sir_gp</code> and run the two jupyter notebooks.<br>
For Neural Processes, simply change to <code>src/sir_np/MA</code>, then go into the desired process and run train.py. This should train and also test + evaluate the model.
","
# Capstone Project (DSC 180B): Active Learning with Neural Processes for Epidemiology Modeling
## [Report](https://drive.google.com/file/d/1Mk2uujYlSpMKpOzAgYlZWoz1AOed6XPl/view), [Poster](https://drive.google.com/file/d/1m3Gy5ldjGqiTkYX6XV3meAU44MSHP9dL/view), [Website](http://apatankar22.github.io/hier-neural-proc/) <br>
Authors: Amogh Patankar <br>
Mentors: Rose Yu, Yian Ma

## [SIR Neural Process and Gaussian Process Data](https://drive.google.com/drive/folders/1osXBkuDuzSmB8__2r3lLoOLHIXqju3G2)
SIR_GP: Save <code>nargp_data</code> and <code>sfgp_data</code> on the same level as <code>src</code>. 

SIR_NP: Save <code>mfnp_nested, nested, and nonnested</code> on the same level as <code>src/sir_np/</code>. 

## Packages
Installed packages through pip. To replicate, run <code> pip install -r requirements.txt.</code> <br>
Alternatively install along as you go, the Gaussian processes don't require all the same packages as the neural processes. 

## How to Run
For Gaussian Processes, simply change to <code>src/sir_gp</code> and run the two jupyter notebooks.<br>
For Neural Processes, simply change to <code>src/sir_np/MA</code>, then go into the desired process and run train.py. This should train and also test + evaluate the model.
"
180,https://github.com/Grizlucks/DSC180B-CapstoneFinalProject,"{'tktran11': 'https://github.com/tktran11', 'Grizlucks': 'https://github.com/Grizlucks'}","{'Jupyter Notebook': 0.72, 'Python': 0.26, 'Dockerfile': 0.02}","# DSC180B-CapstoneFinalProject

Evaluating gender bias within images generated by DallE-2.

Requirements:

This project makes use of OPENAI and requires a 'config.py' file
with the below line. 

OPENAI_API_KEY = ""YOUR_API_KEY_HERE""

If running the test target with a valid OpenAI key, it will generate ten images
(256x256) on that account to be charged (or with the user's remaining free
credits if any).

## Instructions and Project Guide

### run.py

Call `python run.py test` to run the project using test data. 

To run the project on the data used for the actual report, call `python run.py` however this will take a bit of time.

The run.py is setup to work with seperated targets that can be called by using the format `python run.py target1 target2 ...`:

- `power` Performs analysis on BLS like data to give information on the sample size necessary at 1% statistical power for our statistical testing.
- `images` Generates images based on the occupations specified in the main or test `params.json` files. Requires a `config.py` file to be setup as specified.
- `analysis` Performs the statstical tests for the report while generating informative visualizations. Requires labeled image data in the format found under `test/raw/labeled_results.csv`. 

## notebooks/

Included for reference, these notebooks were used in the hand-labeling 
process. 

## src/ 

Contains all source code in generating images and selecting occupations
within a specified range (of a 50/50 gender split). Currently, only tester
images can be generated. ","# DSC180B-CapstoneFinalProject

Evaluating gender bias within images generated by DallE-2.

Requirements:

This project makes use of OPENAI and requires a 'config.py' file
with the below line. 

OPENAI_API_KEY = ""YOUR_API_KEY_HERE""

If running the test target with a valid OpenAI key, it will generate ten images
(256x256) on that account to be charged (or with the user's remaining free
credits if any).

## Instructions and Project Guide

### run.py

Call `python run.py test` to run the project using test data. 

To run the project on the data used for the actual report, call `python run.py` however this will take a bit of time.

The run.py is setup to work with seperated targets that can be called by using the format `python run.py target1 target2 ...`:

- `power` Performs analysis on BLS like data to give information on the sample size necessary at 1% statistical power for our statistical testing.
- `images` Generates images based on the occupations specified in the main or test `params.json` files. Requires a `config.py` file to be setup as specified.
- `analysis` Performs the statstical tests for the report while generating informative visualizations. Requires labeled image data in the format found under `test/raw/labeled_results.csv`. 

## notebooks/

Included for reference, these notebooks were used in the hand-labeling 
process. 

## src/ 

Contains all source code in generating images and selecting occupations
within a specified range (of a 50/50 gender split). Currently, only tester
images can be generated. "
181,https://github.com/ptse8204/airlinedatabias,"{'ptse8204': 'https://github.com/ptse8204', 'JosephPerez4': 'https://github.com/JosephPerez4', 'Maricela99': 'https://github.com/Maricela99'}","{'Jupyter Notebook': 0.85, 'Python': 0.15}","# Welcome to the Airline Pricing Model Bias Repositary

## Introduction
This repositary showcase how we investigate on how airline price discriminate on certain protected groups, including but not limit to:
* Race
* Income
* Geo areas

We understand that airfare pricing is a business decision that was driven by revenues. However, by investigating such factors, it may also drives airline's bottom line as the result could be useful for more attractive pricing for passengers.

## Methodology
### Dataset
We uses the Airline Origin and Destination Survey from the USDOT. The main reason why we chose to use such a dataset, instead of the advertising fare of the flight is because the data point represent a fare that is actually purchase by a customers. 

The detail descrption and the data of the dataset is available on: https://www.transtats.bts.gov/tables.asp?QO_VQ=EFI&QO_anzr=Nv4yv0r

### How does finding bias work?
We aim our investigation (mainly) in 2 directions:
* Investigate whether there is price discrepency in protected groups on existing dataset
* Feed the data on to our custom build models, and see whether the model would generate results that showcase strong bias. In especially models that are extremely accuracte, and has a hard time to correctly identity areas that has a strong influence with protected groups.

## Expected Goals and Outcomes
### Goals
* Discover bias, if any
* Creating an accurate model, that is both accurate and unbias, using various accuracy measurments, and bias mitogation techniques.
* Discover any trend shift of airfare pre-pandemic and post-pandemic

### Outcomes
* An unbias model for extimatting the fair price that customers pay
* An indicator allow consumers to know whether they are price dscriminated and whether they are paying the fair price

## Credits
Tba :)

## Notes
We will keep updating the repo alongside with our progress.
readme file last update: Feb 12, 2023
","# Welcome to the Airline Pricing Model Bias Repositary

## Introduction
This repositary showcase how we investigate on how airline price discriminate on certain protected groups, including but not limit to:
* Race
* Income
* Geo areas

We understand that airfare pricing is a business decision that was driven by revenues. However, by investigating such factors, it may also drives airline's bottom line as the result could be useful for more attractive pricing for passengers.

## Methodology
### Dataset
We uses the Airline Origin and Destination Survey from the USDOT. The main reason why we chose to use such a dataset, instead of the advertising fare of the flight is because the data point represent a fare that is actually purchase by a customers. 

The detail descrption and the data of the dataset is available on: https://www.transtats.bts.gov/tables.asp?QO_VQ=EFI&QO_anzr=Nv4yv0r

### How does finding bias work?
We aim our investigation (mainly) in 2 directions:
* Investigate whether there is price discrepency in protected groups on existing dataset
* Feed the data on to our custom build models, and see whether the model would generate results that showcase strong bias. In especially models that are extremely accuracte, and has a hard time to correctly identity areas that has a strong influence with protected groups.

## Expected Goals and Outcomes
### Goals
* Discover bias, if any
* Creating an accurate model, that is both accurate and unbias, using various accuracy measurments, and bias mitogation techniques.
* Discover any trend shift of airfare pre-pandemic and post-pandemic

### Outcomes
* An unbias model for extimatting the fair price that customers pay
* An indicator allow consumers to know whether they are price dscriminated and whether they are paying the fair price

## Credits
Tba :)

## Notes
We will keep updating the repo alongside with our progress.
readme file last update: Feb 12, 2023
"
182,https://github.com/NicoloWX/CausalTreeInference,"{'huaningliu': 'https://github.com/huaningliu', 'NicoloWX': 'https://github.com/NicoloWX', 'wenqian-zhao': 'https://github.com/wenqian-zhao'}","{'Jupyter Notebook': 0.59, 'R': 0.41, 'Dockerfile': 0.0}","# DSC180 Capstone Project
- `run.R`: a demo of the tree-based method

For models in `src/models/`:

To replicate the table[2][3][4] in the report
- `ageTest.R`
- `yearTest.R`
- `genderTest.R`

To replicate the tree graphs in the report
- `ageTest.R`
- `yearTest.R`
- `genderTest.R`

To replicate the boxplots in the report
- `boxplotGenAge.R`
- `boxplotGenYear.R`
- `boxplotGenGender.R`

To replicate table[5][6][7] in the report
- `forest-TestAge.R`
- `forest-TestYear.R`
- `forest-TestGender.R`
","# DSC180 Capstone Project
- `run.R`: a demo of the tree-based method

For models in `src/models/`:

To replicate the table[2][3][4] in the report
- `ageTest.R`
- `yearTest.R`
- `genderTest.R`

To replicate the tree graphs in the report
- `ageTest.R`
- `yearTest.R`
- `genderTest.R`

To replicate the boxplots in the report
- `boxplotGenAge.R`
- `boxplotGenYear.R`
- `boxplotGenGender.R`

To replicate table[5][6][7] in the report
- `forest-TestAge.R`
- `forest-TestYear.R`
- `forest-TestGender.R`
"
183,https://github.com/alecpanattoni/MissingnessFairnessAnalysis,"{'alecpanattoni': 'https://github.com/alecpanattoni', 'giwi2123': 'https://github.com/giwi2123', 'yuribz': 'https://github.com/yuribz', 'matthewsy217': 'https://github.com/matthewsy217'}","{'Jupyter Notebook': 0.96, 'Python': 0.04, 'Dockerfile': 0.0}","When in the repo directory, in order to produce the test results, one can use target ""test"". In order to produce results with the downloaded data, target name ""all"" can be used. To make use of these targets, simply cd into the repo's director and input ""python3 run.py <target>""

The project directory contains the following:

data folder contains test data, the notebook for generating the test data, as well as the complete allegations data

Dockerfile from which necessary packages will be installed to run project

report folder contains the overleaf report pdf

run.py is the code in which results are produced (main coding file)

src folder contains python code for data cleaning and generation of missing data (methods called in run.py). It also contains the models for ensuring that the model is fair and producing predictions so that fairness notion measurements can be produced. ","When in the repo directory, in order to produce the test results, one can use target ""test"". In order to produce results with the downloaded data, target name ""all"" can be used. To make use of these targets, simply cd into the repo's director and input ""python3 run.py <target>""

The project directory contains the following:

data folder contains test data, the notebook for generating the test data, as well as the complete allegations data

Dockerfile from which necessary packages will be installed to run project

report folder contains the overleaf report pdf

run.py is the code in which results are produced (main coding file)

src folder contains python code for data cleaning and generation of missing data (methods called in run.py). It also contains the models for ensuring that the model is fair and producing predictions so that fairness notion measurements can be produced. "
184,https://github.com/ejsong37/Trustworthy-Recommender-Systems-Capstone,{'ejsong37': 'https://github.com/ejsong37'},"{'Jupyter Notebook': 0.77, 'Python': 0.23, 'Dockerfile': 0.0}","# Trustworthy Recommender Systems via Bayesian Bandits Capsone

### Team Members: Eric Song, Xiqiang Liu, Hien Bui, Vivek Saravanan

### Mentor: Yuhua Zhu

## About
Recommender systems have emerged as a simple yet powerful framework for the suggestion of relevant items to users. However, a potential issue arises when recommender systems overly recommend or spam undesired products to users in which the model loses the trust of the user. We propose a constrained bandit-based recommender system. We show this model outperforms Upper Confidence Bound (UCB) and Thompson sampling in terms of expected regret and does not lose the trust of the users. This work was presented at the Halıcıoğlu Data Science Institute Capstone Showcase on March 15th, 2023 at UC San Diego.

## Resources
- [Website](https://hi3nb1.github.io/capstone/)
- [Poster](https://drive.google.com/file/d/1BjS6ZcwmB4TsGctyS56vNxZsTev8F0zF/view)
- [Report](https://drive.google.com/file/d/10VEKJZ_TWxqBKimkeTWUmmYjivagMGZJ/view)

## Running Experiments
```bash
python run.py all  # run all experiments

python run.py etc  # run Explore-Then-Commit (ETC) experiments
python run.py ucb  # run UCB experiments
python run.py ts  # run Thompson Sampling experiments
python run.py optimal  # run Bayesian Optimal Policy experiments

python run.py linucb  # run LinUCB experiments
python run.py lints  # run Linear Thompson Sampling experiments
```

To run experiments related to Trustworthy Recommender Systems, run code in experiments.ipynb in TrustworthyMAB folder.

All the results are going to be saved in `results/` sub-directory.

## Visualize Results

Notebooks to visualize collected results could be found in `notebooks/` sub-directory.
","# Trustworthy Recommender Systems via Bayesian Bandits Capsone

### Team Members: Eric Song, Xiqiang Liu, Hien Bui, Vivek Saravanan

### Mentor: Yuhua Zhu

## About
Recommender systems have emerged as a simple yet powerful framework for the suggestion of relevant items to users. However, a potential issue arises when recommender systems overly recommend or spam undesired products to users in which the model loses the trust of the user. We propose a constrained bandit-based recommender system. We show this model outperforms Upper Confidence Bound (UCB) and Thompson sampling in terms of expected regret and does not lose the trust of the users. This work was presented at the Halıcıoğlu Data Science Institute Capstone Showcase on March 15th, 2023 at UC San Diego.

## Resources
- [Website](https://hi3nb1.github.io/capstone/)
- [Poster](https://drive.google.com/file/d/1BjS6ZcwmB4TsGctyS56vNxZsTev8F0zF/view)
- [Report](https://drive.google.com/file/d/10VEKJZ_TWxqBKimkeTWUmmYjivagMGZJ/view)

## Running Experiments
```bash
python run.py all  # run all experiments

python run.py etc  # run Explore-Then-Commit (ETC) experiments
python run.py ucb  # run UCB experiments
python run.py ts  # run Thompson Sampling experiments
python run.py optimal  # run Bayesian Optimal Policy experiments

python run.py linucb  # run LinUCB experiments
python run.py lints  # run Linear Thompson Sampling experiments
```

To run experiments related to Trustworthy Recommender Systems, run code in experiments.ipynb in TrustworthyMAB folder.

All the results are going to be saved in `results/` sub-directory.

## Visualize Results

Notebooks to visualize collected results could be found in `notebooks/` sub-directory.
"
185,https://github.com/abhianish0105/DSC180B-Q2-Project,"{'vinhnee': 'https://github.com/vinhnee', 'abhianish0105': 'https://github.com/abhianish0105', 'tpatel00': 'https://github.com/tpatel00'}","{'Python': 0.84, 'HTML': 0.16}","# DSC180B-Q2-Project

# Twitter Sentiment Analysis on Gun Control

## Introduction
This project is on the sentiment analysis of tweets regarding the topic of gun control. In recent years in the United States, there have been an outburst of several horrific events as a result of guns getting in the hands of the wrong people. In 2023, the number of mass shootings in the US has already reached triple digits. With these incidents, we believe it would be an interesting study to do further research into the sentiment and beliefs that people have toward these issues and study this using automation and machine learning. Twitter is a very vocal platform and with the use of our new knowledge regarding sentiment analysis, there are possibly very many interesting discoveries to be made, such as how user sentiment toward one topic may differ from another. We hypothesize that because of the many gruesome events that have occurred in recent years, we will observe a mostly positive sentiment toward gun control, in that people support more regulation of weapon distribution rather than less. Our proposed project period of 10 weeks can be attributed to the fact that we are looking to improve upon our previous work by expanding our knowledge on the Astra Streaming features, and working on more efficient implementation of our architecture to ensure that we receive enough data for sufficient analysis.  We will also look to go further by integrating new features into our project, including displaying results from our visualizations on a website in addition to streaming data to a feature store or database. As we work towards these goals, we recognize that we may come across technical issues that may require us to be more flexible and adjust our project structure, so we would like to stay open minded in regards to what additional features will be added.


## Running the Repository
* The code in this repository runs our sentiment analysis code on a small test set of test tweets, the same code that was ran on our collected database of tweets retrieved from the Twitter API.
* Run the following docker image inside a container: 
  * tepatel/test_q2
* To test the model on the test data: 
  * `python3 run.py test`



","# DSC180B-Q2-Project

# Twitter Sentiment Analysis on Gun Control

## Introduction
This project is on the sentiment analysis of tweets regarding the topic of gun control. In recent years in the United States, there have been an outburst of several horrific events as a result of guns getting in the hands of the wrong people. In 2023, the number of mass shootings in the US has already reached triple digits. With these incidents, we believe it would be an interesting study to do further research into the sentiment and beliefs that people have toward these issues and study this using automation and machine learning. Twitter is a very vocal platform and with the use of our new knowledge regarding sentiment analysis, there are possibly very many interesting discoveries to be made, such as how user sentiment toward one topic may differ from another. We hypothesize that because of the many gruesome events that have occurred in recent years, we will observe a mostly positive sentiment toward gun control, in that people support more regulation of weapon distribution rather than less. Our proposed project period of 10 weeks can be attributed to the fact that we are looking to improve upon our previous work by expanding our knowledge on the Astra Streaming features, and working on more efficient implementation of our architecture to ensure that we receive enough data for sufficient analysis.  We will also look to go further by integrating new features into our project, including displaying results from our visualizations on a website in addition to streaming data to a feature store or database. As we work towards these goals, we recognize that we may come across technical issues that may require us to be more flexible and adjust our project structure, so we would like to stay open minded in regards to what additional features will be added.


## Running the Repository
* The code in this repository runs our sentiment analysis code on a small test set of test tweets, the same code that was ran on our collected database of tweets retrieved from the Twitter API.
* Run the following docker image inside a container: 
  * tepatel/test_q2
* To test the model on the test data: 
  * `python3 run.py test`



"
186,https://github.com/dsaraf-hub/DSC180A-Capstone_Quarter_2,"{'dsaraf-hub': 'https://github.com/dsaraf-hub', 'justincun': 'https://github.com/justincun'}","{'Jupyter Notebook': 0.99, 'Python': 0.01, 'Dockerfile': 0.0}",,
187,https://github.com/DSC-180A/spam.detector.github.io,"{'TysonTran': 'https://github.com/TysonTran', 'l2lee': 'https://github.com/l2lee', 'Skylar1013': 'https://github.com/Skylar1013'}","{'Jupyter Notebook': 0.89, 'Python': 0.11}","# Q2-Project

## How Spam Affects the Sentiment of Tweets

## Contributors
Lucas Lee, Tyson Tran, Yi (Skylar) Li


## Objective
The objective of our research is to develop a pipeline that filters spam content to model noise-reduced sentiments towards abortion on Twitter in real-time and analyze how spam content affects said sentiments. We compared the use of both Naive Bayes and a transfer learning model based on BERT to do spam filtration, and analyzed the impact of spam on sentiment distribution results to gain a deeper understanding of the role it plays in shaping public opinion on social media platforms.


## Pipeline
This project is produced as part of the DSC180B Capstone at UCSD, working with mentors from DataStax. In this project, we utilized Apache Pulsar through Astra Streaming to create a pipeline that ingests live stream of abortion related tweets, incoporate spam-filtering ML models, and performs sentiment analysis. This is primarily done through the following steps:
<img src=""visuals/Untitled drawing (2).jpg"" width=350 height=600> 
  
1. A producer makes Twitter API calls to request a stream of tweets through the FilteredStreamV2 endpoint 
2. The producer then publishes each incoming tweet (stringified Json) to a pulsar topic — Raw Tweet Topic.
3. Pulsar consumers subscribes to the Raw Tweet Topic and
   a) Consumer 1 performs sentiment analysis directly.
   b) Consumer 2 deploys Naive Bayes Model to label spam tweets, then performs SA.
   c) Consumer 3 deploys BERT Model to label spam tweets, then performs SA.
4. Consumers then update data to a data source.
5. Grafana visualizes the finding on <a href=""https://skylar1013.grafana.net/d/_ztsas0Vz/capstone?orgId=1&from=1675065600000&to=1676188799000"">dashboards</a>


With the above described pipeline, we now have a real-time stream of tweets being classified. We used Google Spreadsheet <a href=""https://docs.google.com/spreadsheets/d/1fZ6MsCqtPXHWekonx2QGst0-eGei9ABzMG5LFDMEFbA/edit#gid=0"">Google Spreadsheet</a> to collect real-time tweets with producer running.

## Requirements
- `requirements.txt` provided with all dependencies needed to run the code below
- `capstone_googlesheet_key.json` provided necessary credentials to update tweets to database.
- Astra Streaming account
- Twitter developer account

### Required Envronmental Variables
Running the producers and consumers require Astra Streaming topic keys. Setting up topics and creating an account is free, and more information can be found at the tutorial [here](https://docs.google.com/document/d/1VS31dXTIAmEkIh9o_9FcAhD-rVvcmnTo_Zm1zSMgCmY/edit).

The shell envrionmental variables are used within the `Producer` and `Consumer` classes within `producer.py` and `consumer.py`.
- `ASTRA_STREAMING_TOKEN`
- `ASTRA_STREAMING_URL`
- `ASTRA_TOPIC`


## How to run
`run.py test`. This runs a test pipeline on test data.
This will then run:
`python src/producer.py`. This makes requests to the Twitter API, publishing remote-work related tweets to a pulsar topic. In its current test tag, it will run the `src/producer_offline.py`, which is encouraged to be used to test.
`python src/consumer_*.py`. This captures the cleaned tweets, utilizes spam detection ML models, performs sentiment analysis, alters it so that it feeds your needs for downstream analysis. Note that there are three such consumers that will be simultanouesly run in the background. Please use `ps` to check the list of processes and `kill [pid]` to end the processes. This is fully intended as producer and consumers are long running jobs in the background.



## Usage
* Since the publishing time of the tweet is currently calculated by when the consumer receives the tweet from the topic, it's recommended to use concurrently run both `producer.py` and `consumer.py` simultaneously
* Keep in mind that this requires the setup of the Astra Streaming dashboard, a tutorial is available [here](https://docs.google.com/document/d/1VS31dXTIAmEkIh9o_9FcAhD-rVvcmnTo_Zm1zSMgCmY/edit#heading=h.3znysh7)
* With the above setup, the only thing left to change is the topic that the `Consumer` subscribes to, which is in its constructor.
* The path and nameof the generated CSV is modifiable in the constructor of a `Consumer` in `consumer.py`

## Files
- `src/producer.py`: Main driver class for fetching tweets
- `src/consumer_*.py`: Main driver class for filtering spams and performing sentiment analysis.
- `requirements.txt`: Required dependencies in Python

","# Q2-Project

## How Spam Affects the Sentiment of Tweets

## Contributors
Lucas Lee, Tyson Tran, Yi (Skylar) Li


## Objective
The objective of our research is to develop a pipeline that filters spam content to model noise-reduced sentiments towards abortion on Twitter in real-time and analyze how spam content affects said sentiments. We compared the use of both Naive Bayes and a transfer learning model based on BERT to do spam filtration, and analyzed the impact of spam on sentiment distribution results to gain a deeper understanding of the role it plays in shaping public opinion on social media platforms.


## Pipeline
This project is produced as part of the DSC180B Capstone at UCSD, working with mentors from DataStax. In this project, we utilized Apache Pulsar through Astra Streaming to create a pipeline that ingests live stream of abortion related tweets, incoporate spam-filtering ML models, and performs sentiment analysis. This is primarily done through the following steps:
<img src=""visuals/Untitled drawing (2).jpg"" width=350 height=600> 
  
1. A producer makes Twitter API calls to request a stream of tweets through the FilteredStreamV2 endpoint 
2. The producer then publishes each incoming tweet (stringified Json) to a pulsar topic — Raw Tweet Topic.
3. Pulsar consumers subscribes to the Raw Tweet Topic and
   a) Consumer 1 performs sentiment analysis directly.
   b) Consumer 2 deploys Naive Bayes Model to label spam tweets, then performs SA.
   c) Consumer 3 deploys BERT Model to label spam tweets, then performs SA.
4. Consumers then update data to a data source.
5. Grafana visualizes the finding on <a href=""https://skylar1013.grafana.net/d/_ztsas0Vz/capstone?orgId=1&from=1675065600000&to=1676188799000"">dashboards</a>


With the above described pipeline, we now have a real-time stream of tweets being classified. We used Google Spreadsheet <a href=""https://docs.google.com/spreadsheets/d/1fZ6MsCqtPXHWekonx2QGst0-eGei9ABzMG5LFDMEFbA/edit#gid=0"">Google Spreadsheet</a> to collect real-time tweets with producer running.

## Requirements
- `requirements.txt` provided with all dependencies needed to run the code below
- `capstone_googlesheet_key.json` provided necessary credentials to update tweets to database.
- Astra Streaming account
- Twitter developer account

### Required Envronmental Variables
Running the producers and consumers require Astra Streaming topic keys. Setting up topics and creating an account is free, and more information can be found at the tutorial [here](https://docs.google.com/document/d/1VS31dXTIAmEkIh9o_9FcAhD-rVvcmnTo_Zm1zSMgCmY/edit).

The shell envrionmental variables are used within the `Producer` and `Consumer` classes within `producer.py` and `consumer.py`.
- `ASTRA_STREAMING_TOKEN`
- `ASTRA_STREAMING_URL`
- `ASTRA_TOPIC`


## How to run
`run.py test`. This runs a test pipeline on test data.
This will then run:
`python src/producer.py`. This makes requests to the Twitter API, publishing remote-work related tweets to a pulsar topic. In its current test tag, it will run the `src/producer_offline.py`, which is encouraged to be used to test.
`python src/consumer_*.py`. This captures the cleaned tweets, utilizes spam detection ML models, performs sentiment analysis, alters it so that it feeds your needs for downstream analysis. Note that there are three such consumers that will be simultanouesly run in the background. Please use `ps` to check the list of processes and `kill [pid]` to end the processes. This is fully intended as producer and consumers are long running jobs in the background.



## Usage
* Since the publishing time of the tweet is currently calculated by when the consumer receives the tweet from the topic, it's recommended to use concurrently run both `producer.py` and `consumer.py` simultaneously
* Keep in mind that this requires the setup of the Astra Streaming dashboard, a tutorial is available [here](https://docs.google.com/document/d/1VS31dXTIAmEkIh9o_9FcAhD-rVvcmnTo_Zm1zSMgCmY/edit#heading=h.3znysh7)
* With the above setup, the only thing left to change is the topic that the `Consumer` subscribes to, which is in its constructor.
* The path and nameof the generated CSV is modifiable in the constructor of a `Consumer` in `consumer.py`

## Files
- `src/producer.py`: Main driver class for fetching tweets
- `src/consumer_*.py`: Main driver class for filtering spams and performing sentiment analysis.
- `requirements.txt`: Required dependencies in Python

"
188,https://github.com/DSC180A/transaction_categorization,"{'cepan': 'https://github.com/cepan', 'kylenero99': 'https://github.com/kylenero99', 'kooshaj': 'https://github.com/kooshaj'}","{'Jupyter Notebook': 1.0, 'Python': 0.0}","# transaction_categorization

## Rethinking Credit Scores: Ensuring Fair Lending through NLP for Transaction Categorization

### By Kyle Nero, Chung En (Shawn) Pan, Nathan Van Lingen, Koosha Jadbabaei

#### Industry Mentor: Brian Duke (Petal)
#### Faculty Mentor: Berk Ustun

The invention of credit has reshaped modern personal finance. For most, a credit card provides an opportunity to buy something you can't afford in full, such as a car or a home. However, for others, credit is seen as a barrier. Because of the nature of how credit scores are calculated, certain groups, often referred to as the ""credit invisible"" are neglected. These individuals may be credit invisible for any number of reasons– they may be a young adult, or may have recently immigrated to a new country. Regardless, this group of ""credit invisible"" individuals often struggle to be approved for loans because they have no credit history.

In our project, we aim to cater to this ""credit invisible"" group by creating a tool that is able to classify transaction memos from an individual's checking history. We are working with industry partner Petal, a financial services company who uses an alternative metric, called the ""CashScore"", as opposed to a conventional credit score. Being able to accurately classify transaction memos is important because it allows Petal to more precisely calculate a CashScore for each credit applicant, which will over time increase the amount of people who qualify for credit– especially those with no credit history.

To reproduce this analysis, simply run the run.py file using the included docker image.
","# transaction_categorization

## Rethinking Credit Scores: Ensuring Fair Lending through NLP for Transaction Categorization

### By Kyle Nero, Chung En (Shawn) Pan, Nathan Van Lingen, Koosha Jadbabaei

#### Industry Mentor: Brian Duke (Petal)
#### Faculty Mentor: Berk Ustun

The invention of credit has reshaped modern personal finance. For most, a credit card provides an opportunity to buy something you can't afford in full, such as a car or a home. However, for others, credit is seen as a barrier. Because of the nature of how credit scores are calculated, certain groups, often referred to as the ""credit invisible"" are neglected. These individuals may be credit invisible for any number of reasons– they may be a young adult, or may have recently immigrated to a new country. Regardless, this group of ""credit invisible"" individuals often struggle to be approved for loans because they have no credit history.

In our project, we aim to cater to this ""credit invisible"" group by creating a tool that is able to classify transaction memos from an individual's checking history. We are working with industry partner Petal, a financial services company who uses an alternative metric, called the ""CashScore"", as opposed to a conventional credit score. Being able to accurately classify transaction memos is important because it allows Petal to more precisely calculate a CashScore for each credit applicant, which will over time increase the amount of people who qualify for credit– especially those with no credit history.

To reproduce this analysis, simply run the run.py file using the included docker image.
"
189,https://github.com/kkw002/Quarter2Project,{'kkw002': 'https://github.com/kkw002'},{'Python': 1.0},"# DSC180B Quarter 2 Project: Prediction of Transaction Types using NLP analysis.
This project attempts to identify which phrases, among other features that are generated from given data from Petal are used to predict the categorization of transaction made by a user. Currently, the model in the files only represents the 'base' model in which the features are relatively basic, as well as the model that was used to generate the accuracy.

## Accessing Data
The data needs to be accessed through ``` https://drive.google.com/file/d/10JH-rN5c1cMXIEXgPkPGImWSGOzC19Kx/view?usp=share_link ```.
1) After downloading the data, replace the ``` testdata.parquet ``` file with the downloaded file.
2) In the run.py file, replace ```getData('data/testdata.parquet')``` with ```getData('data/DSC180B.parquet')```

## Viewing Results
To see the accuracy score of the model on the dataset run ``` python run.py test ```
","# DSC180B Quarter 2 Project: Prediction of Transaction Types using NLP analysis.
This project attempts to identify which phrases, among other features that are generated from given data from Petal are used to predict the categorization of transaction made by a user. Currently, the model in the files only represents the 'base' model in which the features are relatively basic, as well as the model that was used to generate the accuracy.

## Accessing Data
The data needs to be accessed through ``` https://drive.google.com/file/d/10JH-rN5c1cMXIEXgPkPGImWSGOzC19Kx/view?usp=share_link ```.
1) After downloading the data, replace the ``` testdata.parquet ``` file with the downloaded file.
2) In the run.py file, replace ```getData('data/testdata.parquet')``` with ```getData('data/DSC180B.parquet')```

## Viewing Results
To see the accuracy score of the model on the dataset run ``` python run.py test ```
"
190,https://github.com/NJMIXI98/DSC180_Q2PROJECT,"{'NJMIXI98': 'https://github.com/NJMIXI98', 'sawadhwa': 'https://github.com/sawadhwa'}","{'Jupyter Notebook': 0.96, 'Python': 0.04}","# Auditing race inequality based on gender in data science related job market

Data Science Capstone Project advised under Stuart Geiger

# Authors

Nancy Jiang and Sahil Wadhwa

# Overview

In the twentieth century, one of sociology’s findings is that race and gender matter in the job market. Jobs were segregated by race and gender with whites earning more than other people of color and men earning more than women. Race inequality in the job market has been a long-standing interest of scholars. Notably, some research indicates that racial gaps are more significant for women than for men. Women face the unique challenges of lower wages and lower rewards in the global workforce. However, as women’s relative share in occupations grows nowadays, the gender inequality gap narrows in most job markets(Stier et al.,2014). Besides, a finding shows that race-based discrimination is weaker in high-paid jobs. Back in 2012, the Harvard Business Review acclaimed data science as “the sexiest job of the 21st century”. Some may pose the question of whether the statement still holds today. According to the U.S. BUREAU of Labor Statistics, employment in data science is projected to grow 36% from 2021 to 2031, much faster than the average for all occupations, which means employers will create more than 13,500 new data science related job opportunities each year on average, over the decade(Bureau of Labor Statistics 2022). Thus, we are curious about the gender-based race inequality in the data science related job market. 

# Code Instruction
Use Docker image: ""ucsdets/datahub-base-notebook:2022.3-stable""
Running python run.py test
","# Auditing race inequality based on gender in data science related job market

Data Science Capstone Project advised under Stuart Geiger

# Authors

Nancy Jiang and Sahil Wadhwa

# Overview

In the twentieth century, one of sociology’s findings is that race and gender matter in the job market. Jobs were segregated by race and gender with whites earning more than other people of color and men earning more than women. Race inequality in the job market has been a long-standing interest of scholars. Notably, some research indicates that racial gaps are more significant for women than for men. Women face the unique challenges of lower wages and lower rewards in the global workforce. However, as women’s relative share in occupations grows nowadays, the gender inequality gap narrows in most job markets(Stier et al.,2014). Besides, a finding shows that race-based discrimination is weaker in high-paid jobs. Back in 2012, the Harvard Business Review acclaimed data science as “the sexiest job of the 21st century”. Some may pose the question of whether the statement still holds today. According to the U.S. BUREAU of Labor Statistics, employment in data science is projected to grow 36% from 2021 to 2031, much faster than the average for all occupations, which means employers will create more than 13,500 new data science related job opportunities each year on average, over the decade(Bureau of Labor Statistics 2022). Thus, we are curious about the gender-based race inequality in the data science related job market. 

# Code Instruction
Use Docker image: ""ucsdets/datahub-base-notebook:2022.3-stable""
Running python run.py test
"
191,https://github.com/brianjhuang/CryptoWho,"{'brianjhuang': 'https://github.com/brianjhuang', 'yu-lily': 'https://github.com/yu-lily'}","{'Jupyter Notebook': 0.61, 'Python': 0.39}","# CryptoWho
Data Science Capstone Project advised under Stuart Geiger

## Authors
#### Brian Huang and Lily Yu

## Overview
In 2020, Bitcoin, other Cryptocurrencies and blockchain investments (NFTs) experienced a major boom. With the endorsements of many major companies and what seemed to be a large scale adoption on the horizon, the Crypto craze had started kicking off.

For many, cryptocurrency and NFTs were the first time they encountered an 'investment'. Many proclaimed crypto/NFT traders had no prior experience with any investing (stock market, retirement accounts, etc.) and looked to crypto/NFT as a get rich quick path. However, with the influx of new and young traders it was only a matter of time before the crypto/NFT scams began to pray on it's new consumers.

While crypto/NFT is not inherently a scam, many bad actors began to manipulate the influx of young and inexperienced investors. Many schemes akin to pump and dumps began to appear and with the popularity of crypto/NFT throughout social media, many inexperienced investors were quick to turn a blind eye to scams that seasoned investors may recognize immediately.

Cases like this have occured with traditional markets as well, where investors pour money into an asset based off of hype with no sound reasoning ($GME or GameStop). Crypto/NFTts however were especially susceptible to this with the combination of a relatively new asset, large demographic of young investors, and many social media influencers promoting these assets (Logan Paul, Doja Cat, etc)

With the most recent scandals in the cryptocurrency/NFT world (FTX, Logan Paul's CryptoZoo), it's more important than ever to investigate the platforms that many of these assets are promoted on. While platforms like YouTube and TikTok may not be intentionally promoting this content, it's important that they're aware of if their algorithms do indeed promote this type of content. To clarify, we do not provide an opinion on whether these scams are run by the founders of these products (FTX's Sam Bankman-Fried and CryptoZoo's Logal Paul) but rather emphasize that the scams have occured.

![crypo_losses](references/figures/crypto_losses.png)

According to the FTC, over a billion dollars has been lost to these types of scams since 2021. Half of which have orginated directly from social media. The most susceptible group of people? Young individuals.

The goal of this project is to investigate YouTube's recommendation algorithm to provide insight on the types of investment recommendations being provided to users across a variety of age groups. We assume that all individuals should be receiving the same proportion of recommendations based on their search trends (within a margin of error) regardless of age. This implies that a user who is younger and searching for general investment advice should not be receiving more crypto/NFT recommendations than someone who is older with similar watch history. By conducting audits on YouTube, we hope to gain valuable insight on YouTube and it's role in propogating this type of content on their platform (whether intentional or not).

### What does this repository offer?

The repository offers all tools used to conduct the audit. Helper functions and classes help download video and metadata from YouTube, run headless browsers to watch seed videos, and query the GPT-3 API for video sentiment classification.

All code can be found in the `src` folder and imported respectively. For any changes in filepath or settings, please look through the `config` folder.

## Table of Contents

- [Overview](#overview)
- [Methodology](#methodlogy)
- [Installation](#installation)
- [Downloading YouTube Data](#downloading-youtube-video-data)
- [GPT Prompting and Fine-Tuning](#gpt-prompting-and-fine-tuning)
- [Conducting the Audit](#conducting-the-audit)
- [Inference](#running-inference)
- [Analyzing Audit Results](#analyzing-audit-results)
- [Acknowledgements](#acknowledgements)

## Methodology

Our project is split into three parts:

**Part 1: Collecting Seed Videos and Creating Persona Users**

Seed videos function as a way for us to evaluate our GPT model and the videos that users watch to build watch history. Given the time we had, we could only collect and label 140 videos (20 for each age, 40 for traditional investments, 40 for blockchain investments, 5 for mixed, and 15 for edge cases where the video discusses money but is actually unrelated).

These seed videos fall into one of four labels:

Blockchain, Traditional, Mixed, and Unrelated.

Using our seed videos, we create six persona users with different watch behaviors.
```
A young individual (18-23) who:

Watches blockchain

Watches traditional

Watches mixed
```
```
A old individual (55-60) who:

Watches blockchain

Watches traditional

Watches mixed
```
By comparing the recommendations across these users, we can determine if YouTube is fairly and evenly recommending this content to all age groups. 

**Part 2: Creating a Prompt that peforms the best for our task at hand**

Creating a prompt is important when using GPT as a classifier. More info about our prompt can be found in the sections that follow.

**Part 3: Running the audit**

Using selenium, we can mimic the behavior of the six persona users above. By having a headless browser click and watch each video, we can collect the recommendations of each user. 

Each user watches 50% of the video by default. This and other config can be changed in the `config/Audit.py` file. 

The diagrams below illustrate the general pipeline for our audit:

![method](references/figures/methods.png)
![audit](references/figures/audit.png)

## Installation

We are using Python version [3.8.5](https://www.python.org/downloads/release/python-385/).

Please install this version of Python, as gensim summarization is still supported.

Versions between 3.8 and 3.9 should work, however we recommend you install the same version we use.

We recommend using [Anaconda](https://www.anaconda.com/) environments to do so.

**Once you've installed Anaconda, please run the following to create your environment.**

```bash
conda create --name <env_name> python=3.8.5
```

**Activate your conda environment like so:**
```bash
# Linux/Mac
source activate <env_name>

# Windows
activate <env_name>
```

**Install required packages**

```bash
pip install -r requirements.txt
```

**If you have Python 3.8.5 working outside of Conda (or any other version of Python that works with gensim) you can create a normal environment if you prefer**

```bash
python3 -m venv .venv

source .venv/bin/activate
```

**Install required packages**

```bash
pip install -r requirements.txt
```

### Headless Browser Setup

We use Selenium with the Firefox Webdriver to conduct our audit and gather YouTube video recommendations.

Note: More recent versions of FireFox will just launch if you have the browser installed. Please install the Firefox browser. If it does not work, install the driver.

To run the scraper, you will need to install the Firefox Webdriver, which can be downloaded [here](https://github.com/mozilla/geckodriver/releases).

To install, place your OS-appropriate executable in a directory locatable by your PATH.

### API Key Setup

Our codebase uses the YouTube Data API to download video metadata, comments, and for many other purposes like searching YouTube and grabbing recommendations. We use the OpenAI API to provide snippets and retrieve classification labels for our downloaded videos.

You can enable the YouTube Data API for your Google account and obtain an API key following the steps <a href=""https://developers.google.com/youtube/v3/getting-started"">here</a>.

The key can be found after you set up your cloud console.

You can fetch your OpenAI API key from <a href=""https://platform.openai.com/"">here</a>.

The key can be found in your profile under ***View API Keys***.

Once you have both API keys, please set the ```YOUTUBE_DATA_API_KEY``` and ```OPENAI_API_KEY``` variable in your environment:

You can do so by going to your home directory and running the following command:

**Mac OS and Linux**

```
nano .bash_profile

# Note Mac Users using zsh shell users should also set their keys in their zsh_profile
nano .zsh_profile
```

Inside your bash profile, you can go ahead and set this at the top:

```
# YOUTUBE API KEY
export YOUTUBE_DATA_API_KEY=""YOUR_API_KEY""
export OPENAI_API_KEY=""YOUR_API_KEY""
```

Close out of your terminal and your code editor to see changes occur.

**Check that updates have been made**
```
echo $YOUTUBE_DATA_API_KEY
echo $OPENAI_API_KEY
```

The following tutorials cover how to do this as well:

https://www.youtube.com/watch?v=5iWhQWVXosU&t=1s (Mac/Linux)

https://www.youtube.com/watch?v=IolxqkL7cD8 (Windows)

If you are not seeing updates, your `bash_profile` may not be sourced. To resolve this, add the following line to your `.bashrc`:

```
. ~/.bash_profile

# Note Mac Users using zsh shell users should do this in .zshrc
. ~/.zsh_profile
```

This can be anywhere, but we've put ours at the very bottom. Use the following command to enter your `.bashrc`.

```
nano .bashrc
# Note Mac Users using zsh shell users should do this
nano .zshrc
```

Now within Python you can access your API key by doing the following:
```
import os

youtube_key = os.environ.get(""YOUTUBE_DATA_API_KEY"")
openai_key = os.environ.get(""OPENAI_API_KEY"")
```

### Connecting to a VPN

To ensure all things are constant, we connected to the UC San Diego VPN for all of our audits and downloads. We recommend you connect to a VPN in the same location as well. There are many different VPN providers.

### Preserving User Agent

We recommend the audits be done on the same device, and if that is not possible, the same operating system. All of our audits were conducted on a Ubuntu device, however the platform you choose to audit with should not matter as long as they are consistent.

## Running The Entire Project Pipeline (Not Recommmended)

Our code uses a `run.py` file to help you run our code out of the box. You have the option to run the entire project pipeline using the folliwng command:

`python run.py all`

This command will download all seed videos, create snippets and evaluate your seed videos using the classifier, conduct the audit and download all videos, and classify the videos from the audit.

We **DO NOT** recommend you run this. The entire pipeline may take multiple days to run depending on the size of your seed videos. (Note: With 140 seed videos, our entire pipeline takes three days to run.)

If you would like to validate that the pipeline and it's sub-targets work, we instead recommended using:

`python run.py test`

Which will run the pipeline end to end on a much smaller subset of data found in `test/youtube`. This takes around an hour.

You can continue reading below for more details, but here is an overview of all targets in the run.py file. They are listed in the order we recommend you run them. Running each target gives you finer control over what is downloaded, avoiding repeated efforts/wasted API calls.
```
seed - The following target downloads and processed seed data. Target will prompt user for what they want to download and if they want to process the seed data (create video snippets.) Video snippets for context is a concatenation of the video title + summarized transcript (via TextRank) + top ten video tags (via TF-IDF)

classify-seed - The following target classifies the seed videos, providing a baseline classification report of your prompt. The confusion matrix is saved in references/figures. Note that the classification report is printed in terminal and not saved.

audit - The following target conducts the audit on a Firefox headless browser. It allows you to run a single or multiple audits. Config for each audit can be found in config/Audit.py

download-audit - The following target allows you to download the results of your audit. You can choose to download homepage results, sidebar results, or both.

create-audit-snippets - The following target creates snippets for each set of audit results. Note this may take a while as some videos have large transcripts. Video's over an hour have no transcript removed, however by modifying the target code this can be removed if you are able to wait the additional time.

classify - Run classification on your audit snippets.
```

## Downloading YouTube Video Data
Using `python run.py seed` will download all seed videos and save it in `data/seed/youtube/videos_{}.csv`.

Using `python run.py download-audit` will download all videos from the audit and save the downloaded videos in `data/audit/youtube/videos_{}.csv`.

Calling `python3 src/data/youTubeDownloader.py <video_ids seperated by spaces>` will download any videos you want and save it in `data/external/youtube/videos_{}.csv`. 

## GPT Prompting and Fine Tuning
Unlike traditional classifiers, GPT-3 is not trained on an existing dataset. Instead, predictions are generated through a set of 'prompts', instructing the large language model what the task is.

At the start of this project, GPT-3 was used. GPT-3 can be fine-tuned using a labelled dataset and prompts. For those interested in using a fine-tuned GPT-3 model, we encourage you to use the following <a href=""https://platform.openai.com/docs/guides/fine-tuning"">resource</a>.

Starting March 1st, GPT-3.5 was released. This API powers ChatGPT. Due to significant reduction in cost and increases in accuracy, we migrated to <a href = ""https://platform.openai.com/docs/models/gpt-3"">GPT 3.5.</a>

You are open to modify the prompt for our model however you see fit. The prompt can be found in `gpt.py` in the `create_message()` function.

The following prompt was used for our classification:
```
{""role"": ""system"", ""content"" : 

""You are a classifier that determines if a YouTube video snippet falls under a label. A snippet is a concatenation of the video title, summarized transcript, and video tags. The labels and additional instructions will be included in the first user message.""},

{""role"": ""user"", ""content"" : 

""""""Labels:

Traditional: Videos that recommend or educate about stocks, bonds, real estate, commodities, retirement accounts, or other traditional investments or keywords related to them.
Blockchain: Videos that recommend or educate about cryptocurrency (BTC, ETH, etc.), NFTs, or other Web3 investments or keywords related to them.
Mixed: Videos that recommend or educate about both blockchain and traditional investments or keywords related to both.
Unrelated: Videos that do not recommend or educate about either blockchain or traditional investments or keywords related to them.

Instructions:
- The classifier should consider the context and meaning of the keywords used to determine whether the snippet is related to traditional or blockchain investments.
- If talks about making money from jobs, side hustles, or other alternative assets (cars, watches, artificial intelligence, trading cards, art, etc), they are Unrelated.
- A video that is only downplaying an investment or discussing it negatively should be classified as Unrelated.
- Please return predictions in the format"" {Label} : {20 word or shorter rationale}""""""},

{""role"": ""assistant"", ""content"": 

""""""Understood. I will classify YouTube video snippets based on the provided labels and instructions. Here's how I will format the predictions:

    {Label} : {20-word or shorter rationale}

Please provide me with the YouTube video snippet you would like me to classify.""""""}
```
Note that unlike GPT-3, the ChatCompletions API endpoint expects a message, not a prompt. The system message biases the model towards a specific task, while the user messages provide prompts and instructions. The assistant messages can be used to affirm what has already been stated in the user messages.

You can test out your prompt on your seed videos!

`python run.py classify-seed`

This will run classification on the seed videos and return a classification report and confusion matrix. The confusion matrix can be found in `references/figures`.

More information on writing prompts can be found <a href = ""https://github.com/openai/openai-cookbook"">here</a>

Note: Occasionally, the output of a prediction will be 'Label'. This is due to GPT not adhering to the format we instruct it to. There is no known fix to this as of now.

For example:
```
### Expected Output
Blockchain: Rationale

### Occasional Output
Label: Blockchain: Rationale
```

By default, our seed classifier will mark videos with a prediction of 'Label' as unrelated. This may result in accuracy being off by 1-2% when in reality it should be higher. Please inspect your output yourself to verify!

## Conducting the Audit

The audit may take a very long time to run. Please make sure you're connected to a VPN before you start. We recommend using some program to keep your computer awake (Caffeine, Amphetamine). 

If you are using `python run.py audit`, the audit will prompt you if you want to conduct a single audit or all audits (in our case six audits.)

If you choose one audit, the one audit will be run based on these parameters in `config/Audit.py`
```
# AUDIT VARIABLES
USER_AGE = ""old""  #'young' or 'old'
FINANCE_VIDEO_TYPE = ""blockchain""  #'traditional', 'blockchain', 'mixed'
```

If you choose multiple audits, the code will iterate through the following dictionary in `config/Audit.py`, sleeping ten minutes between each audit.
```
AUDITS = [
    {""type"": ""traditional"", ""age"": ""young""},
    {""type"": ""mixed"", ""age"": ""young""},
    {""type"": ""blockchain"", ""age"": ""young""},
    {""type"": ""traditional"", ""age"": ""old""},
    {""type"": ""mixed"", ""age"": ""old""},
    {""type"": ""blockchain"", ""age"": ""old""},
]
```

After running the audit, please run:

```python
python run.py download-audit

python run.py create-audit-snippets
```

Audits are saved to `data/audit` with the following sub-folders:
```
raw - The raw video information. Please only keep the results of one run in this folder, as the pre-processing scripts read the entire directory to load in files. Having multiple audits in the folder will result in overriden and missing files in data cleaning steps.

processed - The downloaded videos. The sub-folder `snippets` contains the downloaded videos with snippets appended.

results - The results of our predictions.
```

## Running Inference

Running inference on the audit videos is easy!

`python run.py classify`

Running the following command with run classification on every single audit file. Results are stored in `data/audit/results/`.

## Analyzing Audit Results

GPT models use a parameter called 'temperature' to adjust how much risk their models take. 0 temperature means the model is deterministic, outputting the same response everytime. We've used a temeprate of 0.25 as we want the model to take a bit of risk. This, however, does resukt in cases where our predictions come back in the wrong format.

For example:
```
### Expected Output:

Blockchain: This video is blockchain

### Occassional Output:

Label: Blockchain: This video is blockchain
```

Because of this, we can not include a target for analyzing the results. It's much easier to analyze the result on your own through a notebook. We have a reference notebook: `notebooks/insepectPredictions.ipynb` that demonstrate loading in the result data, cleaning it, and running a Chi-Squared test. Please use that for reference. You can also generate any plots you need using `matplotlib` and `seaborn`. 

## Acknowledgements

We'd like to make a special thanks to the Data Science Capstone faculty, HDSI, Professor Stuart Geiger, and all our friends and family who provied us the opportunity to work on this project.
","# CryptoWho
Data Science Capstone Project advised under Stuart Geiger

## Authors
#### Brian Huang and Lily Yu

## Overview
In 2020, Bitcoin, other Cryptocurrencies and blockchain investments (NFTs) experienced a major boom. With the endorsements of many major companies and what seemed to be a large scale adoption on the horizon, the Crypto craze had started kicking off.

For many, cryptocurrency and NFTs were the first time they encountered an 'investment'. Many proclaimed crypto/NFT traders had no prior experience with any investing (stock market, retirement accounts, etc.) and looked to crypto/NFT as a get rich quick path. However, with the influx of new and young traders it was only a matter of time before the crypto/NFT scams began to pray on it's new consumers.

While crypto/NFT is not inherently a scam, many bad actors began to manipulate the influx of young and inexperienced investors. Many schemes akin to pump and dumps began to appear and with the popularity of crypto/NFT throughout social media, many inexperienced investors were quick to turn a blind eye to scams that seasoned investors may recognize immediately.

Cases like this have occured with traditional markets as well, where investors pour money into an asset based off of hype with no sound reasoning ($GME or GameStop). Crypto/NFTts however were especially susceptible to this with the combination of a relatively new asset, large demographic of young investors, and many social media influencers promoting these assets (Logan Paul, Doja Cat, etc)

With the most recent scandals in the cryptocurrency/NFT world (FTX, Logan Paul's CryptoZoo), it's more important than ever to investigate the platforms that many of these assets are promoted on. While platforms like YouTube and TikTok may not be intentionally promoting this content, it's important that they're aware of if their algorithms do indeed promote this type of content. To clarify, we do not provide an opinion on whether these scams are run by the founders of these products (FTX's Sam Bankman-Fried and CryptoZoo's Logal Paul) but rather emphasize that the scams have occured.

![crypo_losses](references/figures/crypto_losses.png)

According to the FTC, over a billion dollars has been lost to these types of scams since 2021. Half of which have orginated directly from social media. The most susceptible group of people? Young individuals.

The goal of this project is to investigate YouTube's recommendation algorithm to provide insight on the types of investment recommendations being provided to users across a variety of age groups. We assume that all individuals should be receiving the same proportion of recommendations based on their search trends (within a margin of error) regardless of age. This implies that a user who is younger and searching for general investment advice should not be receiving more crypto/NFT recommendations than someone who is older with similar watch history. By conducting audits on YouTube, we hope to gain valuable insight on YouTube and it's role in propogating this type of content on their platform (whether intentional or not).

### What does this repository offer?

The repository offers all tools used to conduct the audit. Helper functions and classes help download video and metadata from YouTube, run headless browsers to watch seed videos, and query the GPT-3 API for video sentiment classification.

All code can be found in the `src` folder and imported respectively. For any changes in filepath or settings, please look through the `config` folder.

## Table of Contents

- [Overview](#overview)
- [Methodology](#methodlogy)
- [Installation](#installation)
- [Downloading YouTube Data](#downloading-youtube-video-data)
- [GPT Prompting and Fine-Tuning](#gpt-prompting-and-fine-tuning)
- [Conducting the Audit](#conducting-the-audit)
- [Inference](#running-inference)
- [Analyzing Audit Results](#analyzing-audit-results)
- [Acknowledgements](#acknowledgements)

## Methodology

Our project is split into three parts:

**Part 1: Collecting Seed Videos and Creating Persona Users**

Seed videos function as a way for us to evaluate our GPT model and the videos that users watch to build watch history. Given the time we had, we could only collect and label 140 videos (20 for each age, 40 for traditional investments, 40 for blockchain investments, 5 for mixed, and 15 for edge cases where the video discusses money but is actually unrelated).

These seed videos fall into one of four labels:

Blockchain, Traditional, Mixed, and Unrelated.

Using our seed videos, we create six persona users with different watch behaviors.
```
A young individual (18-23) who:

Watches blockchain

Watches traditional

Watches mixed
```
```
A old individual (55-60) who:

Watches blockchain

Watches traditional

Watches mixed
```
By comparing the recommendations across these users, we can determine if YouTube is fairly and evenly recommending this content to all age groups. 

**Part 2: Creating a Prompt that peforms the best for our task at hand**

Creating a prompt is important when using GPT as a classifier. More info about our prompt can be found in the sections that follow.

**Part 3: Running the audit**

Using selenium, we can mimic the behavior of the six persona users above. By having a headless browser click and watch each video, we can collect the recommendations of each user. 

Each user watches 50% of the video by default. This and other config can be changed in the `config/Audit.py` file. 

The diagrams below illustrate the general pipeline for our audit:

![method](references/figures/methods.png)
![audit](references/figures/audit.png)

## Installation

We are using Python version [3.8.5](https://www.python.org/downloads/release/python-385/).

Please install this version of Python, as gensim summarization is still supported.

Versions between 3.8 and 3.9 should work, however we recommend you install the same version we use.

We recommend using [Anaconda](https://www.anaconda.com/) environments to do so.

**Once you've installed Anaconda, please run the following to create your environment.**

```bash
conda create --name <env_name> python=3.8.5
```

**Activate your conda environment like so:**
```bash
# Linux/Mac
source activate <env_name>

# Windows
activate <env_name>
```

**Install required packages**

```bash
pip install -r requirements.txt
```

**If you have Python 3.8.5 working outside of Conda (or any other version of Python that works with gensim) you can create a normal environment if you prefer**

```bash
python3 -m venv .venv

source .venv/bin/activate
```

**Install required packages**

```bash
pip install -r requirements.txt
```

### Headless Browser Setup

We use Selenium with the Firefox Webdriver to conduct our audit and gather YouTube video recommendations.

Note: More recent versions of FireFox will just launch if you have the browser installed. Please install the Firefox browser. If it does not work, install the driver.

To run the scraper, you will need to install the Firefox Webdriver, which can be downloaded [here](https://github.com/mozilla/geckodriver/releases).

To install, place your OS-appropriate executable in a directory locatable by your PATH.

### API Key Setup

Our codebase uses the YouTube Data API to download video metadata, comments, and for many other purposes like searching YouTube and grabbing recommendations. We use the OpenAI API to provide snippets and retrieve classification labels for our downloaded videos.

You can enable the YouTube Data API for your Google account and obtain an API key following the steps <a href=""https://developers.google.com/youtube/v3/getting-started"">here</a>.

The key can be found after you set up your cloud console.

You can fetch your OpenAI API key from <a href=""https://platform.openai.com/"">here</a>.

The key can be found in your profile under ***View API Keys***.

Once you have both API keys, please set the ```YOUTUBE_DATA_API_KEY``` and ```OPENAI_API_KEY``` variable in your environment:

You can do so by going to your home directory and running the following command:

**Mac OS and Linux**

```
nano .bash_profile

# Note Mac Users using zsh shell users should also set their keys in their zsh_profile
nano .zsh_profile
```

Inside your bash profile, you can go ahead and set this at the top:

```
# YOUTUBE API KEY
export YOUTUBE_DATA_API_KEY=""YOUR_API_KEY""
export OPENAI_API_KEY=""YOUR_API_KEY""
```

Close out of your terminal and your code editor to see changes occur.

**Check that updates have been made**
```
echo $YOUTUBE_DATA_API_KEY
echo $OPENAI_API_KEY
```

The following tutorials cover how to do this as well:

https://www.youtube.com/watch?v=5iWhQWVXosU&t=1s (Mac/Linux)

https://www.youtube.com/watch?v=IolxqkL7cD8 (Windows)

If you are not seeing updates, your `bash_profile` may not be sourced. To resolve this, add the following line to your `.bashrc`:

```
. ~/.bash_profile

# Note Mac Users using zsh shell users should do this in .zshrc
. ~/.zsh_profile
```

This can be anywhere, but we've put ours at the very bottom. Use the following command to enter your `.bashrc`.

```
nano .bashrc
# Note Mac Users using zsh shell users should do this
nano .zshrc
```

Now within Python you can access your API key by doing the following:
```
import os

youtube_key = os.environ.get(""YOUTUBE_DATA_API_KEY"")
openai_key = os.environ.get(""OPENAI_API_KEY"")
```

### Connecting to a VPN

To ensure all things are constant, we connected to the UC San Diego VPN for all of our audits and downloads. We recommend you connect to a VPN in the same location as well. There are many different VPN providers.

### Preserving User Agent

We recommend the audits be done on the same device, and if that is not possible, the same operating system. All of our audits were conducted on a Ubuntu device, however the platform you choose to audit with should not matter as long as they are consistent.

## Running The Entire Project Pipeline (Not Recommmended)

Our code uses a `run.py` file to help you run our code out of the box. You have the option to run the entire project pipeline using the folliwng command:

`python run.py all`

This command will download all seed videos, create snippets and evaluate your seed videos using the classifier, conduct the audit and download all videos, and classify the videos from the audit.

We **DO NOT** recommend you run this. The entire pipeline may take multiple days to run depending on the size of your seed videos. (Note: With 140 seed videos, our entire pipeline takes three days to run.)

If you would like to validate that the pipeline and it's sub-targets work, we instead recommended using:

`python run.py test`

Which will run the pipeline end to end on a much smaller subset of data found in `test/youtube`. This takes around an hour.

You can continue reading below for more details, but here is an overview of all targets in the run.py file. They are listed in the order we recommend you run them. Running each target gives you finer control over what is downloaded, avoiding repeated efforts/wasted API calls.
```
seed - The following target downloads and processed seed data. Target will prompt user for what they want to download and if they want to process the seed data (create video snippets.) Video snippets for context is a concatenation of the video title + summarized transcript (via TextRank) + top ten video tags (via TF-IDF)

classify-seed - The following target classifies the seed videos, providing a baseline classification report of your prompt. The confusion matrix is saved in references/figures. Note that the classification report is printed in terminal and not saved.

audit - The following target conducts the audit on a Firefox headless browser. It allows you to run a single or multiple audits. Config for each audit can be found in config/Audit.py

download-audit - The following target allows you to download the results of your audit. You can choose to download homepage results, sidebar results, or both.

create-audit-snippets - The following target creates snippets for each set of audit results. Note this may take a while as some videos have large transcripts. Video's over an hour have no transcript removed, however by modifying the target code this can be removed if you are able to wait the additional time.

classify - Run classification on your audit snippets.
```

## Downloading YouTube Video Data
Using `python run.py seed` will download all seed videos and save it in `data/seed/youtube/videos_{}.csv`.

Using `python run.py download-audit` will download all videos from the audit and save the downloaded videos in `data/audit/youtube/videos_{}.csv`.

Calling `python3 src/data/youTubeDownloader.py <video_ids seperated by spaces>` will download any videos you want and save it in `data/external/youtube/videos_{}.csv`. 

## GPT Prompting and Fine Tuning
Unlike traditional classifiers, GPT-3 is not trained on an existing dataset. Instead, predictions are generated through a set of 'prompts', instructing the large language model what the task is.

At the start of this project, GPT-3 was used. GPT-3 can be fine-tuned using a labelled dataset and prompts. For those interested in using a fine-tuned GPT-3 model, we encourage you to use the following <a href=""https://platform.openai.com/docs/guides/fine-tuning"">resource</a>.

Starting March 1st, GPT-3.5 was released. This API powers ChatGPT. Due to significant reduction in cost and increases in accuracy, we migrated to <a href = ""https://platform.openai.com/docs/models/gpt-3"">GPT 3.5.</a>

You are open to modify the prompt for our model however you see fit. The prompt can be found in `gpt.py` in the `create_message()` function.

The following prompt was used for our classification:
```
{""role"": ""system"", ""content"" : 

""You are a classifier that determines if a YouTube video snippet falls under a label. A snippet is a concatenation of the video title, summarized transcript, and video tags. The labels and additional instructions will be included in the first user message.""},

{""role"": ""user"", ""content"" : 

""""""Labels:

Traditional: Videos that recommend or educate about stocks, bonds, real estate, commodities, retirement accounts, or other traditional investments or keywords related to them.
Blockchain: Videos that recommend or educate about cryptocurrency (BTC, ETH, etc.), NFTs, or other Web3 investments or keywords related to them.
Mixed: Videos that recommend or educate about both blockchain and traditional investments or keywords related to both.
Unrelated: Videos that do not recommend or educate about either blockchain or traditional investments or keywords related to them.

Instructions:
- The classifier should consider the context and meaning of the keywords used to determine whether the snippet is related to traditional or blockchain investments.
- If talks about making money from jobs, side hustles, or other alternative assets (cars, watches, artificial intelligence, trading cards, art, etc), they are Unrelated.
- A video that is only downplaying an investment or discussing it negatively should be classified as Unrelated.
- Please return predictions in the format"" {Label} : {20 word or shorter rationale}""""""},

{""role"": ""assistant"", ""content"": 

""""""Understood. I will classify YouTube video snippets based on the provided labels and instructions. Here's how I will format the predictions:

    {Label} : {20-word or shorter rationale}

Please provide me with the YouTube video snippet you would like me to classify.""""""}
```
Note that unlike GPT-3, the ChatCompletions API endpoint expects a message, not a prompt. The system message biases the model towards a specific task, while the user messages provide prompts and instructions. The assistant messages can be used to affirm what has already been stated in the user messages.

You can test out your prompt on your seed videos!

`python run.py classify-seed`

This will run classification on the seed videos and return a classification report and confusion matrix. The confusion matrix can be found in `references/figures`.

More information on writing prompts can be found <a href = ""https://github.com/openai/openai-cookbook"">here</a>

Note: Occasionally, the output of a prediction will be 'Label'. This is due to GPT not adhering to the format we instruct it to. There is no known fix to this as of now.

For example:
```
### Expected Output
Blockchain: Rationale

### Occasional Output
Label: Blockchain: Rationale
```

By default, our seed classifier will mark videos with a prediction of 'Label' as unrelated. This may result in accuracy being off by 1-2% when in reality it should be higher. Please inspect your output yourself to verify!

## Conducting the Audit

The audit may take a very long time to run. Please make sure you're connected to a VPN before you start. We recommend using some program to keep your computer awake (Caffeine, Amphetamine). 

If you are using `python run.py audit`, the audit will prompt you if you want to conduct a single audit or all audits (in our case six audits.)

If you choose one audit, the one audit will be run based on these parameters in `config/Audit.py`
```
# AUDIT VARIABLES
USER_AGE = ""old""  #'young' or 'old'
FINANCE_VIDEO_TYPE = ""blockchain""  #'traditional', 'blockchain', 'mixed'
```

If you choose multiple audits, the code will iterate through the following dictionary in `config/Audit.py`, sleeping ten minutes between each audit.
```
AUDITS = [
    {""type"": ""traditional"", ""age"": ""young""},
    {""type"": ""mixed"", ""age"": ""young""},
    {""type"": ""blockchain"", ""age"": ""young""},
    {""type"": ""traditional"", ""age"": ""old""},
    {""type"": ""mixed"", ""age"": ""old""},
    {""type"": ""blockchain"", ""age"": ""old""},
]
```

After running the audit, please run:

```python
python run.py download-audit

python run.py create-audit-snippets
```

Audits are saved to `data/audit` with the following sub-folders:
```
raw - The raw video information. Please only keep the results of one run in this folder, as the pre-processing scripts read the entire directory to load in files. Having multiple audits in the folder will result in overriden and missing files in data cleaning steps.

processed - The downloaded videos. The sub-folder `snippets` contains the downloaded videos with snippets appended.

results - The results of our predictions.
```

## Running Inference

Running inference on the audit videos is easy!

`python run.py classify`

Running the following command with run classification on every single audit file. Results are stored in `data/audit/results/`.

## Analyzing Audit Results

GPT models use a parameter called 'temperature' to adjust how much risk their models take. 0 temperature means the model is deterministic, outputting the same response everytime. We've used a temeprate of 0.25 as we want the model to take a bit of risk. This, however, does resukt in cases where our predictions come back in the wrong format.

For example:
```
### Expected Output:

Blockchain: This video is blockchain

### Occassional Output:

Label: Blockchain: This video is blockchain
```

Because of this, we can not include a target for analyzing the results. It's much easier to analyze the result on your own through a notebook. We have a reference notebook: `notebooks/insepectPredictions.ipynb` that demonstrate loading in the result data, cleaning it, and running a Chi-Squared test. Please use that for reference. You can also generate any plots you need using `matplotlib` and `seaborn`. 

## Acknowledgements

We'd like to make a special thanks to the Data Science Capstone faculty, HDSI, Professor Stuart Geiger, and all our friends and family who provied us the opportunity to work on this project.
"
192,https://github.com/gprasad125/dsc180b,"{'gprasad125': 'https://github.com/gprasad125', 'AnnieeeeeF': 'https://github.com/AnnieeeeeF'}","{'Jupyter Notebook': 0.94, 'Python': 0.06, 'Dockerfile': 0.0}","# Analyzing U.S. Congressional Tweets with OpenAI GPT-3

# Repository for the Spring 2023 Quarter (DSC180b)

This project covers Tweet sentiment analysis for Tweets originating from US Congresspeople as it relates to China.
This is an extension to Quarter 1's project found for each researcher below:

[Annie's Q1 codebase](https://github.com/AnnieeeeeF/DSC180A_Project1)

[Gokul's Q1 codebase](https://github.com/gprasad125/dsc180a_project)

This project continues the work by exploring the same topic through the lens of a Large Language Model (LLM). 

## Necessary Configurations:

You will *need* an API key from OpenAI to utilize the GPT-3 model.
Sign up for an account and get a key [here](https://openai.com/api/)

You can then pass your API Key to our scripts in one of two ways:

1. Export your key by running the following in your command line:

`export OPENAI_API_KEY=...`

2. Create a .env file in the root directory and paste in your key like so:

`OPENAI_API_KEY=...`

## Data Source:

Raw data can be found [here](https://drive.google.com/drive/u/1/folders/1VSYdGh12UNVNhfxbSeHRdANvHr5xF8Ea). 
Download the file `SentimentLabeled_10112022.csv`, and place it inside the `data/raw` directory. 

You can then run the `run.py` file with the following targets:
- `test`: runs the file on man-made test data
- `data` / `all`: runs the file on Twitter-API sourced data.

## Explanation of File Structure:

### 📁 Folders:

#### config
Contains JSON configuration for optimized & group-selected models. 

#### data
Contains the data for and from the project, divided as such:
- raw: the base uncleaned data
- out: the output cleaned data used for visualizations and modeling
- test: test data used to debug the Python scripts
- results: visuals generated from the EDA and modeling, formatted as PNGs

#### notebooks
Contains initial Jupyter Notebooks for EDA / Modeling.
Not entirely cleaned up yet. Cleaned versions of this code will be found inside our `src` folder.

#### src
Contains the Python scripts needed to run the project, divided as such:
- data: 
    - `make_dataset.py` cleans and processes the raw data
- models: 
    - `classifier.py`: GPT-3 powered classifier to find ""relevant"" Tweets (i.e, Tweets about Chinese governmental impact on America.)
    - `sentiment.py`: GPT-3 powered sentiment scorer to find ""emotion"" of Tweet (i.e, is a Tweet favorable or negative towards China?)
- visuals: 
    - `eda.py`: Generates summary visuals for the two cleaned dataframes going into modeling. Not the full EDA of the dataset. For that, check under `notebooks/EDA.ipynb`
- notebooks:
    - `nb_functions.py`: All necessary functions for notebook report + visuals. Uses the code from other folders with slight modifications to fit an ipynb environment. 

### 📜 Files:

#### run.py
Baseline Python script to run via CLI with targets.
Current targets include `test` (`all`) and `data`. 

    - Creates cleaned data file
    - Generates exploratory visuals and saves them
    - Runs models on data

### requirements.txt
Necessary Python packages to install via `pip install -r requirements.txt`

","# Analyzing U.S. Congressional Tweets with OpenAI GPT-3

# Repository for the Spring 2023 Quarter (DSC180b)

This project covers Tweet sentiment analysis for Tweets originating from US Congresspeople as it relates to China.
This is an extension to Quarter 1's project found for each researcher below:

[Annie's Q1 codebase](https://github.com/AnnieeeeeF/DSC180A_Project1)

[Gokul's Q1 codebase](https://github.com/gprasad125/dsc180a_project)

This project continues the work by exploring the same topic through the lens of a Large Language Model (LLM). 

## Necessary Configurations:

You will *need* an API key from OpenAI to utilize the GPT-3 model.
Sign up for an account and get a key [here](https://openai.com/api/)

You can then pass your API Key to our scripts in one of two ways:

1. Export your key by running the following in your command line:

`export OPENAI_API_KEY=...`

2. Create a .env file in the root directory and paste in your key like so:

`OPENAI_API_KEY=...`

## Data Source:

Raw data can be found [here](https://drive.google.com/drive/u/1/folders/1VSYdGh12UNVNhfxbSeHRdANvHr5xF8Ea). 
Download the file `SentimentLabeled_10112022.csv`, and place it inside the `data/raw` directory. 

You can then run the `run.py` file with the following targets:
- `test`: runs the file on man-made test data
- `data` / `all`: runs the file on Twitter-API sourced data.

## Explanation of File Structure:

### 📁 Folders:

#### config
Contains JSON configuration for optimized & group-selected models. 

#### data
Contains the data for and from the project, divided as such:
- raw: the base uncleaned data
- out: the output cleaned data used for visualizations and modeling
- test: test data used to debug the Python scripts
- results: visuals generated from the EDA and modeling, formatted as PNGs

#### notebooks
Contains initial Jupyter Notebooks for EDA / Modeling.
Not entirely cleaned up yet. Cleaned versions of this code will be found inside our `src` folder.

#### src
Contains the Python scripts needed to run the project, divided as such:
- data: 
    - `make_dataset.py` cleans and processes the raw data
- models: 
    - `classifier.py`: GPT-3 powered classifier to find ""relevant"" Tweets (i.e, Tweets about Chinese governmental impact on America.)
    - `sentiment.py`: GPT-3 powered sentiment scorer to find ""emotion"" of Tweet (i.e, is a Tweet favorable or negative towards China?)
- visuals: 
    - `eda.py`: Generates summary visuals for the two cleaned dataframes going into modeling. Not the full EDA of the dataset. For that, check under `notebooks/EDA.ipynb`
- notebooks:
    - `nb_functions.py`: All necessary functions for notebook report + visuals. Uses the code from other folders with slight modifications to fit an ipynb environment. 

### 📜 Files:

#### run.py
Baseline Python script to run via CLI with targets.
Current targets include `test` (`all`) and `data`. 

    - Creates cleaned data file
    - Generates exploratory visuals and saves them
    - Runs models on data

### requirements.txt
Necessary Python packages to install via `pip install -r requirements.txt`

"
193,https://github.com/x6zeng/NLP-Active-Learning-Pipeline,"{'yunyihuang': 'https://github.com/yunyihuang', 'x6zeng': 'https://github.com/x6zeng'}","{'Jupyter Notebook': 0.99, 'Python': 0.01, 'Dockerfile': 0.0}","# NLP-Active-Learning-Pipeline
This is the repository for our DSC180B section A12 Group B Quarter 2 Project, which consists of 2 machine learning models, with Active Learning approaches, that can be used to predict the relevance and sentiment toward China of the tweets posted by the members of the U.S. Congress, given the tweet's text content.

## Main Content
- __data__: folder to store data, including test data and other data. It is also used to store the results data for the author
  - test: folder to store the test data
  - raw: empty, folder to store the raw data
  - result: folder to store the result data
- __notebook__: folder to store the pre-development notebooks
  - analyses: notebooks containing the active learning results analyses.
  - explorations: code explorations for active learning pipeline.
  - model_comparison: all the pre-development code for relevance and sentiment model, as well as the comparison between models using different hyperparameters.
- __src__: folder to store the files of obtaining the dataset, building the features, and the code for the 2 models
  - `utilities.py` - script to preprocess the raw data
  - `Relevance_AL_Committee.py` - script to train the relevance model
  - `Sentiment_AL_Committee.py` - script to train the sentiment model

## Data Source
The data used in this project was provided by the staffs from the China Data Lab at UC San Diego. Click [here](https://drive.google.com/drive/folders/1VSYdGh12UNVNhfxbSeHRdANvHr5xF8Ea?usp=sharing) for data. If running the models with the raw data, please place the `SentimentLabeled_10112022.csv` in the folder `data/raw`.

## Important Files
- `Dockerfile`: contains the information for building the docker image
- `run.py`: the script to run the models. To run the models on test data, use the following command: 
  - `python3 run.py test`
- `submission.json`: contains the submission information","# NLP-Active-Learning-Pipeline
This is the repository for our DSC180B section A12 Group B Quarter 2 Project, which consists of 2 machine learning models, with Active Learning approaches, that can be used to predict the relevance and sentiment toward China of the tweets posted by the members of the U.S. Congress, given the tweet's text content.

## Main Content
- __data__: folder to store data, including test data and other data. It is also used to store the results data for the author
  - test: folder to store the test data
  - raw: empty, folder to store the raw data
  - result: folder to store the result data
- __notebook__: folder to store the pre-development notebooks
  - analyses: notebooks containing the active learning results analyses.
  - explorations: code explorations for active learning pipeline.
  - model_comparison: all the pre-development code for relevance and sentiment model, as well as the comparison between models using different hyperparameters.
- __src__: folder to store the files of obtaining the dataset, building the features, and the code for the 2 models
  - `utilities.py` - script to preprocess the raw data
  - `Relevance_AL_Committee.py` - script to train the relevance model
  - `Sentiment_AL_Committee.py` - script to train the sentiment model

## Data Source
The data used in this project was provided by the staffs from the China Data Lab at UC San Diego. Click [here](https://drive.google.com/drive/folders/1VSYdGh12UNVNhfxbSeHRdANvHr5xF8Ea?usp=sharing) for data. If running the models with the raw data, please place the `SentimentLabeled_10112022.csv` in the folder `data/raw`.

## Important Files
- `Dockerfile`: contains the information for building the docker image
- `run.py`: the script to run the models. To run the models on test data, use the following command: 
  - `python3 run.py test`
- `submission.json`: contains the submission information"
194,https://github.com/colts661/Incomplete-Text-Classification,{'colts661': 'https://github.com/colts661'},"{'Python': 0.99, 'Dockerfile': 0.01}","# Incomplete Supervision: Text Classification based on a Subset of Labels

In this project, we explore the **I**n**c**omplete **T**ext **C**lassification (IC-TC) setting. We aim to design a text classification model that could suggest class names not belonging to the training corpus to unseen documents, and classify documents into a full set of class names.

<div style=""display:flex;"">
    <div style=""width:100%;float:left"">
        Authors: Luning Yang, Yacun Wang<br>Mentor: Jingbo Shang
    </div>
</div>


### Model Pipeline
- Find seed words from the supervised set: TF-IDF
- Use full corpus to find word embeddings
  - Final Model: Pretrained `BERT` contextualized word embeddings using static representations guided by `XClass`, reduced dimensions using PCA
  - Baseline Model: Trained `Word2Vec` word embeddings
- Find document and class embeddings based on averaged word embeddings, from the documents or seed words
- Compute similarity as confidence score, predict argmax if confidence over threshold
- For other unconfident documents, run clustering and label generation (LI-TF-IDF or Prompted ChatGPT)
<p align=""center""><img width=""60%"" src=""others/model-pipeline.png""/></p>


### Environment

- [**DSMLP Users**]: Since the data for this project is large, please run DSMLP launch script using a larger RAM. The suggested command is `launch.sh -i yaw006/incomplete-tc:final -m 16 -g 1`. Please **DO NOT** use the default, otherwise Python processes will be killed halfway.
- Other options:
  - Option 1: Run the docker container: `docker run yaw006/incomplete-tc:final`;
  - Option 2: Install all required packages in `requirements.txt`.

### Data
#### Data Information
- The datasets used in the experiments can be found on [Google Drive](https://drive.google.com/drive/folders/1kf3AXpKbwbZuQhcVSiaMzCiaSrWTdO7i?usp=sharing).
- The datasets used in the experiments are: `DBPedia`, `DBPedia-small`, `nyt-fine`, `Reddit`
- **Note**: `DBPedia-small` is the default experiment target dataset, as it contains a subset of documents for the full `DBPedia` dataset, and could be run in a few minutes.

#### Get Data
- [**DSMLP Users**]: For the 3 datasets provided, convenient Linux commands to download and get the data are provided in the [documentation of raw data](data/raw/). Please run the commands in the **repository root directory**.
- Generally, under Linux command line, for any Google Drive zip file, 
  - Follow the `wget` [tutorial](https://medium.com/@acpanjan/download-google-drive-files-using-wget-3c2c025a8b99)
    - Find the Large File section (highlighted code section towards the end)
    - Paste the `<FILEID>` from the `zip` file **sharing link** found on Google Drive
    - Change the `<FILENAME>` to your data title
  - Run `cd <dir>` to change directory into the data directory
  - Run `unzip -o <zip name>` to unzip the data
  - Run `rm <zip name>` to avoid storing too many objects in the container
  - Run `cd <root>` to change directory back to your working directory
  - Run `mkdir <data>` to create the processed data directory
- Under non-command line, go to the Google Drive link, download the zip directly, place the files according to the requirements in the **Data Format** section, and manually created the directory needed for processed files. See the **File Outline** section for example.

#### Data Format
- Raw Data: Each dataset must contain a `df.pkl` placed in `data/raw/`. The file should be a compressed Pandas DataFrame using `pickle` containing two columns: `sentence` (for documents) and `label` (for the corresponding label).
- Processed Data: 
  - The corpus will be processed after the first run, and processed files will be placed in `data/processed`.
  - The processed file will be directly loaded for subsequent runs.

### Commands
[**DSMLP Users**]: 
- The `test` target could be easily run as `python run.py test`.
- The `experiment` target could be run as `python run.py exp -d <dataset>`.
- When prompted from the prompt, insert values.

The main script is located in the root directory. It supports 3 targets:
- `test`: Run the test data. All other flags are ignored.
- `experiment` (or `exp`) [default]: Perform one vanilla run.

The full command is:
```
python run.py [-h] target [-d DATA] [-m MODEL]

required: target {test,experiment,exp}
  run target. Default experiment; if test is selected, run baseline model on testdata.

optional arguments:
  -h, --help                 show this help message and exit
  -d DATA, --data DATA       data path, required for non-testdata
  -m MODEL, --model MODEL    model pipeline, {'final', 'baseline'}. Default 'final'
```
**Notes**: 
1. Due to data size constraints to run large BERT embeddings or train embeddings based on the corpus, the `test` target will run the baseline model with pre-trained `glove-twitter-25` word embedding to speed up computation time.
2. Due to time constraints and container constraints, the short experiments are chosen to run fast, which means performance is not guaranteed.


### Code File Outline
```
Incomplete-Text-Classification/
├── run.py                           <- main run script
├── data/                            <- all data files
│   ├── raw                          <- raw files (after download)
│   │   ├── nyt-fine
│   │   |   └── df.pkl               <- required DataFrame pickle file
│   |   └── ...
│   └── processed/                   <- processed files (after preprocessing)
├── src/                             <- source code library
│   ├── data.py                      <- data class definition
│   ├── word_embedding.py            <- word embedding modules
│   ├── similarity.py                <- computing similarities and cutoff
│   ├── unsupervised.py              <- dimensionality reduction and clustering
│   ├── generation.py                <- label or seed word generation
│   ├── evaluation.py                <- evaluation methods
│   ├── models.py                    <- model pipelines
│   └── util.py                      <- other utility functions
└── test/                            <- test target data
```

---
### Citations

#### Word2Vec
```
@article{word2vec,
    title={Efficient estimation of word representations in vector space},
    author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
    journal={arXiv preprint arXiv:1301.3781},
    year={2013}
}
```

#### XClass
```
@misc{wang2020xclass,
      title={X-Class: Text Classification with Extremely Weak Supervision}, 
      author={Zihan Wang and Dheeraj Mekala and Jingbo Shang},
      year={2020},
      eprint={2010.12794},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

#### BERT
```
@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
```
<div style=""float:right"">
    <img width=""25%"" src=""others/HDSI.png"" alt=""Logo"">
</div>","# Incomplete Supervision: Text Classification based on a Subset of Labels

In this project, we explore the **I**n**c**omplete **T**ext **C**lassification (IC-TC) setting. We aim to design a text classification model that could suggest class names not belonging to the training corpus to unseen documents, and classify documents into a full set of class names.

<div style=""display:flex;"">
    <div style=""width:100%;float:left"">
        Authors: Luning Yang, Yacun Wang<br>Mentor: Jingbo Shang
    </div>
</div>


### Model Pipeline
- Find seed words from the supervised set: TF-IDF
- Use full corpus to find word embeddings
  - Final Model: Pretrained `BERT` contextualized word embeddings using static representations guided by `XClass`, reduced dimensions using PCA
  - Baseline Model: Trained `Word2Vec` word embeddings
- Find document and class embeddings based on averaged word embeddings, from the documents or seed words
- Compute similarity as confidence score, predict argmax if confidence over threshold
- For other unconfident documents, run clustering and label generation (LI-TF-IDF or Prompted ChatGPT)
<p align=""center""><img width=""60%"" src=""others/model-pipeline.png""/></p>


### Environment

- [**DSMLP Users**]: Since the data for this project is large, please run DSMLP launch script using a larger RAM. The suggested command is `launch.sh -i yaw006/incomplete-tc:final -m 16 -g 1`. Please **DO NOT** use the default, otherwise Python processes will be killed halfway.
- Other options:
  - Option 1: Run the docker container: `docker run yaw006/incomplete-tc:final`;
  - Option 2: Install all required packages in `requirements.txt`.

### Data
#### Data Information
- The datasets used in the experiments can be found on [Google Drive](https://drive.google.com/drive/folders/1kf3AXpKbwbZuQhcVSiaMzCiaSrWTdO7i?usp=sharing).
- The datasets used in the experiments are: `DBPedia`, `DBPedia-small`, `nyt-fine`, `Reddit`
- **Note**: `DBPedia-small` is the default experiment target dataset, as it contains a subset of documents for the full `DBPedia` dataset, and could be run in a few minutes.

#### Get Data
- [**DSMLP Users**]: For the 3 datasets provided, convenient Linux commands to download and get the data are provided in the [documentation of raw data](data/raw/). Please run the commands in the **repository root directory**.
- Generally, under Linux command line, for any Google Drive zip file, 
  - Follow the `wget` [tutorial](https://medium.com/@acpanjan/download-google-drive-files-using-wget-3c2c025a8b99)
    - Find the Large File section (highlighted code section towards the end)
    - Paste the `<FILEID>` from the `zip` file **sharing link** found on Google Drive
    - Change the `<FILENAME>` to your data title
  - Run `cd <dir>` to change directory into the data directory
  - Run `unzip -o <zip name>` to unzip the data
  - Run `rm <zip name>` to avoid storing too many objects in the container
  - Run `cd <root>` to change directory back to your working directory
  - Run `mkdir <data>` to create the processed data directory
- Under non-command line, go to the Google Drive link, download the zip directly, place the files according to the requirements in the **Data Format** section, and manually created the directory needed for processed files. See the **File Outline** section for example.

#### Data Format
- Raw Data: Each dataset must contain a `df.pkl` placed in `data/raw/`. The file should be a compressed Pandas DataFrame using `pickle` containing two columns: `sentence` (for documents) and `label` (for the corresponding label).
- Processed Data: 
  - The corpus will be processed after the first run, and processed files will be placed in `data/processed`.
  - The processed file will be directly loaded for subsequent runs.

### Commands
[**DSMLP Users**]: 
- The `test` target could be easily run as `python run.py test`.
- The `experiment` target could be run as `python run.py exp -d <dataset>`.
- When prompted from the prompt, insert values.

The main script is located in the root directory. It supports 3 targets:
- `test`: Run the test data. All other flags are ignored.
- `experiment` (or `exp`) [default]: Perform one vanilla run.

The full command is:
```
python run.py [-h] target [-d DATA] [-m MODEL]

required: target {test,experiment,exp}
  run target. Default experiment; if test is selected, run baseline model on testdata.

optional arguments:
  -h, --help                 show this help message and exit
  -d DATA, --data DATA       data path, required for non-testdata
  -m MODEL, --model MODEL    model pipeline, {'final', 'baseline'}. Default 'final'
```
**Notes**: 
1. Due to data size constraints to run large BERT embeddings or train embeddings based on the corpus, the `test` target will run the baseline model with pre-trained `glove-twitter-25` word embedding to speed up computation time.
2. Due to time constraints and container constraints, the short experiments are chosen to run fast, which means performance is not guaranteed.


### Code File Outline
```
Incomplete-Text-Classification/
├── run.py                           <- main run script
├── data/                            <- all data files
│   ├── raw                          <- raw files (after download)
│   │   ├── nyt-fine
│   │   |   └── df.pkl               <- required DataFrame pickle file
│   |   └── ...
│   └── processed/                   <- processed files (after preprocessing)
├── src/                             <- source code library
│   ├── data.py                      <- data class definition
│   ├── word_embedding.py            <- word embedding modules
│   ├── similarity.py                <- computing similarities and cutoff
│   ├── unsupervised.py              <- dimensionality reduction and clustering
│   ├── generation.py                <- label or seed word generation
│   ├── evaluation.py                <- evaluation methods
│   ├── models.py                    <- model pipelines
│   └── util.py                      <- other utility functions
└── test/                            <- test target data
```

---
### Citations

#### Word2Vec
```
@article{word2vec,
    title={Efficient estimation of word representations in vector space},
    author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
    journal={arXiv preprint arXiv:1301.3781},
    year={2013}
}
```

#### XClass
```
@misc{wang2020xclass,
      title={X-Class: Text Classification with Extremely Weak Supervision}, 
      author={Zihan Wang and Dheeraj Mekala and Jingbo Shang},
      year={2020},
      eprint={2010.12794},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

#### BERT
```
@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
```
<div style=""float:right"">
    <img width=""25%"" src=""others/HDSI.png"" alt=""Logo"">
</div>"
195,https://github.com/k6chan/reverse-dictionary-pokemon,{'k6chan': 'https://github.com/k6chan'},"{'Python': 0.76, 'HTML': 0.22, 'Dockerfile': 0.02}","# pokemon-reverse-dictionary

## [Static Website](https://k6chan.github.io/reverse-dictionary-pokemon/)

A reverse dictionary for Pokemon as a Python Flask web app.

## Setup

*With approval from my TA, a `run.py` script is not necessary for this website project.*

### With Docker (automatically installs libraries and starts the dev server)

**The Flask server hosting is not compatible with Docker on DSMLP. This Docker image can only be run locally, not on DSMLP.**

```
DockerHub repository:

https://hub.docker.com/repository/docker/k6chan/reverse-dictionary-pokemon
```

Run the Docker image using `docker run -it --rm -p 5000:5000 k6chan/reverse-dictionary-pokemon:latest`.

The Flask server should automatically start up. Access the server in your browser, usually `http://127.0.0.1:5000/`.

### Without Docker

First, clone the repository to your device with `git clone`.

Install the Python libraries in `requirements.txt` with `pip install --no-cache-dir -r requirements.txt`.

Change directory into `src/models` and use the following command to run the dev server:

`python -m flask --app application run`

Access the server in your browser, usually `http://127.0.0.1:5000/`.

## Contribution Statement

Solo project by Kaitlyn Chan.
","# pokemon-reverse-dictionary

## [Static Website](https://k6chan.github.io/reverse-dictionary-pokemon/)

A reverse dictionary for Pokemon as a Python Flask web app.

## Setup

*With approval from my TA, a `run.py` script is not necessary for this website project.*

### With Docker (automatically installs libraries and starts the dev server)

**The Flask server hosting is not compatible with Docker on DSMLP. This Docker image can only be run locally, not on DSMLP.**

```
DockerHub repository:

https://hub.docker.com/repository/docker/k6chan/reverse-dictionary-pokemon
```

Run the Docker image using `docker run -it --rm -p 5000:5000 k6chan/reverse-dictionary-pokemon:latest`.

The Flask server should automatically start up. Access the server in your browser, usually `http://127.0.0.1:5000/`.

### Without Docker

First, clone the repository to your device with `git clone`.

Install the Python libraries in `requirements.txt` with `pip install --no-cache-dir -r requirements.txt`.

Change directory into `src/models` and use the following command to run the dev server:

`python -m flask --app application run`

Access the server in your browser, usually `http://127.0.0.1:5000/`.

## Contribution Statement

Solo project by Kaitlyn Chan.
"
196,https://github.com/zaxiang/Spam_Filter,"{'gbirch11': 'https://github.com/gbirch11', 'lorraineeeee': 'https://github.com/lorraineeeee', 'zaxiang': 'https://github.com/zaxiang'}","{'Jupyter Notebook': 0.87, 'Python': 0.13, 'Dockerfile': 0.0}","# Weakly Supervised Spam-Label Classification
DSC180 Quarter 2 Capstone Project 

Using a list of categories and words that represent these categories, we classify harmful spam messages into categories such as insurance scams, medical sales, software sales, and more. Doing so, we hope to alleviate the burden on non technical people in todays world as spammers continue to get by detection systems - we want to find and highlight a pattern throughout them all. Leveraging models ranging from simple methods like TFIDF to complex large language models such as ConWea with BERT, we examine the differences between these models and if it is worth using such big, computation costly models.

You can see more details about our project on our [website](https://gbirch11.github.io/SpamLabelClassifier/).

# Data
The data is available on [Google Drive](https://drive.google.com/drive/folders/1uTRzRPkom6nUtRB2D4pOi8uOpSpqst7m?usp=share_link)\
Please unzip and place the files into the following locations; \
Annotated Spam Messages -> ```data/raw/spam/Annotated/``` \
Unannotated Spam Messages -> ```data/raw/spam/Unannotated/``` \
Non-spam (Ham) Messages -> ```data/raw/ham/``` 


The dataset should contain the following files:
1) Annotated Spam Messages \
  ex) ```data/raw/spam/Annotated/medical-sales/xyz.txt```
    * Where xyz is any file name that was annotated to be medical sale spam
    * Other folders follow same pattern for each category
2) Non-spam (ham) Messages \
  ex) ```data/raw/ham/xyz.txt```
3) Seedwords JSON file \
  ex) ```data/out/seedwords.json```

## Running the Project
**DSMLP Command**
``` 
launch.sh -i gbirch11/dsc180b [-m d] [-g 1]
```
Note: -m is an optional argument to include more RAM on the machine; HIGLHLY RECOMMEND setting $d$ to 16 or 32 for faster processing \
Also highly recommended to run with -g 1, especially if running ConWea model.
``` 
launch.sh -i gbirch11/dsc180b -m 32 -g 1
```
<br> <br>
To run this project, execute the following command;
```
python run.py [test | data]
```
Note: If running ```python run.py test``` \
Very simple set of test data will be used to produce results. \
Result trend not consistent with running on full dataset.

If running ```python run.py data```: \
Whole dataset will be used to produce results.

Example commands include: \
``` python run.py test ``` \
``` python run.py data ```

Note: The above commands only run on the TF-IDF, Word2Vec, and FastText models. To run our best model, ConWea, see the section below.

## Running the ConWea Model
Since ConWea is a huge model using BERT, we have separated this model into the following separate commands;
1) Navigate to the ConWea model directory using \
``` cd src/models/ConWea ``` <br> <br>
2) To contextualize the corpus and seed words run \
a) For testing: ``` python contextualize.py --dataset_path ""../../../test/testdata/"" --temp_dir ""temp/"" --gpu_id 0 ``` \
b) For full data: ``` python contextualize.py --dataset_path ""../../../data/raw/spam/Annotated/"" --temp_dir ""temp/"" --gpu_id 0 ```  <br> <br>
3) To train model + observe results run \
a) For testing: ``` python train.py --dataset_path ""../../../test/testdata/"" --gpu_id 0 ``` \
b) For full data: ``` python train.py --dataset_path ""../../../data/raw/spam/Annotated/"" --gpu_id 0 ```  <br> <br>

Note: Be warned that running ConWea on the full dataset will ~ 3 hours to run. Running ConWea on test data runs in ~ 20 minutes. <br>
Note: ConWea trains using multiple layers and tons of epochs, since our test data is small it is safe to interrupt the terminal (CTRL+C) after first iteration has occured. The layers are kept for consistency for full datasets.
","# Weakly Supervised Spam-Label Classification
DSC180 Quarter 2 Capstone Project 

Using a list of categories and words that represent these categories, we classify harmful spam messages into categories such as insurance scams, medical sales, software sales, and more. Doing so, we hope to alleviate the burden on non technical people in todays world as spammers continue to get by detection systems - we want to find and highlight a pattern throughout them all. Leveraging models ranging from simple methods like TFIDF to complex large language models such as ConWea with BERT, we examine the differences between these models and if it is worth using such big, computation costly models.

You can see more details about our project on our [website](https://gbirch11.github.io/SpamLabelClassifier/).

# Data
The data is available on [Google Drive](https://drive.google.com/drive/folders/1uTRzRPkom6nUtRB2D4pOi8uOpSpqst7m?usp=share_link)\
Please unzip and place the files into the following locations; \
Annotated Spam Messages -> ```data/raw/spam/Annotated/``` \
Unannotated Spam Messages -> ```data/raw/spam/Unannotated/``` \
Non-spam (Ham) Messages -> ```data/raw/ham/``` 


The dataset should contain the following files:
1) Annotated Spam Messages \
  ex) ```data/raw/spam/Annotated/medical-sales/xyz.txt```
    * Where xyz is any file name that was annotated to be medical sale spam
    * Other folders follow same pattern for each category
2) Non-spam (ham) Messages \
  ex) ```data/raw/ham/xyz.txt```
3) Seedwords JSON file \
  ex) ```data/out/seedwords.json```

## Running the Project
**DSMLP Command**
``` 
launch.sh -i gbirch11/dsc180b [-m d] [-g 1]
```
Note: -m is an optional argument to include more RAM on the machine; HIGLHLY RECOMMEND setting $d$ to 16 or 32 for faster processing \
Also highly recommended to run with -g 1, especially if running ConWea model.
``` 
launch.sh -i gbirch11/dsc180b -m 32 -g 1
```
<br> <br>
To run this project, execute the following command;
```
python run.py [test | data]
```
Note: If running ```python run.py test``` \
Very simple set of test data will be used to produce results. \
Result trend not consistent with running on full dataset.

If running ```python run.py data```: \
Whole dataset will be used to produce results.

Example commands include: \
``` python run.py test ``` \
``` python run.py data ```

Note: The above commands only run on the TF-IDF, Word2Vec, and FastText models. To run our best model, ConWea, see the section below.

## Running the ConWea Model
Since ConWea is a huge model using BERT, we have separated this model into the following separate commands;
1) Navigate to the ConWea model directory using \
``` cd src/models/ConWea ``` <br> <br>
2) To contextualize the corpus and seed words run \
a) For testing: ``` python contextualize.py --dataset_path ""../../../test/testdata/"" --temp_dir ""temp/"" --gpu_id 0 ``` \
b) For full data: ``` python contextualize.py --dataset_path ""../../../data/raw/spam/Annotated/"" --temp_dir ""temp/"" --gpu_id 0 ```  <br> <br>
3) To train model + observe results run \
a) For testing: ``` python train.py --dataset_path ""../../../test/testdata/"" --gpu_id 0 ``` \
b) For full data: ``` python train.py --dataset_path ""../../../data/raw/spam/Annotated/"" --gpu_id 0 ```  <br> <br>

Note: Be warned that running ConWea on the full dataset will ~ 3 hours to run. Running ConWea on test data runs in ~ 20 minutes. <br>
Note: ConWea trains using multiple layers and tons of epochs, since our test data is small it is safe to interrupt the terminal (CTRL+C) after first iteration has occured. The layers are kept for consistency for full datasets.
"
197,https://github.com/ym820/foreground_window_forcast,"{'ym820': 'https://github.com/ym820', 'AlanXZhang': 'https://github.com/AlanXZhang'}","{'HTML': 0.75, 'Jupyter Notebook': 0.25, 'Python': 0.0, 'Dockerfile': 0.0}","# Intel Capstone: Improving App Launch Time with Deep Learning
Authors: Yikai(Mike) Mao, Alan Zhang, Mandy Lee \
Website: https://ym820.github.io/foreground_window_forcast/

## Abstract
Application launch time is a crucial element of the user experience. Long wait times can cause frustration and prompt users to upgrade to more powerful machines, resulting in increased electronic waste (e-waste) at landfills. Improving app launch time is vital in reducing e-waste by extending the average lifespan of computers. While software has become more resource efficient, it is still challenging to prevent large programs from being bloated and slow to run. In this paper, we propose utilizing neural networks to analyze system usage reports and pre-launch applications in the background before the user needs them. This approach can be universally applied to all computers, making it more economically viable than asking all developers to optimize their applications. We developed our data collection software to minimize resource usage and to identify applications that the system can pre-launch using Hidden Markov Model (HMM) and Long Short-Term Memory (LSTM) models.

## Prerequisites

> __Below we assume the working directory is the repository root.__

### Install dependencies
- Using docker\
You may pull the docker image from `mikem820/intel_capstone:latest` and then clone this github repo
- Using pip3

  ```sh
  # Install the dependencies
  pip3 install -r requirements.txt
  ```

## Run ""test"" code
You can use the below command to run the ""test"" code with a sample of our collected dataset. You **must** pass two arguments. The first is `test` or `all`, indicating whether to run test code or not. The second argument is to choose the model (must be either `hmm` or `lstm`) which corresond to the task we will explain in the later section. 
```
python3 run.py test hmm/lstm
```
You can also run the full pipeline with the entire dataset by the following command, with the default hyperparameters we implemented.
```
python3 run.py all hmm/lstm
```
If you want to explore different sets of hyperparameters, we explained our tasks and specific instructions to run the scripts in the following section.

## Task 1: Next-App Prediction with Hidden Markov Model (HMM)
In this task, our goal is to predict the next application the user will use based on the previous usage data.
### Run
```
cd src/model/HMM
python3 run.py
```
### Arguments

| Parameter                 | Default       | Description   |	
| :------------------------ |:-------------:| :-------------|
| -ts --test_size 	       |	0.2	            |Test set size (percentage of entire dataset)
| -t --top  		       | 1	           | Number of executables to predict for each data point
| -ex  --experiment 	        | 1           | The experiment number

### Notes
After running, there will be a folder created at `outputs` and named as ""HMM_expt_`experiment`"". Then, the parameters, transition matrix, model accuracy, and visualization will be stored in the folder.

## Task 2: App Duration Prediction with Long Short-Term Memory (LSTM)
As for the primary objective for our project, we aim to forecast the amount of time (in seconds) an individual will spend on a specific application within a specific hour. 
### Run
```
cd src/model/LSTM
python3 run.py
```
### Arguments

| Parameter                 | Default       | Description   |	
| :------------------------ |:-------------:| :-------------|
| -exe --exe_name	       |	firefox.exe          |The executable name to predict
| -lb --lookback          | 5           |Lookback window (hyper-parameter) for dataset processing
| -ts --test_size 	       |	0.2	            |Test set size (percentage of entire dataset)
| -e --epochs 	       |	100	            |Number of epochs
| -lr --learning_rate  		       | 0.001	           | Learning rate
| -l --loss 		           | mse             | Loss function
| -ex  --experiment 	        | 1           | The experiment number
| -r  --random	        | False           | Whether to choose the start index for test set randomly

### Notes
After running, there will be a folder created at `outputs` and named as ""LSTM_expt_`experiment`"". Then, the parameters, trained model, Keras training history, loss plot, and prediction plot will be stored in the folder.

## Mentors
We would like to express our gratitude to all the mentors at the Intel DCA & Telemetry team who provided invaluable guidance and support throughout this project. Special thanks to
- Bijan Arbab (Intel)
- Jamel Tayeb (Intel)
- Sruti Sahani (Intel)
- Oumaima Makhlouk (Intel)
- Teresa Rexin (UCSD)
- Praveen Polasam (Intel)
- Chansik Im (Intel)
","# Intel Capstone: Improving App Launch Time with Deep Learning
Authors: Yikai(Mike) Mao, Alan Zhang, Mandy Lee \
Website: https://ym820.github.io/foreground_window_forcast/

## Abstract
Application launch time is a crucial element of the user experience. Long wait times can cause frustration and prompt users to upgrade to more powerful machines, resulting in increased electronic waste (e-waste) at landfills. Improving app launch time is vital in reducing e-waste by extending the average lifespan of computers. While software has become more resource efficient, it is still challenging to prevent large programs from being bloated and slow to run. In this paper, we propose utilizing neural networks to analyze system usage reports and pre-launch applications in the background before the user needs them. This approach can be universally applied to all computers, making it more economically viable than asking all developers to optimize their applications. We developed our data collection software to minimize resource usage and to identify applications that the system can pre-launch using Hidden Markov Model (HMM) and Long Short-Term Memory (LSTM) models.

## Prerequisites

> __Below we assume the working directory is the repository root.__

### Install dependencies
- Using docker\
You may pull the docker image from `mikem820/intel_capstone:latest` and then clone this github repo
- Using pip3

  ```sh
  # Install the dependencies
  pip3 install -r requirements.txt
  ```

## Run ""test"" code
You can use the below command to run the ""test"" code with a sample of our collected dataset. You **must** pass two arguments. The first is `test` or `all`, indicating whether to run test code or not. The second argument is to choose the model (must be either `hmm` or `lstm`) which corresond to the task we will explain in the later section. 
```
python3 run.py test hmm/lstm
```
You can also run the full pipeline with the entire dataset by the following command, with the default hyperparameters we implemented.
```
python3 run.py all hmm/lstm
```
If you want to explore different sets of hyperparameters, we explained our tasks and specific instructions to run the scripts in the following section.

## Task 1: Next-App Prediction with Hidden Markov Model (HMM)
In this task, our goal is to predict the next application the user will use based on the previous usage data.
### Run
```
cd src/model/HMM
python3 run.py
```
### Arguments

| Parameter                 | Default       | Description   |	
| :------------------------ |:-------------:| :-------------|
| -ts --test_size 	       |	0.2	            |Test set size (percentage of entire dataset)
| -t --top  		       | 1	           | Number of executables to predict for each data point
| -ex  --experiment 	        | 1           | The experiment number

### Notes
After running, there will be a folder created at `outputs` and named as ""HMM_expt_`experiment`"". Then, the parameters, transition matrix, model accuracy, and visualization will be stored in the folder.

## Task 2: App Duration Prediction with Long Short-Term Memory (LSTM)
As for the primary objective for our project, we aim to forecast the amount of time (in seconds) an individual will spend on a specific application within a specific hour. 
### Run
```
cd src/model/LSTM
python3 run.py
```
### Arguments

| Parameter                 | Default       | Description   |	
| :------------------------ |:-------------:| :-------------|
| -exe --exe_name	       |	firefox.exe          |The executable name to predict
| -lb --lookback          | 5           |Lookback window (hyper-parameter) for dataset processing
| -ts --test_size 	       |	0.2	            |Test set size (percentage of entire dataset)
| -e --epochs 	       |	100	            |Number of epochs
| -lr --learning_rate  		       | 0.001	           | Learning rate
| -l --loss 		           | mse             | Loss function
| -ex  --experiment 	        | 1           | The experiment number
| -r  --random	        | False           | Whether to choose the start index for test set randomly

### Notes
After running, there will be a folder created at `outputs` and named as ""LSTM_expt_`experiment`"". Then, the parameters, trained model, Keras training history, loss plot, and prediction plot will be stored in the folder.

## Mentors
We would like to express our gratitude to all the mentors at the Intel DCA & Telemetry team who provided invaluable guidance and support throughout this project. Special thanks to
- Bijan Arbab (Intel)
- Jamel Tayeb (Intel)
- Sruti Sahani (Intel)
- Oumaima Makhlouk (Intel)
- Teresa Rexin (UCSD)
- Praveen Polasam (Intel)
- Chansik Im (Intel)
"
198,https://github.com/zeming-zhang/DSC180B-project_artifact,"{'zeming-zhang': 'https://github.com/zeming-zhang', 'afraenkel': 'https://github.com/afraenkel', 'surajrampure': 'https://github.com/surajrampure'}",{'Jupyter Notebook': 1.0},,
199,https://github.com/miloncl/System-Usage-Analysis,"{'thynguyen115': 'https://github.com/thynguyen115', 'miloncl': 'https://github.com/miloncl'}","{'Jupyter Notebook': 0.95, 'Python': 0.03, 'C': 0.02}","# Intel & UCSD HDSI -- Data Science Capstone Project -- 2022-2023

## Introduction
- Hello everyone, we are Thy Nguyen, Milon Chakkalakal, and Pranav Thaenraj from UC San Diego
- Our advisors are Jamel Tayeb, Bijan Arbab, Scruti Sahani, Oumaima Makhlouk, Praveen Polasam, and Chansik Im from Intel
- This is our Github repo including all the source codes and data files to do data collection and analysis for our capstone project _""Discover User-App Interactions and Solutions to Reducing the Initial User-CPU Latency""_

## Overview
- We try to closely follow the template for Data Science projects by <a href=""https://drivendata.github.io/cookiecutter-data-science/"">Cookie Cutter Data Sciece </a>
- Please check out the below template to understand how to navigate our repo
```
Project
├── .gitignore         <- Files to keep out of version control (e.g. data/binaries).
├── run.py             <- run.py with calls to functions in src.
├── README.md          <- The top-level README for developers using this project.
├── data
│   ├── temp           <- Intermediate data that has been transformed.
│   ├── out            <- The final, canonical data sets for modeling.
│   └── raw            <- The original, immutable data dump.
├── notebooks          <- Jupyter notebooks (presentation only).
|   ├── Process and EDA.ipynb
|   ├── HMM.ipynb
|   └── LSTM_RNN.ipynb
├── references         <- Data dictionaries, explanatory materials.
|   ├── data_dictionaries
|   |   ├── dataframe_description.txt
|   |   └── schema.txt
|   ├── weekly_presentation
|   |   ├── [DSC 180B] - Quarter 2 Week 2.pdf
|   |   ├── [DSC 180B] - Quarter 2 Week 3.pdf
|   |   ├── [DSC 180B] - Quarter 2 Week 4.pdf
|   |   ├── [DSC 180B] - Quarter 2 Week 5.pdf
|   |   └── [DSC 180B] - Quarter 2 Week 6.pdf
|   └── poster.pdf
├── requirements.txt   <- For reproducing the analysis environment, 
├── src                <- Source code for use in this project.
│   ├── data           <- Scripts to download or generate data.
│   │   ├── make_dataset.py
│   │   └── foreground
|   |       ├── foreground.c
|   |       └── foreground.h 
│   ├── features       <- Scripts to turn raw data into features for modeling.
│   │   └── build_features.py
│   ├── models         <- Scripts to train models and make predictions.
│   │   ├── hmm_model.py
│   │   └── lstm_model.py
│   └── visualization  <- Scripts to create exploratory and results-oriented viz.
│       └── visualize.py
├── outputs 
|   └── HMM           <- HMM model results (LSTM/RNN model results are inside the notebook)
│       └── emission_mt_user1.txt
|       ├── emission_mt_user2.txt
|       ├── transition_mt_user1_top15apps.txt
|       ├── transition_mt_user1_top1app.txt
|       ├── transition_mt_user2_top15apps.txt
|       └── transition_mt_user2_top1app.txt
└── config
    ├── data-params.json <- Save the inputs for the function calls
    └── submission.json <- GitHub repo and Docker image links

```

## Instruction to Run the code
- For the Methodology Staff
    
    On DSMLP,
    1. Cloning our GitHub repository.
    2. Launching a container with your Docker image.
    3. Running ```python run.py test```.

## Specific Links to Presentations and Source Code

### Week 1: 
- Introduction
### Week 2:
- Presentation: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20-%20Quarter%202%20Week%202.pdf"">Evaluate Data Quality and Conduct EDA</a>
- Source Code: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/Process%20and%20EDA.ipynb"">Process_and_EDA.ipynb</a>

### Week 3:
- Presentation: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20-%20Quarter%202%20Week%203.pdf"">HMM: Transition Matrix, Model Accuracy, and Emission Matrix</a>
- Source Code: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/HMM.ipynb"">HMM.ipynb</a>,  <a href=https://github.com/miloncl/System-Usage-Analysis/blob/main/src/models/hmm_model.py>hmm_model.py</a>
- Outputs: <a href=""https://github.com/miloncl/System-Usage-Analysis/tree/main/outputs/HMM"">HMM outfiles</a>

### Week 4:
- Presentation: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20-%20Quarter%202%20Week%204.pdf"">Study LSTM: Data Prep, Data Viz, Research on RNN/LSTM</a>
- Source Code: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/Process%20and%20EDA.ipynb""> Process_and_EDA.ipynb</a>

### Week 5:
- Presentation: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20-%20Quarter%202%20Week%205.pdf"">RNN (Vanilla + LSTM)</a>
- Source Code: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/LSTM_RNN.ipynb"">LSTM_RNN.ipynb</a>,  <a href=https://github.com/miloncl/System-Usage-Analysis/blob/main/src/models/lstm_model.py>lstm_model.py</a>

### Week 6:
- Presentation: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20%20-%20Quarter%202%20Week%206.pdf"">LSTM Experiments</a>
- Source Code: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/LSTM_RNN.ipynb"">LSTM_RNN.ipynb</a>, <a href=https://github.com/miloncl/System-Usage-Analysis/blob/main/src/models/lstm_model.py>lstm_model.py</a>

### Week 7-9:
- Project Poster: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/references/poster.pdf"">Poster</a>
- Practice presentation and elevator pitch in class
","# Intel & UCSD HDSI -- Data Science Capstone Project -- 2022-2023

## Introduction
- Hello everyone, we are Thy Nguyen, Milon Chakkalakal, and Pranav Thaenraj from UC San Diego
- Our advisors are Jamel Tayeb, Bijan Arbab, Scruti Sahani, Oumaima Makhlouk, Praveen Polasam, and Chansik Im from Intel
- This is our Github repo including all the source codes and data files to do data collection and analysis for our capstone project _""Discover User-App Interactions and Solutions to Reducing the Initial User-CPU Latency""_

## Overview
- We try to closely follow the template for Data Science projects by <a href=""https://drivendata.github.io/cookiecutter-data-science/"">Cookie Cutter Data Sciece </a>
- Please check out the below template to understand how to navigate our repo
```
Project
├── .gitignore         <- Files to keep out of version control (e.g. data/binaries).
├── run.py             <- run.py with calls to functions in src.
├── README.md          <- The top-level README for developers using this project.
├── data
│   ├── temp           <- Intermediate data that has been transformed.
│   ├── out            <- The final, canonical data sets for modeling.
│   └── raw            <- The original, immutable data dump.
├── notebooks          <- Jupyter notebooks (presentation only).
|   ├── Process and EDA.ipynb
|   ├── HMM.ipynb
|   └── LSTM_RNN.ipynb
├── references         <- Data dictionaries, explanatory materials.
|   ├── data_dictionaries
|   |   ├── dataframe_description.txt
|   |   └── schema.txt
|   ├── weekly_presentation
|   |   ├── [DSC 180B] - Quarter 2 Week 2.pdf
|   |   ├── [DSC 180B] - Quarter 2 Week 3.pdf
|   |   ├── [DSC 180B] - Quarter 2 Week 4.pdf
|   |   ├── [DSC 180B] - Quarter 2 Week 5.pdf
|   |   └── [DSC 180B] - Quarter 2 Week 6.pdf
|   └── poster.pdf
├── requirements.txt   <- For reproducing the analysis environment, 
├── src                <- Source code for use in this project.
│   ├── data           <- Scripts to download or generate data.
│   │   ├── make_dataset.py
│   │   └── foreground
|   |       ├── foreground.c
|   |       └── foreground.h 
│   ├── features       <- Scripts to turn raw data into features for modeling.
│   │   └── build_features.py
│   ├── models         <- Scripts to train models and make predictions.
│   │   ├── hmm_model.py
│   │   └── lstm_model.py
│   └── visualization  <- Scripts to create exploratory and results-oriented viz.
│       └── visualize.py
├── outputs 
|   └── HMM           <- HMM model results (LSTM/RNN model results are inside the notebook)
│       └── emission_mt_user1.txt
|       ├── emission_mt_user2.txt
|       ├── transition_mt_user1_top15apps.txt
|       ├── transition_mt_user1_top1app.txt
|       ├── transition_mt_user2_top15apps.txt
|       └── transition_mt_user2_top1app.txt
└── config
    ├── data-params.json <- Save the inputs for the function calls
    └── submission.json <- GitHub repo and Docker image links

```

## Instruction to Run the code
- For the Methodology Staff
    
    On DSMLP,
    1. Cloning our GitHub repository.
    2. Launching a container with your Docker image.
    3. Running ```python run.py test```.

## Specific Links to Presentations and Source Code

### Week 1: 
- Introduction
### Week 2:
- Presentation: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20-%20Quarter%202%20Week%202.pdf"">Evaluate Data Quality and Conduct EDA</a>
- Source Code: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/Process%20and%20EDA.ipynb"">Process_and_EDA.ipynb</a>

### Week 3:
- Presentation: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20-%20Quarter%202%20Week%203.pdf"">HMM: Transition Matrix, Model Accuracy, and Emission Matrix</a>
- Source Code: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/HMM.ipynb"">HMM.ipynb</a>,  <a href=https://github.com/miloncl/System-Usage-Analysis/blob/main/src/models/hmm_model.py>hmm_model.py</a>
- Outputs: <a href=""https://github.com/miloncl/System-Usage-Analysis/tree/main/outputs/HMM"">HMM outfiles</a>

### Week 4:
- Presentation: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20-%20Quarter%202%20Week%204.pdf"">Study LSTM: Data Prep, Data Viz, Research on RNN/LSTM</a>
- Source Code: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/Process%20and%20EDA.ipynb""> Process_and_EDA.ipynb</a>

### Week 5:
- Presentation: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20-%20Quarter%202%20Week%205.pdf"">RNN (Vanilla + LSTM)</a>
- Source Code: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/LSTM_RNN.ipynb"">LSTM_RNN.ipynb</a>,  <a href=https://github.com/miloncl/System-Usage-Analysis/blob/main/src/models/lstm_model.py>lstm_model.py</a>

### Week 6:
- Presentation: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20%20-%20Quarter%202%20Week%206.pdf"">LSTM Experiments</a>
- Source Code: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/LSTM_RNN.ipynb"">LSTM_RNN.ipynb</a>, <a href=https://github.com/miloncl/System-Usage-Analysis/blob/main/src/models/lstm_model.py>lstm_model.py</a>

### Week 7-9:
- Project Poster: <a href=""https://github.com/miloncl/System-Usage-Analysis/blob/main/references/poster.pdf"">Poster</a>
- Practice presentation and elevator pitch in class
"
200,https://github.com/KeaganBenson/DSC180Flock,"{'benduong2001': 'https://github.com/benduong2001', 'KeaganBenson': 'https://github.com/KeaganBenson', 'Keaggyb123': 'https://github.com/Keaggyb123', 'radumanea23': 'https://github.com/radumanea23'}","{'Jupyter Notebook': 0.93, 'Python': 0.07, 'Dockerfile': 0.0}","# DSC180Flock

This project's model comprises of 3 sub-models (predicting average, amount, and standard deviation) that are used for an algorithm at the end
On Command Prompt,for the first time, run the following to clone the repo for the first time:
```
git clone https://github.com/KeaganBenson/DSC180Flock.git
```
Then open Anaconda Prompt, for the first time, enter the new folder, and run the following to create a new conda environment with the requirements.txt. 
```
cd dsc180flock
conda create --name flock_env --file requirements.txt
conda activate flock_env
python run.py all
```
While python run.py all is being ran, plots and maps may open up on seperate windows during the execution, and the execution pauses until those windows are closed. After the execution is complete, observe the metrics printed in the console and close the anaconda
Now that the repo is cloned locally and the environment is created, anytime you want to run the model again, you open anaconda prompt and run:
```
cd dsc180flock
conda activate flock_env
python run.py all
```


Targets supported:
* **data** - performs the ETL that extracts data from online, and fills the empty data/raw folder with the raw data
* **features** - performs the data-cleaning and feature engineering for the intermediate data in data/temp folder and final data in data/out folder
* **model** - trains the data on the final data made by the features target, and outputs prediction accuracy metrics and confusion matrix from the validation data
* **clear** - empties the data folders raw, temp, and out
* **all** - complete cycle of ETL, data preparation (cleaning, feature engineering), training, and prediction. Equivalent of ""python run.py clear data features model"". 
* **test-data**: all subsequent arguments will be using only the test-data folder, not the data folder.
* **test** - complete cycle but only on the ""test data"". Equivalent of ""python run.py test-data clear feature model""



","# DSC180Flock

This project's model comprises of 3 sub-models (predicting average, amount, and standard deviation) that are used for an algorithm at the end
On Command Prompt,for the first time, run the following to clone the repo for the first time:
```
git clone https://github.com/KeaganBenson/DSC180Flock.git
```
Then open Anaconda Prompt, for the first time, enter the new folder, and run the following to create a new conda environment with the requirements.txt. 
```
cd dsc180flock
conda create --name flock_env --file requirements.txt
conda activate flock_env
python run.py all
```
While python run.py all is being ran, plots and maps may open up on seperate windows during the execution, and the execution pauses until those windows are closed. After the execution is complete, observe the metrics printed in the console and close the anaconda
Now that the repo is cloned locally and the environment is created, anytime you want to run the model again, you open anaconda prompt and run:
```
cd dsc180flock
conda activate flock_env
python run.py all
```


Targets supported:
* **data** - performs the ETL that extracts data from online, and fills the empty data/raw folder with the raw data
* **features** - performs the data-cleaning and feature engineering for the intermediate data in data/temp folder and final data in data/out folder
* **model** - trains the data on the final data made by the features target, and outputs prediction accuracy metrics and confusion matrix from the validation data
* **clear** - empties the data folders raw, temp, and out
* **all** - complete cycle of ETL, data preparation (cleaning, feature engineering), training, and prediction. Equivalent of ""python run.py clear data features model"". 
* **test-data**: all subsequent arguments will be using only the test-data folder, not the data folder.
* **test** - complete cycle but only on the ""test data"". Equivalent of ""python run.py test-data clear feature model""



"
201,https://github.com/taekunkim/flock-freight,"{'taekunkim': 'https://github.com/taekunkim', 'GihoKim-DataScientist': 'https://github.com/GihoKim-DataScientist', 'JoyceY14': 'https://github.com/JoyceY14'}","{'Python': 0.9, 'Jupyter Notebook': 0.07, 'Cython': 0.03, 'C': 0.01, 'C++': 0.0, 'Fortran': 0.0, 'JavaScript': 0.0, 'HTML': 0.0, 'PowerShell': 0.0, 'Smarty': 0.0, 'CSS': 0.0, 'Roff': 0.0, 'Shell': 0.0, 'Forth': 0.0, 'Batchfile': 0.0, 'Dockerfile': 0.0, 'Lua': 0.0}","

Data:
This repo uses Orders and Offers data provided by FlockFreight.
The threeDigitZipCode.json file comes from https://github.com/billfienberg/zip3.
Canadian postal codes are from postalcodes_ca, a Python library.","

Data:
This repo uses Orders and Offers data provided by FlockFreight.
The threeDigitZipCode.json file comes from https://github.com/billfienberg/zip3.
Canadian postal codes are from postalcodes_ca, a Python library."
202,https://github.com/ESR76/Capstone-Brick-Modeling,"{'ESR76': 'https://github.com/ESR76', 'Jbomwell': 'https://github.com/Jbomwell', 'Xenonition': 'https://github.com/Xenonition', 'alisebruevich': 'https://github.com/alisebruevich'}","{'Jupyter Notebook': 0.96, 'Python': 0.04, 'Dockerfile': 0.0}","# Energy Cost and HVAC Optimization in Smart Buildings

In this project, we attempt to make predictions about future energy usage and cost in a building using energy data collected from UC San Diego's EBU-3B (the Computer Science & Engineering) building's HVAC system.

We also have a public-facing [poster](https://www.canva.com/design/DAFZKQlLOLo/2ALw0oHRO8qrPj--Q-8huw/view?utm_content=DAFZKQlLOLo&utm_campaign=designshare&utm_medium=link&utm_source=publishsharelink) from our poster session and [website](https://xenonition.github.io/) associated with this project.

## Running the Code
To test our code, please first clone the repository. 

If you're concerned that you won't have the proper packages to run something in our code, use the [Docker image](https://hub.docker.com/repository/docker/esr76/capstone-brick-modeling/general) associated with this project - we recommend using the **""final""** tag (the ""latest"" tag should also work). It is designed to be run on UCSD's JupyterHub service using the base UCSD notebook as a base, for which instructions to use it can be found [here](https://github.com/ucsd-ets/datahub-example-notebook).

Another option is running ""pip install -r requirements.txt"".

- To run the modeling pipeline on the original data and compare to our paper/poster run the line:
    - ""python3 run.py all""
    **OR** 
    - ""python3 run.py data features model optimize visualize""

If you try to run a later part of the pipeline (ie. model) before running the earlier parts (ie. data, features), this **will** raise an error.

- To use a smaller set of test data and test the output for the pipeline run:
    - ""python3 run.py test""

Results from running this line will appear in the /test directory, with data in /test/testdata and visualizations in /test/testviz.

- To remove any files created by running the script, please run:
    - ""python3 run.py clean""

After any call of run.py, the script will run through the steps called, creating files/file organization as necessary, and then will print which steps it took in the order it took them in.

***NOTE:***
Once you've run one of the stages in the main training pipeline or run whole the pipeline before, calling it again will SKIP regenerating the files and print what was skipped. Running the ""clean"" keyword is the only way to ensure that the files will regenerate. The test pipeline will rerun each time the ""test"" keyword is called.

## Data Notes

### Getting the Data
Ideally in our pipeline, data would be obtained by pairing sensor data with mappings from our building's [Brick Schema](https://brickschema.org/) in order to query the locations and floors for relevant sensors to perform our calculation, then using UCSD's Brick server.

However, we were not able to obtain access to the Brick server in the time we had for the project, so we used data from [a data pull from a previous project](https://github.com/HYDesmondLiu/B2RL/tree/master/real_building_buffers). This represents 15 rooms worth of data on floors 2, 3, and 4 of UC San Diego's EBU-3B (Computer Science) building with data from July 2017 to early January 2019. This data should download automatically when the data part of the pipeline runs - the code should also generate several directories for you, including: data and its subdirectories, test and its subdirectories, and visualizations.

The features part of the data pipeline is split into two parts that perform data cleaning steps that are detailed more on our website, but essentially:
1. We separate our data into training and testing sets based on dates in the original datasets, where approximately 70% of the data is before August 1, 2018 and the rest is August 1, 2018 and onwards.
2. We floor timestamps in the dataset to the nearest hour and use medians to aggregate into buckets of that time, since the energy values range because of the 15 unidentified rooms in the dataset.
3. We impute the training dataset with data based on the median for the value at that hour - this was the most stable trend that we found in the original data. We do not impute the testing dataset because we want to ensure that we are not evaluating the model on predictions of false values.

### Other Data and Goals
Along with predicting future energy usage using the energy values from the data pull above, we will be using data from UCSD's pricing plan to scale this for energy. We will only be scaling our data by a constant (derived from UCSD's cost of electricity in fiscal years 2017-2018/2018-2019), although we understand that with UCSD using both its own energy and energy from San Diego Gas and Electric, this likely leads to an underestimation.

While we don't use it in the final version of our model, we also have the EBU 3B Turtle file (ie. the building's representation in Brick) in our raw data to understand relationships between components of the HVAC system in the building. If you'd like to take a look at this, here's the [link](https://brickschema.org/ttl/ebu3b_brick.ttl). This will not auto-download for you.

We also initially pulled other temperature and climate information to use in this project from [NOAA (the National Oceanic and Atmospheric Administration)](https://www.noaa.gov/) and the [EIA (U.S. Energy Information Administration)](https://www.eia.gov/), but we did not end up having time to incorporate this data in our final model.

## Credits

### Authors
We are four undergraduate Data Science students in our final year at UC San Diego.

If you're interested in our work, here's where you can find more about us:
| Name | GitHub | LinkedIn |
| ---- | ---- | ---- |
| **Jonah Bomwell** | [Link](https://github.com/Jbomwell) | [Link](https://www.linkedin.com/in/jonah-bomwell-0756191b7/) | 
| **Alise Bruevich** | [Link](https://github.com/alisebruevich) | [Link](https://www.linkedin.com/in/alisebruevich/) |
| **William Nathan** | [Link](https://github.com/Xenonition) | [Link](https://www.linkedin.com/in/william-nathan-5019661b2/) |
| **Esperanza Rozas** | [Link](https://github.com/ESR76) | [Link](https://www.linkedin.com/in/esperanza-r/) |


### Acknowledgments
This project was completed as a capstone project for the Data Science major at UC San Diego in Winter of 2023.
For more information on the course, please read about the class [here](https://dsc-capstone.github.io/).

We'd also like to thank: 
- Rajesh Gupta, our mentor and one of the creators of the Brick Schema.
- Xiaohan Fu and Hsin-Yu Liu, who provided us with the data we used and additional mentoring.
- Keaton Chia and the DERConnect team, who helped us generate ideas for this project, provided us with UCSD's cost model, and discussed the possibilities of using Brick for future work in this area with us.
- Suraj Rampure, our instructor for the course.","# Energy Cost and HVAC Optimization in Smart Buildings

In this project, we attempt to make predictions about future energy usage and cost in a building using energy data collected from UC San Diego's EBU-3B (the Computer Science & Engineering) building's HVAC system.

We also have a public-facing [poster](https://www.canva.com/design/DAFZKQlLOLo/2ALw0oHRO8qrPj--Q-8huw/view?utm_content=DAFZKQlLOLo&utm_campaign=designshare&utm_medium=link&utm_source=publishsharelink) from our poster session and [website](https://xenonition.github.io/) associated with this project.

## Running the Code
To test our code, please first clone the repository. 

If you're concerned that you won't have the proper packages to run something in our code, use the [Docker image](https://hub.docker.com/repository/docker/esr76/capstone-brick-modeling/general) associated with this project - we recommend using the **""final""** tag (the ""latest"" tag should also work). It is designed to be run on UCSD's JupyterHub service using the base UCSD notebook as a base, for which instructions to use it can be found [here](https://github.com/ucsd-ets/datahub-example-notebook).

Another option is running ""pip install -r requirements.txt"".

- To run the modeling pipeline on the original data and compare to our paper/poster run the line:
    - ""python3 run.py all""
    **OR** 
    - ""python3 run.py data features model optimize visualize""

If you try to run a later part of the pipeline (ie. model) before running the earlier parts (ie. data, features), this **will** raise an error.

- To use a smaller set of test data and test the output for the pipeline run:
    - ""python3 run.py test""

Results from running this line will appear in the /test directory, with data in /test/testdata and visualizations in /test/testviz.

- To remove any files created by running the script, please run:
    - ""python3 run.py clean""

After any call of run.py, the script will run through the steps called, creating files/file organization as necessary, and then will print which steps it took in the order it took them in.

***NOTE:***
Once you've run one of the stages in the main training pipeline or run whole the pipeline before, calling it again will SKIP regenerating the files and print what was skipped. Running the ""clean"" keyword is the only way to ensure that the files will regenerate. The test pipeline will rerun each time the ""test"" keyword is called.

## Data Notes

### Getting the Data
Ideally in our pipeline, data would be obtained by pairing sensor data with mappings from our building's [Brick Schema](https://brickschema.org/) in order to query the locations and floors for relevant sensors to perform our calculation, then using UCSD's Brick server.

However, we were not able to obtain access to the Brick server in the time we had for the project, so we used data from [a data pull from a previous project](https://github.com/HYDesmondLiu/B2RL/tree/master/real_building_buffers). This represents 15 rooms worth of data on floors 2, 3, and 4 of UC San Diego's EBU-3B (Computer Science) building with data from July 2017 to early January 2019. This data should download automatically when the data part of the pipeline runs - the code should also generate several directories for you, including: data and its subdirectories, test and its subdirectories, and visualizations.

The features part of the data pipeline is split into two parts that perform data cleaning steps that are detailed more on our website, but essentially:
1. We separate our data into training and testing sets based on dates in the original datasets, where approximately 70% of the data is before August 1, 2018 and the rest is August 1, 2018 and onwards.
2. We floor timestamps in the dataset to the nearest hour and use medians to aggregate into buckets of that time, since the energy values range because of the 15 unidentified rooms in the dataset.
3. We impute the training dataset with data based on the median for the value at that hour - this was the most stable trend that we found in the original data. We do not impute the testing dataset because we want to ensure that we are not evaluating the model on predictions of false values.

### Other Data and Goals
Along with predicting future energy usage using the energy values from the data pull above, we will be using data from UCSD's pricing plan to scale this for energy. We will only be scaling our data by a constant (derived from UCSD's cost of electricity in fiscal years 2017-2018/2018-2019), although we understand that with UCSD using both its own energy and energy from San Diego Gas and Electric, this likely leads to an underestimation.

While we don't use it in the final version of our model, we also have the EBU 3B Turtle file (ie. the building's representation in Brick) in our raw data to understand relationships between components of the HVAC system in the building. If you'd like to take a look at this, here's the [link](https://brickschema.org/ttl/ebu3b_brick.ttl). This will not auto-download for you.

We also initially pulled other temperature and climate information to use in this project from [NOAA (the National Oceanic and Atmospheric Administration)](https://www.noaa.gov/) and the [EIA (U.S. Energy Information Administration)](https://www.eia.gov/), but we did not end up having time to incorporate this data in our final model.

## Credits

### Authors
We are four undergraduate Data Science students in our final year at UC San Diego.

If you're interested in our work, here's where you can find more about us:
| Name | GitHub | LinkedIn |
| ---- | ---- | ---- |
| **Jonah Bomwell** | [Link](https://github.com/Jbomwell) | [Link](https://www.linkedin.com/in/jonah-bomwell-0756191b7/) | 
| **Alise Bruevich** | [Link](https://github.com/alisebruevich) | [Link](https://www.linkedin.com/in/alisebruevich/) |
| **William Nathan** | [Link](https://github.com/Xenonition) | [Link](https://www.linkedin.com/in/william-nathan-5019661b2/) |
| **Esperanza Rozas** | [Link](https://github.com/ESR76) | [Link](https://www.linkedin.com/in/esperanza-r/) |


### Acknowledgments
This project was completed as a capstone project for the Data Science major at UC San Diego in Winter of 2023.
For more information on the course, please read about the class [here](https://dsc-capstone.github.io/).

We'd also like to thank: 
- Rajesh Gupta, our mentor and one of the creators of the Brick Schema.
- Xiaohan Fu and Hsin-Yu Liu, who provided us with the data we used and additional mentoring.
- Keaton Chia and the DERConnect team, who helped us generate ideas for this project, provided us with UCSD's cost model, and discussed the possibilities of using Brick for future work in this area with us.
- Suraj Rampure, our instructor for the course."
203,https://github.com/SamuelBAguirre/DSC180A_Project,"{'j1u': 'https://github.com/j1u', 'SamuelBAguirre': 'https://github.com/SamuelBAguirre'}","{'Jupyter Notebook': 1.0, 'Python': 0.0, 'Dockerfile': 0.0}","This is a repo for our DSC180A Q1 project. In this project we explore measuring the change in surface water over time for Lake Oroville.
Please download image data from https://drive.google.com/drive/folders/1b5-gGvu5K4WNVqeRQ1BLvdQ5DwA2KkQg?usp=share_link, make sure to unzip file and place images directory inside of ./data/

A quick breakdown of the structure of this repository:

config: Contains config files for params used in our notebook  
notebooks: Contains our data exploration notebooks  
src: Contains our source code  
test: Contains some of our testing data  
out: Contains the processed images outputted from our source code

Website: https://samuelbaguirre.github.io/
","This is a repo for our DSC180A Q1 project. In this project we explore measuring the change in surface water over time for Lake Oroville.
Please download image data from https://drive.google.com/drive/folders/1b5-gGvu5K4WNVqeRQ1BLvdQ5DwA2KkQg?usp=share_link, make sure to unzip file and place images directory inside of ./data/

A quick breakdown of the structure of this repository:

config: Contains config files for params used in our notebook  
notebooks: Contains our data exploration notebooks  
src: Contains our source code  
test: Contains some of our testing data  
out: Contains the processed images outputted from our source code

Website: https://samuelbaguirre.github.io/
"
204,https://github.com/alexmak001/SAR-satelite-image-ship-detection,"{'alexmak001': 'https://github.com/alexmak001', 'ngseann': 'https://github.com/ngseann'}","{'Jupyter Notebook': 1.0, 'Python': 0.0, 'Dockerfile': 0.0, 'JavaScript': 0.0}","# Maritime Ship Detection Using Synthetic Aperture Radar Satellite Imagery
Satellites are being launched into space at an exponential rate and are able to produce high quality images in relatively short intervals of time on any part of Earth. The amount of data and types of it are also increasing significantly and in this paper we specifically use Synthetic Aperture Radar (SAR) satellite imagery in order to detect ships traveling through bodies of water. We created a ship counting tool that intakes a start date, end date, and an area of interest and returns the number of ships for each day between the two dates. The images are first classified into offshore or inshore and a separate object detection algorithm counts the number of ships per image. The classifier and object detection networks are trained using the Large-Scale SAR Ship Detection Dataset-v1.0 (LS-SSDD-v1.0) and deployed on Google Earth Engine.

## Testing:
When running on DSMLP, be sure to use use the launch script
`launch-scipy-ml.sh -g 1 -i snng/sar_ship_detection` to launch a pod with a GPU. Otherwise the script will fail to run. 

To run the test, simply run python run.py test

## run.py file
Using the ""data"" target, it downaloads and formats the dataset locally. This also downloads the models as well. 
The ""train_ret"" target will start the training of the RetinaNet model, which requires the data to be loaded. Similarly, the ""train_faster"" will begin to train the Faster R-CNN model. The hyperparameters can be configured in the src/models/ folder for each of the models.
The ""predict"" target uses the model to predict on all of the test data and returns the key metrics for both models. The ""viz"" target causes the models to predict the bounding boxes on the tif files saved in the src/visualization folder. It then saves the resulting image with bounding boxes in the same folder as a jpg.

## HOW TO RUN THE SHIP COUNTING SCRIPT:
[//]: <> (Have to figure out what to do about json key)
1. Once you have activated your DSMLP environment using the launch script above, please ensure you have the private json key downloaded. This will be used for initializing and authenticating Google Earth Engine. [website](https://developers.google.com/earth-engine/guides/service_account)
2. We will have to get the coordinates for the desired area of interest from the Google Earth Engine [website](https://code.earthengine.google.com/).
    - Note: If you do not have a Google Earth Engine account you will have to make one.
    - To get the coordinates, navigate on the map to your desired place of interest
    - Then draw a bounding box over your area of interest using the shape tool. (The shape tool is button with the gray square underneath the scripts panel. 
    ![tut1](https://user-images.githubusercontent.com/69220036/221438416-ca8513ea-412e-43c6-8a8e-5b87e30ac128.png)
    ![tut2](https://user-images.githubusercontent.com/69220036/221438475-eac5c729-4478-46bd-8691-88648845255a.png)
    - Once you get the desired vertices, the middle panel which is usually labeled new script will have a geometry variable and you expand that until you get the list of 5 vertices and those are the place coordinate values to pass into image downloader function.
  ![tut3](https://user-images.githubusercontent.com/69220036/221438515-9acf67df-450b-4f66-b4a7-deed39eb1013.png)
3. Once you have obtained your coordinates, run this command in terminal to start counting ships.
[//]: <> (Might have to change this command depending on how we implement shipcounter.py.)

`python -c from shipcounter.py import shipcounter(place_coords, start_date, end_date, del_images)`

where:
- `place_coords` are the coordinates from Google Earth Engine
- `start_date` is the desired start date in the format 'MM/DD/YYYY'
- `end_date` is the desired end date in the format 'MM/DD/YYYY'
- `del_images` set to `True` if you want to delete the images locally afterwards, `False` if not.
","# Maritime Ship Detection Using Synthetic Aperture Radar Satellite Imagery
Satellites are being launched into space at an exponential rate and are able to produce high quality images in relatively short intervals of time on any part of Earth. The amount of data and types of it are also increasing significantly and in this paper we specifically use Synthetic Aperture Radar (SAR) satellite imagery in order to detect ships traveling through bodies of water. We created a ship counting tool that intakes a start date, end date, and an area of interest and returns the number of ships for each day between the two dates. The images are first classified into offshore or inshore and a separate object detection algorithm counts the number of ships per image. The classifier and object detection networks are trained using the Large-Scale SAR Ship Detection Dataset-v1.0 (LS-SSDD-v1.0) and deployed on Google Earth Engine.

## Testing:
When running on DSMLP, be sure to use use the launch script
`launch-scipy-ml.sh -g 1 -i snng/sar_ship_detection` to launch a pod with a GPU. Otherwise the script will fail to run. 

To run the test, simply run python run.py test

## run.py file
Using the ""data"" target, it downaloads and formats the dataset locally. This also downloads the models as well. 
The ""train_ret"" target will start the training of the RetinaNet model, which requires the data to be loaded. Similarly, the ""train_faster"" will begin to train the Faster R-CNN model. The hyperparameters can be configured in the src/models/ folder for each of the models.
The ""predict"" target uses the model to predict on all of the test data and returns the key metrics for both models. The ""viz"" target causes the models to predict the bounding boxes on the tif files saved in the src/visualization folder. It then saves the resulting image with bounding boxes in the same folder as a jpg.

## HOW TO RUN THE SHIP COUNTING SCRIPT:
[//]: <> (Have to figure out what to do about json key)
1. Once you have activated your DSMLP environment using the launch script above, please ensure you have the private json key downloaded. This will be used for initializing and authenticating Google Earth Engine. [website](https://developers.google.com/earth-engine/guides/service_account)
2. We will have to get the coordinates for the desired area of interest from the Google Earth Engine [website](https://code.earthengine.google.com/).
    - Note: If you do not have a Google Earth Engine account you will have to make one.
    - To get the coordinates, navigate on the map to your desired place of interest
    - Then draw a bounding box over your area of interest using the shape tool. (The shape tool is button with the gray square underneath the scripts panel. 
    ![tut1](https://user-images.githubusercontent.com/69220036/221438416-ca8513ea-412e-43c6-8a8e-5b87e30ac128.png)
    ![tut2](https://user-images.githubusercontent.com/69220036/221438475-eac5c729-4478-46bd-8691-88648845255a.png)
    - Once you get the desired vertices, the middle panel which is usually labeled new script will have a geometry variable and you expand that until you get the list of 5 vertices and those are the place coordinate values to pass into image downloader function.
  ![tut3](https://user-images.githubusercontent.com/69220036/221438515-9acf67df-450b-4f66-b4a7-deed39eb1013.png)
3. Once you have obtained your coordinates, run this command in terminal to start counting ships.
[//]: <> (Might have to change this command depending on how we implement shipcounter.py.)

`python -c from shipcounter.py import shipcounter(place_coords, start_date, end_date, del_images)`

where:
- `place_coords` are the coordinates from Google Earth Engine
- `start_date` is the desired start date in the format 'MM/DD/YYYY'
- `end_date` is the desired end date in the format 'MM/DD/YYYY'
- `del_images` set to `True` if you want to delete the images locally afterwards, `False` if not.
"
205,https://github.com/rtvo20/dsc180_capstone_q2,{'rtvo20': 'https://github.com/rtvo20'},"{'Jupyter Notebook': 0.63, 'Python': 0.37}","# DSC 180 Capstone Project

*Most of this description was from the README/documentation located on our previous repo, with some slight changes. The repo can be found here* [here](https://github.com/rtvo20/dsc180_quarter1_submission).

This project uses data from David Fenning's Solar Energy Innovation Laboratory (SOLEIL) that creates solar cell samples. The data they collect are information from the manufacturing process of these solar cells, along with data they collect when testing the samples. Our project cleans and transforms the data, which are originally in JSON format, before saving them as CSVs that allow it to be imported and graphed in Neo4j, a Graph DBMS. The purpose of this task is to have a pipeline that can organize and transform the data so that it can be graphed in Neo4j and can be queried.

Input data are JSON files containing information from the SOLEIL lab in the form of a worklist. Our functions extract data from these worklists, such as step names (e.g. 'drop', 'spin', 'hotplate'), chemical names, and output data (measurements and tests done on the resulting sample). Running run.py on the input data gives us the output data file, which can be used to generate our output, a graph representation of the data.

The output data (CSVs) after they are cleaned and transformed are run through a function that generates a script file with queries in Neo4j's query language, Cypher. The script (a .cypher file) automates the process to graph the data using Neo4j's Cypher shell terminal. To run the script, we use a Docker container running Neo4j, which requires us to copy this 'output.cypher' file into the docker container's root directory before we can run it with a command. All of the instructions to do so are located in the section below.

This current iteration includes test data under test/testdata, which is one sample from some actual data to show how our code works on ""barebones"" test data.

More detailed instructions are below, however an overview of the process to reproduce our results:

run python run.py test to generate a script file with queries to generate a graph's nodes and links.
Using Docker, pull the latest Neo4j Docker image and start a container with this image.
Copy the script file from DSMLP to the local setting, then copy it to the docker container's root directory
Open a terminal in the docker container and run the script file and produce the results.

More detailed instructions are below, however an overview of the process to reproduce our results:
1. run ```python run.py test``` to generate a script file with queries to generate a graph's nodes and links.
2. Using Docker, pull the latest Neo4j Docker image and start a container with this image.
   * Copy the script file from DSMLP to the local setting, then copy it to the docker container's root directory
   * Open a terminal in the docker container and run the script file and produce the results.

## To run the project use run.py and follow the instructions below.

* The filepaths to the test data are already coded into ```run.py``` and are under the folder ""test/testdata"".
* The available targets for running ```python run.py <target>``` and the order of the targets are:
    * ```data```>```features```>```queries```
    * Alternatively, running the command ```python run.py test``` is equivalent to running each of the above targets sequentially.
* Running ```run.py``` cleans and transforms the data and creates queries in Neo4j's query language (Cypher) that allows for nodes and links to be graphed. Each graph in our implementation currently requires 6 queries to create and link all the nodes, so to help automate the process, the output of ```run.py``` is a Neo4j script-type file (.cypher file) that performs all of these queries in less inputs than doing so manually.
  * Our output file is named ""output.cypher"" and will be located in the project's root directory.

## To run the script generated by the run.py script above, use Docker

* The following docker run command sets up a docker container with all of the necessary flags and config settings. The command is all one line, it should be copied and pasted in its entirety in a local terminal.
    * ```docker run -p 7474:7474 -p 7687:7687 -v $PWD/data:/data -v $PWD/plugins:/plugins --name neo4j-apoc -e NEO4J_apoc_export_file_enabled=true -e NEO4J_apoc_import_file_enabled=true -e NEO4J_apoc_import_file_use__neo4j__config=true -e NEO4JLABS_PLUGINS=\[\""apoc\""\] neo4j:4.0```
    * In short, the flags set up permissions that allow for moving files between the Docker container's storage volume and local storage. It also enables for usage of APOC, a Neo4j package used to help export queried data.
    * Wait for the Docker container to initialize and start up, and in an internet browser navigate to `localhost:7474`.
    * This opens up Neo4j's browser UI and upon accessing it for the first time, should prompt the user to create a username/password; although it is possible to set it up with no authentication required under the ""Authentication type"" drop-down menu.
    * The default username/password is neo4j/neo4j. Once entered, it will then ask for a new password.

* The next steps require copying the output of the `run.py` file, ""output.cypher"" and the output CSVs into the Docker container's directory.
    * If the output.cypher file is located on DSMLP, it should be downloaded/copied to a local directory first.
    * In a local terminal, change directory to the location of the cypher file and CSVs, then run the following command
      * ```docker cp output.cypher neo4j-apoc:/var/lib/neo4j/import/output.cypher```
    * Then, perform the same process and copy the CSVs into the Docker container.
      * ```docker cp b19_sample0_chem.csv neo4j-apoc:/var/lib/neo4j/import/```
      * ```docker cp b19_sample0_action.csv neo4j-apoc:/var/lib/neo4j/import/```
      * ```docker cp b19_sample0_link.csv neo4j-apoc:/var/lib/neo4j/import/```

* With the necessary files copied over to the docker container, go to the docker container and select ""Open in Terminal"", as seen in the image below. 

![image](https://user-images.githubusercontent.com/59627502/218381794-04ed9f95-5fc9-4102-aa9c-d5c87adcee41.png)

  * In this docker terminal, `cd import`
  * and again, in the docker terminal, run the following command:
    * `cypher-shell -f output.cypher -u neo4j -p test` (replacing your username and password where `neo4j` and `test` are respectively.  
  * Back in the browser at `localhost:7474`, `MATCH(n) RETURN n` can be entered in the query field and run to return the nodes and relationships graphed by our output.

![image](https://user-images.githubusercontent.com/59627502/218383326-7880d998-0aeb-4b63-8ce6-24aad0ae5f85.png)
* This is our result from a graph containing multiple samples, but the test data will contain just 1 sample.

## Additional features
At the final stage of our project, we moved from using Neo4j Docker to Neo4j Desktop, as this version has more practical use for the lab team. Our run.py and this README file contain instructions and functionality for Neo4j Docker in order to support reproducibility; however we have included a `preprocessing.ipynb` that contains several features used with Neo4j Desktop that is useful in practice for the lab team. 
To briefly describe those features:
  * Saving `action`, `chem`, and `link` CSV files along with cypher files to the appropriate folders within the local Neo4j Desktop installation
    * CSV files are saved to the `import` folder within the Neo4j Desktop database directory
    * Cypher files are saved to the `bin` folder within the Neo4j Desktop database directory 
  * Version control to keep track of batches of data that have already been processed, to avoid unnecessary redundant processing of batchs.
  * Deleting CSV files after they are used to load data into Neo4j to reduce clutter in file storage.
","# DSC 180 Capstone Project

*Most of this description was from the README/documentation located on our previous repo, with some slight changes. The repo can be found here* [here](https://github.com/rtvo20/dsc180_quarter1_submission).

This project uses data from David Fenning's Solar Energy Innovation Laboratory (SOLEIL) that creates solar cell samples. The data they collect are information from the manufacturing process of these solar cells, along with data they collect when testing the samples. Our project cleans and transforms the data, which are originally in JSON format, before saving them as CSVs that allow it to be imported and graphed in Neo4j, a Graph DBMS. The purpose of this task is to have a pipeline that can organize and transform the data so that it can be graphed in Neo4j and can be queried.

Input data are JSON files containing information from the SOLEIL lab in the form of a worklist. Our functions extract data from these worklists, such as step names (e.g. 'drop', 'spin', 'hotplate'), chemical names, and output data (measurements and tests done on the resulting sample). Running run.py on the input data gives us the output data file, which can be used to generate our output, a graph representation of the data.

The output data (CSVs) after they are cleaned and transformed are run through a function that generates a script file with queries in Neo4j's query language, Cypher. The script (a .cypher file) automates the process to graph the data using Neo4j's Cypher shell terminal. To run the script, we use a Docker container running Neo4j, which requires us to copy this 'output.cypher' file into the docker container's root directory before we can run it with a command. All of the instructions to do so are located in the section below.

This current iteration includes test data under test/testdata, which is one sample from some actual data to show how our code works on ""barebones"" test data.

More detailed instructions are below, however an overview of the process to reproduce our results:

run python run.py test to generate a script file with queries to generate a graph's nodes and links.
Using Docker, pull the latest Neo4j Docker image and start a container with this image.
Copy the script file from DSMLP to the local setting, then copy it to the docker container's root directory
Open a terminal in the docker container and run the script file and produce the results.

More detailed instructions are below, however an overview of the process to reproduce our results:
1. run ```python run.py test``` to generate a script file with queries to generate a graph's nodes and links.
2. Using Docker, pull the latest Neo4j Docker image and start a container with this image.
   * Copy the script file from DSMLP to the local setting, then copy it to the docker container's root directory
   * Open a terminal in the docker container and run the script file and produce the results.

## To run the project use run.py and follow the instructions below.

* The filepaths to the test data are already coded into ```run.py``` and are under the folder ""test/testdata"".
* The available targets for running ```python run.py <target>``` and the order of the targets are:
    * ```data```>```features```>```queries```
    * Alternatively, running the command ```python run.py test``` is equivalent to running each of the above targets sequentially.
* Running ```run.py``` cleans and transforms the data and creates queries in Neo4j's query language (Cypher) that allows for nodes and links to be graphed. Each graph in our implementation currently requires 6 queries to create and link all the nodes, so to help automate the process, the output of ```run.py``` is a Neo4j script-type file (.cypher file) that performs all of these queries in less inputs than doing so manually.
  * Our output file is named ""output.cypher"" and will be located in the project's root directory.

## To run the script generated by the run.py script above, use Docker

* The following docker run command sets up a docker container with all of the necessary flags and config settings. The command is all one line, it should be copied and pasted in its entirety in a local terminal.
    * ```docker run -p 7474:7474 -p 7687:7687 -v $PWD/data:/data -v $PWD/plugins:/plugins --name neo4j-apoc -e NEO4J_apoc_export_file_enabled=true -e NEO4J_apoc_import_file_enabled=true -e NEO4J_apoc_import_file_use__neo4j__config=true -e NEO4JLABS_PLUGINS=\[\""apoc\""\] neo4j:4.0```
    * In short, the flags set up permissions that allow for moving files between the Docker container's storage volume and local storage. It also enables for usage of APOC, a Neo4j package used to help export queried data.
    * Wait for the Docker container to initialize and start up, and in an internet browser navigate to `localhost:7474`.
    * This opens up Neo4j's browser UI and upon accessing it for the first time, should prompt the user to create a username/password; although it is possible to set it up with no authentication required under the ""Authentication type"" drop-down menu.
    * The default username/password is neo4j/neo4j. Once entered, it will then ask for a new password.

* The next steps require copying the output of the `run.py` file, ""output.cypher"" and the output CSVs into the Docker container's directory.
    * If the output.cypher file is located on DSMLP, it should be downloaded/copied to a local directory first.
    * In a local terminal, change directory to the location of the cypher file and CSVs, then run the following command
      * ```docker cp output.cypher neo4j-apoc:/var/lib/neo4j/import/output.cypher```
    * Then, perform the same process and copy the CSVs into the Docker container.
      * ```docker cp b19_sample0_chem.csv neo4j-apoc:/var/lib/neo4j/import/```
      * ```docker cp b19_sample0_action.csv neo4j-apoc:/var/lib/neo4j/import/```
      * ```docker cp b19_sample0_link.csv neo4j-apoc:/var/lib/neo4j/import/```

* With the necessary files copied over to the docker container, go to the docker container and select ""Open in Terminal"", as seen in the image below. 

![image](https://user-images.githubusercontent.com/59627502/218381794-04ed9f95-5fc9-4102-aa9c-d5c87adcee41.png)

  * In this docker terminal, `cd import`
  * and again, in the docker terminal, run the following command:
    * `cypher-shell -f output.cypher -u neo4j -p test` (replacing your username and password where `neo4j` and `test` are respectively.  
  * Back in the browser at `localhost:7474`, `MATCH(n) RETURN n` can be entered in the query field and run to return the nodes and relationships graphed by our output.

![image](https://user-images.githubusercontent.com/59627502/218383326-7880d998-0aeb-4b63-8ce6-24aad0ae5f85.png)
* This is our result from a graph containing multiple samples, but the test data will contain just 1 sample.

## Additional features
At the final stage of our project, we moved from using Neo4j Docker to Neo4j Desktop, as this version has more practical use for the lab team. Our run.py and this README file contain instructions and functionality for Neo4j Docker in order to support reproducibility; however we have included a `preprocessing.ipynb` that contains several features used with Neo4j Desktop that is useful in practice for the lab team. 
To briefly describe those features:
  * Saving `action`, `chem`, and `link` CSV files along with cypher files to the appropriate folders within the local Neo4j Desktop installation
    * CSV files are saved to the `import` folder within the Neo4j Desktop database directory
    * Cypher files are saved to the `bin` folder within the Neo4j Desktop database directory 
  * Version control to keep track of batches of data that have already been processed, to avoid unnecessary redundant processing of batchs.
  * Deleting CSV files after they are used to load data into Neo4j to reduce clutter in file storage.
"
206,https://github.com/nahmann/DSC180-B16,"{'nahmann': 'https://github.com/nahmann', 'AlexGuan123': 'https://github.com/AlexGuan123', 'mchan415': 'https://github.com/mchan415', 'miyazakia2552': 'https://github.com/miyazakia2552'}","{'HTML': 0.49, 'Jupyter Notebook': 0.31, 'Python': 0.19, 'CSS': 0.0, 'Procfile': 0.0}","# DSC 180 B16 - Decentralized Location Consensus

Group Members:  
Nathan Ahmann  
Mason Chan  
Alex Guan  
Alan Miyazaki  
Mentor: Haojian Jin


Our Heroku site can be found [here](https://dsc180-decentralized-location.herokuapp.com/)  
A static verison of our website can be found [here](https://nahmann.github.io/DSC180-B16/)

Our original work from Fall Quarter:  
[Video Demo](https://youtu.be/Ixj5MV3JIbA) that shows the server working with GET and POST requests.
  
Our new video showcasing the final project:  
[Video Demo](https://youtu.be/sOopTH0-ghM) that walks through an example scenario.


## Content

This repository contains all the files sent to our Heroku server which can be accessed [here](https://dsc180-decentralized-location.herokuapp.com/).  

This main server has the API for the backend and a landing page for the website. In conjunction with B16-2, the app team, an Android device will send get/post requests to this backend which will allow the app to verify the location of users. The verification is done via blacklisting malicious users by having Heroku run `trust_algorithm.py` on a set timer (currently on the hour). This file updates the database for the blacklist on the backend.

If you would like to run this website locally, you can install the necessary packages and run the following code in the terminal. Note that the local development version of the server utilizes sqlite3 as the database since it is easier to locally utilize than Postgres which we use on our Heroku server.

`python manage.py makemigrations` - sets up the migrations for the database  
`python manage.py migrate` - makes the migration files  
`python manage.py runserver` - will run the server which can then be accessed at 127.0.0.1  

## Contains parts of code modified from the following tutorials:
https://docs.djangoproject.com/en/4.1/intro/   
https://github.com/heroku/python-getting-started  
https://devcenter.heroku.com/articles/getting-started-with-python  
https://www.django-rest-framework.org/tutorial/quickstart/   
https://www.django-rest-framework.org/tutorial/1-serialization/ 

and our static webpages contain CSS from Bootstrap
","# DSC 180 B16 - Decentralized Location Consensus

Group Members:  
Nathan Ahmann  
Mason Chan  
Alex Guan  
Alan Miyazaki  
Mentor: Haojian Jin


Our Heroku site can be found [here](https://dsc180-decentralized-location.herokuapp.com/)  
A static verison of our website can be found [here](https://nahmann.github.io/DSC180-B16/)

Our original work from Fall Quarter:  
[Video Demo](https://youtu.be/Ixj5MV3JIbA) that shows the server working with GET and POST requests.
  
Our new video showcasing the final project:  
[Video Demo](https://youtu.be/sOopTH0-ghM) that walks through an example scenario.


## Content

This repository contains all the files sent to our Heroku server which can be accessed [here](https://dsc180-decentralized-location.herokuapp.com/).  

This main server has the API for the backend and a landing page for the website. In conjunction with B16-2, the app team, an Android device will send get/post requests to this backend which will allow the app to verify the location of users. The verification is done via blacklisting malicious users by having Heroku run `trust_algorithm.py` on a set timer (currently on the hour). This file updates the database for the blacklist on the backend.

If you would like to run this website locally, you can install the necessary packages and run the following code in the terminal. Note that the local development version of the server utilizes sqlite3 as the database since it is easier to locally utilize than Postgres which we use on our Heroku server.

`python manage.py makemigrations` - sets up the migrations for the database  
`python manage.py migrate` - makes the migration files  
`python manage.py runserver` - will run the server which can then be accessed at 127.0.0.1  

## Contains parts of code modified from the following tutorials:
https://docs.djangoproject.com/en/4.1/intro/   
https://github.com/heroku/python-getting-started  
https://devcenter.heroku.com/articles/getting-started-with-python  
https://www.django-rest-framework.org/tutorial/quickstart/   
https://www.django-rest-framework.org/tutorial/1-serialization/ 

and our static webpages contain CSS from Bootstrap
"
207,https://github.com/acanonig/DSC180B-Proxensus-,{'tjuacalla123': 'https://github.com/tjuacalla123'},{'Java': 1.0},"# DSC 180B Project Code - Proxensus
Group Members:
Frans Timothy Juacalla,
Andrew Canonigo,
Martin Thai,
Aryaman Sinha,

This code was adapted and modified from the Pre-standard Codebase of the DP3T SDK for Android.
https://github.com/DP-3T/dp3t-sdk-android/releases/tag/prestandard

DP3T SDK
https://github.com/DP-3T/dp3t-sdk-android

### How to run project
To Run the project in Android Studio, Please open the 'calibration-app' instead of the entire project. The project needs at least 3 Android smartphones with working Bluetooth.

Demonstration Video: https://youtu.be/stuOTvUJUmk
","# DSC 180B Project Code - Proxensus
Group Members:
Frans Timothy Juacalla,
Andrew Canonigo,
Martin Thai,
Aryaman Sinha,

This code was adapted and modified from the Pre-standard Codebase of the DP3T SDK for Android.
https://github.com/DP-3T/dp3t-sdk-android/releases/tag/prestandard

DP3T SDK
https://github.com/DP-3T/dp3t-sdk-android

### How to run project
To Run the project in Android Studio, Please open the 'calibration-app' instead of the entire project. The project needs at least 3 Android smartphones with working Bluetooth.

Demonstration Video: https://youtu.be/stuOTvUJUmk
"
208,https://github.com/pnagasam/dsc180a_capstone_project,"{'Lalaluke413': 'https://github.com/Lalaluke413', 'pnagasam': 'https://github.com/pnagasam', 'wshunter': 'https://github.com/wshunter'}","{'Jupyter Notebook': 1.0, 'Python': 0.0}","# dsc180a_capstone_project

This program takes care of all data loading and preprocessing, model building and training, and visualizations regarding our DSC 180 Capstone project.

## Usage
All functionality is used by running `python3 run.py` followed by some arguments in the root directory. The first argument is `exec_type` which indicates what action you would like to take. Possible values are:
```
data     # for data loading

train    # for model training

OT       # for optimal transport

eval     # for gathering results

viz      # for creating visualizations

all      # for doing all of the above

test     # for testing all of the above on dummy data
         # (except downloading the data)
```

The second and final argument is `-c` or `--clean`, which, when included cleans the save directories used by the specified `exec_type`.

### All
The `all` function runs the other functions sequentially, so you don't have to run each individual function. It is up to the user to set values in the `config/` files correctly. To run:
```bash
python3 run.py all
```

To remove all models, OT, results, and visualizations generated by the program, run:
```bash
python3 run.py all -c    # or --clean
```
Note: running this will NOT remove the 13.1 GB of data downloaded by running the `data` function. This is to prevent headache on behalf of the user. You're welcome.

### Data Loading
The dataset is downloaded from the WILDS project using their python package, which is a prerequisite. To install run:
```bash
pip install wilds
```

Running the following command in the root project directory will download the dataset to the proper location:
```bash
python3 run.py data
```

### Model Training
The model we used for this project was a custom CNN built in pytorch and trained entirely on either urban or rural data from one country. To train a model with settings specified in the `config/train.json`, run:
```bash
python3 run.py train
```
To change which country the model should be trained on, whether an urban/rural model should be trained, the percentile cutoffs the model uses for classification, or other training parameters, please consult the config section below.

To remove all trained models, run:
```bash
python3 run.py train -c    # or --clean
```

### Optimal Transport
Optimal transport was achieved using the python optimal transport package, which is a prerequisite. To install run:
```bash
pip install ot
```

In our implementation, optimal transport is used for domain adaptation. The goal is to adapt the color profiles of one country to another, in the hope that the CNN trained on only one country can more accurately classify images from another country.
In order to fit the optimal transport to transport images from a source country to a target country, first edit the source and target country fields in `config/OT.json`, then run:
```bash
python3 run.py OT
```

To remove all saved OT models, run:
```bash
python3 run.py OT -c    # or --clean
```

### Results
The results are gathered and using data, models and OT objects saved from above. This function will error if run without running `data`, `train`, and `OT` first with aligning configuration to generate objects. To run:
```bash
python3 run.py eval
```

To remove all saved results, run:
```bash
python3 run.py eval -c    # or --clean
```

### Visulizations
Similarly to results, the `viz` function requires previous data to be present in the directories specified in each `config/` file. Visualizations are generated using matplotlib and pandas, both prerequisites. To run:
```bash
python3 run viz
```

To remove all saved visualizations, run:
```bash
python3 run.py viz -c    # or --clean
```

### Test
The `test` function is similar to the `all` function except it uses randomly generated data. A valid `config/` is still required for the program to run entirely. And onjects will still be saved. To run:
```bash
python3 run.py test -c    # or --clean
```

Running `python3 run.py test -c    # or --clean` won't remove anything. To remove all saved objects, run:
```bash
python3 run.py all -c    # or --clean
```

## Config
Below is the default `config/` files along with descriptions of each option.

### Train
`config/train.json`

```json
{
    ""country"": ""nigeria"",  // the country to train the model on (usually same as ""target_country"")
    ""train_proportion"": 0.7,  // proportion of data to use to train the model
    ""valid_proportion"": 0.2,   // proportion of data to use to validate the model
    ""urban"": true,  // whether to train a model on urban data
    ""rural"": true,  // whether to train a rural on urban data
    ""low_quantile"": 0.3333333,  // the lower percentile used for classification cutoff
    ""high_quantile"": 0.6666666,  // the lower percentile used for classification cutoff
    ""n_epochs"": 300,   // number of epochs to train the model on (best one will be used for evaluation)
    ""batch_size"": 48,  // batch size to use
    ""save_path"": ""models/"",  // path to save models at
    ""random_seed"": 10  // random seed to use for training
}
```

### OT
`config/OT.json`

```json
{
    ""target_country"": ""nigeria"",  // country to transport to
    ""source_country"": ""mali"",  // country to transport from
    ""n_samples"": 500,  // number of pixel samples to take from each country
    ""reg"": 0.1,  // sinkhorn's regularization parameter
    ""batch_size"": 10,  // batch size to use for OT
    ""save_path"": ""OT/"",  // path to save OT objects to
    ""random_seed"": 10  // random seed to use for OT
}
```

### Results
`config/eval.json`

```json
{
    ""target_country"": ""nigeria"", // OT ""target_country"" (usually the same as ""country"" in config/train.json)
    ""source_country"": ""mali"",  // OT ""source_country""
    ""urban"": true,  // whether to evaluate results on urban model
    ""rural"": true,  // whether to evaluate results on rural model
    ""batch_size"": 10,  // batch size to use when evaluating
    ""save_path"": ""results/"",  // path to save results at
    ""random_seed"": 76  // random seed to use for evaluation
}
```

### Visualizations
`config/viz.json`

```json
{
    ""asset_index_dist"": true,  // whether to show asset index distribution visualization
    ""clf_cutoffs"": true,  // whether to show classification cutoff visualization
    ""target_country"": ""nigeria"",  // OT ""target_country"" and train ""country""
    ""source_country"": ""mali"",  // OT ""source_country""
    ""low_quantile"": 0.33333,  // the lower percentile used for classification cutoff
    ""high_quantile"": 0.66666,  // the lower percentile used for classification cutoff
    ""training_info"": {
        ""urban"": true,  // whether to show training info for urban model
        ""rural"": true  // whether to show training info for rural model
    },
    ""source_confusion_matrix"": {
        ""urban"": {
            ""without_OT"": true,  // whether to show confusion matrix for urban model without OT
            ""with_OT"": true  // whether to show confusion matrix for urban model with OT
            
        },
        ""rural"": {
            ""without_OT"": true,  // whether to show confusion matrix for rural model without OT
            ""with_OT"": true  // whether to show confusion matrix for rural model with OT
        }
    },
    ""show_changed"": true,  // whether to an example of where optimal transport changed the models prediction
    ""save_path"": ""viz/"", // path to save visualizations to
    ""random_seed"": 53 // random seed to use for visualizations
}
```
","# dsc180a_capstone_project

This program takes care of all data loading and preprocessing, model building and training, and visualizations regarding our DSC 180 Capstone project.

## Usage
All functionality is used by running `python3 run.py` followed by some arguments in the root directory. The first argument is `exec_type` which indicates what action you would like to take. Possible values are:
```
data     # for data loading

train    # for model training

OT       # for optimal transport

eval     # for gathering results

viz      # for creating visualizations

all      # for doing all of the above

test     # for testing all of the above on dummy data
         # (except downloading the data)
```

The second and final argument is `-c` or `--clean`, which, when included cleans the save directories used by the specified `exec_type`.

### All
The `all` function runs the other functions sequentially, so you don't have to run each individual function. It is up to the user to set values in the `config/` files correctly. To run:
```bash
python3 run.py all
```

To remove all models, OT, results, and visualizations generated by the program, run:
```bash
python3 run.py all -c    # or --clean
```
Note: running this will NOT remove the 13.1 GB of data downloaded by running the `data` function. This is to prevent headache on behalf of the user. You're welcome.

### Data Loading
The dataset is downloaded from the WILDS project using their python package, which is a prerequisite. To install run:
```bash
pip install wilds
```

Running the following command in the root project directory will download the dataset to the proper location:
```bash
python3 run.py data
```

### Model Training
The model we used for this project was a custom CNN built in pytorch and trained entirely on either urban or rural data from one country. To train a model with settings specified in the `config/train.json`, run:
```bash
python3 run.py train
```
To change which country the model should be trained on, whether an urban/rural model should be trained, the percentile cutoffs the model uses for classification, or other training parameters, please consult the config section below.

To remove all trained models, run:
```bash
python3 run.py train -c    # or --clean
```

### Optimal Transport
Optimal transport was achieved using the python optimal transport package, which is a prerequisite. To install run:
```bash
pip install ot
```

In our implementation, optimal transport is used for domain adaptation. The goal is to adapt the color profiles of one country to another, in the hope that the CNN trained on only one country can more accurately classify images from another country.
In order to fit the optimal transport to transport images from a source country to a target country, first edit the source and target country fields in `config/OT.json`, then run:
```bash
python3 run.py OT
```

To remove all saved OT models, run:
```bash
python3 run.py OT -c    # or --clean
```

### Results
The results are gathered and using data, models and OT objects saved from above. This function will error if run without running `data`, `train`, and `OT` first with aligning configuration to generate objects. To run:
```bash
python3 run.py eval
```

To remove all saved results, run:
```bash
python3 run.py eval -c    # or --clean
```

### Visulizations
Similarly to results, the `viz` function requires previous data to be present in the directories specified in each `config/` file. Visualizations are generated using matplotlib and pandas, both prerequisites. To run:
```bash
python3 run viz
```

To remove all saved visualizations, run:
```bash
python3 run.py viz -c    # or --clean
```

### Test
The `test` function is similar to the `all` function except it uses randomly generated data. A valid `config/` is still required for the program to run entirely. And onjects will still be saved. To run:
```bash
python3 run.py test -c    # or --clean
```

Running `python3 run.py test -c    # or --clean` won't remove anything. To remove all saved objects, run:
```bash
python3 run.py all -c    # or --clean
```

## Config
Below is the default `config/` files along with descriptions of each option.

### Train
`config/train.json`

```json
{
    ""country"": ""nigeria"",  // the country to train the model on (usually same as ""target_country"")
    ""train_proportion"": 0.7,  // proportion of data to use to train the model
    ""valid_proportion"": 0.2,   // proportion of data to use to validate the model
    ""urban"": true,  // whether to train a model on urban data
    ""rural"": true,  // whether to train a rural on urban data
    ""low_quantile"": 0.3333333,  // the lower percentile used for classification cutoff
    ""high_quantile"": 0.6666666,  // the lower percentile used for classification cutoff
    ""n_epochs"": 300,   // number of epochs to train the model on (best one will be used for evaluation)
    ""batch_size"": 48,  // batch size to use
    ""save_path"": ""models/"",  // path to save models at
    ""random_seed"": 10  // random seed to use for training
}
```

### OT
`config/OT.json`

```json
{
    ""target_country"": ""nigeria"",  // country to transport to
    ""source_country"": ""mali"",  // country to transport from
    ""n_samples"": 500,  // number of pixel samples to take from each country
    ""reg"": 0.1,  // sinkhorn's regularization parameter
    ""batch_size"": 10,  // batch size to use for OT
    ""save_path"": ""OT/"",  // path to save OT objects to
    ""random_seed"": 10  // random seed to use for OT
}
```

### Results
`config/eval.json`

```json
{
    ""target_country"": ""nigeria"", // OT ""target_country"" (usually the same as ""country"" in config/train.json)
    ""source_country"": ""mali"",  // OT ""source_country""
    ""urban"": true,  // whether to evaluate results on urban model
    ""rural"": true,  // whether to evaluate results on rural model
    ""batch_size"": 10,  // batch size to use when evaluating
    ""save_path"": ""results/"",  // path to save results at
    ""random_seed"": 76  // random seed to use for evaluation
}
```

### Visualizations
`config/viz.json`

```json
{
    ""asset_index_dist"": true,  // whether to show asset index distribution visualization
    ""clf_cutoffs"": true,  // whether to show classification cutoff visualization
    ""target_country"": ""nigeria"",  // OT ""target_country"" and train ""country""
    ""source_country"": ""mali"",  // OT ""source_country""
    ""low_quantile"": 0.33333,  // the lower percentile used for classification cutoff
    ""high_quantile"": 0.66666,  // the lower percentile used for classification cutoff
    ""training_info"": {
        ""urban"": true,  // whether to show training info for urban model
        ""rural"": true  // whether to show training info for rural model
    },
    ""source_confusion_matrix"": {
        ""urban"": {
            ""without_OT"": true,  // whether to show confusion matrix for urban model without OT
            ""with_OT"": true  // whether to show confusion matrix for urban model with OT
            
        },
        ""rural"": {
            ""without_OT"": true,  // whether to show confusion matrix for rural model without OT
            ""with_OT"": true  // whether to show confusion matrix for rural model with OT
        }
    },
    ""show_changed"": true,  // whether to an example of where optimal transport changed the models prediction
    ""save_path"": ""viz/"", // path to save visualizations to
    ""random_seed"": 53 // random seed to use for visualizations
}
```
"
209,https://github.com/BillChen24/DSC180B-Project-B319-2,"{'Alina-Zhi': 'https://github.com/Alina-Zhi', 'BillChen24': 'https://github.com/BillChen24', 'Guanlin-99': 'https://github.com/Guanlin-99'}","{'Jupyter Notebook': 0.99, 'Python': 0.01, 'Dockerfile': 0.0}","# DSC180B-Project-B319-2
# Domain Adaptation of CNN in Animal Classification Task

### For Test Trails:
After downloading the githubt repo (including the sample data in data folder), run the following code in terminal
```
python run.py test
```
This code will train a custom CNN model on the sample data and generate a loss curve over epochs.

The loss plot will be saved in the path printed at the end of the execution.

The trained model will be saved in the path printed at the end of the execution.

Code will create a ""result/"" folder if such folder doesn't exist in the local repository and store the loss plot and model in this folder

#### After Runing
To clear all output files, run the following command in terminal:
```
rm -r result/
```


### To Get Whole dataset
Create environment for loading iwildcam dataset 

0: Open a terminal 

1: ssh in tothe dsmlp environment:
ssh <user_name>@dsmlp-login.ucsd.edu

Example: ssh zhc023@dsmlp-login.ucsd.edu

2: launch-scipy-ml.sh -c 8 -m 50 -i billchen24/dsc180b-project -P Always // request 8 cpu and 50 GB, and create specific environment with my docker image 

3: Create new terminal 

4: ssh -N -L 8889:127.0.0.1:16585 zhc023@dsmlp-login.ucsd.edu

5: Go to http://localhost:8889/user/zhc023/tree/ 

### To Run the Full Experiment
Run the following command
```
python run.py main 10
```
""10"" specified the maximum number of epoch the model will train, Feel free to update it to any value.
Some other hyperparameters can be view and change in ""run.py""

Similar to the test trail, all output from can be removed by 
```
rm -r result/
```
","# DSC180B-Project-B319-2
# Domain Adaptation of CNN in Animal Classification Task

### For Test Trails:
After downloading the githubt repo (including the sample data in data folder), run the following code in terminal
```
python run.py test
```
This code will train a custom CNN model on the sample data and generate a loss curve over epochs.

The loss plot will be saved in the path printed at the end of the execution.

The trained model will be saved in the path printed at the end of the execution.

Code will create a ""result/"" folder if such folder doesn't exist in the local repository and store the loss plot and model in this folder

#### After Runing
To clear all output files, run the following command in terminal:
```
rm -r result/
```


### To Get Whole dataset
Create environment for loading iwildcam dataset 

0: Open a terminal 

1: ssh in tothe dsmlp environment:
ssh <user_name>@dsmlp-login.ucsd.edu

Example: ssh zhc023@dsmlp-login.ucsd.edu

2: launch-scipy-ml.sh -c 8 -m 50 -i billchen24/dsc180b-project -P Always // request 8 cpu and 50 GB, and create specific environment with my docker image 

3: Create new terminal 

4: ssh -N -L 8889:127.0.0.1:16585 zhc023@dsmlp-login.ucsd.edu

5: Go to http://localhost:8889/user/zhc023/tree/ 

### To Run the Full Experiment
Run the following command
```
python run.py main 10
```
""10"" specified the maximum number of epoch the model will train, Feel free to update it to any value.
Some other hyperparameters can be view and change in ""run.py""

Similar to the test trail, all output from can be removed by 
```
rm -r result/
```
"
210,https://github.com/TallMessiWu/dota2-drafting-backend,{'TallMessiWu': 'https://github.com/TallMessiWu'},"{'Jupyter Notebook': 0.99, 'Python': 0.01}","Project DOTA 2 drafting.

[API Documentation](https://docs.opendota.com/)
","Project DOTA 2 drafting.

[API Documentation](https://docs.opendota.com/)
"
