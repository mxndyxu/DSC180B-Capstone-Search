{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPEN_AI_KEY = os.getenv('OPEN_AI_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = OPEN_AI_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report_pdf_content_dict = pd.read_pickle(\"../data/report_pdf_content_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(report_pdf_content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "github_df = pd.read_csv(\"../data/github_df.csv\", index_col=0)\n",
    "github_df[\"readme\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_github_dfs = github_df[~github_df[\"readme\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "    max_chunk_size = 2048\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in text.split(\".\"):\n",
    "        if len(current_chunk) + len(sentence) < max_chunk_size:\n",
    "            current_chunk += sentence + \".\"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \".\"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request the summarization using ChatGPT\n",
    "# summarization_response = openai.Completion.create(\n",
    "#     engine=\"text-davinci-002\",\n",
    "#     prompt=\"Please summarize the following text\",\n",
    "#     max_tokens=max_tokens,\n",
    "#     api_key=OPEN_AI_KEY\n",
    "# )\n",
    "\n",
    "# https://medium.com/muthoni-wanyoike/implementing-text-summarization-using-openais-gpt-3-api-dcd6be4f6933\n",
    "# Code from here: https://www.kdnuggets.com/2023/04/text-summarization-development-python-tutorial-gpt35.html\n",
    "def generate_summarizer(text):\n",
    "    \n",
    "    res = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        temperature=0.7,\n",
    "        top_p=0.5,\n",
    "        frequency_penalty=0.5,\n",
    "        messages=\n",
    "    [\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant for text summarization.\",\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Summarize this: {text}\",\n",
    "        },\n",
    "        ],\n",
    "    )\n",
    "    return (res[\"choices\"][0][\"message\"][\"content\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>github_url</th>\n",
       "      <th>readme</th>\n",
       "      <th>contributors</th>\n",
       "      <th>language_breakdown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/DSC-Capstone/projects-2019-...</td>\n",
       "      <td># wiki-capstone\\nPublic repository for DSC 180...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/DSC-Capstone/projects-2019-...</td>\n",
       "      <td># DSC180B Wikipedia Engagement\\n\\nThis project...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/DSC-Capstone/projects-2019-...</td>\n",
       "      <td># DSC180B Final Project: Investigating the Bia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/DSC-Capstone/projects-2019-...</td>\n",
       "      <td># Name That Raga: Classification and Analysis ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/DSC-Capstone/projects-2019-...</td>\n",
       "      <td>&lt;p align=\"center\"&gt;\\n    &lt;!-- &lt;b style=\"font-si...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>https://github.com/nahmann/DSC180-B16</td>\n",
       "      <td># DSC 180 B16 - Decentralized Location Consens...</td>\n",
       "      <td>{'nahmann': 'https://github.com/nahmann', 'Ale...</td>\n",
       "      <td>{'HTML': 0.49, 'Jupyter Notebook': 0.31, 'Pyth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>https://github.com/acanonig/DSC180B-Proxensus-</td>\n",
       "      <td># DSC 180B Project Code - Proxensus\\nGroup Mem...</td>\n",
       "      <td>{'tjuacalla123': 'https://github.com/tjuacalla...</td>\n",
       "      <td>{'Java': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>https://github.com/pnagasam/dsc180a_capstone_p...</td>\n",
       "      <td># dsc180a_capstone_project\\n\\nThis program tak...</td>\n",
       "      <td>{'Lalaluke413': 'https://github.com/Lalaluke41...</td>\n",
       "      <td>{'Jupyter Notebook': 1.0, 'Python': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>https://github.com/BillChen24/DSC180B-Project-...</td>\n",
       "      <td># DSC180B-Project-B319-2\\n# Domain Adaptation ...</td>\n",
       "      <td>{'Alina-Zhi': 'https://github.com/Alina-Zhi', ...</td>\n",
       "      <td>{'Jupyter Notebook': 0.99, 'Python': 0.01, 'Do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>https://github.com/TallMessiWu/dota2-drafting-...</td>\n",
       "      <td>Project DOTA 2 drafting.\\n\\n[API Documentation...</td>\n",
       "      <td>{'TallMessiWu': 'https://github.com/TallMessiWu'}</td>\n",
       "      <td>{'Jupyter Notebook': 0.99, 'Python': 0.01}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            github_url  \\\n",
       "0    https://github.com/DSC-Capstone/projects-2019-...   \n",
       "1    https://github.com/DSC-Capstone/projects-2019-...   \n",
       "2    https://github.com/DSC-Capstone/projects-2019-...   \n",
       "3    https://github.com/DSC-Capstone/projects-2019-...   \n",
       "4    https://github.com/DSC-Capstone/projects-2019-...   \n",
       "..                                                 ...   \n",
       "203              https://github.com/nahmann/DSC180-B16   \n",
       "204     https://github.com/acanonig/DSC180B-Proxensus-   \n",
       "205  https://github.com/pnagasam/dsc180a_capstone_p...   \n",
       "206  https://github.com/BillChen24/DSC180B-Project-...   \n",
       "207  https://github.com/TallMessiWu/dota2-drafting-...   \n",
       "\n",
       "                                                readme  \\\n",
       "0    # wiki-capstone\\nPublic repository for DSC 180...   \n",
       "1    # DSC180B Wikipedia Engagement\\n\\nThis project...   \n",
       "2    # DSC180B Final Project: Investigating the Bia...   \n",
       "3    # Name That Raga: Classification and Analysis ...   \n",
       "4    <p align=\"center\">\\n    <!-- <b style=\"font-si...   \n",
       "..                                                 ...   \n",
       "203  # DSC 180 B16 - Decentralized Location Consens...   \n",
       "204  # DSC 180B Project Code - Proxensus\\nGroup Mem...   \n",
       "205  # dsc180a_capstone_project\\n\\nThis program tak...   \n",
       "206  # DSC180B-Project-B319-2\\n# Domain Adaptation ...   \n",
       "207  Project DOTA 2 drafting.\\n\\n[API Documentation...   \n",
       "\n",
       "                                          contributors  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "203  {'nahmann': 'https://github.com/nahmann', 'Ale...   \n",
       "204  {'tjuacalla123': 'https://github.com/tjuacalla...   \n",
       "205  {'Lalaluke413': 'https://github.com/Lalaluke41...   \n",
       "206  {'Alina-Zhi': 'https://github.com/Alina-Zhi', ...   \n",
       "207  {'TallMessiWu': 'https://github.com/TallMessiWu'}   \n",
       "\n",
       "                                    language_breakdown  \n",
       "0                                                  NaN  \n",
       "1                                                  NaN  \n",
       "2                                                  NaN  \n",
       "3                                                  NaN  \n",
       "4                                                  NaN  \n",
       "..                                                 ...  \n",
       "203  {'HTML': 0.49, 'Jupyter Notebook': 0.31, 'Pyth...  \n",
       "204                                      {'Java': 1.0}  \n",
       "205           {'Jupyter Notebook': 1.0, 'Python': 0.0}  \n",
       "206  {'Jupyter Notebook': 0.99, 'Python': 0.01, 'Do...  \n",
       "207         {'Jupyter Notebook': 0.99, 'Python': 0.01}  \n",
       "\n",
       "[204 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_null_github_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(non_null_github_dfs[\"readme\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/nickthegroot/recipe-recommendation Done. Waiting 15 seconds\n",
      "https://github.com/mjw49/DSC180B-Quarter-2-Project Done. Waiting 15 seconds\n",
      "https://github.com/camille-004/Graph-HSCN Done. Waiting 15 seconds\n",
      "https://github.com/bliu8923/dsc180b-project Done. Waiting 15 seconds\n"
     ]
    }
   ],
   "source": [
    "# report_text_dict = {}\n",
    "# report_text_dict = pd.read_pickle(\"../data/report_text_dict.pkl\")\n",
    "# not_finished_text = []\n",
    "# i = 0\n",
    "# for k, v in report_pdf_content_dict.items():\n",
    "#     if i < 46:\n",
    "#         i += 1\n",
    "#     else:\n",
    "#         text = v[0]\n",
    "#         try:\n",
    "#             summary_text = generate_summarizer(text)\n",
    "#             report_text_dict[k] = [text, summary_text]\n",
    "#         except openai.InvalidRequestError:\n",
    "#             print(\"Invalid Request Error\")\n",
    "#             not_finished_text.append(k)\n",
    "\n",
    "#         print(f\"{k} Done. Waiting 10 seconds\")\n",
    "#         time.sleep(10)\n",
    "#         i += 1\n",
    "\n",
    "report_readme_dict = pd.read_pickle(\"../data/report_readme_dict.pkl\")\n",
    "not_finished_readmes = []\n",
    "for i, row in non_null_github_dfs.iterrows():\n",
    "    if row[\"github_url\"] not in report_readme_dict:\n",
    "        text = row[\"readme\"]\n",
    "        try:\n",
    "            summary_text = generate_summarizer(text)\n",
    "            report_readme_dict[row[\"github_url\"]] = [text, summary_text]\n",
    "            print(f\"{row['github_url']} Done. Waiting 15 seconds\")\n",
    "            # time.sleep(15)\n",
    "        except openai.InvalidRequestError:\n",
    "            print(\"Invalid Request Error\")\n",
    "            not_finished_readmes.append(row[\"github_url\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_01/': ['# wiki-capstone\\nPublic repository for DSC 180B senior capstone project exploring racial bias in Oscars and Golden Globes Award Shows.\\n\\nHow to Run:\\n  This project consists of two parts: data collection and creating visuals. Both \"test-project\" and \"full-project\" targets\\n  will run both parts. \\n\\n  Targets:\\n  1. \"clean\" : Removes any test data created and removes any visualizations created. Does not remove full data because that is needed for the visualizations.\\n  2. \"test-project\": Runs the data collection process for only years 1934-1935. It also creates visuals using data from years 1934-2008.\\n  3. \"full-project\": Runs the data collection process for all years 1934- 2008. It also creates visuals using this data. (Delete \"data/\" folder before running this target)\\n  \\n  \\n',\n",
       "  'This is a public repository for a senior capstone project called \"wiki-capstone\" that focuses on exploring racial bias in Oscars and Golden Globes Award Shows. The project consists of two parts: data collection and creating visuals. There are three targets: \"clean\" removes test data and visualizations, \"test-project\" runs the data collection process for years 1934-1935 and creates visuals from years 1934-2008, and \"full-project\" runs the data collection process for all years 1934-2008 and creates visuals (requires deleting the \"data/\" folder before running).'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_02/': ['# DSC180B Wikipedia Engagement\\n\\nThis project is focused on helping people understand engagement on a wikipedia page through analysis of supply and demand on the page.\\n\\n## Usage\\n```\\nlaunch-scipy-ml.sh\\ngit clone https://github.com/Jwlin17/DSC180B.git\\ncd DSC180B\\npython run.py test-project\\n```\\n\\n```\\nlaunch-scipy-ml.sh -i jwlin/wiki-engagement\\n```\\n\\n## Files\\n\\n**./config/test-params.json** - holds input values to run test-project command\\n\\n**./config/env.json** - docker config data\\n\\n**./config/data-params.json** - directory where data should be output to\\n\\n**./notebooks/WikiEngagementEDA.ipynb** - EDA analysis for our data set\\n\\n**./src/engagement_score.py** - contains relevant functions for creating the engagement score\\n\\n**./src/wikiparser.py** - contains relevant functions for parsing wikipedia dump files\\n\\n**./website_data/article_titles.txt** - contains article titles to populate search bar\\n\\n**./website_data/content_score.png** - content score formula\\n\\n**./website_data/editor_score.png** - editor score formula\\n\\n**./website_data/engagement_scores.json** - engagement scores to graph on website\\n\\n**./index.html** - website html\\n\\n**./requirements.txt** - required packages\\n\\n**./run.py** - call run.py to run data analysis\\n\\n## Output Files\\n\\n**./data/raw/zips** - holds all the zipped files from wikidump / lightdump\\n\\n**./data/raw/extracted** - holds all the extracted zip files\\n\\n**./data/temp** - holds the created lightdump data parsed from en-wiki dump\\n\\n**./data/out** - contains the .png charts of mscore over time and csv of article title / mscore\\n\\n## Sources\\n\\nLink to my writeup report: https://docs.google.com/document/d/17lQ96NeAWvI133FgFkBsB234vdQLecg9g2wfLc_Y79o/edit?usp=sharing\\n\\nWiki-Media Dumps I Used: https://dumps.wikimedia.org/enwiki/20200101/\\n\\nWiki-Monitor Lightdump information: http://wwm.phy.bme.hu/\\n\\nPaper that I replicated: https://arxiv.org/pdf/1107.3689.pdf\\n\\nAlternative methods of studying controversy: https://www.opensym.org/ws2012/p18wikisym2012.pdf\\n',\n",
       "  'This project focuses on analyzing supply and demand on a Wikipedia page to understand engagement. It provides instructions for usage and includes various files such as code, data, and website resources. The output files include zipped and extracted data, as well as charts and CSV files. The project sources include a writeup report, Wikipedia dumps, information on Wiki-Monitor Lightdump, and a research paper on studying controversy.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_03/': ['# DSC180B Final Project: Investigating the Biaseness of Wikipedia and the Media in the Scope of COVID-19\\nWhich is the most unbiased source of COVID-19 information: A analysis of 20 News agency and Wikipedia\\n\\n## Overview\\n\\nCOVID-19 has gained increasing attention since its breakout in China in the early 2020. At this stage where full understanding of the virus is still developing, information related to COVID-19 in the media coverage may sometimes be misleading. There is diverging, even self-conflicting, news coverage from newspapers. It is becoming increasingly difficult to find out to figure out which statements being thrown at us are facts that we should be listening to or simple just rumors we should ignore. Wikipedia viewing and editing data is a record of people’s information-seeking and engagement behavior that could be used as an unbiased indicator of public opinion. It might reveal what people actually learn from the media coverage and how people react to the COVID-19. On the other hand, it could be seen as a platform where “random people” get to contribute and therefore making it a biased and untrustworthy source. We defined the trustworthiness of a source through a topic distribution matrix. Since we can divide the broad topic of COVID-19 into sub topics such as origins of the virus, symptoms, economic implications, how it spreads etc, we will measure how trustworthy a source is by how many of these subtopics it covers. Specifically, we wanted to answer the question, “How biased are certain popular news outlets in how they cover the effects and the spread of COVID-19?\"\\n\\n## Usage\\n\\nOur project was implemented with a combination of python and R. We use python for data ingesting, cleaning, and transformation. We use R for running the STM model since the package is R-exclusive. And finally, Python for final measurements and visualization.\\n\\nThe project pipeline could be run by:\\n```\\nlaunch-scipy-ml.sh\\ngit clone git@github.com:SchootHuang/DSC180B-Coronavirus-Wikipedia.git\\ncd DSC180B\\npython run.py #USER-SPECIFIED-TARGET#\\n```\\n\\nFor demoing purpuse:\\n1. Please refer to EDA_news.ipynb for the EDA process of news dataset\\n\\n2. please refer to 2020-COVID.R for topic modeling in R.\\n\\n3. Finally, refer to the visualization.ipynb notebook for visualization, and Biaseness_measurement.ipynb for the biaseness evaluation with Wasserstain distance and Forbenius Norm distance.\\n\\n\\n## Resources\\n\\nLink to our Project Website: https://schoothuang.github.io/DSC180B-Coronavirus-Wikipedia/ \\n\\nWiki-Media Dumps dataset Used: https://dumps.wikimedia.org/enwiki/20200101/\\n\\nWiki-Monitor Lightdump information: http://wwm.phy.bme.hu/\\n\\nAll the News 2.0 dataset: https://components.one/datasets/all-the-news-2-news-articles-dataset/ \\n',\n",
       "  'This project investigates the bias in COVID-19 information from news agencies and Wikipedia. The goal is to determine which source is the most unbiased. The project uses Python and R for data analysis and visualization. The pipeline for running the project is provided, along with resources such as datasets and a project website.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_04/': ['# Name That Raga: Classification and Analysis of Indian Classical Music\\n\\n## Background\\nIndian Classical music contains two primary divisions - North Indian Classical (_Hindustani_), and South Indian Classical (_Carnatic_). While both styles have their fundamental differences, the underlying structure of styles can be captured in a _raga/raag/ragam_. A raga is defined as “a pattern of notes having characteristic intervals, rhythms, and embellishments, used as a basis for improvisation.” A raga can be compared to a type of scale in Western classical music. Though Western classical music does not have a direct equivalent to this concept, a raga is somewhat comparable to certain scales, such as a natural harmonic minor or a major scale. Every song has a raga that it is set to. For example, the raga _Kirvani_ is equivalent to the natural harmonic scale in Western music.\\n\\nOur goal with this project is to quantify ragas in a way that we can then build a raga identification tool. This tool would be able to “listen” to an audio clip and be able to identify the raga that the song is set to. It would mimic what seasoned listeners of Indian classical music do already: try to identify a raga while listening to music. By quantifying the features of a ragam, we will attempt to build a raga identification classifier. \\n\\nTo manage the scope of this project with the time we have, we will build this tool to be functional for 10 prominent Hindustani/Carnatic ragas. These 10 prominent ragas are known as _thaats_. In Hindustani music, these are known as: _Asavari, Bilawal, Bhairav, Bhairavi, Kafi, Kalyan, Khamaj, Marva, Poorvi,_ and _Todi_. The corresponding ragas in Carnatic are known as _Natabhairavi, Dheerashankarabharanam, Mayamalavagowlai, Hanumatodi, Karaharapriya, Kalyani, Harikhamboji, Gamanashama, Kamarvardhani_, and _Shubhapantuvarali_. \\n\\n\\n## How To Run: Example\\n\\nOur repository includes a small amount of test data that you can run this pipeline on and obtain results. \\n\\n1. Clone this repository to have a local copy of these files.\\n2. On the command line, navigate to this repository locally \\n3. The command *python run.py test-project* runs the pipeline with the test-project target. This will load, clean, extract features, and build a model with the small amount of data included in the *testdata_raw* folder. \\n4. You should now have a csv for loaded data, cleaned data, and the model that was built. \\n\\n## How To Run: Classification of Your Own Audio Files\\n\\n1. Clone this repository. \\n2. On the command line, navigate to this repository locally. \\n3. Add your own data to *data/raw*\\n4. On the command line, the command *python run.py full-project* runs the pipeline with the full-project target. This will load, clean, extract features, and build a model with the data that you have included in the *data/raw* folder.\\n5. You should now have a csv for loaded data, cleaned data, and the model that was built.\\n',\n",
       "  'This article discusses the classification and analysis of Indian classical music, specifically focusing on the concept of ragas. Ragas are patterns of notes used as a basis for improvisation in Indian classical music. The goal of the project is to develop a tool that can identify the raga of a song by analyzing its audio clip. The project focuses on 10 prominent ragas from both Hindustani and Carnatic music traditions. The article also provides instructions on how to run the tool using test data or your own audio files.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_05/': ['<p align=\"center\">\\n    <!-- <b style=\"font-size: 45px;\">Red Means Go</b><br> -->\\n    <img width=\"614\" height=\"345\" src=\"https://raw.githubusercontent.com/codencoding/Red-Means-Go/gh-pages/images/site-logo.png\">\\n</p>\\n\\n## Abstract\\nYouTube has become a significant source of income for many content creators, and they are always looking for the best way to grow their channel. When a YouTube video gets more views, the Content Creator makes more money. The purpose of our project is to analyze the significance of the various features of a YouTube preview thumbnail that can contribute to a video’s success. We believe that there are features in YouTube thumbnails that can be extracted and used to identify what makes a video more appealing to potential viewers. \\n\\n## Introduction\\nOur research question is what YouTube thumbnail features, if any, have an effect on the amount of views that the video gets. Our hypothesis is that the more popular videos will likely have more provocative thumbnails that grab that attention of viewers. We believe that popular thumbnails will have more contrast, brightness, engaging subject matter, or just general attractiveness to potential viewers. \\nWe used YouTube’s Data API (v3) to create a data set that’s sourced from a YouTube search for “Fortnite”. We scrape the first 200 query results, and then scrape the first 100 videos from each unique channel, resulting in around 10,000 videos. Then we create a gamut of statistics from the metadata to best assess how well the video is doing according to YouTube. Because our data is from YouTube, which is an ever changing ecosystem, our results are only indicative of the data we scraped on April 16th, 2020. The thumbnails were created by YouTube content creators and/ or their thumbnail designers, while video metadata was provided by YouTube.\\nWe think our analysis is an interesting investigation because YouTube is a very prominent cultural influence, and so being able to better attract a larger audience would be of general interest to anyone pursuing a YouTube-based career. Because of the recent monetization of YouTube on a large scale, more and more people are trying to make a living off of YouTube. We hope that our results will help give any potential YouTuber insight into what features are most relevant in a thumbnail. In addition, as avid YouTube viewers, we have an intuition that thumbnails play a factor in YouTube’s recommendation algorithm. We hoped to learn more about this rather vague recommendation system and figure out what factors help in a video’s success, if any.\\n\\n## Methods\\nThe features we will use to address our question are a combination of metadata and image features. For the metadata features, we use the view count of the YouTube video as well as a z-scored view count, calculated by taking fortnite videos from the same channel within the same month, that way we can compare channel to channel through the z-scores. For our image features, we are using image brightness, saturation, hue, unique_rgb_ratio (number of unique rgb values divided by the number of pixels), and the number of faces present (using DeepFace). For our metadata features, we used the YouTube Data API (v3) to get our metadata such as views. For the image features, we used the skimage library to extract image brightness, saturation, hue, and rgb values. For face recognition, we used DeepFace from DLib. We  computed our own extracted features such as unique_rgb_ratio and z-score views. \\nThe analytical techniques we are using are as follows. For initial eda, we computed the correlation between each image feature column and the z_views column. This way we can see if there is any correlation between a specific image feature and the amount of views the video got (relative to similar videos from the same channel). For a deeper analysis, we wanted to combine images features to see if a certain combination of image features would attract more viewers. To do this, we first tried feeding all image feature columns into a random forest regressor and gradient boosted regressor. Then we plotted the predicted values vs. the real values to see any patterns in the predictions. We also multiplied sets of two/three/four features together to make higher level features, then trained a linear regression model with these features. We also looked at the predictions for this model to see any patterns in the predictions. We did not use a neural net to generate features for regression as the generated high level features are not clear enough to make a distinction between which image features have impact on the video views.\\n\\n## Results\\nThe results of our deep dive into how image features of thumbnails relate to video views are that there is no strong correlation between our thumbnail image features and video views. To show this, we’ve selected a couple visualizations to help see these results.\\n\\n<p align=\"center\">\\n    <img width=\"614\" height=\"345\" src=\"https://raw.githubusercontent.com/codencoding/Red-Means-Go/gh-pages/images/fig1.png\">\\n</p>\\n\\n<p align=\"center\">\\n    <small> Figure (1): Random Forest Regression on the z-score for video views </small>\\n</p>\\n\\n<p align=\"center\">\\n    <img width=\"614\" height=\"345\" src=\"https://raw.githubusercontent.com/codencoding/Red-Means-Go/gh-pages/images/fig2.png\">\\n</p>\\n\\n<p align=\"center\">\\n    <small> Figure (2): Gradient Boosted Regression on the z-score for video views </small>\\n</p>\\n\\nThe first two charts we chose to include are scatterplots of the predicted z_views vs the actual z_views. These predictions were obtained by training a random forest regressor and a gradient boosted regressor on the numerical image features gathered from our image processing (\\'unique_rgb_ratio\\',\\'mean_hue\\', \\'mean_saturation\\',\\'mean_brightness\\', \\'contrast\\', \\'edge_score\\', numFaces’). The purpose of this graph is to show the lack of correlation between the predictions and the real values, which is shown by the score (coefficient of determination R<sup>2</sup>) for each being close to 0. A score of 0 is achieved by always predicting the mean value of z_views, making these regressors worse than the most naive approach. This shows the lack of relationship between the image features we used and the value of z_views that the video gets.\\n\\n<p align=\"center\">\\n    <img width=\"640\" height=\"360\" src=\"https://raw.githubusercontent.com/codencoding/Red-Means-Go/gh-pages/images/fig3.png\">\\n</p>\\n\\n<p align=\"center\">\\n    <small> Figure (3): Thumbnail image statistics </small>\\n</p>\\n\\nThe third visualization we chose to include is a combination of  bar charts comparing the good performance videos (z_views > 1) and the poor performance videos (z_views < -1). We plot the values of the standard descriptive statistics to give a summary of values for the selected numerical image features. The adjacent bars allow for easy comparison for the different subsets of videos. This represents a conditional subset approach we used. By splitting up the data and looking at videos that did “well” and videos that did “poorly” , we are able to see some promising differences in thumbnail image features. Looking at the image features “unique_rgb_ratio” and “mean_hue”, we see consistent higher values for good videos than bad videos. This sheds light on the theory that more colorful thumbnails see greater success. We also see consistent differences in “contrast” and “edge_score”, this time the poorly performing videos having higher values. This alludes to “busier” thumbnails seeing less success. However, these differences are slight and not statistically significant, but we have hope that these features will direct some future analysis. \\n\\n## Discussion\\nFor all models, the predictions scored worse than predicting the mean for all values, indicating no such patterns exists, alluding to a lack of correlation between our image features and video views. The score for each model is either negative or very close to 0. According to SKLearn’s documentation, a score of 0 would be achieved by predicting the mean of the target column for all values. Because our models scored around 0 while trained on thumbnail features, we cannot say that there is any significant correlation between our thumbnail features and video views. We think this result is likely due to the relative importance of the thumbnail to the content of the video. We additionally looked at the correlation of the individual image features and video views, and found no significant correlation.\\n\\nSince there are so many aspects that go into whether someone watches the video such as title, thumbnail, video duration, and most importantly, the contents of the video, it makes sense that there would not be a strong correlation between thumbnail image features and video views. It is also worth noting that these results are specifically for Fortnite Gaming videos uploaded in March/April 2020, and our image features were relatively basic. If the scope of this project was larger, we could look at more genres and more videos per genre, along with more advanced image features. We still think that video thumbnails affect video views, but we lack the quantifiable results to say so. \\n\\nWith the growth of social media platforms such as YouTube, the job title of ‘content creator’ has become a more common and financially viable occupation. As such, our work on YouTube thumbnails will help creators put numbers to the trends they inherently sense in the ever changing YouTube thumbnail meta, allowing them to make changes to their thumbnails with less guesswork and backed with more relevant data. Besides the impact on those creating thumbnails, our work also explores how audiences of specific genres of videos react to different types of thumbnails because thumbnails are the front cover of a video and holds the potential to attract millions of users who aren’t already followers.\\n\\nOur approach takes advantage of the digital format as we were able to scrape and process mass amounts of data which wouldn’t have been easy to do manually which enabled us to acquire and work with a substantially larger dataset. Since we parameterized our work, it is very flexible and can be configured to analyze different genres of Youtube videos which could be useful for a Youtuber as it would provide them with a snapshot of the current YouTube thumbnail meta and act as a soft guide when they are making their own.\\nWe could expand our project scope in the future by looking at other video games in the YouTube Gaming category or even other YouTube categories (such as make-up videos or VLOGS). Another direction is creating a live thumbnail meta analysis. We suspect that certain features in thumbnails rise and fall in popularity similar to fashion, so having a live trend analysis of thumbnails could prove useful. For instance, if faces in the thumbnail start becoming less popular, YouTubers might want to stay away from putting their face on a thumbnail. However, they might use the thumbnail meta analysis as a way to go against the meta which would make their thumbnails stick out. \\n\\n## Getting Started / Extending this Project\\n\\n### Running the Pipeline\\n\\nTo run the main pipeline, run the command “python run.py” in the repository’s root directory. To run this command you also need a youtube API key. The key can be obtained by following this guide: (https://developers.google.com/youtube/v3/getting-started). Once obtained, you need to create a file named “api_key.json” in the root directory of the repo. It needs to have one key called “api_keys”, which is a list of API keys as strings. You can include as many keys as you’d like to gather more datasets, as the youtube API has a daily limit. Additionally, the command “python run.py test-project” can be run, which runs the analysis on a curated test dataset. This is good for a quick option that does not require fiddling with configuration parameters or creating an API key. \\n\\n### Output Files\\n\\nWhen running this pipeline locally, 5 output files will be generated in the “data/local/*selected-game*/video_data/” directory. The first file is the “scrape_MM_DD_YY.json” file, which represents a dataset of video_ids returned by the search result, and for each video_id, includes the specified number of video_ids from that channel that also match the *selected-game*. The second file is “*selected-game*_full_features_MM_DD_YY.csv”, which represents the dataset of metadata features, basic image features, and advanced image features for all videos in the dataset created on the date in the filename. The third file is “*selected-game*_full_metadata_MM_DD_YY.csv”, which contains only the metadata features for the dataset gathered on the specified date. The fourth file is “*selected-game*_requests.json”, which contains the massive amount of data returned by the youtube api for each video_id. This is constantly updated for any new videos that are scraped, which is why it does not have a date in the filename. The last file is “*selected-game*_summary_metadata_MM_DD_YY.csv”, this contains just the metadata for only the videos that appear in the search results, and not the videos within the channels that are contained in “*selected-game*_full_features_MM_DD_YY.csv”.\\n\\n\\n### Further Analysis\\n\\nOnce the pipeline has been run, further analysis of the data gathered can be found in the notebook “notebooks/eda/cwynne_combined_analysis.ipynb”. In the third cell of the notebook, specify which dataset that the analysis should be done on, usually selecting one of the generated output files mentioned above. The variable “data_path” should be set to the desired dataset. The target columns can also be changed in this cell, although it is recommended that the defaults are used. \\n\\n\\n### Configuration Parameters\\nThe run.py script will access the configuration file stored in “config/config-scraping.json”. This json file contains different parameters that allow you to change what data is scraped / analyzed. Below are the parameters in further detail.\\n\\n\\n- selected-game: this  key represents what keyword will be used to search for videos. This does not have to be a game, but can be any keyword of your choosing. \\n- thumbnail-qual: This key holds a dictionary which can be thought of as a switch, put a 1 in any quality that you want a dataset of thumbnails for. \\n- test-videos-dir: This is the filepath of the test video folder, which contains files such as the video ids and metadata of the test subset.\\n- test-thumbs-dir: This is the filepath of the directory of thumbnails for the test dataset.\\n- api-service-name: This generally should not change, if the youtube api changes this argument, then this should be changed. \\n- api-version: This is the version of the API, currently the most updated version is “v3”, but this could be changed to use later or earlier versions, however the code has not been tested on different api versions.\\n- videos-dir: This is the location of the currently scraped video data directory. Files in this folder are scraped based on the keyword and include the csv of video ids in the dataset and metadata information. This is only a local directory and is used for anything that is not the test dataset\\n- full-metadata-csv-write-path: This is the filepath to write the metadata for all videos that were scraped. This includes the search result videos and the specified number of videos per channel by the key “videos-per-channel”).\\n- summary-metadata-csv-write-path: This is the filepath to write only the initial search result videos and not the extra channel videos. \\n- requests-dic-read-path: This is the file path for a local json of request data that could exist from a previous run of this pipeline. If nothing is provided it will create a new file using “requests-dic-write-path” that can be used for reruns of the pipeline. This is to prevent re-requesting the same info for videos and making unnecessary API calls.\\n- requests-dic-write-path: This is the path where the requests data json file is written for quicker reruns or saving of new requests data. \\n- num-recent-videos: This is the number of search results that will be gathered based on the “selected-game” keyword being searched with the youtube api. \\n- videos-per-channel: This is the videos per channel that are scraped, based on the different channels that uploaded videos in the search result for our keyword.\\n- scrape-write-dir: This is the directory that all of our scraped video data gets written to. This includes the dataset of video ids, and the metadata dataset for these videos. This changes based on the “selected-game” keyword. \\n- full-features-write-name: This is the name of the file that contains metadata, basic image features, and advanced image features. It is stored in the directory specified by “videos-dir”\\n- overwrite: if set to true, it will overwrite any previously scraped data that was scraped on the same calendar date. If set to false, it will simply add a number to the end of the new requests data file. \\n\\n\\n## References\\nLouise Myers 2019, accessed April 6, 2020, \\n\\n[https://louisem.com/198803/how-to-youtube-thumbnails](https://louisem.com/198803/how-to-youtube-thumbnails)\\n\\nEmpLemon 2020, accessed May 4, 2020,\\n\\n[https://www.youtube.com/watch?v=-6-i75wDIBE](https://www.youtube.com/watch?v=-6-i75wDIBE)\\n',\n",
       "  'The purpose of this project was to analyze the significance of various features in YouTube thumbnails that contribute to a video\\'s success. The researchers used YouTube\\'s Data API to create a dataset of around 10,000 videos sourced from a search for \"Fortnite\". They analyzed both metadata and image features of the thumbnails to determine if there was a correlation with the number of views. However, their analysis found no strong correlation between thumbnail image features and video views. They also explored different visualizations and statistical analyses but did not find any significant results. The researchers suggest that there are many factors that contribute to video views, such as title, video duration, and content, making it difficult to establish a strong correlation with thumbnail image features alone. They also mention that their results are specific to Fortnite gaming videos uploaded in March/April 2020 and that more advanced image features and a larger dataset could yield different results. The researchers believe that their work can still provide insights for content creators in improving their thumbnails and understanding audience reactions. They also suggest future directions for expanding the project, such as analyzing other genres of YouTube videos or creating a live trend analysis of thumbnails.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_06/': [\"# DSC180B Project - Quantifying Style\\n# RestoreNet: Quantifying the Restoration of WWII Documents\\n\\nWelcome to the my current version of the project.\\n\\n### Prerequisites\\n\\nWhat things you need to use the program (TBD)\\n\\n```\\nBeautifulSoup - pip install beautifulsoup4\\nREGEX - pip install regex\\nRequests - pip3 install requests\\n\\nTensorFlow - pip3 install tensorflow\\nPyTorch - pip3 install torch torchvision\\n```\\n\\n### Getting Started\\n\\nFirst, go ahead and clone this repository to your local directory\\n```\\ngit clone https://github.com/Emmanuel-Diaz/DSC180B-Project.git\\n```\\n\\nthen install all necessary packages\\n\\n```\\npip install -r requirements.txt\\n```\\n\\n\\n### Usage\\n\\n\\n**Collecting World War II Images**\\n\\nEnter the following command to collect from [ww2db](http://ww2db.com/photo.php)\\n\\n```\\npython run.py scrape [NUM_IMAGES] [TIME_PERIOD]\\n```\\n\\n```NUM_IMAGES``` Number of World War II images to scrape<br>\\n```TIME_PERIOD``` Time period of images [Pre-War, Mid-War, Late-War]<br>\\n\\n\\n**Computing features**\\nEnter the following command to compute features on the data collected\\n\\n```\\npython run.py features\\n```\\n\\n**Restoring a test example**\\n\\nEnter the following command to run the network on the 'Spoils of War' image, just like in the report output.\\n```\\npython run.py test-project\\n```\\n\\n### Training\\n\\n**Performing a trained restoration**\\nIn order to do a trained restoration, you will either need to\\n1. Download Pre-trained weights found [here]() and place them into your ```data/out``` directory.\\n2. Train your own weights using your collected data. (TBD)\\n\\n**Running with custom image and mask**\\n\\nTo use your own custom image and mask for restoration, please perform the following steps.\\n1. Place your image to be restored ```img``` in the ```data/input``` directory. (```img``` can be ```.png```,```.jpeg```,```.jpg```)\\n2. Place your custom mask ```mask``` in the ```data/input``` directory. (```mask``` must be ```.png```)\\n3. Run the following command with your file names.\\n\\n```\\npython run.py -t [True/False] -i <img> -m <mask.png>\\n```\\n\\n```-t``` Use a trained model (e.g weights are located in ```data/out```)<br>\\n```-i``` Degraded image file name<br>\\n```-m``` Mask image file name<br>\\n\\n\\n## Built With\\n\\n* [Python](https://www.python.org/) - Language used\\n* [PyTorch](https://www.pytorch.org) - Net Framework\\n\\n\\n## Version\\n\\n1.2 Added selective mask inpainting\\n\\n## Author\\n\\n* [Emmanuel Diaz](https://github.com/Emmanuel-Diaz)\\n\\n* *Deep Generative Prior* by Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, Ping Luo \\n\\t* [Report](https://arxiv.org/abs/2003.13659)\\n\\t* [GitHub](https://github.com/XingangPan/deep-generative-prior)\\n\",\n",
       "  'This project, called \"RestoreNet: Quantifying the Restoration of WWII Documents,\" focuses on quantifying the restoration of World War II documents. The project provides instructions on how to collect World War II images, compute features on the collected data, and restore test examples using a network. It also includes information on training the network and running it with custom images and masks. The project is built with Python and PyTorch. The current version is 1.2 and it was authored by Emmanuel Diaz.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_08/': ['# SuperBowlCapstone\\n\\nCollects superbowl and non-superbowl commercials from ispot and adforum respectively.\\nGenerates features from their audio and video content\\nBuilds multiple models based off of the generated features.\\nCreates multiple visualizations.\\n\\n## Usage Instructions\\n\\nPotential run.py arguments:\\n* data: does the webscraping\\n* fxt: feature extraction, requires data to be run\\n* analyze: model and visual generation, requires fxt to be run\\n* test-project: using given test commercials, tests project.\\n\\n## Project Contents\\n\\n```\\nROOT FOLDER\\n├── .gitignore\\n├── README.md\\n├── config\\n│\\xa0\\xa0 ├── data-params.json\\n│\\xa0\\xa0 ├── test-params.json\\n│\\xa0\\xa0 └── env.json\\n├── chosen data folder\\n│\\xa0\\xa0 ├── chosen superbowl commercial folder\\n│\\xa0\\xa0 │\\xa0\\xa0 └── chosen audio folder\\n│\\xa0\\xa0 └── chosen non-superbowl commercial folder\\n│\\xa0\\xa0     └── chosen audio folder\\n├── notebooks\\n│\\xa0\\xa0 └── .gitkeep\\n├── run.py\\n└── src\\n    └── etl.py\\n```\\n\\n### `src`\\n\\n* `ad_scraper.py`: Library code that collects non-superbowl commercials from adforum based off of collected superbowl commercials.\\n* `ispot_scrape.py`: Library code that collects superbowl commercials from ispot.\\n* `video_processing.py`: Library code that generates visual features.\\n* `extract_audio_features.py`: Library code that generates audio features.\\n* `vis_video.py`: Library code that generates visualizations for generated features.\\n* `predictor.py`: Library code that generates predictions from Logistic Regression and Random Forest models.\\n\\n### `config`\\n\\n* `data-params.json`: Common parameters for getting data, serving as\\n  inputs to library code.\\n  \\n* `test-params.json`: parameters for running small process on small\\n  test data.\\n\\n### `notebooks`\\n\\n* Jupyter notebooks for analysis, mainly audio-orientated.\\n',\n",
       "  'This project is focused on collecting Super Bowl and non-Super Bowl commercials from iSpot and Adforum, respectively. It then generates features from the audio and video content of these commercials and builds multiple models based on these features. The project also includes visualizations for the generated features. The `src` folder contains various library code files for different tasks such as scraping commercials, processing video and audio, generating visualizations, and making predictions using logistic regression and random forest models. The `config` folder contains parameter files for data collection and testing, while the `notebooks` folder contains Jupyter notebooks for analysis, mainly focusing on audio-related tasks.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_09/': ['# IlliterateBot\\n\\n\\n### To test our code use the command:\\n\\npython run.py test\\n\\n\\n## Limitations of Code \\n\\n### CNN Feature Vectors\\nParts of the code that we have used we are unable to include into our pipeline. \\nThis includes creating the VGG16 CNN feature vectors, when transforming our book cover images\\nto feature vectors. When running our code that transforms our book cover images to feature vectors on our code, \\nour code would constantly time out, and thus we were only able to run it on our jupyter notebooks where we\\nwould need to split the data accordingly, and save the feature vectors immediately before the kernel crashes, \\nand we would have to run the code again, and save the feature vectors separately before it crashes again. Because of\\nthis, it is difficult to include it into our pipeline without changing the code each time we run. \\n\\n\\n\\n### Tesseract OCR\\nIn our report we discussed our experiences using Tesseract OCR. The code used to generate the visualizations found in our report is in our Tesseract Jupyter Notebook. \\nWe were not able to add this code to our main pipeline since PyTesseract, a Python wrapper for Tesseract, requires a local installation of the Tesseract engine and requires placement directly\\nin the local AppData folder.\\n\\n\\n',\n",
       "  'The code has limitations with regards to creating CNN feature vectors and using Tesseract OCR. The code for creating VGG16 CNN feature vectors constantly times out and can only be run on Jupyter notebooks, requiring data splitting and saving before crashes. The code for generating visualizations using Tesseract OCR is not included in the main pipeline as it requires a local installation of the Tesseract engine in the AppData folder.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_10/': ['# Fair-Policing-Capstone',\n",
       "  'The topic is Fair Policing Capstone.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_11/': ['# DSC180B Capstone Project\\n\\n## Description of Contents\\n\\nThe project consists of these portions:\\n```\\nPROJECT\\n├── .gitignore\\n├── README.md\\n├── config\\n│   ├── data-params.json\\n│   ├── env.json\\n│   ├── model.json\\n│   ├── process-params.json\\n│   ├── test-data-params.json\\n│   ├── test-model.json\\n│   └── test-process-params.json\\n├── imgs\\n├── notebooks\\n│   ├── prop_score.ipynb\\n│   └── sandbox.ipynb\\n├── references\\n│   └── .gitkeep\\n├── requirements.txt\\n├── run.py\\n├── index.html\\n├── index.md\\n└── src\\n│   ├── etl.py\\n│   ├── model_vod.py\\n│   ├── model.py\\n│   └── viz.py\\n```\\n\\n### `src`\\n\\n* `etl.py`: Script to perform Extract Transform Load.\\n* `model_vod.py`: Script to perform Veil of Darkness analysis.\\n* `model.py`: Script to perform propensity score analysis.\\n* `viz.py`: Script to visualize findings from both sets of analysis.\\n\\n### `config`\\n\\n* `data-params.json`: Common parameters for getting data, serving as inputs to library code.\\n* `env.json`: Environment paramters for GitHub repository and Docker image.\\n* `model.json`: Model parameters for for performing propensity score analysis.\\n* `process-params.json`: Common parameters for cleaning and processing data.\\n* `test-data-params.json`: Common parameters for getting test data, serving as inputs to library code.\\n* `test-model.json`: Model parameters for for performing propensity score analysis on test data.\\n* `test-process-params.json`: Common parameters for cleaning and processing test data.\\n\\n### `notebooks`\\n\\n* `prop_score.ipynb`: Imports code from `src` for the purpose of running the propensity score analysis. \\n* `sandbox.ipynb`: Imports code from `src` for the purpose of analysis. \\n\\n### Description of Targets\\n\\n* `!python run.py data`: Collects traffic stops data from Stanford Open Policing Portal and cleans it.\\n* `!python run.py model`: Performs propensity score and veil of darkness analysis on traffic stops.\\n* `!python run.py test-project`: Ingests, cleans, and runs model on a subset of the traffic stops data for the purpose of testing.',\n",
       "  'The DSC180B Capstone Project consists of several components including source code, configuration files, notebooks, and targets. The `src` directory contains scripts for ETL, Veil of Darkness analysis, propensity score analysis, and visualization. The `config` directory contains JSON files with parameters for data processing and model analysis. The `notebooks` directory includes Jupyter notebooks for running the propensity score analysis and performing general analysis. The project targets include collecting and cleaning traffic stops data, performing propensity score and veil of darkness analysis on the data, and testing the model on a subset of the data.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_12/': ['# DSC180B\\nExploring Predictive Policing in San Diego for DSC180B Capstone Project\\n\\n[Here](https://chuanyuanyeh.github.io/predpol_study/) is the link to the website.\\n\\nLink to the GIS map can be found at https://arcg.is/1CmX0r\\n\\n## Usage Instructions\\n\\nTo replicate the entire (or subsets of the) project, copy and paste `python run.py` in the command line while in the root directory followed by the arguments below:\\n* `data`: Ingests raw data from online sources.\\n* `process`: Runs the pipeline for cleaning and formatting raw datasets.\\n* `eda`: Performs exploratory data analysis and outputs visualizations.\\n* `analyze`: Performs statistical tests on differences in observed proportions between PredPol and non-PredPol instances.\\n* `test-project`: Runs the entire pipeline from start to end on a smaller, versioned test data.\\n\\nFor example, running the code below would reproduce the entire project:\\n\\n`python run.py data process eda analyze`\\n\\n## Description of Contents\\n\\nThe project consists of these portions:\\n```\\nPROJECT\\n├── config\\n    ├── data-params.json\\n    ├── process-params.json\\n    ├── eda-params.json\\n    ├── analyze-params.json\\n    ├── test-data-params.json\\n    ├── test-process-params.json\\n    ├── test-eda-params.json\\n    ├── test-analyze-params.json\\n    └── env.json\\n├── data\\n    ├── raw\\n    └── cleaned\\n├── notebooks\\n    └── .gitkeep\\n├── references\\n    ├── arrest_charges.json\\n    ├── arrest_types.json\\n    ├── crime_charges.json\\n    ├── crime_types.json\\n    ├── divisions_mapper.json\\n    ├── nhgis0005_ds172_2010_block_codebook.txt\\n    └── races.json\\n└── src\\n    ├── etl.py\\n    ├── eda.py\\n    ├── analyze.py\\n    └── geospatial.py\\n├── test_data\\n    ├── raw\\n    └── cleaned\\n├── viz\\n    ├── EDA\\n        ├── Arrests\\n        ├── Crime\\n        └── Stops\\n    └── Analysis\\n        ├── Arrests\\n        ├── Crime\\n        └── Stops\\n├── .gitignore\\n├── Dockerfile\\n├── README.md\\n├── requirements.txt\\n├── run.py\\n```\\n\\n### `config/`\\n\\n* `data-params.json`: Common parameters for getting data, serving as\\n  inputs to library code.\\n* `process-params.json`: Parameters for processing data.\\n* `eda-params.json`: Parameters for exploratory analysis on each dataset.\\n* `analyze-params.json`: Parameters for statistical testings and analyses.\\n* `env.json`: Parameters for loading virtual environment.\\n* Also contains similar configurations for test data.\\n  \\n### `data/`\\n\\n* `raw/`: Raw datasets from original source.\\n* `cleaned/`: Cleaned datasets.\\n\\n### `notebooks/`\\n\\n* Jupyter notebooks for *analyses* and *code development*\\n  - notebooks will be removed after migration to library code.\\n\\n### `references/`\\n\\n* Data Dictionaries, references to external sources.\\n\\n### `src/`\\n\\n* `etl.py`: Library code that executes tasks useful for getting data.\\n\\n### `test_data/`\\n\\n* Versioned test data.\\n\\n### `viz/`\\n\\n* Visual outputs from EDA and analyses pipelines.\\n\\n### `Dockerfile`\\n\\n* Docker image to replicate the environment the project was developed in. \\n\\n### `requirements.txt`\\n\\n* Python libraries/modules used as well as their corresponding versions.\\n\\n### `run.py`\\n\\n* Main driver for project replication\\n',\n",
       "  'This is a summary of the DSC180B Capstone Project on Exploring Predictive Policing in San Diego. The project includes various components such as data ingestion, data processing, exploratory data analysis, statistical tests, and visualization. The project also provides instructions on how to replicate the project using the command line. The project structure includes directories for configuration files, raw and cleaned data, notebooks, references, source code, test data, visualizations, Dockerfile for replication environment, requirements.txt for required libraries/modules, and run.py as the main driver for project replication.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_13/': ['# RML vs Traffic Collisions\\nHow recreational marijuana legalization(RML) affects traffic-related scenes in California using Difference in Differences model.\\n',\n",
       "  'This study examines the impact of recreational marijuana legalization (RML) on traffic-related incidents in California using a Difference in Differences model.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_14/': ['# DSC180B Visualization\\n\\nhttps://siqihuang47.github.io/dsc180b_visualization/\\n',\n",
       "  'The link provided leads to a webpage for DSC180B Visualization.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_15/': ['# DSC180BPipeline\\nOur pipleine allows you to download data from the GWAS Catalog, Clean the data, Merge the data, and then create plots such as Q-Q plots, P-value Histograms, and Manhattan Plots. These plots allow you to analyze the GWAS data and to determine which SNPs are significant. Our pipeline runs on a small data set but the images it produces is not very significant because the data is to small. When we run the data on the actual data we are using the images are much better and meaningful to analyze. \\nTo run the pipepline on the sample data in the direction run this command in the terminal:\\n  python run.py test-project\\nThis command should output a histogram plot, a q-q plot, and a Manhattan plot.\\n\\nTo run the data downloading portion run this command:\\n  python run.py test\\nThis command should output a directory of the data you are trying to download.\\n\\nTo run the cleaning and plot making portion run this command:\\n  python run.py clean\\n This command should clean the data and output the charts.\\n \\nYou can modify the code to work for you specific trait you are analyzing from the GWAS Catalog in the data collection portion and the whole pipeline should work\\n',\n",
       "  'The DSC180BPipeline allows you to download, clean, merge, and analyze GWAS data. It generates plots such as Q-Q plots, P-value histograms, and Manhattan plots to identify significant SNPs. The pipeline works better with larger datasets. To run the pipeline on sample data, use the command \"python run.py test-project\" in the terminal. To download data, use \"python run.py test\", and to clean and generate plots, use \"python run.py clean\". The code can be modified for specific traits in the GWAS Catalog.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_16/': [\"# Predicting Disease Risk Through Machine Learning\\n\\nTraditional epidemiology techniques, most notably polygenic risk scoring, have been used by researchers and well-known companies, such as Takeda, MiCom Labs, and 23andMe, to calculate the disease risk of patients and consumers. However, recent research has shown limitations in polygenic risk scoring due to its inability to model high dimensional data with complex interactions (Wai, 2019). As humans, millions of potentially disease-contributing genetic variants exist in the genome, so the inability to leverage such information limits the power of polygenic risk scoring to accurately determine the disease risk of individuals. In this project, the viability of machine learning in disease risk prediction for Coronary Artery Disease, Alzheimer’s, and Diabetes Mellitus is explored. It is shown how machine learning models, including Support Vector Machines (SVMs), Logistic Regression, K Nearest Neighbors, Decision Trees, Random Forest, and Gaussian Naive Bayes, compare in their ability to effectively predict disease risk and how they may offer alternate and possibly better methods over traditional techniques. \\n\\n## Usage Instructions\\n\\nIn order to use the different components of this project, please run `python run.py` along with a target of your choice:\\n\\n* `clean`: Cleans the data directory\\n* `data`: Downloads the data from GWAS Catalog according to data-params.json\\n* `simulate-one`: Simulates a SNP population for the training GWAS\\n* `simulate-both`: Simulates a SNP population for both the training GWAS and the test GWAS\\n* `model`: \\n   * If there is no simulated data for the test GWAS: \\n          Splits the training GWAS simulated data into a train and test subset. Model is trained on the training subset, filtered to only contain SNPs also present in the test GWAS, in order to simulate sampling. The model is then tested on the test subset and results are reported (and saved).\\n   * If there is simulated data for the test GWAS (run via `simulate-both` target):\\n          Model is trained on simulated data (filtered to contain SNPs present in both GWAS's) from the training GWAS. Model is tested on simulated data from the test GWAS and results are reported (and saved).\\n* `test-project`: Tests project using test data\\n* `run-project`: Runs entire project according to config files\\n\\n## Description of Contents\\n\\nThe project consists of these portions:\\n```\\nPROJECT\\n├── config\\n│\\xa0\\xa0 ├── data-params.json\\n│\\xa0\\xa0 ├── env.json\\n│\\xa0\\xa0 ├── model-params.json\\n│\\xa0\\xa0 └── test-params.json\\n├── notebooks\\n│\\xa0\\xa0 ├── Build_Model.ipynb\\n│\\xa0\\xa0 └── Simulate_Data.ipynb\\n├── src\\n│\\xa0\\xa0 ├── etl.py\\n│\\xa0\\xa0 ├── model.py\\n│   └── visualize_data.py\\n├── testdata\\n│\\xa0\\xa0 ├── alzheimer's\\n│\\xa0\\xa0 ├── coronary_artery\\n│\\xa0\\xa0 └── diabetes_type1_melittus\\n├── .gitignore\\n├── README.md\\n├── requirements.txt\\n└── run.py\\n```\\n\\n### `root`\\n\\n* `run.py`: Python script to run main command.\\n\\n### `src`\\n\\n* `etl.py`: Library code that executes tasks useful for getting data and transforming it into a machine-learning-ready format.\\n\\n* `model.py`: Library code that builds and tests multiple models, and reports the results.\\n\\n* `visualize_data.py`: Library code that generates a variety of visualization that are useful for analysis.\\n\\n### `config`\\n\\n* `data-params.json`: Parameters for downloading data from the GWAS Catalog and preparing for model building\\n\\n* `env.json`: Environment information\\n\\n* `model-params.json`: Contains the (sklearn) models that are tested, and the parameters to use for each model.\\n\\n* `test-params.json`: Parameters for preparing test data for model building\\n\\n### `testdata`\\n\\nThis directory contains two summary data files from the GWAS catalog for different diseases, one is used for building a training set and the other is use for a test set. Which is which can be found in the `data-params.json` configuration file (and changed).\\n\\n* `alzheimer's`: Contains two summary statistics CSV's from GWAS studies on Alzheimer's Disease.\\n\\n* `coronary_artery`: Contains two summary statistics CSV's from GWAS studies on Coronary Artery Disease.\\n\\n* `diabetes_type1_melittus`: Contains two summary statistics CSV's from GWAS studies on Diabetes Type I.\\n\\n### `notebooks`\\n\\n* `Build_Model.ipynb`: Notebook walking through the model building/validation process\\n\\n* `Simulate_Data.ipynb`: Notebook walking through the population simulation process\\n\",\n",
       "  \"This project explores the use of machine learning in predicting disease risk for Coronary Artery Disease, Alzheimer's, and Diabetes Mellitus. Traditional techniques like polygenic risk scoring have limitations in modeling complex data interactions. The project compares different machine learning models like Support Vector Machines, Logistic Regression, K Nearest Neighbors, Decision Trees, Random Forest, and Gaussian Naive Bayes to determine their effectiveness in predicting disease risk. The project also provides usage instructions and a description of its contents, including code files and data directories.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_17/': ['# Clustering-Germ-Layers\\n\\n## __General__\\n\\nData scraping from https://portal.gdc.cancer.gov/ through selenium for analysis across the different germ layers and cancers. \\n\\n<br>\\nThis is done through the use of Google\\'s chromedriver. The chromedriver.exe included is for Windows with Chrome 83. If this does not work for your computer, go to this site https://chromedriver.chromium.org/ and download the matching driver and change chrome_driver_location to that of the correct one. MacOS may need to rename the driver to include the .exe extension.\\n\\n<br><br>\\n\\n## __File Usage__\\n\\n### [Param_config.json](config/param_config.json)\\n\\n- createDict \\n    - Required keys: \\n        - chrome_driver_location\\n        - data_dict (if not specified in command line)\\n            - if done through command line, remember to change it in param_config to file you want to use\\n    - Optional:\\n        - headless\\n        - time_wait, implicit_wait, after_sort_wait\\n- queryData\\n    - Required:\\n        - chrome_driver_location\\n        - data_dict (made from createDict)\\n        - samples\\n    - Optional:\\n        - headless\\n        - time_wait, implicit_wait, after_sort_wait\\n        - sort_using, sort_direction\\n        - file_names\\n- downloadData\\n    - Required:\\n        - chrome_driver_location\\n        - keep_tar_files, tar_dir, maf_dir\\n        - manual_csv_files (if not specified by pattern on the command line)\\n        - download_inds\\n    - Optional:\\n        - headless\\n        - time_wait, implicit_wait, after_sort_wait, download_wait\\n\\n\\nExample:\\n```\\n{\\n    \"chrome_driver_location\" : \"chromedriver.exe\",\\n    \"headless\" : true,\\n    \"data_dict\": \"references/data_dictionary.csv\",\\n    \"file_names\" : [\"Q1.csv\", \"Q2.csv\"],\\n    \"sort_using\" : [\"Size\", \"Project\"],\\n    \"sort_direction\": [\"up\", \"up\"],\\n    \"samples\": [133, 272],\\n    \"time_wait\" : 2,\\n    \"implicit_wait\" : 3,\\n    \"after_sort_wait\" : 5,\\n    \"download_wait\" : 60,\\n    \"manual_csv_files\" : [\"Q1.csv\"],\\n    \"download_inds\" : [\"1-2,7-12\"],\\n    \"keep_tar\" : false,\\n    \"tar_dir\" : \"testdata/tars\",\\n    \"maf_dir\": \"testdata/mafs\"\\n}\\n```\\n<br>\\n\\n### [Query_config.json](config/query_config.json)\\n\\n- queryData\\n    - All categories are up to user to pick and choose. A list of categories, types, and descriptions are in data dictionary file made using createDict.\\n        - Number after is the number of queries to be made using it so make sure that the numbers used match. The queries are made in order.\\n            - Must match with files_names and samples from param_config.json\\n        - Ex.  \\n        > { \"files.data_format\" : [\"maf\", 2] } -> [\"files.data_format in [\"maf\"]\", \"files.data_format in [\"maf\"]\"] \\n\\n        > { \\\\\\n            \"data_format\" : [\"maf\", 1, \"vcf\", 1], \\\\\\n            \"access : [\"open\", 2] \\\\\\n          } -------> \\\\\\n          [\"files.data_format in [\"maf\"] and files.access in [\"open\"], \"files.data_format in [\"vcf\"] and files.access in [\"open\"]]\\n\\n    - Incomplete category names may be autofilled to include the starting class name by the dictionary file, if they match to existing entries. \\n        - Ex.\\n        > { \"data_format\" : [\"maf\", 2] } -> [\"files.data_format in [\"maf\"]\", \"files.data_format in [\"maf\"]\"]\\n\\n        > { \"data_for\" : [\"open\", 2] } -> Error\\n\\n\\nExample:\\n```\\n{\\n    \"data_category\" : [\"Simple Nucleotide Variation\", 2],\\n    \"data_format\" : [\"maf\", 1, \"vcf\", 1],\\n    \"cases.primary_site\" : [\"bronchus and lung\", 2],\\n    \"file_size_min\": [\"> 1000\", 2],\\n    \"file_size_max\": [\"< 10MB\", 2],\\n    \"access\" : [\"open\", 2]\\n}\\n```\\n<br><br>\\n\\n## __Command Line used by [run.py](run.py)__\\n\\n<> means optional argument\\n```\\n(base) py run.py createDict config/param_config.json <Data_dict.csv>\\n```\\n- createDict creates a dictionary file in location specified in param_config.file. Overidden by CSV path on command line. \\n    - if want to use command line CSV in queryData, then change the data_dict path in the param_config.file\\n\\n<br>\\n\\n```\\n(base) py run.py queryData config/param_config.json config/query_config.json\\n```\\n- queryData creates CSV files that match parameters in both the parameter and query config files with the URLs that link to the matching files.\\n\\n<br>\\n\\n```\\n(base) py run.py downloadData config/param_config.json <CSV matching pattern> <pattern 2> <...>\\n```\\n- downloadData uses indicies from param_config file and either the entered CSV files in param_config or the ones specified in the command line to find the files to download. The created annotations file is the same name as the created maf.gz file for ease of matching together.\\n\\n<br><br>\\n\\n## Appendix\\n\\nParameter file keys are the only ones included here as all information on the query keys can be found from the data file created by createDict. Grouped if highly related.\\n\\nUsed by more than one command\\n\\n- chrome_driver_location \\\\\\n    Path location of the chromedriver.exe file \\n\\n- data_dict \\\\\\n    Data dictionary file made from createDict and can be used in queryData. In createDict, this value can be overridden by the CSV named in the commandline and only the latter CSV file is created.\\n\\n    - It is scraped from TCGA using a vague query that holds all the different combinations for the keys. However, the possible values for those keys must be found from https://portal.gdc.cancer.gov/query and searching those terms in the query bar. This gives a list for all the possible values for that key. \\n    - This file is only to locally see all the keys, types, and descriptions, as well as to help fix some user input errors in queryData.\\n\\n- headless \\\\\\n    Set to False if want to see movement across the site. \\\\\\n    **WARNING**: Webpages load faster when True so if code breaks, then either change back or increase wait times.\\n    \\n- time_wait, implicit_wait, after_sort_wait, download_wait \\\\\\n    Timing variables to change. There are default values if they are not specified; however, slower connections must be fixed by increasing these times so page is loaded.\\n\\n    - Time_wait is the time the code waits after each step.\\n    - Implicit_wait is the time the code waits after loading a new page.\\n    - After_sort_wait is the time waiting for the data to be sorted which usually is kept higher than the others as it takes much longer to do.\\n        - However, if you are not sorting the data then setting the time much longer works.\\n    - Download_wait is the time that the program waits to download each file so larger files can be given more time to download.\\n\\nqueryData\\n\\n- file_names \\\\\\n    The list of CSV names that the queryData results will be saved to. If not specified, uses default names for queries.\\n\\n- samples \\\\\\n    The number of top results from a query search that will be saved to that query\\'s CSV file. The max samples per CSV file is 1000 results.\\n\\n- sort_using, sort_direction \\\\\\n    Before the data is scraped, it is sorted by user specifications and then the top results are saved to the CSV file in queryData. If not specified, then uses the default sorting that TCGA stores files as. \\n    - Sorts are done in order given so sorting by [\"Size\",\"Project\"] is different than [\"Project\", \"Size\"]\\n    - Sort_using values are limited to:\\n        - \"Access\", \"Data Category\", \"Data Format\", \"File Name\", \"Project\", \"Size\"\\n    - Sort_direction gives direction of sorting and is limited to:\\n        - \"Up\", \"Down\"\\n\\n\\ndownloadData\\n\\n- download_inds \\\\\\n    Indexes of the files in manual_csv_files that want to be downloaded to make downloading specific samples easier.\\n    - Ex. \\n    > array_conv([\"1-3,5\", \"1,3-4\"]) => [[1,2,3,5], [1,3,4]]\\n    \\n    This downloads the files at these positions from the specified CSV files. \\\\\\n    **WARNING** : As the first row of the CSV file is the column names, row 2 is the first data value and python starts at a 0 index so [1] points to the 3rd row in the CSV file. \\n\\n- keep_tar_files, tar_dir, maf_dir    \\n    - keep_tar_files is a boolean, that when True, says to keep the downloaded tar files along with the extracted maf.gz files. Otherwise, only the maf.gz files.\\n    - tar_dir is the directory where the chromedriver will save the downloaded tar files into.\\n    - maf_dir is the directory that the extracted maf.gz and corresponding annotation files are placed into.\\n\\n    **WARNING**:  If there is an outer directory used by tar_dir and maf_dir and it is not created before running downloadData, the code will break.\\n    - In the param_config.json example, the testdata directory must be made in advance. However, the individual directories of tar_dir and maf_dir will be made through the code if they do not already exist.\\n\\n- manual_csv_files \\\\\\n    CSV file names that hold the location of the desired files to be downloaded. If new pattern is given in command line, this is overridden by CSVs found using that pattern. However, these CSVs found using the pattern are still subject to matching the download_inds.\\n',\n",
       "  \"This document provides an overview of the Clustering-Germ-Layers project, including information on file usage and command line instructions. The project involves data scraping from a cancer database using Selenium and Google's chromedriver. The document includes details on the parameter and query configuration files used in the project, as well as examples of their contents. It also provides a summary of the command line commands used in the project, including createDict, queryData, and downloadData.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_18/': ['\\n# Understanding miRNA in pre-Type 1 Diabetes\\n\\nBy Pete Sheurpukdi & Derrick Liu\\n\\nThis repository contains the code used our the paper [Understanding miRNA in pre-Type 1 Diabetes](https://github.com/Derrick56007/miRNA_preT1Diabetes/raw/master/report.pdf)\\n\\nRequirements\\n------------\\n\\n- Docker: https://docs.docker.com/get-docker/\\n\\nInstall\\n--------------\\n\\n```\\ndocker pull derrick56007/mirna_pre_t1d:latest\\n```\\n\\nUsage\\n------------\\n\\n```\\ndocker run -ti derrick56007/mirna_pre_t1d:latest python run.py test-project\\n```\\n',\n",
       "  'This repository contains the code used in the paper \"Understanding miRNA in pre-Type 1 Diabetes\". The code can be installed using Docker and run using the provided command.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_19/': [\"# DSC180B_Genome_Project -- GWAS on Alzheimer’s Disease\\n\\n### Tony Zhang, Zhuoyuan Ren, Haoshu Qin\\n\\nVisualization Website: https://tonyzhanghm.github.io/adgwas_website/\\n\\n## Introduction\\n\\nIn this study, we conducted a Genome-Wise Analysis Study on Late-Onset Alzheimer's Disease (LOAD). For experiment details, please refer to the [paper](https://github.com/TonyZhanghm/DSC180B_Genome_01/blob/master/GWAS_on_Alzheimer_s_Disease_report.pdf). \\n\\n## Usage\\n\\n### Environment (Docker)\\nDocker images: https://hub.docker.com/repository/docker/tonyzhanghm/genetics\\n\\n### Commands\\nClone the repo: `git clone https://github.com/TonyZhanghm/DSC180B_Genome_01.git`\\n\\nTo run the whole experiment: `python run.py test-project`\\n\\nTo run the project step by step: `python run.py` with following flags:  \\n`get_data`: download the raw data  and the tools needed.   \\n`filter`: filter the dataset with [PLINK 1.9](https://www.cog-genomics.org/plink/). The specific parameter choices could be found in the paper.    \\n`pca`: run principal component analysis with [PLINK 1.9](https://www.cog-genomics.org/plink/).  \\n`plot_pca`: plot pariplots for the first 5 principal components with [seaborn](https://seaborn.pydata.org/).  \\n`plot_eigenval`: plot the scree plot.   \\n`logistic`: run the association test with logistic regression.   \\n`manhattan`: plot the manhattan plot with [bioinfokit](https://reneshbedre.github.io//blog/howtoinstall.html).  \\n`regional`: plot regional plots for the nine genes of interests.   \\n`qqplot`: plot a qqplot on the test results.  \\n`meta`: run metal analysis with [METAL](https://genome.sph.umich.edu/wiki/METAL_Documentation).  \\n\\nThe data will be stored in `data/` and the experiment results will be store in `data/output/`. \\n\\n## Development Updates\\n\\n### Checkpoint-1 (04/12/2020)\\n- Request data from source: UK Biobank and NIAGADS\\n- Understand the analysis methods: meta analysis, Manhattan plot, regional association plot. \\n- Write a survey of the data you are using, the relationship and appropriateness of the data to the problem under examination, and the context in which the data was created.\\n- Summarize relevant details of the data generating process, describing the population that the data represents, whether that population is relevant to the question at hand, while addressing possible questions of data reliability.\\n- Understand how to use population stratification on our data so that it can apply to other races besides European descent.\\n- no new code added\\n\\n### Checkpoint-2 (04/26/2020)\\n- Describe the source of the backup dataset, the population that the data represents, whether that population is relevant to the question at hand, while addressing possible questions of data reliability. (Scott)\\n- Perform preprocessing quality controls using Plink commands (Jared, Tony)\\n- Statistically assess the quality of the data (Tony)\\n- EDA (Barplot, PCA, Scatter matrix plot, Scree Plot) (All)\\n- Perform multi-covariate association analysis with logistic regression (Tony)\\n\\n\",\n",
       "  \"This study conducted a Genome-Wise Analysis Study on Late-Onset Alzheimer's Disease (LOAD). The researchers provide a visualization website and instructions on how to run the experiment. The study includes steps such as data filtering, principal component analysis, association testing, and plot generation. Development updates mention data collection, analysis methods, data reliability, and quality control.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_20/': ['# HinDroid-with-Embeddings\\n\\n![Docker Cloud Build Status](https://img.shields.io/docker/cloud/build/davidzz/hindroid-xl)\\n\\n## Overview\\n\\nMalware detection for android applications is an expanding field with the introduction of the [Hindroid](https://www.cse.ust.hk/~yqsong/papers/2017-KDD-HINDROID.pdf) model (DOI:[10.1145/3097983.3098026](https://doi.org/10.1145/3097983.3098026)). It proposes a method that transforms the semantic relationships between Android applications and their decompiled source code to a Heterogeneous Information Network (HIN) and uses similarities from various meta-paths between apps to construct a model for malware classification. To further explore the field, we aim to extend the HinDroid model to improve the accuracy in specific subsets of the AMD dataset. Our effort will be focused on finding better representations for both apps as well as APIs and discovering methods to incorporate them as additional features in a new model. In the meantime, we plan to evaluate how the proposed model captures the features that are relevant to the classification task and compare to that of the HinDroid baseline. Our contributions can be utilized in systems where the analysis of malware and interpretable features are more important than mere detection.\\n\\n## Usage\\nDocker image on Docker Hub:\\n[davidzyx/hindroid-xl](https://hub.docker.com/repository/docker/davidzz/hindroid)\\n\\nOn Datahub, create a pod using a custom image with 4 CPU and 32 GB of memory. If you use another configuration, use at least 6GB memory per CPU.\\n```bash\\nlaunch.sh -i davidzz/hindroid-xl -c 4 -m 32\\n```\\n\\nModify the config file located in `config/data-params.json`. If you want to run a test drive, use `config/test-params.json`. Put either `data` or `test` as the first argument.\\n\\nThe HinDroid baseline uses the driver file `run.py` with 3 targets: `ingest`, `process`, and `model`. Put each target space-separated as arguments in the call. To run the whole pipeline, use\\n```bash\\npython run.py data ingest process model\\n```\\n\\n`process` target will save `.npz` files in `data/processed/` for generating various embeddings.\\n\\n## System Prerequisites and Definitions\\n\\n- APK - Executable file for Android\\n- Smali code - Human readable code decompiled from Dalvik bytecode contained the APK\\n- Heterogeneous Information Network (HIN) - A graph where its nodes may not be of the same type\\n- API Extraction\\n  - Use regex to match specific patterns\\n  - API calls and method blocks\\n![api](https://i.imgur.com/nx3rKKv.png)\\n\\n## HinDroid Efficiency Improvements\\n\\nIn HinDroid, the amount of APIs that are used in the final gram matrix calculations can have dimension of several millions. Even in sparse format, these matrices take up huge computational resources for calculation. We are able to reduce the number of API used in HinDroid to 1000 APIs while retaining the same level of accuracy. Both the time and space complexity for training and inference can be improved by a few orders of magnitude. We achieve this by selecting the top important (larger absolute coefficients) APIs (words) from fitting a logistic regression on each app (document) after applying BM25 extraction on the counts of each API for each app.\\n\\n| Method            | # of Apps | # of APIs used | RAM used | train+test time |\\n|-------------------|-----------|----------------|----------|-----------------|\\n| HinDroid-original | 1670      | 2024313        | 68GB     | 3h29m41s        |\\n| HinDroid-reduced  | 1670      | 1000           | 0.3GB    | 20s             |\\n\\n## Embedding Techniques Explored\\n\\nAs we are using NLP approaches such as word2vec which cannot be directly applied to application source code and matrices from HinDroid, our data ingestion pipeline generates text corpus by traversing the graphs following an user defined metapaths and the length of a random walk. Using a metapath `ABPBA` with random walk length 5000, the text corpus may look like \\n\\n`app_3 -> api_500 -> api_321 -> api_234 -> api_578 -> app_321 -> api_123…`\\n\\nwhere each token represents an application or api node and the neighbouring tokens are\\nconnected by edges in the graph.\\n\\n### Word2Vec\\n\\nIn a graph where there are two types of nodes: application and api, Word2Vec is the first approach that we attempt to capture the relationship beyond application and apis that have a direct connection in the graph. This traditional and powerful NLP embeddings techniques helps us to learn the similarity between applications not just limited to the shared api, and also the ability to identify the clusters connection between application and api that do not always have a direct connection. Using gensim’s word2vec model, we are able to generate vector embeddings for each application and api found in the text corpus. We successfully converted decompiled Android source code into a vector of numbers for each application and this information can be easily used in a machine learning model.\\n\\nTo evaluate the effectiveness of the generated embeddings, we visualize the embedding clusters by applying dimensionality reduction into two dimensional vectors. The embedding visualization for metapath APA is shown below:\\n\\n![APA](https://i.imgur.com/TnPyamV.png)\\n\\nAs word2vec does not generate embeddings for unseen words, test applications in our case, we trained a decision tree regressor using the true embeddings for training application as the labels, and the average of all the embeddings of each application’s associated api as the training data. Using this regressor we are able to generate embeddings for test applications using its associated api appear in the training corpus.\\n\\n### Node2Vec\\n\\nIn node2vec, the entire Heterogeneous Information Network is regarded as an large homogeneous graph and the only theoretical difference to the word2vec approach is the random walk procedure. The graph traversal method is based on a graph where all different types of edges are merged together to be one. This change is adapting to the inability of node2vec to traverse according to a metapath but instead a truly random walk with no specific rules restricting where the next node would be. We choose a return parameter of 2 and a in-out parameter of 1 empirically to perform walks beginning on each app node for 100 times. This results in a corpus similar to the word2vec approach, so we could use the same methodology to match and predict different distributions of app and API embeddings.\\n\\nAs node2vec also does not generate embeddings for test appication, we use the similar approach to generate the embeddings for test application using associated API embeddings.\\n\\n![node2vec](https://i.imgur.com/auK5rqj.png)\\n\\n### Metapath2Vec\\n\\n![metapath2vec equation](https://i.imgur.com/AINz4lr.png) (1)\\n\\nMetapath2Vec is used as a technique of sampling our next node. We sample our next node using equation (1). Let\\'s use an example to illustrate the process.\\n\\nImagine that we have these matrices set up, and our defined metapath is **ABA**. Our metapath-chosen sentence will look like \"app_A API_Y API_Z app_B\". An sample matrix looks something like the following:  \\n![Matrices](https://i.imgur.com/XIYFrc3.png)\\n\\nSimplified steps:\\n\\n1. Pick an app. This will replace app_A.\\n2. Go to the matrix corresponding to the metapath. For example, the first path in **ABA** is A, so we will look at the A matrix.\\n3. Go to the row corresponding to the app or API that was chosen.\\n4. Pick an API. Within a row, the APIs that have a value of 1 is picked using a uniform probability.\\n5. Repeat 2, 3, and 4 until you are ready to pick an app (app_B). With the API that was chosen (API_Z), look at the column and pick an app that has value 1 with uniform probaility.\\n\\n![ABA](https://i.imgur.com/8N8IeYi.png)\\n![ABPBPBBPA](https://i.imgur.com/Wi5C3KW.png)\\n![ABABBABBBABBBBABBBBBA](https://i.imgur.com/etgIVjM.png)\\n\\n\\n## Results\\n\\nLet\\'s take a look at the different accuracies for the original HinDroid approach and the HinDroid approach with additional embedding techniques.\\n\\n**HinDroid:**\\n\\n| Metapath | train_acc | test_acc | F1     | TP    | FP   | TN    | FN   |\\n|----------|-----------|----------|--------|-------|------|-------|------|\\n| AA       | 1.0000    | 0.9561   | 0.9562 | 158   | 10   | 147   | 4    |\\n| APA      | 1.0000    | 0.9373   | 0.9412 | 155   | 14   | 145   | 6    |\\n| ABA      | 0.9149    | 0.8558   | 0.8671 | 147   | 27   | 130   | 19   |\\n| APBPA    | 0.9040    | 0.8339   | 0.8408 | 140   | 32   | 126   | 22   |\\n\\n**Reduced:**\\n\\n| Metapath | train_acc | test_acc | F1     | TP    | FP   | TN    | FN   |\\n|----------|-----------|----------|--------|-------|------|-------|------|\\n| AA       | 1.0000    | 0.9561   | 0.9562 | 158   | 10   | 147   | 4    |\\n| APA      | 1.0000    | 0.9373   | 0.9412 | 155   | 14   | 145   | 6    |\\n| ABA      | 0.9149    | 0.8558   | 0.8671 | 147   | 27   | 130   | 19   |\\n| APBPA    | 0.9040    | 0.8339   | 0.8408 | 140   | 32   | 126   | 22   |\\n\\n**Word2Vec**\\n\\n| Metapath  | Accuracy | F1     | TN  | FP | FN | TP  |\\n|-----------|----------|--------|-----|----|----|-----|\\n| ABA       | 95.06%   | 95.02% | 639 | 32 | 34 | 630 |\\n| ABPBA     | 94.61%   | 94.59% | 634 | 37 | 35 | 629 |\\n| APA       | 95.73%   | 95.68% | 647 | 24 | 33 | 631 |\\n| APBPA     | 94.61%   | 94.55% | 638 | 33 | 39 | 625 |\\n\\n**Node2Vec**\\n\\n| Metapath  | Accuracy | F1     | TN  | FP | FN | TP  |\\n|-----------|----------|--------|-----|----|----|-----|\\n| N/A       | 97.75%   | 97.77% | 648 | 18 | 12 | 657 |\\n\\n**Metapath2Vec:**\\n\\n| Metapath              | train_acc | test_acc | F1     | TP    | FP   | TN    | FN   |\\n|-----------------------|-----------|----------|--------|-------|------|-------|------|\\n| AA                    | 0.9736    | 0.9476   | 0.9466 | 621   | 27   | 644   | 43   |\\n| APA                   | 0.9955    | 0.9296   | 0.9277 | 603   | 33   | 638   | 61   |\\n| ABA                   | 0.9864    | 0.9633   | 0.9626 | 630   | 15   | 656   | 34   |\\n| APBPA                 | 0.9900    | 0.9438   | 0.9419 | 608   | 19   | 652   | 56   |\\n| ABPBA                 | 0.9982    | 0.9524   | 0.9524 | 614   | 10   | 661   | 50   |\\n| ABPBPBBPA             | 0.9973    | 0.9476   | 0.9455 | 607   | 13   | 658   | 57   |\\n| ABABBABBBABBBBABBBBBA | 0.9545    | 0.9026   | 0.9027 | 603   | 69   | 602   | 61   |\\n\\n## Conclusion\\n\\nFrom our initial testing, all of our proposed graph embedding techniques are able to achieve similar accuracy and metrics scores. Although it does not seem that the different graph embeddings obtained a higher accuracy score, we believe that using these other graph embedding techniques not only matches with the results of HinDroid, but are also more robust in the sense that hackers are not able to easily rearrange APIs to avoid detection.  \\n\\nThere is definitely further research to do. One being where dummy nodes are added. This will provide us with more evidence of the robustness of the different graph techniques used. The clusters that are present in the visualizations are suggestive of further analysis. Also, we should test out several more different meta-paths. We can test which meta-paths work the best and investigate why it works. We could also do concatenation of embeddings from different meta-paths to form a longer representation and test its effectiveness. \\n\\n## References\\n\\n[1] Passi, Harpreet. Introduction to Malware: Definition, Attacks, Types and Analysis. GreyCampus  \\n[2] Hou, Shifu and Ye, Yanfang and Song, Yangqiu and Abdulhayoglu, Melih. 2017. HinDroid: An Intelligent Android Malware Detection System Based on Structured Heterogeneous Information Network.  \\n[3] Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey. 2013. Efficient Estimation of Word Representations in Vector Space.  \\n[4] Grover, Aditya and Leskovec, Jure. 2016. node2vec: Scalable Feature Learning for Networks.  \\n[5] Dong, Yuxiao and Chawla, Nitesh and Swami, Ananthram. 2017. metapath2vec: Scalable Representation Learning for Heterogeneous Networks  \\n',\n",
       "  'The HinDroid model is a method for malware detection in Android applications. It uses a Heterogeneous Information Network (HIN) to represent the relationships between apps and their decompiled source code. The model constructs a graph using various meta-paths between apps and uses similarities to classify malware. The goal of this project is to extend the HinDroid model to improve accuracy in specific subsets of the AMD dataset by finding better representations for apps and APIs. The project also explores different embedding techniques such as Word2Vec, Node2Vec, and Metapath2Vec. The results show that these techniques achieve similar accuracy scores compared to the original HinDroid approach. Further research is needed to investigate different meta-paths and evaluate the effectiveness of concatenating embeddings from multiple meta-paths.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_21/': ['# Hinreddit\\n\\n[project website](https://syeehyn.github.io/hinreddit/)\\n\\n![Docker Cloud Build Status](https://img.shields.io/docker/cloud/build/syeehyn/hinreddit)\\n\\nAs social platforms become accessible nowadays, more and more people get used to posting opinions on various topics online. The existence of nagetive online behaviors such as hateful comments is also unavoidable. These platforms thus become prolific sources for hate detection, which motivates many people to apply various techniques in order to detect hateful users or hateful speeches.\\n\\nThis project investigates contents from Reddit. its goal is to classify hateful posts from the normal ones. This not only enables platforms to improve user experiences, but also helps to maintain a positive online environment.\\n\\n- [Hinreddit](#hinreddit)\\n  - [Getting Started](#getting-started)\\n    - [Prerequisite](#prerequisite)\\n      - [Use Dockerfile](#use-dockerfile)\\n    - [Usage](#usage)\\n      - [etl](#etl)\\n      - [graph](#graph)\\n      - [embedding](#embedding)\\n      - [pipeline](#pipeline)\\n  - [For Developers](#for-developers)\\n  - [Contribution](#contribution)\\n    - [Authors](#authors)\\n    - [Advisors](#advisors)\\n  - [References](#references)\\n  - [License](#license)\\n\\n----\\n\\n## Getting Started\\n\\n### Prerequisite\\n\\nThe project is mainly built upon following packages:\\n\\n- Data Preprocessing & Feature Extraction\\n\\n  - [Pandas](https://pandas.pydata.org/)\\n  \\n- Labeling & ML Deployment\\n\\n  - [Pytorch = 1.5](https://pytorch.org/)\\n  \\n  - [PyTorch Geometric = 1.5](https://github.com/rusty1s/pytorch_geometric)\\n\\n#### Use Dockerfile\\n\\n  You can build a docker image out of the provided [DockerFile](Dockerfile)\\n\\n  ```bash\\n  docker build . # This will build using the same env as in a)\\n  ```\\n\\n  Run a container, replacing the ID with the output of the previous command\\n\\n  ```bash\\n  docker run -it -p 8888:8888 -p 8787:8787 <container_id_or_tag>\\n  ```\\n\\n  The above command will give an URL (Like http://(container_id or 127.0.0.1):8888/?token=<sometoken>) which can be used to access the notebook from browser. You may need to replace the given hostname with \"localhost\" or \"127.0.0.1\".\\n\\n### Usage\\n\\n#### etl\\n\\n - Modify the config file located in `config/data-params.json`. For testing, use `config/test-params.json`, you may define an output root `[data-path]` under `config/data-params.json`.\\n\\n - **The HinReddit\\'s etl process uses the python script file `run.py` with target `data[-test]`.**\\n\\n - You may change the `nlp_model.zip` file with custom nlp labeling rules.\\n\\n - The etl process result will be under \"\\\\<data-path>/raw\" and \"\\\\<data-path>/interim/label\" directories.\\n\\n#### graph\\n\\n- **The HinReddit\\'s graph process uses the python script file `run.py` with target `graph[-test]`.**\\n\\n- The graph process result will be under \"\\\\<data-path>/interim/graph/*.mat\"\\n\\n#### embedding\\n\\n- Modify the config files located in `config/embedding/graph_<1/2>/[test-]<informax/metapath2vec/node2vec>.json` for corresponding parameters of the embedding models.\\n\\n- **The HinReddit\\'s embedding process uses the python script file `run.py` with following targets:**\\n\\n  - `node2vec[-test]`: for node2vec embedding.\\n  - `metapath2vec[-test]`: for metapath2vec embedding.\\n  - `infomax[-test]`: for deep graph infomax (DGI) embedding.\\n\\n#### pipeline\\n\\n- run `$ python run.py data[-test] graph[-test] node2vec[-test] metapath2vec[-test] infomax[-test]`\\n\\n- You can find a detailed explaination of configuration arguments [here](./writeups/PARAMS.md)\\n\\n----\\n\\n## For Developers\\n\\n[Development Guide](./writeups/DEVGUIDE.md) is provided and under `./writeups/DEVGUIDE.md`\\n\\n----\\n\\n## Contribution\\n\\n### Authors\\n\\n- [Chengyu Chen](https://github.com/anniechen0127)\\n- [Yu-chun Chen](https://github.com/yuc330)\\n- [Yanyu Tao](https://github.com/lilytaoyy)\\n- [Shuibenyang Yuan](https://github.com/shy166)\\n\\n### Advisors\\n\\n- [Aaron Fraenkel](https://afraenkel.github.io/)\\n- [Shivam Lakhotia](https://github.com/shivamlakhotia)\\n\\n<sup>Authors contributed equally to this project</sup>\\n\\n----\\n\\n## References\\n\\n``` \\n@paper{Hou/Ye/2017,\\n  title={HinDroid: {An Intelligent Android Malware Detection System Based on Structured Heterogeneous Information Network}},\\n  author={Hou, Ye, Song, Abdulhayoglu}\\n  year={2017}\\n}\\n@inproceedings{Fey/Lenssen/2019,\\n  title={Fast Graph Representation Learning with {PyTorch Geometric}},\\n  author={Fey, Matthias and Lenssen, Jan E.},\\n  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},\\n  year={2019},\\n}\\n@article{turc2019,\\n  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},\\n  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\\n  journal={arXiv preprint arXiv:1908.08962v2 },\\n  year={2019}\\n}\\n@course{Koutra/2018,\\n  title={Mining Large-scale Graph Data},\\n  author={Danai Koutra},\\n  link={http://web.eecs.umich.edu/~dkoutra/courses/W18_598/},\\n  year={2018}\\n}\\n@collection{src-d/2019,\\n  title={Awesome Machine Learning On Source Code},\\n  author={src-d},\\n  link={https://github.com/src-d/awesome-machine-learning-on-source-code},\\n  year={2019}\\n}\\n```\\n\\n----\\n\\n## License\\n\\n[Apache License 2.0](LICENSE)\\n',\n",
       "  'The Hinreddit project aims to classify hateful posts from normal ones on the social platform Reddit. By detecting hateful users or speeches, this project helps improve user experiences and maintain a positive online environment. The project uses various packages such as Pandas, PyTorch, and PyTorch Geometric for data preprocessing, feature extraction, labeling, and machine learning deployment. Developers can refer to the provided development guide for more information. The project is contributed by Chengyu Chen, Yu-chun Chen, Yanyu Tao, and Shuibenyang Yuan, with advisors Aaron Fraenkel and Shivam Lakhotia. The project is licensed under Apache License 2.0.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_22/': ['# recordLinkage\\nDSC 180B Project: Probabilistic Record Linkage\\n\\n---\\n![Machine Learning Pipeline](./reports/images/pipeline.png)\\n\\nThe machine learning pipeline can be broken down in 3 steps:\\n1. graph construction\\n2. node2vec embedding\\n3. classifier.\\n\\n---\\nTo run the program, refer to the config folder for the parameters that can be changed for this program, and use the *python run.py* command to train, test, and evualate the model.Refer to the run.py file for the appropriate command line inputs.\\n\\nFor visualizations and testing can be found in the notebooks directory.\\n\\nThe paper associated with this paper can be found [here](./reports/final_report.pdf)\\n\\n---\\n## Example Code\\n\\nIn order to generate the artificial dataset, you can use the command below:\\n```\\npython3 gen-data\\n```\\n\\nIn order to perform the graph construction, use the command:\\n\\n```\\npython3 create-small-graphs\\n```\\n\\nLastly, in order to test any changes to the pipeline on example test, use the command:\\n```\\npython3 test-project\\n```\\n\\n---\\n## Configuration\\n\\n1. refer to ./config/datasetGenConfig.json to change the hyperparameters associated with the artificial dataset generation.\\n2. refer to ./config/test_config.json and ./config/train_config.json to change the hyperparamters associated with generating the graphs and training the classifier.\\n\\n(add information about the configurations)',\n",
       "  'This document provides an overview of the DSC 180B project on probabilistic record linkage. It describes the machine learning pipeline, which consists of three steps: graph construction, node2vec embedding, and classification. The program can be run by changing the parameters in the config folder and using the \"python run.py\" command. The paper associated with this project is also provided. Additionally, example code is given for generating an artificial dataset, performing graph construction, and testing changes to the pipeline. The configuration files are located in the ./config directory.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_23/': [\"# Malware-detection\\n\\n## Abstract\\n\\nRecent work introduced a model using a Heterogeneous Information Network (HIN) representation of Android applications utilizing a meta-path approach to link applications through the API calls contained within them. It was found with multi-kernel learning, the model was able to identify malicious applications with high accuracy. This recent work was the first approach of this kind to be published; therefore, a replication process would allow for deeper understanding of this approach. In this paper, we introduce a framework for improving upon the model through scalability and testable measures with the purpose of maintaining or increasing accuracy while creating an easily executable pipeline. In particular, we employ dimensionality reduction and stochastic techniques to achieve reasonably replicable results. Additionally, we attempt to understand, through model explainability practices, the inner mechanisms of the complex model to better understand possible inaccuracies which may arise in creating a scaled version of a HIN approach.\\n\\n\\n## Usage Instructions\\n\\nIn a terminal or command window, navigate to the top-level project directory `malware-detection/` and run\\n\\n`python3 run.py test`\\n\\nor\\n\\n`python3 run.py`\\n\\n\\n* Instructions\\n\\n`python3 run.py test` - Runs code on test set of 3 benign and 3 malware apps\\n\\n\\n`python3 run.py` - Runs project on 1000 benign and 1000 malware apps including data collection pipeline\\n\\n## Output \\n\\n\\n`test-data/processes/app_to_api.json`: structure to create A matrix\\n\\n`test-data/processes/code_block.json`: structure to create B matrix\\n\\n`test-data/processes/library_dic.json`: structure to create P matrix\\n\\n`test-data/processes/test_app_api.json`: structure to create A matrix for test set\\n\\n`test-data/processes/unique_api.text`: txt file with all unique API's\\n\\n`test-data/matrix/a_matrix.npz`: Sparse format of A matrix \\n\\n`test-data/matrix/b_matrix.npz`: Sparse format of B matrix \\n\\n`test-data/matrix/p_matrix.npz`: Sparse format of P matrix \\n\\n`results/scores.csv`: Performace metrics of model on diffrent kernels\\n\\n`charts`: Conatins all ouput charts \\n\\n\\n## Description of Contents\\n\\nThe project consists of these portions:\\n```\\nPROJECT\\n\\n├── README.md\\n├── config\\n│\\xa0\\xa0 ├── data-params.json\\n|\\xa0\\xa0 └── test-params.json\\n|   └── env.json\\n├── test-data\\n│\\xa0\\xa0 ├── smalli\\n│\\xa0\\xa0 └── processes\\n│\\xa0\\xa0     ├── app_to_api.json\\n|       ├── code_block.json\\n|       └── libray_dic.json \\n├── notebooks\\n│\\xa0\\xa0 ├── eda.ipynb\\n|   ├── coefficient_explaining.ipynb\\n|   ├── feature_extraction.ipynb\\n|   ├── model.ipynb\\n|   └── malware_type.ipynb\\n├── reports\\n|   └── malware detection.pdf    \\n├── src\\n|    ├── __init__.py\\n|    ├── build_features.py\\n|    ├── make_dataset.py\\n|    ├── elt.py\\n|    ├─ multi-kernel.py\\n|    ├─ model.py\\n|    └──coefficient_analysis.py\\n├── requirements.txt\\n├── run.py\\n\\n``` \\n\\n### `src`\\n\\n* `etl.py`: Library code that executes tasks useful for getting data.\\n* `make_dataset.PY`: Library code that excutes task useful to cleaning and building dataset\\n* `build_features.py`: Library code that excutes task to extract features from dataset\\n* `model.py`: Library code  that excutes task to create and test model.\\n* `Multi-kernel.py`: Library code pertaining the creation of multi kernel\\n* `coefficient_analysis.py` : Library code that conatins analysis of model outputs \\n\\n### `config`\\n* `env.json`: conatains dokcer conatiner id and outpaths of all files created after running run.py\\n\\n* `params.json`: Common parameters for getting data, serving as\\n  inputs to library code.\\n  \\n* `test-set.json`: parameters for running small process on small\\n  test data.\\n\\n\\n### `notebooks`\\n\\n* Jupyter notebooks for *analyses*\\n  - Contains data cleaning, eda, building features and building the model.\\n\\n\\n\\n\\n\",\n",
       "  'This paper introduces a framework for improving the model used in malware detection. The model utilizes a Heterogeneous Information Network (HIN) representation of Android applications and a meta-path approach to link applications through API calls. The framework aims to increase accuracy while maintaining scalability and testability. It employs dimensionality reduction and stochastic techniques to achieve replicable results. The paper also explores model explainability practices to better understand possible inaccuracies in a scaled version of the HIN approach.\\n\\nThe project directory structure includes various files and folders for data processing, feature extraction, model building, and analysis. The `src` folder contains library code for tasks such as data extraction, dataset cleaning, feature extraction, model creation, multi-kernel creation, and coefficient analysis. The `config` folder includes JSON files with parameters for data processing and testing. The `notebooks` folder contains Jupyter notebooks for data cleaning, exploratory data analysis (EDA), feature extraction, and model building.\\n\\nTo run the project, navigate to the top-level project directory in a terminal or command window and use the command `python3 run.py test` to run the code on a test set of benign and malware apps. Alternatively, use `python3 run.py` to run the project on a larger dataset including data collection pipeline.\\n\\nThe output of the project includes various files such as JSON structures for creating matrices, a text file with unique APIs, sparse format matrices, performance metrics of the model on different kernels stored in a CSV file, and output charts stored in the \"charts\" folder.\\n\\nOverall, this project aims to enhance malware detection using a HIN approach by improving scalability, replicability, and accuracy while providing insights into possible inaccuracies through model explainability practices.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_24/': ['# The Food Chain - A Personalized Restaurant Recommender System\\n\\nLeveraging Yelp data to develop a user-customizable, similarity-based restaurant recommendation system.\\n\\n- About this project: http://team05.pythonanywhere.com/about  \\n\\n- Try it for yourself:  http://team05.pythonanywhere.com/  \\n\\n## Getting Started \\n\\n1.  `pip install -r requirements.txt`\\n2.  `python -m nltk.downloader all`\\n3.  `python -m textblob.download_corpora lite`\\n4.  `python main.py`\\n\\n\\n## Test Project Locally\\n\\n1.  Install dependencies (see **Getting Started** through step 3).\\n2.  `python run.py test-project`\\n\\n---\\nQuestions? Please reach out to lpdoyle@ucsd.edu or dmarcusthierry@gmail.com :) \\n',\n",
       "  'This project is about developing a personalized restaurant recommendation system using Yelp data. You can learn more about the project and try it out for yourself by visiting the provided links. To get started, you need to install the required dependencies and run the main.py file. There is also an option to test the project locally. If you have any questions, you can reach out to lpdoyle@ucsd.edu or dmarcusthierry@gmail.com.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_25/': [\"# CodeHonestly\\r\\n### Utilize AST graphs to detect code plagiarism\\r\\n\\r\\n![](logo.png)\\r\\n\\r\\n`python run.py test-project` to run the project\\r\\n\\r\\nNote 1: Please make sure there are at least two Python files in data folder, not any subfolder. It is functional out of box.\\r\\n\\r\\nNote 2: We didn't wrap all the codes into library codes as possible because we might use the code in actual product that we don't want to release a version that 's easy for future development by others.\\r\\n\\r\\nWebsite: https://www.codehonestly.com/\\r\\n\\r\\nThis project was founded in DSC 180B, a capstone project for data science undergraduate students at UCSD.\\r\\n\",\n",
       "  'CodeHonestly is a project that uses AST graphs to detect code plagiarism. It can be run by executing the command `python run.py test-project`. The project requires at least two Python files in the data folder. It was founded in DSC 180B, a capstone project for data science undergraduate students at UCSD.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_26/': [\"Presentation: https://drive.google.com/open?id=1ick7HAF_w0bjRSVxWURykakYpwKsDwDY\\n\\n\\nWebsite: https://nancyvuong.github.io/dsc180b_website/\\n\\nTo run pipeline on test data run python3 run.py test-project.\\n \\n \\nThe project consists of these portions:\\n```\\nPROJECT\\n├── .gitignore\\n├── README.md\\n├── config\\n│   ├── data-params.json\\n│   ├── test-params.json\\n│   └── env.json\\n├── Notebooks\\n│   ├── API_Relationships (Model Explainability).ipynb\\n│   ├── Co_Occurence_EDA.ipynb\\n│   ├── Create_matrix.ipynb\\n│   ├── EDA.ipynb\\n│   ├── Lime.ipynb\\n│   ├── SVM_Explained.ipynb\\n│   ├── Scrape-APK.ipynb\\n│   └── Statistical test.ipynb\\n├── references\\n│   └── Hindroid.pdf\\n├── run.py\\n├── test-data\\n│   ├── APK/dating\\n│   ├── apk_data\\n│   └── xml_files/dating\\n└── src\\n    ├── Correlation_Coef.py\\n    ├── Filter_Coef.py\\n    ├── Malware_Types.py\\n    ├── Percent_API.py\\n    ├── baseline.py\\n    ├── create_matricies.py\\n    ├── model.py\\n    └── scrape_apk.py\\n    \\n```\\n\\n### `src`\\n\\n* `Correlation_Coef.py`: Calculated correlation coef for each API in each kernel\\n\\n* `Filter_Coef.py`: Filters API by corelation coef values\\n\\n* `Malware_Types.py`: Obtains type and category of malware from amd.arguslab.org \\n\\n* `Percent_API.py`: Ranking Algorithim for APIs\\n\\n* `baseline.py`: Baseline Hindorid model\\n\\n* `create_matricies.py`: Create A,B and P matricies/kernels for the Hindroid Model\\n\\n* `model.py`: Final model used for multi-class malware detection\\n\\n* `scrape_apk.py`: Obtains the Benign apps from https://apkpure.com/sitemap.xml\\n\\n\\n\\n### 'Reference'\\n\\n  * http://yes-lab.org/files/HinDroid_KDD2017_Slides_Ye.pdf\\n  \\n\",\n",
       "  'The project consists of a presentation and a website. It includes various notebooks, source code files, and references. The source code files perform tasks such as calculating correlation coefficients, filtering APIs, obtaining malware types, ranking algorithms for APIs, creating matrices for the Hindroid model, implementing the final model for multi-class malware detection, and scraping APKs. The reference provides additional information on the project.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_27/': ['# Graph-Based-Malware-Prediction\\nThis project analyzes multiple graph embeddings for malware predictions based on their smali codes. Project results and interactive demonstrations can be found on our [project website](https://maishuliang.github.io/malware-detection-viz/)\\n\\n## Overview\\n\\nNowadays, Android is dominating the smartphone market as an open source and customizable operating system. Many hackers targeted Android applications by disseminating malwares, posing serious threats to users. Historically, mobile security products such as Norton and Lookout, are heavily relied upon as major defense against such threats. Recently, many machine learning based methods have been invented for malware detection. A successful one of them is creating features from a Heterogeneous Information Network ([HinDroid](https://www.cse.ust.hk/~yqsong/papers/2017-KDD-HINDROID.pdf)). However, it is confined in such a way that it ignores more comprehensive information which can be extracted from graph representation. In this project, we will explore different meta-paths and incorporate various graph embedding methods in the task of malware prediction. We propose to build upon our previous work in HinDroid replication, more specifically we will attempt to use deep learning graph embedding techniques including Node2vec and Metapath2vec.\\n\\n## Usage\\nDocker image on Docker Hub:\\n[b4zhang/malware_detection_with_graph_embedding](https://hub.docker.com/r/b4zhang/malware_detection_with_graph_embedding)\\n\\nOn Datahub, create a pod using the custom image.\\n\\nTo test-run the project, use or modify the default `config/test-params.json` and run\\n```bash\\npython run.py test\\n```\\n\\nTo run the project, modify the config file `config/data-params.json` and run\\n```bash\\npython run.py data process\\n```\\nor run `data` and `process` separately.\\n\\n`data` target will save decompiled apk files under the folder as specified by the argument `apk_out_path` in `config/data-params.json`.\\n\\n`process` target will save the Word2Vec model and Neural Network Model under the folder as specified by the argument `model_out_path` in `config/data-params.json`.\\n',\n",
       "  'This project focuses on analyzing multiple graph embeddings for malware predictions based on their smali codes. The goal is to explore different meta-paths and incorporate various graph embedding methods in the task of malware prediction. The project builds upon previous work in HinDroid replication and utilizes deep learning graph embedding techniques such as Node2vec and Metapath2vec. The project provides a Docker image for easy usage and includes instructions on how to test-run and run the project.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_53': ['# FaceMaskDetection\\nIn an attempt to bring more transparency in artificial intelligence in a high stakes situation such as the Coronavirus pandemic, our aim was to create a model that would be able to determine if an individual was wearing a mask correctly, incorrectly, or not at all. Utilizing a subsection of the dataset\\xa0MaskedFace-Net, we were able to train a model with the Inception Resnet V1 model. Moreover, as this dataset further breaks down incorrect mask usage into why, such as uncovered chin, mouth, or nose area, we aimed to apply GradCAM in order to build transparency and trust, and ultimately ensure that our model was coming to the conclusion for the right reasons.\\n\\n\\n## Project Stucture\\n\\n#### Config files\\n\\nThese files contain important links and file paths to images and our dataset that are used by our run files \\n\\n#### Presentation\\n\\nThese contain brief snippets of our notebook to give you an idea of how our model was built and the underlying code and output for different features of our project. Our notebook GradCam EDA looks at implementing an algorithm that can identify what our neural network would look at to identify whether a mask would be work correctly and this is displayed in the gradcam presentation\\n\\n#### run.py\\n\\nThis is our main file run file that calls in methods given in gradcam.py and etl.py. To run on a given image type the command  ```python run.py test``` and in order to edit the file_path for another image, run the command  ```python run.py run_grad``` and give the file path in the data_input.py file to test it out. \\n\\n### src\\nThe src folder contains information on the functions used to train the model and our config folder contains parameter information that simplifies the working of run.py. \\n\\n## Usage of GradCam\\n\\nIn order to run out code on a given input path, type the command ```python run.py test``` from the main directory. \\n\\nThis calls the etl.py function which presents a list of stats for our images in our dataset as well as invokes the gradcam class defined in **gradcam.py** \\n\\nBased on a predefinied path, Gradcam will be applied to the image rendering a heatmap of what the netowrk looked at to make our prediction. As seen below here are examples of what GradCam looked at to make a prediction regarding the correct wearing of FaceMaks. This increases one trust in the Neural Netowrk as it bceomes more Explainable to the Human Eye. \\n\\n![image](https://drive.google.com/uc?export=view&id=10EIantVsmZLYXwfyJI6VtpXCQ1fwNJhS)\\n![image](https://drive.google.com/uc?export=view&id=1kqw8QJYPR7vOBCco7p4XcVZ7xQKexdIR)\\n\\nLooking at these images, neural networks make a lot more sense intuitively as we know why our network made that prediction.\\n\\n\\n# Results and Discussion\\n\\nThe result of our model was an accuracy of 96% in being able to classify between the three classes: improper face mask usage, no mask, and proper face mask usage. In terms of Grad-CAM, the implementation was successful in building trust and transparency within our model: the model was looking at the correct areas to determine the face mask usage.\\n\\n# Website and Frontend links\\nCheckout our website and demo: https://elizabethmkim.github.io/FaceMaskDetection/\\n\\nCheckout our frontend repo:  https://github.com/elizabethmkim/FaceMaskDetection \\n',\n",
       "  \"The aim of the project was to create a model that can determine if an individual is wearing a mask correctly, incorrectly, or not at all. The model was trained using the MaskedFace-Net dataset and the Inception Resnet V1 model. GradCAM was applied to provide transparency and trust in the model's predictions. The project structure includes config files, presentation snippets, and a main run file. The usage of GradCam involves running the code on a given input path. The results showed an accuracy of 96% in classifying mask usage, and Grad-CAM successfully identified the correct areas for determining mask usage. The website and frontend repository links are provided for further exploration.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_52': [\"# DSC180B Face Mask Detection\\n\\nThis repository focuses on the creation of a Face Mask Detection report.\\nLink to website: https://athena112233.github.io/DSC180B_Project_Webpage/\\n\\n-----------------------------------------------------------------------------------------------------------------\\n\\n### Introduction\\n* This repo is about training an Convolutional Neunral Network(CNN) image classification model on MaskedFace-Net. MaskedFace-Net is a dataset that contains more than 60,000 images of person either wearing a mask not. For images that contain a person wearing a mask, the dataset is further splited into either a person is wearing a mask properly or not. In this repo, we've trained a model on this dataset and also implemented a Grad-CAM algorithm on the model.\\n\\n##### config \\n* This folder contains the parameters for running each target. (Make sure the paths to the model and image are correct!)\\n\\n##### model\\n* This folder contains a trained model parameters(model.pt)\\n\\n##### my_image\\n* This folder contains all the custom images that you want the model to test on. This folder will only be created when a custom image path is provided in /config\\n\\n##### notebook\\n* This folder contains the exploratory data analysis(EDA) of the MaskedFace-Net.\\n\\n##### result\\n* This folder contains the images that display the result of model prediction, Grad-CAM algorithm, and Integrated Gradient.\\n\\n##### src\\n* This folder contains the .py files for model architecture, training procedure, testing procedure, Integrated Gradient, and Grad-CAM algorithm.\\n\\n##### run.py\\n* This `run.py` file will the specified target.\\n\\n##### submission.json\\n* `submission.json` contains the general structure of this repo.\\n\\n### How to run this repo with explanation:\\n*  Please visit the `EDA.ipynb` inside the `notebook folder` to understand the MaskedFace-Net. Once you understand the daatset, then you can process to run the repo\\n\\nTo run this repo on GPU (highly recommended), run the following lines in a terminal\\n\\n```\\nlaunch-scipy-ml-gpu.sh -i j0e2r1r0/face-mask-detection -c 4 -m 8\\ngit clone https://github.com/gatran/DSC180B-Face-Mask-Detection\\ncd DSC180B-Face-Mask-Detection\\n```\\n\\nOR\\n\\nTo run this repo on CPU, run the following lines in a terminal\\n\\n```\\nlaunch.sh -i j0e2r1r0/face-mask-detection -c 4 -m 8\\ngit clone https://github.com/gatran/DSC180B-Face-Mask-Detection\\ncd DSC180B-Face-Mask-Detection\\n```\\n\\nThen you can start to run the various targets we've provided in ```src/``` folder\\n\\nTo train a model on your own, run the following line in a terminal\\n\\n```\\npython run.py training\\n```\\n\\nTo test the model, run the following line in a terminal\\n\\n```\\npython run.py testing\\n```\\n\\nTo implement the Grad-Cam algorithm on the model, run the following in a terminal\\n\\n```\\npython run.py gradcam\\n```\\n\\nTo implement the Integrated Gradient algorithm on the model, run the following in a terminal\\n\\n```\\npython run.py ig\\n```\\n\\n##### Contributions\\n* Gavin Tran: train the model and generate output\\n* Che-Wei Lin: implement gradcam and generate output\\n* Athena Liu: create a report based on those outputs\\n\",\n",
       "  'This repository focuses on Face Mask Detection using a Convolutional Neural Network (CNN) image classification model trained on the MaskedFace-Net dataset. The repository includes various folders such as config, model, my_image, notebook, result, and src. It also provides a run.py file for running different targets such as training the model, testing the model, implementing the Grad-CAM algorithm, and implementing the Integrated Gradient algorithm. The contributions to this project were made by Gavin Tran, Che-Wei Lin, and Athena Liu.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_51': [\"The purpose of this code is generating captions from an image and creating attention maps to help explain the model’s reasoning for the captions generated.\\nIn order to test the robustness of the model we also use counterfactual images to see how the model's prediction changes when certain object are removed. \\nUsing this infomation we can also generate an object importance map to show which object in an image are most important to the caption generation process.\\n\\nThis project has six targets: data, train, evaluate_model, generate_viz, counterfactual_production, and explain_model. \\n  - **data**: This target loads in the COCO dataset and prepares it for our image captioning model. \\n  - **train**: This target builds the encoder and decoder in our image captioning model and trains it with the COCO dataset. \\n  - **evaluate_model**: This target evaluates the trained model using beam search caption generation and BLEU score. \\n  - **generate_viz**: This target generates a visualization of the attention maps at each stage of the caption generation process.\\n  - **counterfactual_production**: This target creates all of the files necessary to generate the counterfactuals (such as masks) and \\n                                   then produces the counterfactual images.\\n  - **explain_model**: This target takes all of the counterfactual images and generates caption based on the new counterfactuals. \\n                       Then compares the caption change from the original caption to generate a visualization to explain object \\n                       importance using BERT similarity score.\\n\\nTo run the four targets, clone our repo to the dsmlp server and execute the command ‘python run.py all’ to run all the targets in sequence or \\n'python run.py <target>' to run a single target. To run on a small set of test data execute: ‘python run.py test’. The output images will be saved \\n to the same directory as run.py.\\n\\n\\nDocker Repo: https://hub.docker.com/layers/140345085/afosado/capstone_project/final_docker/images/sha256-198c698d15e7a67d1bba8180a30c21dbf00dfd5e839189a94c06e6ffe96f9fac?context=explore\\n\\nDemo Website: https://afosado.github.io/180b_capstone_xai/index.html\\n\",\n",
       "  \"This code generates captions from an image and creates attention maps to explain the model's reasoning. It also uses counterfactual images to test the model's robustness. The project has six targets: data, train, evaluate_model, generate_viz, counterfactual_production, and explain_model. To run the targets, clone the repo and execute the command 'python run.py all' or 'python run.py <target>'. The output images will be saved in the same directory as run.py. A Docker repo and a demo website are also provided.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_50': ['# StockMarket_explainableAI\\nContributing Members: \\n- Sohyun Lee\\n- Shin Ehara\\n- Jou-Ying Lee\\n\\n## Abstract\\nDeep learning architectures are now publicly recognized and repeatedly proven to be powerful in a wide range of high-level prediction tasks. While these algorithms’ modeling generally have beyond satisfactory performances with apposite tuning, the long-troubling issue of this specific learning lies in the un-explainability of model learning and predicting. This interpretability of “how” machines learn is often times even more important than ensuring machines outputting “correct” predictions. Especially in the field of finance, users’ ability to dissect how and why an algorithm reached a conclusion from a business standpoint is integral for later applications of i.e., to be incorporated for business decision making, etc. This project studies similar prior work done on image recognition in the financial market and takes a step further on explaining predictions outputted by the Convolutional Neural Network by applying the Grad-CAM algorithm. \\n\\nProject Website at: https://connielee99.github.io/Explainable-AI-in-Finance/\\n\\n## Instructions on Runing Project\\n* This project aims to apply the Grad-CAM technique to a CNN model trained on images that represent closing prices during the first hour of market exchange. \\n* **To engineer data and create a CNN model**, you would need to run each notebook in `notebooks` folder in the following order:\\n\\t* **1. Run every cell in `Data Processing.ipynb`**\\n\\t\\t* This notebooke is preprocessing the raw data by extracting closing prices during first hour after market open and labeling depends on prices increasing or decreasing\\n\\t\\t* **Input:** `raw_NIFTY100.csv`\\n\\t\\t* **output:** `first_combined.csv` contains closing prices during the first hour of market exchange\\n\\t* **2. Run every cell in `Image Conversion.ipynb`**\\n\\t\\t* This notebook is for an image conversion with `first_combined.csv` data. We will converse data into image with Gramian Angular Algorithm.\\n\\t\\t*  **Input:**`first_combined.csv`\\n\\t\\t*  **output** `.png` images in `imgs` folder\\n\\t* **3. Run every cell in `CNN.ipynb`**\\n\\t\\t* This notebook uses FastAI, a PyTorch-based deep learning library, to build the neural network, which is able to figure out the relationship between input features and find hidden relationship with them. The input data is an image dataset with labels, which is converted from time series with Gramian Angular Field algorithm as described in the previous sections.\\n\\n* **To run Grad-CAM**: \\n\\t- Clone the Grad-CAM submodule we have included in repo homepage.\\n\\t- Navigate to <i>StockMarket_explainableAI/test</i> and put <i>test_imgs</i> folder inside this cloned submodule folder.\\n\\t- Set your directory to be in this submodule, and run the following command (feel free to modify the last part in the code for specific images):\\n\\t\\t* python3 main.py demo1 -a resnet34 -t layer4 -i test_imgs/2017-01-03.png -k 1\\n\\n## Directory Structure\\n* **config**</br>\\n\\tThis folder contains json files for main and testing parameters\\n\\t* `data_params.json`</br>contains parameters for running main on all data\\n\\t* `test_params.json`</br>contains parameters for running main on test data\\n* **data**</br>\\n\\tThis folder contains all stock data from time series to image representation</br>\\n\\t**imgs**</br>\\n\\t* This folder contains all images converted from time series. ex) 2017-01-02.png\\n\\t\\n\\t**raw data**</br>\\n\\t* `raw_NIFTY100.csv`</br>contains raw stoack market data; time series data\\n\\n\\t**processed data**</br>\\n\\t* `first_combined.csv`</br>contains closing prices during the first hour of market exchange\\n\\t* `gramian_df.csv`</br>contains data after implementing gramian angular algorithm\\n\\t* `label_dir_2.csv`</br>contains data with label Whether the price goes up or down that day\\n* **gradcam_submodule @ fd10ff7**</br>\\n\\tThis folder is the submodule for gradcam\\n\\t\\n* **notebooks**</br>\\n\\tThis folder is the notebook directory\\n\\t\\n\\t* `CNN + Grad-CAM.ipynb`</br>is the development notebook for CNN and GradCam implementation\\n\\t* `Data Processing.ipynb`</br>is the notebook that wraps together data cleaning to feature engineering\\n\\t* `EDA.ipynb`</br>is the notebook with eda work demonstration\\n\\t* `Image Conversion.ipynb`</br>is the notebook with image conversion work done\\n* **references**</br>\\n\\tThis folder contains additional information/references in regards to our project\\n\\t\\n\\t**report_img**</br>\\n\\t* This folder contains images extracted from coded notebooks and included in the written report\\n\\n* **src**</br>\\n\\tThis folder contains library codes extracted from notebooks\\n\\t\\n\\t**features**</br>\\n\\t* `build_features.py`</br>scripts to build features from merged data\\n\\t* `build_labels.py`</br>scripts to create labels for image classification\\n\\t* `build_images.py`</br>scripts to convert and save time series data to images\\n\\t\\n\\t**model**</br>\\n\\t* `gradcam.py`</br>scripts to implement gradcam\\n\\n* **test**</br>\\n      This folder contains test results and test images\\n      \\t\\t\\n* **`Dockerfile`**</br>\\n\\tThis is the dockerfile necessary to build the environment for this project development\\n* **`run.py`**</br>\\n\\tThis is the main python file to execute our program\\n',\n",
       "  'This project focuses on applying the Grad-CAM algorithm to explain predictions made by a Convolutional Neural Network (CNN) trained on stock market data. The project aims to provide interpretability in the field of finance by allowing users to understand how and why the algorithm reached its conclusions. The project website provides more information and instructions on running the project. The directory structure includes folders for data, notebooks, references, and source code, among others.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_49': ['# Racial_Classification_XAI_Model\\n\\nkeyword: Deep Learning, Convolutional Neural Network, Integrated-Gradient, Grad-CAM, Web Application\\n\\nWebsite: https://michael4706.github.io/XAI_Website/\\n\\nStatic Web app: this [demo](https://nicole9925.github.io/facial-analysis-frontend/) (once you clicked the demo, just press submit to run it) is our web application that runs sample image. Make sure you visit the [Web Application](https://michael4706.github.io/XAI_Website/webapp/) (or just click this link) to play with it. If you want to run the web application with your own image, please visit the Web Application section below and follow the intrusctions.\\n\\n\\n![sample result](sample_result.png)\\n\\n### Introduction\\nThis project is about visualizing Convolutional Neural Network (CNN) with XAI techniques: Grad-cam and Integrated-Gradient. We used the FairFace dataset to train our models. This dataset contains about 80000+ training images and 10000+ validation images. The dataset contains three different categories(labels): age range(9 classes), race(7 classes), and gender(2 classes). We implemented a model that combined the first 14 layers from resnet50 as pre-trained layers with our self-defined layers. We trained three models on each of the different categories using the same model structure except changing the number of outputs from the final layer to match each category\\'s number of classes. Then, we applied XAI to visualize models\\' decision-making with heatmaps. We want to examine what features or regions the models focus on given an image. Also, we are interested in comparing the heatmaps generated by the biased and unbiased models. The FairFace Dataset has an equal distribution of race. Therefore, we created a dataset with an unequal distribution of race and trained a biased model with this dataset.\\n\\n##### config\\n* The parameters to run the scripts. Make sure to visit this file before running the code.\\n\\n##### run.py\\n* script to train the model, run integrated-gradients, and calculate the statistics for the model.\\n\\n##### src\\n* folder that contains the source code.\\n\\n##### models\\n* Contained dlib_mod that helps to preprocess the images. You also recommend you to save your trained model here.\\n\\n##### test_data\\n* Contains sample data from FairFace Dataset.\\n\\n### How to run the code\\n1. please the my docker image: `michael459165/capstone2:new8` and run the code inside this container.\\n2. please go to the config file to change the parameters. This file has 5 sections, each corresponds to a set of parameters to execute a particular task.\\n3. Type `python run.py train_model` to train your model.\\n4. Type `python run.py generate_stats` to generate statistics and plots.\\n5. Type `python run.py run_test` to generate just ONE heatmap for both Integrated Gradient and Grad-CAM on the test sample. This will generate the heatmaps of the class with the HIGHEST predictive probability.\\n6. Type `python run.py run_custom_img` to generate just ONE heatmap for both Integrated Gradient and Grad-CAM on YOUR own image. This will generate the heatmaps of the class with the HIGHEST predictive probability.\\n\\nNote: All the functions in util.py are well documented. Please feel free to explore and modify the code!\\n\\n### Web Application\\nWe also made a Web App to showcase our work. Please clone [this repository](https://github.com/nicole9925/facial-analysis-webapp) and follow the instruction to run it locally. If you want to deploy the Web App online, please visit [frontend](https://github.com/nicole9925/facial-analysis-frontend) and [backend](https://github.com/nicole9925/facial-analysis-backend) repositories for further instruction. \\n\\n### More stuff you can do\\n* If you don\\'t want to train the model (because it takes a long time), then please visit the \"temp\" branch from this repository. There are trained models under the models folder and sample integrated-gradient results and statistics under the visualization folder. Please just download the files you need and run the code from this main branch. \\n\\n### Reference\\n[1]Selvaraju, Ramprasaath R., et al. \"Grad-cam: Visual explanations from deep networks via gradient-based localization.\" Proceedings of the IEEE international conference on computer vision. 2017.\\n\\n[2]Grad-CAM implementation in Keras[Source code]. https://github.com/jacobgil/keras-grad-cam.\\n\\n[3]Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. \"Axiomatic attribution for deep networks.\" International Conference on Machine Learning. PMLR, 2017.\\n\\n[4]Integrated Gradients[Source code]. https://github.com/hiranumn/IntegratedGradients.\\n\\n[5]@inproceedings{karkkainenfairface,\\n      title={FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation},\\n      author={Karkkainen, Kimmo and Joo, Jungseock},\\n      booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},\\n      year={2021},\\n      pages={1548--1558}\\n    }\\n\\n[6] FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age[Source code].https://github.com/dchen236/FairFace.\\n\\n[7] Draelos, Rachel. “Grad-CAM: Visual Explanations from Deep Networks.” Glass Box, 29 May 2020. https://glassboxmedicine.com/2020/05/29/grad-cam-visual-explanations-from-deep-networks/#:~:text=Grad%2DCAM%20can%20be%20used%20for%20understanding%20a%20model\\'s%20predictions,choice%20than%20Guided%20Grad%2DCAM.\\n',\n",
       "  \"This project focuses on visualizing a Convolutional Neural Network (CNN) using XAI techniques such as Grad-CAM and Integrated-Gradient. The models were trained on the FairFace dataset, which contains images categorized by age range, race, and gender. The project aims to examine the features and regions that the models focus on when making decisions. A biased model was also trained using a dataset with an unequal distribution of race. The code can be run using a Docker image and various commands are available for training the model, generating statistics, and generating heatmaps. A web application is also provided to showcase the project's work. Additional information, references, and instructions are available in the original source.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_54': ['# Snake NeuralBackedDecisionTrees\\nDSC180B Group 6 Snake Classification using Neural Backed Decision Trees\\n\\nTeam Members:\\n\\nNikolas Racelis-Russell - A15193225\\n\\nWeihua (Cedric) Zhao - A14684029 \\n\\nRui Zheng - A15046475\\n\\n## Abstract\\n\\nOur project focuses on building explanable image classification models on snake images from https://www.aicrowd.com/challenges/snakeclef2021-snake-species-identification-challenge/dataset_files. Our plan is to apply gradcam to images of different snake species, construct a Densenet, and transform them into decision trees to visualize the classification process. \\n\\n### Demo\\n\\nLink to our website:  https://nikolettuce.github.io/DSC180B_06_NeuralBackedDecisionTrees/\\n\\n#### GradCAM\\n\\nOur approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses the class-specific gradient information flowing into the final convolutional layer of a CNN to produce a coarse localization map of the important regions in the image. [1]\\n\\n#### Neural Backed Decision Tree\\n\\nNowadays, machine learning has been applied in multifaceted areas of our life. While its prominence grows, its interpretabilty leaves people insecure because of the fact that people hardly see through the classfication decision process. Many attempts to solve this problem either ends up with the cost of interpretability or the cost of accuracy. Intended to avoid this dilemma in our snake classification process, we applied Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network\\'s final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model\\'s accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging.[2] \\n\\n##### Induced Hierarchy\\n\\nWhen creating the hierarchy tree, one thing to note is that \"[it] requires pre-trained model weights\". We took row vectors wk : k ∈ [1, K], each representing a class, from the fully- connected layer weights W; then, we ran hierarchical agglomerative clustering on the normalized class representatives wk/kwkk2. Last but not least, we built the leaf nodes based on the weights. [2]\\n\\n##### Loss Conversion\\n\\n### Practical Use\\n\\nOur project has valid applications in real life. Wild snakes are prevalent on mountains, and hikers have high possibilities to encounter them. A genuine snake classifier would provide useful information to hikers about whether the snake is venomous or not; thus, they can avoid the snake if a certain dangerous species emerge.\\n\\n### Methods\\nOur classiciation starts with a baseline Densenet model with 5 epochs. In terms of performance, it reached a F-score of 0.495 on validation data and accuracy of 0.66 on validation data. We then managed to improve the model later with higher accuracy.\\n\\nThen we applied Grad-CAM to five different snake pictures with different features. The first category includes pictures where snakes blend in with the background. The second category includes pictures where snakes differ from the background. The third category includes pictures where snakes appear with other objects, like hand. Grad-CAM performs well on localizing the target object. [3]\\n\\nLast, but not least, we are currently working on transforming the CNN models to decision trees. For now, we have a general hierarchy tree where you can see each decision. Though the decision is not clear for now, we will manage to elucidate them in the upcoming weeks.\\n\\nWhen creating the hierarchy tree, one thing to note is that \"[it] requires pre-trained model weights\". We took row vectors wk : k ∈ [1, K], each representing a class, from the fully- connected layer weights W; then, we ran hierarchical agglomerative clustering on the normalized class representatives wk/kwkk2. Last but not least, we built the leaf nodes based on the weights. [4]\\n### Results\\n\\n#### Heatmaps\\n\\n<img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/0a00cdd2b8.jpg\" width=\"200\"/> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam%201.jpg\" width=\"200\"/> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam_gb%201.jpg\" width=\"200\"> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/gb%201.jpg\" width=\"200\"> \\n\\n\\n<img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/0a7eded849.jpg\" width=\"200\"/> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam%204.jpg\" width=\"200\"/> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam_gb%204.jpg\" width=\"200\"> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/gb%204.jpg\" width=\"200\"> \\n\\n<img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/0a54501d6d.jpg\" width=\"200\"/> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam%207.jpg\" width=\"200\"/> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam_gb%207.jpg\" width=\"200\"> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/gb%207.jpg\" width=\"200\"> \\n\\n#### Hierarchy Trees\\n\\n<img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/Screen%20Shot%202021-02-07%20at%205.32.34%20PM.png\">\\n\\n### Conclusions\\n\\n## Installation\\n\\nTo use this project, please run build.sh and allocate at least 30 GB of hard drive space to install the data.\\n\\nThen run python run.py data to first process the data before using the test target\\n\\nTo run the CNN and test on the snake dataset, run python run.py test\\n\\n## Resources\\n\\n1. Grad-CAM Paper https://arxiv.org/pdf/1610.02391v1.pdf\\n2. @misc{wan2021nbdt, title={NBDT: Neural-Backed Decision Trees}, author={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Henry Jin and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez}, year={2021}, eprint={2004.00221}, archivePrefix= {arXiv}, primaryClass={cs.CV} }\\n\\n3. Grad-CAM code  https://github.com/jacobgil/pytorch-grad-cam\\n\\n4. NBDT: https://github.com/alvinwan/neural-backed-decision-trees\\n\\n\\n',\n",
       "  'This project focuses on building explainable image classification models for snake images. The team plans to use Grad-CAM to visualize important regions in the images and transform a Densenet model into decision trees. The project has practical applications in identifying venomous snakes for hikers. The team has also applied Grad-CAM to different snake pictures and is currently working on transforming CNN models into decision trees. Results include heatmaps and hierarchy trees. The installation process is provided, along with resources used for the project.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_16': ['# DSC-180B-Team6\\n\\nWe were able to replicate the ThunderHill race track using the Unity 3D game engine and integrated Unity with the track and robot into the LGSVL simulator. Once the integration was complete we were able to see our robot with the Thunderhill Track as our map in the simulator. We were then able to virtualize the functions of the IMU, odometry and lidar sensors and RGB-D cameras to better visualize what our robot perceives in the simulation. Finally we were able to fully visualize what our robot sees with the virtual sensors using Autoware Rviz which displays the location and point cloud map of the vehicle and its surroundings.\\n',\n",
       "  \"The team successfully replicated the ThunderHill race track using Unity 3D game engine and integrated it with the LGSVL simulator. They virtualized the IMU, odometry, lidar sensors, and RGB-D cameras to visualize the robot's perception in the simulation. Autoware Rviz was used to display the vehicle's location and point cloud map.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_14': ['DSC 180B Autonomous Vehicle Team 1 [Organization\\nRepository](https://github.com/UCSDAutonomousVehicles2021Team1) to\\nserve as a collection of all the repositories built for this project.\\n\\n* [autonomous_navigation_image_segmentation](autonomous_navigation_image_segmentation)\\n* [autonomous_navigation_light_sensitivity](autonomous_navigation_light_sensitivity)\\n* [autonomous_nav_mapping_docker](autonomous_nav_mapping_docker)\\n* [camera_mapping_navigation_website](camera_mapping_navigation_website)\\n* [f1tenth_racecar_dl_custom](f1tenth_racecar_dl_custom)\\n* [rtabmap_mapping_tuning](rtabmap_mapping_tuning)\\n\\n\\n',\n",
       "  'The DSC 180B Autonomous Vehicle Team 1 has created an organization repository on GitHub to house all the repositories built for their project. The repositories included are: autonomous_navigation_image_segmentation, autonomous_navigation_light_sensitivity, autonomous_nav_mapping_docker, camera_mapping_navigation_website, f1tenth_racecar_dl_custom, and rtabmap_mapping_tuning.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_13': ['# Data Visualizations and Interface For Autonomous Robots\\n\\nThis project aims to create data visualizations and an interactive interface for autonomous robots. The intent and design of visualizations created for this project were catered towards optimizing racing performance on the [Thunderhill track](https://www.thunderhill.com/). Visualizations include birdseye view of optimal path on mapped track, live camera feed, lidar readings, IMU data (position and orientation) visualized, battery status display, and various other visualizations to show the health and status of the vehicle. The interface that displays all of these various tools and visualizations are meant to be interactive and communicate with the Autonomous Robot via Rosbridge, which not only allow users to control the Gazebo Robot action by simple interface interaction like clicking button or inputing text, but also monitor the realtime situation of the robot navigating in map. The visualizations will be primarily illustrated through Python, ROS, Gazebo, RViz, and other robotics software. After analyzing the performance of A* algorithm versus the performance of RRT* algorithm, it is determined that RRT* performs better. Ultimately, RRT* algorithms is the primary navigation algorithm used and illustrated in this project.\\n\\n## Running the project\\nFirst clone the repository:\\n```\\n$ git clone https://github.com/dannyluo12/Autonomous_robot_data_visualization_and_interface.git\\n```\\nLaunch docker container using image:\\n```\\n$ launch.sh -i dannyluo12/visualization_and_interface:latest -c 4 -m 8 -P Always\\n```\\n* This command launches a [dockerhub](https://hub.docker.com/repository/docker/dannyluo12/visualization_and_interface) container with the necessary OS libraries, tools, and dependencies to successfully run the project. Certain dependencies will be vital for creating the visualizations and genearting the interface.\\n\\n## Building the project using `run.py`\\n* Use the command `python run.py data` to create data folder. Will contain directories to properly store image and sensor data that is outputted.\\n* Use the command `python run.py clean` to ensure that data is scaled properly to optimize runtime. Includes imaging data for running navigation algorithms as well as executing interface.\\n* Use the command `python run.py analyze` to compare the performance of A* algorithm to RRT* for navigation on the same map.\\n* Use the command `python run.py test` to run the visualization of RRT algorithm in test data, output images can be found in the testdata/step_out and testdata/test_out directories.\\n* Use the command `python run.py all` to run the visualization of RRT algorithm on cleaned data/map, output images can be found in the data/step_out and data/test_out directories.\\n\\n### Contributions:\\n<b>Yuxi Luo</b> <br />\\nContributed to developing visualizations for RRT* and A* algorithms. Tested performance of each navigation algorithm to benchmark each and determine better performer. Collected and cleaned data from alternative groups to enable visualization and interface development. Tested different ROSBAGS for data type compatibility. Investigated various forms of visualization from different ROS topics (diff sensors, camera, lidar, etc.). Helped in managing and updating Github repo, report, and project website.\\n\\n<b>Seokmin Hong</b> <br />\\nContributed by implementing the UCSD simulated track inside the Gazebo simulator, as well as implementing the RRT* and A* algorithms that can be used for G-Mapping SLAM. Also wrote Rviz scripts and interactive interface scripts to allow autonomous navigation with a simple pressing of a button. Helped teammates by creating and writing the report, as well as creating the demonstration videos of the interactive interface and Gazebo simulations.\\n\\n<b>Jia Shi</b> <br />\\nContributed to the research of visualization and interface. Developed an interactive interface with roslibjs and webridge to connect ROS with web page. Worked with teammate to integrate interface with Gazebo robot to allow controlling. Also created visualization demo with ROS bag data from other teams. Helped teammates with the coding and helped with the setup and completion of github repo.\\n',\n",
       "  \"This project focuses on creating data visualizations and an interactive interface for autonomous robots, specifically optimizing racing performance on the Thunderhill track. The visualizations include a birdseye view of the optimal path, live camera feed, lidar readings, IMU data visualization, battery status display, and other visualizations to show the vehicle's health and status. The interface allows users to control the robot's actions and monitor its real-time situation using Rosbridge. The primary navigation algorithm used in this project is RRT*. \\n\\nTo run the project, clone the repository and launch a docker container with the necessary dependencies. The `run.py` script can be used to create data folders, clean data for optimization, analyze algorithm performance, and run visualizations.\\n\\nThe contributions of team members include developing visualizations for RRT* and A* algorithms, implementing the UCSD simulated track in Gazebo simulator, implementing navigation algorithms for G-Mapping SLAM, creating an interactive interface with ROS integration, and assisting with coding and project management tasks.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_12': ['# DSC 180 Autonomous Systems\\n\\nNeghena Faizyar, Garrett Gibo, Shiyin Liang\\n\\n## Data\\nTo get sample data: \\n[Link to Data](https://drive.google.com/drive/folders/1wh7EtgtrS8Wi8xBIe1VwzFDBnp751XHv?usp=sharing)\\n\\nDownload this data to put into the data/raw folder.\\n\\n## Usage \\n\\n### ROS\\n\\nA large portion of this project is a series of ROS packages that can be launched\\ndirectly or integrated in other ROS packages. This repo has been made in such a\\nway that it serves as a full ROS workspace, thus to run the packages that are\\ncontained here, simply run:\\n\\n``` sh\\n# build project\\ncatkin_make\\n\\n# source ROS packages\\nsource devel/setup.bash\\n\\n# launch main node\\nroslaunch simulation main.launch\\n```\\n\\nThis will launch a gazebo simulation containing a test vehicle with sensors that\\nwere used for this project. Because the simulation requires both ROS and gazebo\\nwhich have large graphical portions, these packages must be run a system that\\nhas ROS setup already and also has some type of grapical interface, for example\\nX on linux based systems.\\n\\n### Analysis\\n\\nThe second portion of this project is analysis that is done on the data that\\nis gathered from both simulation and real sensors. \\n\\nTo run any of the following targets, the command is:\\n\\n```sh\\npython run.py <target>\\n```\\n\\nInformation on the targets is found below.\\n\\n#### Targets\\n\\n* `cep`: Calculates the Circular Error Probable (CEP), and \\n2D Root Mean Square (2DRMS), and then plots and creates a graph of the CEP \\nand 2DRMS circles with the datapoints. \\n\\n* `clean_data`: Extract, transform, and clean the raw GPS data so\\nthat it can be used for anaylsis.\\n\\n* `get_path`: Takes in CSV of GPS coordinates and cleans/filters points to create\\na usable path.\\n\\n* `ground_truth`: Plot ground truth coordinates against estimated coordinates \\nfor reported GPS values.\\n\\n* `robot`: Creates an instance of the\\n[dronekit-sitl](https://dronekit-python.readthedocs.io/en/latest/develop/sitl_setup.html),\\nwhich can be used to generate realistic sensor data that can be used\\nas a template for the following targets.\\n\\n* `robot_client` Provides the interface to connect to a specified robot.\\nThe client connects over tcp or udp and uses the\\n[MAVLink](https://mavlink.io/en/messages/common.html), standard for\\nthe messages.\\n\\n* `test`: Runs our projects test code by extracting, transforming, and then \\ncleaning the raw GPS test data such that it could be used. \\n\\n* `visualize`: Create visualizations for all of our data using bokeh. It will \\nplot the line the GPS reports it has traveled and uploads it into the vis \\nfolder of our repository. \\n',\n",
       "  'This document provides information about the DSC 180 Autonomous Systems project. It includes details about the data used, usage instructions for running ROS packages, and analysis targets. The analysis targets include calculating error probabilities, cleaning and transforming GPS data, creating usable paths, plotting ground truth coordinates, generating realistic sensor data, connecting to a robot, running test code, and creating visualizations using bokeh.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_11': ['# Autonomous Vehicles Capstone: Odometry and IMU \\n\\n\\nWe are Team 4 in the Autonomous Vehicles Data Science Capstone Project. Our project revolves around the IMU, Odometry efforts while we collectively work to build a 1/5 scale racing autonomous vehicle. \\n\\nFor a vehicle to successfully navigate istelf and even race autonomously, it is essential for the vehicle to be able localize itself within its environment. This is where Odometry and IMU data can greatly support the robot’s navigational ability. Wheel Odometry provides useful measurements to estimate the position of the car through the use of wheel’s circumference and rotations per second. IMU, which stands for Interial Measurement Unit, is 9 axis sensor that can sense linear acceleration, angular velocity, and magnetic fields. Together, these data sources can provide us crucial information in deriving a Position Estimate (how far our robot has traveled) and a Compass Heading (orientation of the robot/where it’s headed).\\n\\nOur aim is to calibrate, tune, and analyze Odometry and IMU data to provide most accurate Position Estimate, Heading, and data readings to achieve high performant autonomous navigation and racing ability.\\n\\n*Developed by: Pranav Deshmane and Sally Poon*\\n\\n### Usage\\n\\n```\\npython run.py <target>\\n```\\nThe Targets are: \\n \\n* `conversion` - This will extract the data from the raw ROS bags, clean them, and convert them to csv files to be analyzed\\n \\n* `viz_analysis` - This will run the visualizations used in our analysis for IMU and Odometry calibration, tuning, and testing\\n\\n* `test` - This will test the conversion and visualization process with sample data chosen from our raw data\\n\\n### Resources\\nIn the resources folder:\\n\\n* `Openlog_Artemis_IMU_Guide` - Guide we developed for SparkFun Openlog Artemis IMU to improve future experience and aid in the installation, setup, and integration with Jetson NX and ROS.\\n\\n* `Calibration_OLA_Artemis` - Calibration guide we developed for SparkFun Openlog Artemis IMU to aid in calibration process, analysis for future students/users\\n\\n* `Setup for Odometry_IMU` - Guide we developed for the Odometry to aid in tuning process, analysis, and setup of Odometry and VESC interaction to improve experience for future students/users.\\n\\n* `Apollo3`, `ICM-20948`, `Artemis_Hardware` - Sparkfun Openlog Artemis IMU Hardware Specifications, used to cross reference PIN headers that were needed to be configured correctly during integration process. \\n\\n\\n\\n\\n### Additional ROS package \\n* `ros_imu_yaw_pkg` \\nROS package we developed to aid in the integration of the OLA Artemis IMU to ROS. It allows the orientation quaternion readings derived from the IMU to be easily converted into Euler angles and Yaw heading. This is to improve the debugging process within ROS and helps to easily visualize the Yaw heading. This package can be run in parallel as a complement to the main ROS package used to interface with the OLA Artemis IMU and can easily integrate with the rest of your current ROS system in place as a separate node. Overall, this is to aid in the development process within ROS when deriving Yaw Heading from the OLA Artemis IMU. \\n\\n',\n",
       "  \"Team 4 is working on building a 1/5 scale racing autonomous vehicle for the Autonomous Vehicles Data Science Capstone Project. They are focusing on the IMU and Odometry efforts to help the vehicle navigate and race autonomously. Wheel Odometry uses measurements from the wheels to estimate the car's position, while the IMU is a 9-axis sensor that senses linear acceleration, angular velocity, and magnetic fields. By calibrating and analyzing the Odometry and IMU data, they aim to achieve accurate position estimates, heading, and data readings for high-performance autonomous navigation and racing ability. They have also developed resources such as guides for the IMU calibration and setup, as well as a ROS package for integrating the IMU with ROS.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_55': ['# Webpage Link\\n[https://yikaihao.github.io/DSC180_Webpage/](https://yikaihao.github.io/DSC180_Webpage/)\\n',\n",
       "  'The provided link leads to a webpage.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_58': [\"# DSC180B-Project\\nThe data we have are grabbed from the /teams directory: malware and popular-apps.\\n\\nThe purpose is to perform search on each software folder to find its smali files\\nand perform method-call analysis to build markov chain and get holistic\\ninformation of specific software and finally build a improved MAMADroid to\\nclassify specific ware to be benign or malware.\\n\\nIt consists process_smali() to parse smali file and generate call-analysis.\\n\\nTo run it, execute python run.py <targets>.\\nTargets including 'feature', 'model', 'analysis', 'test'\\n\\n### Responsibilities\\n\\n* Jian Jiao developed code which parses content, generates features,\\n  builds model, perform analysis, improve model, generate results.\\n* Zihan Qin developed report and help partner to test code and debug.\\n\\n### Project Webpage\\nhttps://kamui-jiao.github.io/DSC180B-Page/\\n\",\n",
       "  'The project involves analyzing software files to classify them as benign or malware. The data is obtained from the /teams directory, specifically the malware and popular-apps folders. The process includes searching for smali files in each software folder, performing method-call analysis to build a markov chain, and using an improved MAMADroid to classify the software. The responsibilities of the team members are also mentioned, with Jian Jiao developing the code and Zihan Qin assisting with testing and debugging. More information can be found on the project webpage: https://kamui-jiao.github.io/DSC180B-Page/'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_57': [\"# hindroid_replication\\n## HinDroid: An Intelligent Android Malware Detection System. Based on Structured Heterogeneous Information Network.\\n# Malware Detection Using API Relationships + Hindroid\\n# HOW TO RUN \\n`python run.py test`\\n+ `requirements.txt`\\n\\nBy July 2020, Android OS is still a leading mobile operating system that holds 74.6% of market share worldwide, attracting numerous crazy cyber-criminals who are targeting at the largest crowd.¹ Also, due to its open-resource feature and flexible system updating policy, it is 50 times more likely to get infected compared to ios systems.² Thus, developing a strong malware detection system becomes the number one priority.\\n\\nThe current state of malware(malicious software) detection for the growing android OS application market involves looking solely at the API(Application Programming Interface) calls. API is a set of programming instructions that allow outside parties or individuals to access the platform or the application.³ A good and daily example will be the login options displaying on the app interface like “Login with Twitter”.4 Malwares can collect personal information easily from APIs, so analyzing APIs is a critical part of identifying malwares.\\n\\nHindroid formulates a meta-path based approach to highlight relationships across API calls to aggregate similarities and better detect malware.Individual APIs appearing in the ransomware could be harmless, but the combination of them could indicate “this ransomware intends to write malicious code into system kernel.”5 You wouldn’t want to see a group of “write”, “printStackTrace”, and “load” APIs appearing in your app’s smali file.5\\n\\n\\n# Data Generation Process\\nThe data generation process and its relationship to the problem (i.e. for domain problems)\\nThe data for identifying malware is primarily the android play store, although in order to obtain the respective APK’s for these apps the data is directly downloaded from `https://apkpure.com/`.\\n\\nThis data is then unpackaged using the apktools library that allows us to view the subsequent smali code and app binaries.\\n\\nThe smali code and app binaries contain a lot of the information derived from the Java source code that allows us to map the number of API calls and the relationships between them. \\n\\n# Observed Data \\nOverall, from each android app, what’s most relevant to classifying Malware vs Benign - are the API calls, code blocks(methods) and packages these API calls occur in, classified as matrices \\nA, P, B, I. This form of organizing data explains the relationship between these API calls. It provides a story, more in depth, than just the sheer number of API calls per app. \\n\\nThe Hindroid model observes the same relationship of data to better classify malware or not, through the relationship of the above defined matrices. Reducing the ability of apps to just add a larger number of API calls to get classified as benign. \\n\\n# Conclusion\\nThe process of identifying the relationship of API calls, is taking the idea of the subsequent network it creates - thus to not just look at the information queried by the call but also the way it interacts with other API’s in different levels of the codebase. The applicability of this lies beyond that of malware detection in android apps, but probably in the roots of graph theory and how relationships with API calls can be better identified and mapped out to provide more insight\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n## Responsibilities:\\n## Report:\\nNeel Shah: Malware Detection Using API Relationships + Hindroid, Data Generation Process, Observed Data, Conclusion\\nMandy Ma: Malware Detection Using API Relationships + Hindroid, Citation\\n## Code:\\n\\tNeel Shah: Refining codes, create repository, transfer code format to be able to run from terminal\\n\\tMandy Ma: Code algorithms,Debug code, Refining code\\n\\n\\n## Citation\\nO'Dea, Published by S., and Aug 17. “Mobile OS Market Share 2019.” Statista, 17 Aug. 2020, www.statista.com/statistics/272698/global-market-share-held-by-mobile-operating-systems-since-2009/. \\nPanda Security Panda Security specializes in the development of endpoint security products and is part of the WatchGuard portfolio of IT security solutions. Initially focused on the development of antivirus software. “Android Devices 50 Times More Infected Compared to IOS - Panda Security.” Panda Security Mediacenter, 14 Jan. 2019, www.pandasecurity.com/en/mediacenter/mobile-security/android-more-infected-than-ios/\\nApp-press.com. 2020. What Is An API And SDK? - App Press. [online] Available at: <https://www.app-press.com/blog/what-is-an-api-and-sdk#:~:text=API%20%3D%20Application%20Programming%20Interface,usually%20packaged%20in%20an%20SDK.> [Accessed 31 October 2020].\\n“5 Examples of APIs We Use in Our Everyday Lives: Nordic APIs |.” Nordic APIs, 10 Dec. 2019, nordicapis.com/5-examples-of-apis-we-use-in-our-everyday-lives/. \\nShifu Hou, Yanfang Ye ∗ , Yangqiu Song, and Melih Abdulhayoglu. 2017. HinDroid: An Intelligent Android Malware Detection System Based on Structured Heterogeneous Information Network. In Proceedings of KDD’17, August 13-17, 2017, Halifax, NS, Canada, , 9 pages. DOI: 10.1145/3097983.3098026\\n\",\n",
       "  'The paper titled \"HinDroid: An Intelligent Android Malware Detection System\" discusses the importance of developing a strong malware detection system for the Android operating system. The paper highlights the use of API relationships in detecting malware and introduces the Hindroid model, which utilizes meta-path based approaches to identify similarities and detect malware. The data generation process involves obtaining APKs from the Android Play Store and analyzing the smali code and app binaries. The observed data focuses on API calls, code blocks, and packages to classify malware versus benign apps. The conclusion emphasizes the significance of understanding the relationships between API calls and suggests that this approach can be applied beyond malware detection.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_56': ['# Malware Detecting using Control Flow Graphs\\nBy [Edwin Huang](https://www.linkedin.com/in/edwin-huang-671a77100/), [Sabrina Ho](https://www.linkedin.com/in/sabrinaho7/)\\n\\n[Link to Webpage](https://iamsabhoho.github.io/dsc180b-malware/)\\n\\n## Introduction\\nThere are many malware detection tools available in the market, including pattern-based, behavior-based methods, etc, with the prompt development of artificial intelligence, many modern data analysis methods are applied to detecting malware in recent years. We are interested in investigating the effectiveness of different data analysis methods for detecting certain types of malware.\\n\\nAs the number of malicious software (malware) increases throughout the past few decades, malware detection has become a challenge for app developers, companies hosting the apps, and people using the apps. There are many pieces of research conducted on malware detection since it first appeared in the early 1970s. Just like the paper we studied in our first quarter, it uses the HIN (Heterogeneous Information Network) structure to classify the Android applications. It also compared its own method against other popular methods such as Naive Bayes and Decision Tree, and other known commercial mobile security products, to test its performance. The result showed that their method performs better than the other methods with an accuracy of 98% while other others only achieve an average of 90\\\\%. After studying the paper, we are more curious about the detecting effectiveness of an analysis method when applied to a certain type of malware.\\n\\n![Project Pipeline](/img/flow.png)\\n\\nNot everyone has access to tools that can detect whether or not the app they just downloaded is malicious or not. Our motivation to conduct this research is to hope to produce a recommending tool that can be easily accessed by the general public for detecting malware. Optimistically, we want to reduce the chance of people downloading malicious apps and potentially prevent their devices from being hacked. To achieve that, we will be classifying applications using Control Flow Graphs and different similarity-based methods including k-nearest neighbors (kNN) as well as Random Forest classifier to see if different methods can detect certain types of malware or any specific features.\\n\\nWe are interested in analyzing whether one classifier has better performance in detecting certain types of malware or specific features, and designing a framework for recommending a method with a specific set of parameters for a certain type of malware and provide users a more friendly interface. With the similarity-based approach, we believe that it will detect malware with much higher accuracy and will be more flexible for applications that evolved over time as they become more complicated.\\n\\n\\n## Related Work\\n\\n### Mamadroid\\nMamaDroid is a system that detects Android malware by the apps\\' behaviors. This method extract call graphs from APKs, which are represented using nodes and edges in a graph object. From each graph, sequences of probabilities are extracted, representing one feature vector per APK. These probabilistic feature vectors are used for malware classification. MamaDroid also abstracts each API call to the family and package level, which inspired us to abstract to the class level. This is discussed further later.  \\n\\n### Hindroid\\nHindroid is a system that parses SMALI code extracted from APKs and uses them to create four different graphs, which are represented by large matrices. Within these matrices, each value in a matrix corresponds to an edge. A combination of these matrices are used to classify malicious software and benign apps.  \\n\\n### Metapath2Vec\\nMetapath2Vec is a node representation learning model that uses predefined paths based on the node types. These paths define where the the program can traverse the graph. Following is an example of a metapath. In this case below, the metapath is Type 1 → Type 2 → Type 1 → Type 3 → Type 1.\\n\\n![Metapath2vec Example](/img/m2v.png) \\n\\nWith predefined meta-paths, we can traverse a graph according to these node types to generate a large corpus, which is then fed into Node2Vec to obtain representations of words. This method will obtain one vector for one node within the graph.\\n\\n### Word2Vec\\nWord2vec is a model that turns text into numerical representations. It is trained on a large corpus, and outputs a representation for each word in the corpus. Below is a famous example of Word2Vec: King and Queen and Men and Women.\\n\\n![Word2vec Example](/img/w2v.png) \\n\\nSince Word2Vec measures the similarity between words using Cosine similarity, we can see from the above vector space that the word King is similar to Queen, and Men is similar to Women.\\n\\n### Doc2Vec\\nSimilar to Word2Vec, Doc2Vec turns a whole document/paragraph into numerical representations instead of word representations. If we can obtain one corpus from each of the apps by applying metapath2vec, then we can treat each corpus as its own document, and then feed it into the Doc2Vec model to learn representations for each of the documents. These vector representations can then be used in the classification process.\\n\\n\\n## Data\\nThe data we will be using is randomly downloaded from APK Pure and AMD malware dataset. It consists of labeled malware and other popular and unpopular (random) applications. Among our random apps downloaded from APK Pure, there might be one app out of five that might be a malware since they are apps that have little or no reviews. Rather than using .SMALI files, we will be working with APK files directly. From the APK files, we will be extracting a new form of representation called Control Flow Graphs. With APK files, we can easily generate control flow graphs through Androguard, which is a powerful tool to disassemble and decompile Android applications.\\n\\n### Control Flow Graphs\\nA Control Flow Graph (CFG) is a representation using graph notation of all paths that might be traversed through a program during execution. Firstly, a CFG consists of nodes and edges. Control Flow is the order in which individual statements, instructions, or function calls of an imperative program are executed or evaluated. Imperative meaning statements that change a program’s state. Each node in the CFG represents a basic block, or a straight-line piece of code without any jumps or jump targets. In our case, a node in the CFG is an API or method call. A jump statement is a statement that changes the program’s flow into another place of the source code. For example, from line 4 to line 60, or from file 1 to file 6. The following figures are two simple control flow graphs.\\n\\n![A Simple Control Flow Graph](/img/cfg.png)\\n\\nA node in our CFG can call another API (node). A node can be visualized as one of the circles in Figure 4, and the \"call\" action can be visualized by the arrow(edge). Each node has attributes. There are 7 Boolean attributes for each node, and 3 different edge types.\\n\\n| Node Attributes | |\\n| --- | --- |\\n| External | If a node is an external method |\\n| Entrypoint | If a node is not called by anything |\\n| Native | If a node is native |\\n| Public | If the node is a public method |\\n| Static | If the node is a static method |\\n| Node | If none of the above are True |\\n| APK Node | If the node is an APK |\\n\\nWe can pick from the 6 Boolean attributes and create node types based, such as: “external, public Node”, “external, static Node” and “entrypoint, native Node”. There can be more than 20 different node types.  \\n\\n| Edge Types | |\\n| --- | --- |\\n| Calls | API to API |\\n| Contains | APK to API |\\n| In | API to APK |\\n\\nTogether, nodes and edges can build paths like: “external, public Node - calls -> external, static Node” or “APK - contains -> external, public Node”. The following is a control flow graph example with code block to explain how nodes are called:\\n\\n```\\nClass: Lclass0/package0/exampleclass; ## let’s call this A\\n# direct methods\\n.method public constructor <init>()V\\n    if ....:\\n    \\t# api, call this B:\\n    \\tLclass1/package1/example;->doSomething(Ljava/lang/String;)V\\n\\telse:\\n    \\t# api, call this C:\\n    \\tLclass2/package2/example;->perform(L1/2/3)F\\n```\\n\\n![Control Flow Graph With Code Block Example](/img/cfg1.png)\\n\\nThe method **constructor** calls API A, which calls API B: **doSomething** and calls API C: **perform**. Since API B and API C will jump to other places within the source code, the flow of the program is broken, and this jump is recorded in the control flow graph. \\n\\nThe Control Flow Graph from an app records tens of thousands of these calls, and represents them as edges, where each edge contains two nodes.\\n\\n\\n### Common Graph\\nSince we obtained a large number of CFGs for a large number of APKs, we need to figure out a way to connect all of these graphs so the representations for each API will be the same. We want the API representations to be the same so when we are classifying we know that all feature vectors are built the same way. This is to avoid us creating random feature vectors, and will result in the model classifying randomly. To make sure we are building features correctly, we must create a common graph that links every CFG together. Also, during the testing phase, we can use these node embeddings to build a feature vector for an unseen app.\\n\\nThe common graph we built contains a total of 1,950,729 nodes, and 215,604,110 edges. Our common graph is simply a union of all the control flow graphs that we obtained from separate apps. This is not only to make sure that each distinct API node are consistent throughout our training and testing process, but to make sure that all our CFGs are on the same space. First, all the edges of each separate graphs are extracted, along with their weights and node types of each edge. Then, these information are loaded altogether to become a common graph. \\n\\n![Common Graph Example](/img/common_graph2.png)\\n\\nThe figure above demonstrates what a common graph looks like by combining two control flow graphs from two different apps. On the left we have red and blue applications, which both have five nodes consisting of A, B, and C nodes connected together and other nodes of its own. When combining them, we generate the graph on the right, which merge the shared nodes with each other. Duplicate nodes are joined to be one, while the edges are still preserved. As you can see, the similar A, B, C sequence is preserved as well. This is important when we are building feature vectors for an app. The common graph ensures the same node representations for two graphs. This means that when we are building feature vectors for apps, the same representations are used for two similar apps. Conversely, if two apps are not similar and do not share the same sequences, then their representations will be very different. Like in the common graph in Figure 6, this will make sure that the similarity and differences between the apps are preserved.\\n\\n### Data Generating Process/ETL\\nThe raw data we are investigating is code written by app developers. In order to turn something into a malware, you have to alter the source code, which will allow hackers to plant certain types of malicious code. If a developer were to hijack a device, then the app would need special Root permissions. Often targeting API calls that represent System Calls is one of the ways to alter the source code. For that reason, source code is an essential part in determining whether an application is malicious or not. \\n\\nAs mentioned above, we will be using control flow graphs converted directly from the APK files. We are looking at the sequence of which these system calls are made and define them as meta paths. We were able to obtain one CFG for each APK. We extracted this by using Androguard’s AnalysisAPK method in its misc module which returns an analysis object. Afterwards, we called .get\\\\_call\\\\_graph() on the analysis object to obtain the CFG. At this stage, we also perform some feature extraction specifically on the nodes of the graph. We extract the string representation of these nodes as well as node type. The string representations of nodes is used to build the corpus, and the node type is used to build meta paths. We then exported this graph as a compressed gml file to save on disk. We hypothesize that our method will perform better than our baseline model, since metapath2vec can capture the relations and context within the graphs, giving the feature vector much more information. Also, our metapaths are traversed in the beginning of our process to learn all possible metapaths. Using this method, we ensure that the model learns the different sequences that a malware could have, and use this information in future classification. \\n\\n### EDA\\nWe have a total of 8435 malicious software and a total of 802 benign applications, which is a combination of popular apks and random apps. While generating control flow graph objects from the APK files, there was an error of “*Missing AndroidManifest.xml*,” so we were not able to generate those graphs and will be working with fewer benign apps \\\\footnote{We looked into why there might be missing Android Manifest files error, interestingly, we found that some of the apps having this issue contain the manifest while some do not. However, the apps that have this issue do not decompile correctly, and do not create a graph correctly as well.}. To counteract the imbalance between malware and benign apps, we calculated class weights and used it in the classification process to ensure we are penalizing the model in a balanced way.\\n\\n![Benign vs. Malicious](/img/bm_counts.png)\\n\\nTo further understand our benign and malicious data, we perform analysis on these graph objects by comparing the node types, as well as the counts of both nodes and edges.\\n\\n![Benign vs. Malicious: Node Type Counts](/img/bv_node_types.png)\\n\\nThe figure above shows the comparisons of node types between benign applications and malicious code. From the two distributions, we can see that malware contains a lot more types of nodes compared to benign apps. Specifically, most of the malware contains five types of node. If we limit the range of that bar, we can see the figure below for a more clear distribution of benign apps. The left distribution also indicates that majority of the benign apps have five types of nodes.\\n\\n![Benign vs. Malicious: Node Type Counts](/img/bv_node_types1.png)\\n\\nBut because of how imbalanced our data is, we plot the number of node types based on the percentage, as shown in the below figure. From this figure we can conclude that over half of both benign and malicious apps have more than 5 types of nodes. \\n\\n![Benign vs. Malicious: Node Type Counts (%)](/img/bv_node_types_p.png)\\n\\nTo further look into what are these node types, we analyze the top node types from both benign and malware separately. The following figures are node types distributions. On the left shows the benign node types spread, which over 50\\\\% of the nodes are Public Node, followed by Node, External Node, Public Static Node, Static Node, and the other types. Similarly, for malware, Public Node is the top most node type found in the graphs. Followed by External Node, Node, Public Static Node, Static Node, and the others. Both benign and malware have similar top nodes.\\n\\n![Benign: Top Node Types and The Counts](/img/counts_pie_b.png)\\n![Malicious: Top Node Types and The Counts](/img/counts_pie_m.png)\\n\\nThe figure below is a scatter plot of number of edges and number of nodes for both benign (red) and malware (blue). Although it seems that benign has a lot more apps in this plot, malware are just all packed together. We can also see that benign apps have larger number of edges and nodes compared to malware, this is because benign apps are larger in terms of APK sizes and that they might be more complicated.\\n\\n![Benign vs. Malicious: Number of Edges and Number of Nodes](/img/bm_edges_nodes.png)\\n\\nSince the figure above has outliers for benign apps and we want to focus on the malware, we limit the range so that it looks like the figure below. From this figure we see that malware is indeed packed together and that they have a lot less edge and nodes compared to the benign apps.\\n\\n![Benign vs. Malicious: Number of Edges and Number of Nodes (Malware Focused)](/img/bm_edges_nodes1.png)\\n\\n\\n## Methods\\n\\n### Feature Extraction\\nFor our baseline, we extract probabilistic sequences from all possible edges of the APK, which serves as the feature vector for classification. For the Metapath2Vec model, we first create a common graph, then traverse it using Metapath2Vec to learn representations of nodes, which is used to build feature vectors. For our Doc2Vec method, we treat each APK as one document, and Doc2Vec produces one feature vector for one document.\\n\\n### Baseline: Mamadroid\\nWe build Mamadroid as our baseline model. As introduced earlier, it extract call graphs that are represented using nodes and edges. With the graphs, it extracts all the possible edges based on the family or package level. It then extract sequences of probabilities of the edges occurring. This probabilistic feature vector is used for classification. We abstract API calls to both Family and Package level.\\n\\nFor example, \\n```\\nExample API call = \"LFamily/Package\"\\nFamily Level = \"LFamily\"\\nPackage Level = \"LFamily/Package\"\\n```\\n\\nIn Family level, there are seven possible families and 100 total possible edges. However, in Package level, there are 226 possible packages and a total of 51,239 possible edges. The number of possible families and packages are found on Android\\'s Developers page. Those families and packages that are not found on that webpage is abstracted to \"self-defined\". Specifically, in family level, we will obtain a feature vector of 100 elements for one app. In package level, we obtain a feature vector with 51,239 elements for one app. These feature vectors are then used for classification. After obtaining the vector embeddings, we classify using Random Forest model, 1-Nearest Neighbors, and 3-Nearest Neighbors.\\n\\n### Metapath2Vec Model Using Common Graph\\nAs mentioned earlier, we also abstracted our API calls to the class level. For example, an API call looks like this: \"Lfamily/package/class; → doSomething()V\" at the class level, it is: \"Lfamily/package/class;\". The reason for this is there could be user-defined classes, which is not picked up in MamaDroid. We hope that we can obtain more information by abstracting to the class level, but not get too much information at the API level which might result in performance issues. We do not abstract anything to be \"self-defined\" as MamaDroid has.\\n\\n1. Run a Depth First Search to explore all the node types that could be in an APK, and create metapaths.\\n2. Build a common graph by combining all the separate control flow graphs representing different apps.\\n3. Perform an uniform metapath walk on the common graph to obtain a huge corpus.\\n4. Perform node2vec to learn node representations of the huge corpus.\\n5. Build feature vectors for each app, using the node embeddings learned from step 3, by combining embeddings of unique nodes of each app.\\n6. Classification using built feature vectors.\\n\\nExplanations: \\nWe run a depth first search to explore all node types and metapaths since we do not know how convoluted an app\\'s CFG may be and we need flexible metapaths for each app. Also, there is the possibility of the malware being intentionally obfuscated. Therefore, we need flexible metapaths for each app, which we will later use as the predefined metapaths in our metapath2vec step. The reason our feature vector is a component wise combination of node embeddings is because when two vectors are added together, a new vector is obtained. As visualized below:  \\n\\n![Vector Addition And Subtraction](/img/vec.png)\\n\\nThis will provide more information about the APKs that we will classify. The model can more easily learn the distinction between similar and different vectors, by the direction and magnitude to where they point. Of course, the component wise combination can also be other aggregations, such as taking an average, percentiles, and dot products. When encountering an unseen app, unique nodes of that app is extracted. Representations of each of the unique nodes are then found from the trained word2vec model, and some component wise combination is performed to obtain a feature vector for classification. \\n\\n### Doc2Vec Model\\n1. For each app, extract all possible metapaths using Depth First Search, as well as perform metapath2vec on that app to obtain a corpus.\\n2. Take each corpus from each app, append them, and turn them into a list of Tagged Documents.\\n3. Run the Tagged Documents into Doc2Vec to obtain a vector representation for each app.\\n4. Take the vector representations for each tagged document, and use them as feature vectors for classification.\\n\\nThe Doc2Vec model is very straight forward, taking in documents and returning representations for those documents. When there is an unseen app, a corpus is extracted from that app using metapath and is treated as a document. This document is then fed into the Doc2Vec model, and a vector representation is \"inferred\" using the .infer_vector() method.  \\n\\n\\n## Results and Analysis\\n### Baseline Results\\nThe following tables are results from our baseline model, MamaDroid, corresponding to Family and Package mode. Surprisingly, our MamaDroid using control flow graphs performs better than its original model. Let\\'s first take a look at the Family mode. The table below is the confusion matrix, we calculated precision and recall scores based on it.\\n\\n| | Random Forest | 1-NN | 3-NN |\\n| --- | --- | --- | --- |\\n| True Negative | 68 | 61 | 59 |\\n| False Negative | 4 | 9 | 8 |\\n| False Positive  | 11 | 18 | 20 |\\n| True Positive | 1269 | 1264 | 1265 |\\n\\nWe compare the results to the original MamaDroid model. In the table below, we list the original MamaDroid results on it as well to better compare it. We see that our version of MamaDroid has better performance in all F1-Score, precision and recall scores, where we obtain an F measure of 0.994 and the original model only has 0.880. Similarly to precision and recall scores, we obtain 0.991 and 0.997, where the original model has 0.840 and 0.920 as their results. In addition to Random Forest, our 1-NN and 3-NN models also outperform the original MamaDroid model. But all our three models have similar results.\\n\\n<table>\\n    <tbody>\\n        <tr>\\n            <td rowspan=2>PCA = 10 Component</td>\\n            <td colspan=2>Random Forest</td>\\n            <td rowspan=2>1-NN</td>\\n            <td rowspan=2>3-NN</td>\\n        </tr>\\n        <tr>\\n            <td>Original</td>\\n            <td>Ours</td>\\n        </tr>\\n        <tr>\\n            <td>F1-Score</td>\\n            <td>0.880</td>\\n            <td>0.994</td>\\n            <td>0.980</td>\\n            <td>0.994</td>\\n        </tr>\\n        <tr>\\n            <td>Precision</td>\\n            <td>0.840</td>\\n            <td>0.991</td>\\n            <td>0.985</td>\\n            <td>0.994</td>\\n        </tr>\\n        <tr>\\n            <td>Recall</td>\\n            <td>0.920</td>\\n            <td>0.997</td>\\n            <td>0.994</td>\\n            <td>0.994</td>\\n        </tr>\\n    </tbody>\\n</table>\\n\\nNext, we have results for our MamaDroid Package mode. The following table is the confusion matrix. The numbers are close to what we obtain for Family level. However, the true negatives for all three similarity-based models are slightly larger.\\n\\n| | Random Forest | 1-NN | 3-NN |\\n| --- | --- | --- | --- |\\n| True Negative | 70 | 70 | 70 |\\n| False Negative | 1 | 6 | 1 |\\n| False Positive  | 15 | 15 | 15 |\\n| True Positive | 1266 | 1261 | 1266 |\\n\\nWe also compared the results of Package mode to the original MamaDroid results. In the table shown below, we also list out the results of original MamaDroid on the left to compare it with the ones we obtain. As a result, our model was able to achieve an F measure of 0.993, precision score of 0.988, and recall score of 0.999, whereas the original model only has performance of 0.940, 0.940, and 0.950. Both 1-NN and 3-NN models also have very similar numbers as Random Forest model.\\n\\n<table>\\n    <tbody>\\n        <tr>\\n            <td rowspan=2>PCA = 10 Component</td>\\n            <td colspan=2>Random Forest</td>\\n            <td rowspan=2>1-NN</td>\\n            <td rowspan=2>3-NN</td>\\n        </tr>\\n        <tr>\\n            <td>Original</td>\\n            <td>Ours</td>\\n        </tr>\\n        <tr>\\n            <td>F1-Score</td>\\n            <td>0.940</td>\\n            <td>0.993</td>\\n            <td>0.992</td>\\n            <td>0.994</td>\\n        </tr>\\n        <tr>\\n            <td>Precision</td>\\n            <td>0.940</td>\\n            <td>0.988</td>\\n            <td>0.988</td>\\n            <td>0.988</td>\\n        </tr>\\n        <tr>\\n            <td>Recall</td>\\n            <td>0.950</td>\\n            <td>0.999</td>\\n            <td>0.995</td>\\n            <td>0.999</td>\\n        </tr>\\n    </tbody>\\n</table>\\n  \\n### Metapath2Vec/Common Graph (Partial)\\nFor our Metapath2Vec model, we unfortunately do not have the complete results due to large computational time it takes to build the common graph with all the data we have. The complete common graph consists of 1,950,729 nodes, and 215,604,110 edges. However, we did obtain results working with a smaller subset of the Common Graph, consisting of: 87,539 nodes and 15,617,223 edges.\\n\\n| | Random Forest | 1-NN | 3-NN |\\n| --- | --- | --- | --- |\\n| True Negative | 94 | 89 | 73 |\\n| False Negative | 16 | 31 | 36 |\\n| False Positive  | 20 | 25 | 41 |\\n| True Positive | 1679 | 1664 | 1659 |\\n\\nWe tested on the entire test set, and surprisingly the performance was okay. Initially, we thought that there might be an error, however, upon inspecting our code, we were traversing the smaller common graph correctly. We believe we can obtain this result because of the large amount of edges in the common graph as well as our walk length of 500. We set the walk length to 500 to compensate for the smaller subset of graphs that we are using, and therefore can capture more information per walk. Because of this, the Word2Vec model can learn more about those nodes and provide a better representation. \\n\\n| | Random Forest | 1-NN | 3-NN |\\n| --- | --- | --- | --- |\\n| TF1-Score | **0.989** | 0.983 | 0.977 |\\n| Precision | 0.992 | 0.966 | 0.982 |\\n| Recall | 0.642 | 0.959 | 0.959 |\\n\\nEven though the F1-Scores were high, our True Negative and False Positives are higher than our baseline and Doc2vec model. This is again due to the smaller subset that we are using for this experiment. Because of the smaller subset, we do not have a lot of representations for nodes. There could have been some nodes that the word2vec model has never seen before, and therefore cannot infer a good representation for it. \\n\\n### Doc2Vec\\nThe following tables are the results for Doc2Vec with our similarity-based models: Random Forest, 1-NN, and 3-NN. Table below is the confusion matrix, which is used to compute the precision and recall scores. The Doc2Vec performed worse than the baseline model on the Random Forest model, and it seems like it was struggling with classifying benign apps.\\n\\n| | Random Forest | 1-NN | 3-NN |\\n| --- | --- | --- | --- |\\n| True Negative | 109 | 56 | 43 |\\n| False Negative | 606 | 68 | 71 |\\n| False Positive  | 5 | 58 | 31 |\\n| True Positive | 1089 | 1627 | 1664 |\\n\\nIn the table below, we have our F measure, precision, and recall scores. We notice that our Random Forest model only has an F measure of 0.781, which is a lot lower than our baseline. On the other hand, both our k Nearest Neighbors perform much better than Random Forest. The Random Forest classifier has an emphasis on certain features when training, and focuses on some feature more than others. However, the 1-NN and 3-NN models both look at an unseen vector\\'s closest neighbors, therefore utilizing all the features in the vector. We believe this is why the 1-NN and 3-NN models performed better in this experiment.\\n\\n| | Random Forest | 1-NN | 3-NN |\\n| --- | --- | --- | --- |\\n| TF1-Score | 0.781 | 0.963 | **0.970** |\\n| Precision | 0.992 | 0.966 | 0.982 |\\n| Recall | 0.642 | 0.959 | 0.959 |\\n\\n\\n## Conclusion\\nIn conclusion, our baseline model is able to achieve a better performance than the original work that we have studied. Although our Doc2Vec did not perform better than the baseline Random Forest model, our k Nearest Neighbors models performed almost as good as our baseline. From this, we can see that Control Flow Graphs might be a good choice when it comes to choosing representations for source code. Again, control flow graphs show the jumps in code. From our EDA: Figure 12, even though malware has a small number of nodes, they have a large amount of edges. This means that there could be lots of instances where the program is jumping around in the source code. All this is recorded in the CFG representation and could provide much more information about an APK.\\n\\n Although we successfully created a complete common graph, we were unable to obtain all the node embeddings from it due to time and memory constraints. Therefore we built a smaller common graph to see how it performs. If time and resources allowed, we hope to finish the metapath traversal of the complete common graph. Judging from the results using the smaller common graph, if we were to scale up the model might out-perform our baseline.\\n\\nFor our future work, we plan on investigating other vector embeddings technique and perhaps instead of using only similarity-based models, we could also implement graph neural networks (GNN). In addition to neural networks, we are also interested in graph classification specifically. Since our data format is already in the form of multiple apps, it can be easy to normalize and transform data for a GNN model. Of so many researches we have seen on malware detection, not a lot of them uses control flow graphs as their input data. Since our experiments confirmed that using control flow graphs is not any worse than using other forms of data, we are curious to know if control flow graphs can outperform in other models.\\n\\n## Acknowledgement\\nWe would like to express our gratitude to our mentors for our Capstone project: Professor Aaron Fraenkel, who provided us with lots of resources and ideas throughout the entire process of our research, and Shivam Lakhotia, who guided us through the project and assisted us every week. \\n\\n## References\\n[1] MamaDroid, https://arxiv.org/pdf/1612.04433.pdf\\n\\n[2] Hindroid, https://www.cse.ust.hk/~yqsong/papers/2017-KDD-HINDROID.pdf\\n\\n[3] Metapath2Vec, https://ericdongyx.github.io/papers/KDD17-dong-chawla-swami-metapath2vec.pdf\\n\\n[4] Word2Vec, https://radimrehurek.com/gensim/models/word2vec.html\\n\\n[5] Doc2Vec, https://radimrehurek.com/gensim/models/doc2vec.html\\n\\n[6] Androguard, https://androguard.blogspot.com/2011/02/android-apps-visualization.html\\n\\n[7] StellarGraph, https://github.com/stellargraph/stellargraph\\n\\n[8] SDK, https://developer.android.com/studio/releases/platforms\\n\\n[9] x2vec, https://iopscience.iop.org/article/10.1088/2632-072X/aba83d/pdf\\n\\n[10] Learning Embeddings of Directed Networks with Text-Associated Nodes—with Application in Software Package Dependency Networks, https://arxiv.org/pdf/1809.02270.pdf\\n\\n',\n",
       "  'The authors of this research paper investigate the effectiveness of different data analysis methods for detecting certain types of malware. They focus on analyzing control flow graphs and using similarity-based methods such as k-nearest neighbors and random forest classifiers to detect malware. They also compare their results to a baseline model called MamaDroid, which extracts call graphs from APKs and uses probabilistic feature vectors for classification. The authors also explore the use of metapath2vec and doc2vec models for feature extraction. They analyze the data, evaluate the performance of their models, and discuss future work and potential improvements.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_59': ['![Docker Cloud Build Status](https://img.shields.io/docker/cloud/build/rcgonzal/m2v-adversarial-hindroid)\\n\\n# m2vDroid: Perturbation-resilient metapath-based Android Malware Detection\\nAn extension of the [HinDroid malware detection system](https://www.cse.ust.hk/~yqsong/papers/2017-KDD-HINDROID.pdf), but using [metapath2vec](https://ericdongyx.github.io/metapath2vec/m2v.html) to encode apps in the Heterogeneous Information Network. We then hope to make the model resilient to adversarial ML like Android HIV. See our [blog post](https://rcgonzalez9061.github.io/m2v-adversarial-hindroid/) for more details on our methods.\\n\\n## Setup and Usage\\nTo recreate our results, please use our Docker image: `rcgonzal/m2v-adversarial-hindroid` and have access to a directory of Android apps decompiled into smali. (This can be done with the [APKtool](https://ibotpeaches.github.io/Apktool/) and [Smali](https://github.com/JesusFreke/smali) -- included with our Docker image)\\n\\nOur project can be run using `python run.py [data] [analysis] [model]` with each tag corresponding to different workflows and being executed in the order shown.\\n\\nThe `data` flag will trigger our ETL workflow. It parses apps, constructs the HIN, and then generates `features.csv` using metapath2vec. It will read additional parameters from `config/etl-params/etl-params.json`. Note that including `data_source` will search for apps in that directory and subdirectories, creating `app_list.csv`. Otherwise, an `app_list.csv` can be specified by simply placing it within `outfolder`:\\n\\n```json\\n{\\n    \"outfolder\": \"Path where graph data will be saved\",\\n    \"parse_params\": {\\n        \"data_source\": \"Path to folder of decompliled apps, optional.\",\\n        \"nprocs\": \"Number of threads to use when parsing\",\\n        \"recompute\": \"Boolean, whether or not to reparse apps. Default, skips apps that exist in app data heap\"   \\n    },\\n    \"feature_params\": {\\n        \"walk_args\": \"Arguments for stellargraph.data.UniformRandomMetaPathWalk\",\\n        \"w2v_args\": \"Arguments for gensim.models.Word2Vec, excl. walks\"\\n    }\\n}\\n```\\n\\nThe `analysis` flag will generate analysis on our data and any necessary plots, reading in additional parameters from `config/analysis-params/analysis-params.json`. `jobs` is a dictionary of jobs to be performed along with their parmeters. Currently the only available job is `plots`:\\n\\n```json\\n{\\n    \"data_path\": \"path to folder of data to load (akin to the outfolder in etl-params)\",\\n    \"jobs\": {\\n        \"plots\": {\\n            \"update_figs\": \"Boolean, whether or not to update figures in report and blog post\",\\n            \"no_labels\": \"Boolean, whether or not to include class labels on plots\"\\n        }\\n    }\\n}\\n```\\n\\n',\n",
       "  'The m2vDroid project is an extension of the HinDroid malware detection system that uses metapath2vec to encode apps in the Heterogeneous Information Network. The goal is to make the model resilient to adversarial machine learning attacks like Android HIV. To recreate the results, use the Docker image rcgonzal/m2v-adversarial-hindroid and have access to a directory of Android apps decompiled into smali. The project can be run using python run.py [data] [analysis] [model], with each tag corresponding to different workflows. The data flag triggers the ETL workflow, while the analysis flag generates analysis on the data and necessary plots.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_38': ['### Background\\n\\nThe purpose of phrase mining is to extract high-quality phrases from a large amount of text corpus. It identifies the phrases instead of an unigram word, which provides a much more understanding of the text.  In this study, we apply AutoPhrase method into two different datasets and compare the decreasing quality ranked list of phrase ranked list in multi-words and single word. Our datasets are from the abstract of Scientific papers in English with the English knowledge base from Wikipedia. Through this project, we will be able to understand the advantages of the AutoPhrase method and how to implement Autophrase in two datasets by identifying different outcomes it produces. \\n\\n\\n### Requirements\\n##### If you run in the local:\\nLinux or MacOS with g++, Java and gensim installed.\\n\\n\\n##### You can also use our docker images to run the code. No need any install. It stores in submission.json file.\\n\\n\\n### Purpose of the Code\\n\\nFor Final Replication, our code would do the data ingestion proportion first, to pull data as the input corpus for future use from the cloud. Then to perform some basic EDA on it. We would run the autophrase algorithm along with phrasal segmentation, analyzing the results. At the end, we manually label the high-quality phrases and select 3 phrases and put those into the phrase embedding model to return five most similar phrases as the result.\\n\\n### Code Content\\nSome Python Scripts, involved in etl.py, eda.py, auto.py,visual.py, and example.py to download, process data, analyze, visualize data and find the most similar phrase by building the model.\\n\\n\\t\\n### How to Run the Code\\n\\n##### To get the data:     -      run python run.py etl\\n\\n\\nThis downloads the data from Illinois University in the directory specified in config/etl-params.json and do data cleaning process. You can find the result in the data/raw folder.\\n\\n\\n\\n\\n##### To do the EDA for the data     -       run python run.py eda\\n\\n\\nThis performs exploratory data analysis and saves the figures in the location specified in config/eda-params.json. You can find the graphs in the data/eda folder.\\n\\n\\n\\n##### To run autophrase algorithm and get the segementation result       -        run python run.py auto\\n\\n\\nThis performs autophrase algorithm and phrasal segmentation saves the results in the location specified in config/auto-params.json. You can find the result in the data/output folder.\\n\\n\\n\\n##### To analyze the output of autophrase           -         run python run.py visual\\n\\n\\nThis performs analysis on the results and saves the figures in the location specified in config/visual-params.json. You can find the two distributions in the data/output folder.\\n\\n\\n##### To find the most 5 similar phrases           -         run python run.py example\\n\\n\\nThis ask the users to manually label the high-quality phrase. It builds the word2vec model on the phrasal segmentation results to obtain phrase embedding based on random sampleing. It also report the top-5 similar phrases based on the 3 high-quality phrases from your previous annotations.However, if the users want to try their own sampling, they can manually label the high-quality phrases in sample.txt, which stored the output file by changing the configuration. You can find the result in the data/output/example folder.\\n\\n\\n##### To run whole project       -          run python run.py all\\n\\nIt will complete the whole process with results. The defualt is the dataset DBLP.txt, if you want to try other dataset, edit the configuation file to your own dataset. All of the input and result can find the in the data folder.\\n\\n\\n##### To make a test run.          -        run python run.py test\\n\\nIt will implement dataset DBLP.5k.txt, which is a test data to check the whole process is working. DBLP.5k.txt is sampled from the original dataset DBLP.txt. This compares the result between tf-idf scores, autophrase quality scores, and their multiplication.\\n\\n\\n### Notebook Contents\\nIn the notebook, it will help the users visulized all the results from the run.py with some brief explanations.\\n\\n\\n\\n### Work Cited\\n\\nProfessor Jingbo Shang’s Github: https://github.com/shangjingbo1226/AutoPhrase\\n\\n\\nJingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, Jiawei Han, \"Automated Phrase Mining from Massive Text Corpora\", accepted by IEEE Transactions on Knowledge and Data Engineering, Feb. 2018.\\n\\n\\n\\n\\n### Responsibilities\\nWe discussed the general idea of the replication project and outlined the steps of the process together.\\n\\n\\nTiange Wan: some of code portion and report portion, and revised the report portion.\\n\\n\\nYicen Ma: some of code portion and report portion, and revised the code portion.\\n\\n\\nAnant Gandhi: participated in another repo\\n\\n\\n\\n\\n\\n',\n",
       "  'The purpose of this study is to extract high-quality phrases from a large text corpus using the AutoPhrase method. The study compares the quality of phrases ranked in multi-words and single words using two different datasets. The code provided performs data ingestion, exploratory data analysis, autophrase algorithm, phrasal segmentation, and analysis of the results. It also allows users to manually label high-quality phrases and find the five most similar phrases using a phrase embedding model. The code can be run using different commands specified in the instructions. The notebook provides visualizations of the results obtained from running the code. The work cited includes a GitHub repository and a research paper on automated phrase mining. The responsibilities for this project were divided among the team members.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_42': ['# Analyzing Movies Using Phrase Mining\\n\\nhttps://a04-capstone-group-02.github.io/movie-analysis-webpage/\\n\\n## Setup\\n\\n### Clone the repository\\n\\n```\\ngit clone --recursive https://github.com/A04-Capstone-Group-02/movie-analysis.git\\n```\\n\\n### Download dataset\\n\\nDownload the [CMU Movie Summary Corpus dataset](http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz) and move its files to `data/raw/`, or run the `download` target.\\n\\nNote that to run this repository on the UCSD DSMLP server, the dataset must be manually uploaded, since the DSMLP server cannot connect to the data source link.\\n\\n### Docker\\n\\nBuild a docker container with the `Dockerfile` or the remote image `991231/movie-analysis` in the docker hub.\\n\\n### Note\\n\\nTo run the `clustering` target, we highly recommend enabling GPU to ensure reasonable running time, since this target heavily interacts with a transformer model. Running other targets without GPU will not be an issue.\\n\\n## Run\\n\\nExecute the running script with the following command:\\n\\n```\\npython run.py [all] [test] [download] [data] [eda] [classification] [clustering]\\n```\\n\\n### `all` target\\n\\nRun `data`, `eda`, `classification` and `clustering` targets in this exact order.\\n\\n### `test` target\\n\\nRuns the same 4 targets in the same order as the `all` target, but using the test data in `test/data/raw` and the test configurations.\\n\\n### `download` target\\n\\nDownload the CMU Movie Summary Corpus dataset and set up the `data` directory.\\n\\n### `data` target\\n\\nRun the ETL pipeline to process the raw data. This target will run AutoPhrase to extract quality phrases, clean categories, combine the processed data into a dataframe, and generate a profile report of the dataset.\\n\\nThe configuration file for this target is `etl.json` (or `etl_test.json` for `test` target), which contains the following items:\\n\\n- `data_in`: the path to the input data (relative to the root)\\n- `false_positive_phrases`: phrases to remove from the quality phrase list\\n- `false_positive_substrings`: substrings to remove from the quality phrase list\\n\\nThe configuration file for the AutoPhrase submodule is `autophrase.json`, which contains the following items:\\n\\n- `MIN_SUP`: the minimum count of a phrase to include in the training process\\n- `MODEL`: the path to the output model (relative to the root)\\n- `RAW_TRAIN`: the path to the raw corpus for training (relative to the root)\\n- `TEXT_TO_SEG`: the path to the raw corpus for segmentation (relative to the root)\\n- `THREAD`: the number of threads to use\\n\\n### `eda` target\\n\\nRun the EDA pipeline. This target will find the temporal change of quality phrase distributions and generate visualizations to show the findings.\\n\\nThe configuration file for this target is `eda.json` (or `eda_test.json` for `test` target), which contains the following items:\\n\\n- `data_in`: the path to the input data (relative to the root)\\n- `data_out`: the path to the output directory (relative to the root)\\n- `example_movie`: example movie to profile\\n- `year_start`: the earliest year to analyze\\n- `year_end`: the latest year to analyze\\n- `decade_start`: the earliest decade to analyze\\n- `decade_end`: the latest decade to analyze\\n- `phrase_count_threshold`: the minimum count of a quality phrase to be included in the analysis\\n- `stop_words`: the stop words to ignore in the analysis\\n- `compact`: whether to output a full or compact visualization\\n- `n_bars`: number of bars to display in the bar plots\\n- `movie_name_overflow`: number of characters in visualization until ellipses\\n- `dpi`: subplot dpi (dot per inches)\\n- `fps`: fps (frame per second) of the bar chart race animation\\n- `seconds_per_period`: the time each subplot will take in the bar chart race animation\\n\\n### `classification` target\\n\\nRun the classification pipeline. This target will transform the data into a TF-IDF matrix, fit a one-vs-rest logistic regression as the classifier and tune the parameters if specified.\\n\\nThe configuration file for this target is `classification.json`, which contains the following items:\\n\\n- `data`: the path to the input data (relative to the root)\\n- `baseline`: a boolean indicator to specify running baseline (true-like) or parameter tuning (false-like)\\n- `top_genre`: a number to specify the number of genres in the final output plot, default is 10 \\n- `top_phrase`: a number of specify the number of words/phrases in the final output plot, default is 10\\n\\n### `clustering` target\\n\\nRun the clustering pipeline. This target will pick representative sentences based on average sublinear TF-IDF score on the quality phrases, calculate document embeddings by average the Sentence-BERT embeddings of the representative sentences, and visualize the clusters.\\n\\nThe configuration file for this target is `clustering.json`, which contains the following items:\\n\\n- `clu_num_workers`: the number of workers to use\\n- `clu_rep_sentences_path`: the path to the checkpoint representative sentences file (relative to the root), or an empty string `\"\"` to disable the checkpoint\\n- `clu_doc_embeddings_path`: the path to the checkpoint document embeddings file (relative to the root), or an empty string `\"\"` to disable the checkpoint\\n- `clu_dim_reduction`: the dimensionality reduction method to apply on the document embeddings for visualization, choose one from `{\"PCA\", \"TSNE\"}`\\n- `clu_sbert_base`: the sentence transformer model to use, can be either a pretrained model or a path to the saved model\\n- `clu_sbert_finetune`: enable finetuning or not\\n- `clu_sbert_finetune_config`: configurations for finetuning, will only be used if finetuning is enabled\\n  - `train_size`: total number of training pairs to sample\\n  - `sample_per_pair`: number of training pairs to sample per sampled document pair\\n  - `train_batch_size`: batch size for training\\n  - `epochs`: number of epochs to train\\n- `clu_num_clusters`: number of clusters to generate\\n- `clu_num_rep_features`: number of top representative features to store\\n- `clu_rep_features_min_support`: the minimum support of a feature to be analyzed with summarizing the clusters\\n\\n## Contributors\\n\\n- Daniel Lee\\n- Huilai Miao\\n- Yuxuan Fan\\n',\n",
       "  'The given text provides instructions and information about analyzing movies using phrase mining. It includes details on how to set up the project, run different targets, and the configuration files required for each target. The text also mentions the contributors of the project.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_41': [\"# AutoPhrase for Financial Documents Interpretation \\n\\nOur main targets are data preparation, feature encoding, eda (optional), train, report (optional), and test. Users can configure parameters for these targets in the ./config files.\\n\\n\\n## Data Prep\\n\\nThe data preparation target scrapes, cleans, and consolidates companies' 8-K documents. Furthermore, it curates features such as EPS as well as price movements for the given companies.\\n<br />\\n* `data_dir` is the file path to download files: 8-K's, EPS, etc.\\n* `raw_dir` is the directory to the raw data\\n* `raw_8k_fp` is the file path with newly downloaded 8-K's (should be the same as to_dir)\\n* `raw_eps_fp` is the file path with newly downloaded EPS information (should be the same as to_dir)\\n* `processed_dir` is the directory to the processed data\\n* `testing` is the status of whether we are doing testing (by default is false)\\n\\n\\n## Feature Encoding\\n\\nThe feature encoding target creates encoded text vectors for each 8-K: both unigrams and quality phrases outputed by the AutoPhrase method.\\n<br />\\n* `data_file` is the file path with all data files from data prep target: processed, raw, models, etc.\\n* `phrase_file` is the file path to the quality phrases outputted by AutoPhrase\\n* `n_unigrams` sets the top n unigrams to be encoded based on PMI (may not be exacly `n_unigrams` total due to overlap of top unigrams within each class)\\n* `threshhold` takes quality phrases with a quality score above it to be encoded\\n\\n\\n## Train\\n\\nTrains Random Forest models using 3 feature sets on encoded data: baseline, baseline + unigrams, and baseline + phrases. The selected classifier and set model parameters were decided through comparing validation accuracy.  \\n\\n* `data_dir` is the file path with all data files from data prep targed: processed, raw, models, etc.\\n* `input_file` is the file path (from `data_dir`) to outputed files by the feature encoding target\\n* `output_file` is the desired file path to download trained, outputed models\\n* `testing` is the status of whether we are doing testing (by default is false)\\n\\n\\n## EDA and Report (optional)\\n\\nExports Jupyter notebooks to HTML with EDA and result analysis from the models.\\n<br />\\n* `report_name` is the desired name of report\\n* `data_dir` is the file path with all data files from data prep target: processed, raw, models, etc.\\n* `notebook_dir` is the file path containing the repo's notebooks\\n* `notebook_file` is the desired file path (from `notebook_dir`) of outputed notebook\\n* `report_dir` is the desired directory our outputed HTML report\\n* `report_file` is the desired file name of outputed report in `report_dir`\\n\\n\\n## Test\\n\\nTest target will run the whole project with only test data\\n\\n\\n## Correct order of excution\\n\\n* data_prep: `python run.py data_prep`\\n* feature_encoding: `python run.py feature_encoding`\\n* (optional) eda: `python run.py eda`\\n* train: `python run.py train`\\n* (optional) report: `python run.py report`\\n\\n\\n## Project Links\\n\\n* [Project Website](https://shy218.github.io/dsc180-project/)\\n* [AutoPhrase](https://github.com/shangjingbo1226/AutoPhrase)\\n\",\n",
       "  \"The AutoPhrase for Financial Documents Interpretation project has several targets: data preparation, feature encoding, exploratory data analysis (optional), training, reporting (optional), and testing. Users can configure parameters for each target in the config files. \\n\\nThe data preparation target involves scraping, cleaning, and consolidating companies' 8-K documents. It also curates features such as EPS and price movements for the companies. \\n\\nThe feature encoding target creates encoded text vectors for each 8-K using unigrams and quality phrases generated by the AutoPhrase method. \\n\\nThe training target trains Random Forest models using three feature sets: baseline, baseline + unigrams, and baseline + phrases. The selected classifier and model parameters are determined by comparing validation accuracy.\\n\\nThere is an optional EDA and report target that exports Jupyter notebooks to HTML with exploratory data analysis and result analysis from the models.\\n\\nThe test target runs the entire project with only test data.\\n\\nThe correct order of execution is: data_prep, feature_encoding, (optional) eda, train, (optional) report.\\n\\nProject links include a project website and the AutoPhrase repository on GitHub.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_40': [\"# DSC180B-NER-Project\\nThis project focuses on the task of document classification using a BBC News Dataset and a 20 News Group Dataset. We implemented various feature based classification models and compared the results. We have analysed the advantages and shortcomings of each method.\\n\\n## Webpage\\n* https://dsc180b-a04-capstone-group-06.github.io/News-Classification-Webpage/\\n\\n## Datasets Used\\n* BBC news: https://www.kaggle.com/pariza/bbc-news-summary </br>\\n  * Download this dataset\\n* 20 news group: http://qwone.com/~jason/20Newsgroups/ \\n  * This dataset is fetched by using the sklearn package\\n## Environment Required\\n* Please use the docker image: ``` littlestone111/dsc180b-ner-project  ```\\n\\n## Run\\n```\\n$ launch-180.sh -i littlestone111/dsc180b-ner-project -G [group]\\n$ python run.py [all] [preprocessing] [autophrase] [model] [test]\\n```\\n\\n```test``` :        target will build the Tf-Idf models on the small subset of 20 new groups dataset and save the models to the model folder.</br>\\n```all```:          target will run everthing inlcuded in project, and return the final prediction on the test dataset for document classification.</br>\\n```preprocessing```: target will preprocess 20 news group data for AutoPhrase, so that they can be used for training the model.</br>\\n```autophrase```:   target will run Professor Shang's Autophrase model to extract quality phrases from the dataset.</br>\\n```model```:        target will build the SVM+ALL+TF-IDF combined vocab list model for 20 news group dataset. </br>\\n\\n</br>\\nOutput: <br>\\n\\n* ```model.pkl```: the parameter of the final model.\\n\\n## Group Members\\n* Rachel Ung\\n* Siyu Dong\\n* Yang Li\\n\\n## Our Findings\\n\\nThe BERT classification on the five-class BBC News dataset does not outperform any of our implemented models. From our results table, we observed that our models have F1-Score and Accuracy performances at around 0.95, indicating they are high-performing classifiers. The best of them is the SVM+ALL(TF-IDF) classifier, or the Support Vector Machine with the All Vector Vocabulary List and Tf-Idf Representations, which uses the vocabulary from both NER results and AutoPhrase results. Because the quality phrases between different domains are likely to differ, we expect these results to be optimal features for our predictors. \\n\\nFor the 20 News Group dataset, the SVM+ALL(TF-IDF) classifer also outperformed the other models, with the F1-Score and Accuracy being 0.84. Considering the classes are huge (i.e. 20 classes), these results verify our model is high-performing. Applying our best model on the five-class BBC News dataset, we attained a F1-Score at 0.9525, and Accuracy at 0.9528; while for the 20 News Group, we yielded a F1-Score at 0.8463 and Accuracy at 0.8478. \\n\\n\\n\\n\\n\",\n",
       "  'This project focuses on document classification using the BBC News Dataset and the 20 News Group Dataset. Various feature-based classification models were implemented and compared. The advantages and shortcomings of each method were analyzed. The best performing model was the SVM+ALL(TF-IDF) classifier, which used a combination of NER results and AutoPhrase results as features. For both datasets, this model achieved high F1-Score and Accuracy values.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_39': ['# AutoLibrary - A Personal Digital Library to Find Related Works via Text Analyzer\\r\\n- Website: https://yichunren.pythonanywhere.com/autolibrary/\\r\\n- DSC180B - Capstone Project (Winter 2021)\\r\\n- Section A04 Group03: Yichun Ren, Jiayi Fan, Bingqi Zhou\\r\\n- Note: This is an application of [AutoPhrase](https://github.com/shangjingbo1226/AutoPhrase) by Jingbo Shang.\\r\\n\\r\\n## Docker\\r\\n- The docker repository is `jfan1998/dsc180a-docker`.\\r\\n- Note: The docker uses dsmlp base container. Please login to a dsmlp jumpbox before entering the command below.\\r\\n```\\r\\nlaunch-scipy-ml.sh -i jfan1998/dsc180a-docker:latest\\r\\n```\\r\\nUse port-forwarding on dsmlp to open the website:\\r\\n  - Instruction: https://docs.google.com/document/d/15ehCaVIKSXwgh2jvH3034l5uSPNLrZRgkczwl-xWNEU/edit?usp=sharing\\r\\n\\r\\n## Website\\r\\n- Our published website enables users to upload papers via URLs/Local Machine\\r\\n- If you are curious about our code for published website, you can switch the branch to website.\\r\\n\\r\\n## Local Run\\r\\n- Please refer to `requirements.txt` to check if all the packages and libraries needed are installed.\\r\\n### Default Run: open AutoLibrary website\\r\\n```\\r\\npython run.py\\r\\n```\\r\\n- The home page of website: `http://127.0.0.1:8000/autolibrary/`\\r\\n### Target 1: Convert the input .pdf file into .txt\\r\\n```\\r\\npython run.py data\\r\\n```\\r\\n### Target 2: Run AutoPhrase on the input file\\r\\n```\\r\\npython run.py autophrase\\r\\n```\\r\\n### Target 3: Apply weight to the quality scores of phrases according the corresponding quality score in its domain\\r\\n```\\r\\npython run.py weight\\r\\n```\\r\\n### Target 4: Webscrape the search results on Semantic Scholar with keywords and domains\\r\\n```\\r\\npython run.py webscrape\\r\\n```\\r\\n### Target 5: Convert jupyter notebooks to html\\r\\nNote: Files with human annotations are in the references directory.\\r\\n```\\r\\npython run.py report\\r\\n```\\r\\n### Target 6: Test all previous targets on test data\\r\\nNote: For the test run, raw test data and domain for search is in test/testdata directory.\\r\\n```\\r\\npython run.py test\\r\\n```\\r\\n### Target 7: Activating the website\\r\\n```\\r\\npython run.py website\\r\\n```\\r\\n\\r\\n#### Note for local run:\\r\\nSince AutoLibary does not have access right to your local documents, if you would like to try other papers, please put the papers in ```~/AutoLibrary/website/autolibrary/documents``` and refresh the local website.\\r\\n\\r\\n### Responsbilities: \\r\\n- Yichun Ren: Dataset, Weight, Website Development\\r\\n- Jiayi Fan: Data, Result Analysis, Website Development\\r\\n- Bingqi Zhou: Dataset, Webscrap, Experiments\\r\\n',\n",
       "  'AutoLibrary is a personal digital library that helps users find related works through text analysis. It is a capstone project developed by Yichun Ren, Jiayi Fan, and Bingqi Zhou. The project utilizes AutoPhrase by Jingbo Shang.\\n\\nTo run AutoLibrary locally, make sure all the required packages and libraries are installed as specified in the `requirements.txt` file. The default run opens the AutoLibrary website at `http://127.0.0.1:8000/autolibrary/`. Other targets include converting a PDF file to TXT, running AutoPhrase on the input file, applying weights to quality scores of phrases, webscraping search results on Semantic Scholar, converting Jupyter notebooks to HTML, testing previous targets on test data, and activating the website.\\n\\nThe responsibilities of the team members are as follows:\\n- Yichun Ren: Dataset, Weight, Website Development\\n- Jiayi Fan: Data, Result Analysis, Website Development\\n- Bingqi Zhou: Dataset, Webscraping, Experiments'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_37': ['# Restaurant Recommender System\\nThere are multiple factors that go into a rating: wait time, service, quality of food, cleanliness, or even atmosphere - for example, a restaurant could have positive sentiment towards the food but negative sentiment towards the service. In order to solve this problem, our aim is to include such sentiments that can be found in the review text and turn that into data which can be used to further improve business recommendations to users.\\n\\nThis repository is a recommender system with a primary focus on the text reviews analysis through TF-IDF (term frequency-inverse document frequency) and targeted sentiment analysis with AutoPhrase to attach sentiments to aspects of a restaurant. In building the recommender system, we learned that review texts can hold the same importance as the numerical statistics because they contain key phrases that characterize how they felt about the review. The ultimate goal is designing a website for deploying our recommender system and showing its functionality.\\n\\nVisit our `website` branch to try some queries on a preprocessed Las Vegas / Phoenix dataset!\\n\\n## Important Things:\\n* This repository is contains two branches. The `main` branch contains the source code for our methods. The `website` branch contains the code to run our recommender sebsite on Flask.\\n* In our implementation and analysis, we use the Autophrase as our core NLP analysis method by submoduling into our repository.\\n* The Docker Image and Tag is `launch.sh -i catherinehou99/yelp-recommender-system:latest -c 8 -m 20 -P Always`\\n- If you would like to learn more details about the AutoPhrase method, please refer to the original github repository: https://github.com/shangjingbo1226/AutoPhrase. Namely, you will find the system requirements, all the tools used and detailed explanation of the output.\\n- Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, Jiawei Han, \"**[Automated Phrase Mining from Massive Text Corpora](https://arxiv.org/abs/1702.04457)**\", accepted by IEEE Transactions on Knowledge and Data Engineering, Feb. 2018.\\n\\n## Before You Run:\\n* In order to use Yelp\\'s academic dataset, you will need to go to their [Website](https://www.yelp.com/dataset) and agree to the Terms of Use Agreement before you download the dataset. Save the dataset to the directory `data/raw`\\n* This repo uses AutoPhrase as the git submodule, run the command `git submodule update --init` after cloning this repo.\\n\\n## How to Use this Repository:\\n1. Run the test target in the `main` branch if you would like to test the targets.\\n2. In `config/data-params` you can choose which city you would like to subset. For test target, the city can be Las Vegas or Phoenix.\\n3. Once you successfully run the targets, the generated files will be saved to `data/tmp`. These files need to be used in the `data` folder of the`website` branch.\\n4. If you would like to run the website on Flask, head over to the website branch!\\n\\n## Default Run\\n\\n```\\n$ python3 run.py -- all\\n```\\nThe default running file is run.py and can be simply run through the command line: python3 run.py -- all\\nThis will run all the targets below (data, sentiment, eda, tfidf)\\n\\nFor each of the target:\\n* data: prepares necessary folders, reads in Yelp json files, and filters the dataset to contain rows relevant to the specified city.\\n* sentiment: performs sentiment analysis on the reviews. It will take in the reviews dataframe and output the positive/negative sentences counts.\\n* eda: performs the eda analysis of the dataset and autophrase result.\\n* test: runs the above targets on a test dataset which runs around 3mins.\\n* clean: removes all the files generated with keeping the html report in the `data/eda` folder.\\n\\n```\\n$ python3 run.py -- data\\n```\\nThe default running file is run.py and can be simply run through the command line: python3 run.py -- data\\n* Check if the reference folder exists in the user local drive. If not, create all the necessary folder for projects\\n* Read the dataframes for further analysis.\\n\\n```\\n$ python3 run.py -- sentiment\\n```\\nThe default running file is run.py and can be simply run through the command line: python3 run.py -- sentiment\\n* Perform sentiment analysis on the reviews text\\n* Outputs a city_name.csv in the `data/tmp` folder contains, for each restaurant, the positive phrases and the number of times they were mentioned in a review.\\n\\n```\\n$ python3 run.py -- eda\\n```\\nThe default running file is run.py and can be simply run through the command line: python3 run.py -- eda\\n* Perform the EDA analysis on the AutoPhrase result of individual user\\n* Perform the EDA analysis on individual city review dataset such as sentiment analysis, feature exploration\\n* Convert all the EDA analysis into an HTML report stored under data/eda\\n* After running this tag go to the data/eda directory to see the report.html\\n\\n```\\n$ python3 run.py -- tfidf\\n```\\nThe default running file is run.py and can be simply run through the command line: python3 run.py -- tfidf\\n* Run restaurant-restaurant based recommendation methods using TF-IDF and cosine similarity score\\n* Generate the TF-IDF results CSV file of two cities, Las Vegas and Phoenix and store in the reference folder\\n* Two TF-IDF results CSV are using in the website building as backend database for generating recommendation\\n\\n\\n```\\n$ python3 run.py -- clean\\n```\\nThe default running file is run.py and can be simply run through the command line: python3 run.py -- clean\\n* Remove all the generated files, plots, dataframes under the reference folder\\n* Keep the HTML file in the eda/data folder for report visualization\\n* Be careful: this will clear all the outputs running so far and can not be reversed!!\\n\\n```\\n$ python3 run.py -- test\\n```\\nThe default running file is run.py and can be simply run through the command line: python3 run.py -- test\\n* Run all the targets on the test data set we generated \\n\\n### Responsibilities\\n* Catherine Hou developed the sentiment/eda/data tag and the food query on the website.\\n* Vincent Le created the dockerfile and developed the website (not in main branch).\\n* Shenghan Liu developed TF-IDF/clean/data tag, user AutoPhrase EDA, and the restaurant query on the website.\\n',\n",
       "  \"This repository is a restaurant recommender system that focuses on analyzing text reviews using TF-IDF and sentiment analysis. The goal is to improve business recommendations by incorporating sentiments found in the review text. The repository contains two branches: the main branch with the source code and the website branch for running the recommender system on Flask. The repository also uses AutoPhrase as a core NLP analysis method. To use Yelp's academic dataset, you need to agree to their terms of use agreement and save the dataset to the specified directory. The repository provides several targets for running different tasks such as data preparation, sentiment analysis, exploratory data analysis (EDA), and TF-IDF-based recommendation methods. Each target can be run using the command line. The responsibilities for developing different parts of the repository are also mentioned.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_48': [\"# ForumRec\\n\\n## Introduction\\nThis repository is for the ForumRec project, a recommendation system, that recommends users questions they are adept to answer on the [Super User](superuser.com) forum of [StackExchange](https://stackexchange.com). The website can be reached at [jackzlin.com](jackzlin.com) and the repository for the website can be reached at [ForumRecWeb](https://github.com/okminz/ForumRecWeb).\\n\\n##  Files\\n\\nFor this project, we have files for retriving the data, running the models, and processing it into the desired output. The files are described below and explain the purpose of each part of the repository.\\n\\n> etl.py: Passes in the configs file related to it. The process for taking the data from the data files, extracting the necessary information, and splitting them into questions and answers after a certain date. This is to maintain a higher predictive function and while utilizing as much of the information as e can after extracting.\\n\\n> api_etl.py: Passes in the configs file path related to it. It uses the [Stack Exchange API](https://api.stackexchange.com/) on the Super User forum to gather recent questions and answers from the Super User forum, concatenates the questions and answers and saves them into a continuously updating file so it can be used for new recommendations.\\n\\n> run.py: Runs etl on the data. Runs api_etl on the api parameters. Runs the hybrid model (collaborative and content-based filterling) creation and recommendation files creation along with evaluating the model. Runs the baseline file to get the baseline comparison values. Can also run all of these files on test data.\\n\\n> create_model.py: Passes in the configs file related to it. This file will run create_inputs.py and gather inputs from that file. It will then use those inputs to a generate a model. \\n\\n> create_inputs.py: Passes in the configs file related to it. This file will use processed questions and answers to generate the necessary matrices and files and save them so that they may be used in create_model.py to create a model.\\n\\n> model.py: Passes in the configs file related to it a boolean parameter to determine if running for baselines (default is False). This file the generated model and gather its inputs in order to make recommendations for based on the interactions between users, questions, answers, text, and tags. These recommendations will be returned in a file. It will also produce the evaluations of the model using precision, recall, auc score, and recriprical rank.\\n\\n> new_user.py: Passes in the configs file related to it. This file will take in user response data from the website in order to gather new and fresh recommendations for the user by fitting partially to the already generated model and replacing it. This script is used for the website's cold start function mostly. \\n\\n> create_baseline.py: Passes in the configs file related to it. Baseline file that return the recommendations given to a user using a simple collaborative filtering model so that it can be compared against the model's recommendations and evaluation metrics.\\n\\n> requirements.txt: Contains the amount of processing resources recommended to run the files and the packages needed and the versions that were used to run all the processes.\\n\\n> LICENSE: A file that contains the reuse and use licenses for this repository.\\n\\n> SuperUser EDA.ipynb: Inside the notebook directory. Notebook containing the exploratory data analyses that was taken on the data to further understand and gain insight on the data we were using, and how we can use the data to build the recommendation system we wanted.\\n> \\n> SuperUser API.ipynb: Inside the notebook directory. Notebook containing the explorationa and trials of using the Stack Exchange API to further understand and gain insight on how to get the desire questions and answers from it. It was used to build the api-etl process.\\n\\n> ETL.ipynb: Inside the notebook directory. Notebook containing the etl processes and the results to look at the etl.\\n\\n> NLP.ipynb: Inside the notebook directory. Notebook containing the natural language processing code that looks at the text data and creates a model through that code.\\n\\n##  Directories\\n\\nThe following directories were created by us in order to be able to store and retain the necessary information needed for different purposes.\\n\\n> config: Contains a list of all the config files that determines the parameters of each file. Use these files according to their use to change the parameters and change which subset of data you are running the processes on. Make sure you are changing the file paths correctly and throughout the entire config file.\\n\\n> data: Location to put the original raw data in. It also would contain a final directory which would contain the data retrieved after running the repository.\\n\\n> test: Contains inside the data directory and inside that the data used to run the repository on a small subset of the data to ensure the models and the scripts are running correctly.\\n\\n> src: Contains inside the src, models, and baselines folders which contain the etl, model creation, input creation, model, new user, and baselines python files that are described above.\\n\\n> notebooks: Contains multiple notebooks that explore the data. The notebooks are described above.\\n\\n## Running the Code\\nPrior to running the code, make sure that you install all the packages listed in *requirements.txt* \\n\\nIn order to obtain the data, one can follow the processes below:\\n\\n### Creating the Data\\n\\nTo create the processed data, run this following command on the command line terminal:\\n```\\npython run.py data\\n```\\nWhere the data will be replaced processed and be returned into new files usable by our models in this project and placed in the data directory.\\n\\n### Gathering data from the API\\n\\nTo get new questions and answers from Super User since the last pull of API data, run this following command on the command line terminal:\\n```\\npython run.py api\\n```\\nWhere the API data will be processed and be returned into a usable continuous file placed in the data directory.\\n\\n### Running the cosine models\\n\\nTo run the hybrid filtering model on the data, run this following command on the command line terminal:\\n```\\npython run.py models\\n```\\nWhere hybrid filtering model and its inputs will be created and used to generate new recommendations and then evaluated using specific metrics.\\n\\n### Comparing the model to baselines\\n\\nTo determine how our model compares to the baselines model, run this following command on the command line terminal:\\n```\\npython run.py baselines\\n```\\nWhere the baseline model will be evaluated and recommendations will be returned using the same data as the hybrid filtering model.\\n\\n### Running all the model targets\\n\\nIf you want to run all of these together, run this following command on the command line terminal:\\n```\\npython run.py all\\n```\\nWhere the all 4 targets (excluding *test*) will run one after another in the order presented above.\\n\\n### Testing all the model targets\\n\\nTo test how if the repository and all the models and scripts are working, run this following command on the command line terminal\\n```\\npython run.py test\\n```\\nWhere all of the targets ('data', 'api', 'models' and 'baselines') above will all be run one after another in the order presented above, but on small test data so that we can observe how the models and scripts are working.\\n\\n## Responsibilities\\nYo Jerimijenko-Conley: \\n\\nJasraj Johl: Created the ETL process, worked with the Super User API, created the code repository and made sure it was clean and runnable, and created the website's design through HTML and CSS integrating it partially with Flask.\\n\\nJack Lin: \\n\",\n",
       "  'This repository is for the ForumRec project, a recommendation system that recommends users questions they are adept to answer on the Super User forum of StackExchange. The repository contains various files and directories for data retrieval, model running, and processing. The code can be run to create processed data, gather data from the API, run the hybrid filtering model, compare the model to baselines, and test all the targets. The responsibilities of each team member are also mentioned.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_47': ['\\n\\n# OnSight: Outdoor Rock Climbing Recommendations\\n\\nRecommendations for outdoor rock climbing has historically been limited to word of mouth, guide books, and most popular climbs. With our project OnSight, we believe we can offer personalized recommendations for outdoor rock climbers.\\n\\nDisclaimer: With rock climbing, especially outdoors, there is an inherent risk that is taken when you decide to climb. Although our recommender tries to offer routes similar to the ones users have done, there is still a risk that the route may be too hard and therefore dangerous. This is not a problem that is solely put on the recommender, but a problem with rock climbing as a whole. There is no standard in climbing grades, but rather it is an agreement among the climbers that have climbed that route. Therefore climbing grades are subjective, and climbs may be harder and more dangerous than a user expects. We realize this, and we encourage everyone to look at the safety information of each climb on its corresponding climbing page on Mountain Project.\\n\\n## How To Run\\n\\nWe suggest for casual users to simply use our website to run the project. The website URL is https://dsc180b-rc-rec.herokuapp.com/. Note that this project runs on a free dyno, so if you are the first user to open the website in about half an hour, then the website may take a minute to load. Be patient!\\n\\nHowever, if you are interested in making changes or diving deep into the code, you can run the project and customize it by either creating your own Heroku project or running the project on the command line.\\n\\n### Creating your own Heroku project\\nTo have your own version of OnSight running on Heroku, do the following steps:\\n 1. Fork the OnSight GitHub repository to your own GitHub account\\n 2. Create a new project on Heroku\\n 3. In the Heroku app dashboard, go to the \"Deploy\" tab\\n 4. Under the \"Deployment method\" section, select GitHub and connect the Heroku app to your forked repository\\n 5. In the Heroku app dashboard, go to the \"Settings\" tab\\n 6. Under the \"Config Vars\" section, click on \"Reveal Config Vars\" and fill in the config variables as shown in the table below. \\n\\n|Config Vars|Value|Description|\\n|-|-|-|\\n|PROJECT_PATH|mysite/|Since the django webserver is stored in the \"mysite/\" folder, but the Procfile (tells Heroku how to start the web server) is in the project root, we need to tell Heroku to look in the \"mysite/\" folder for the webserver code|\\n|GOOGLE_MAPS_API_KEY|Your API key|Your Google Maps API key needs to have the following APIs enabled on the key: \"Maps JavaScript API\", \"Places API\", and \"Maps Embed API\". This key is not strictly necessary, but without the key the map will not work and location can only be entered by manually typing in a latitude and longitude, which is not very UX friendly.|\\n\\n7. Make sure you deploy the Heroku app again from the \"Deploy\" tab on the Heroku dashboard and you should be all set!\\n\\n### Running the Project on the Command Line\\n\\nTo run the project, every command must start with \"python run.py\" from the root directory of the project. By default, \"python run.py\" will do absolutely nothing. You must use at least one of the following flags to actually get some response:\\n\\n|Flag|Type|Default Value|Description|\\n|-|-|-|-|\\n|-d, --data|bool|False|Use this flag to run all data scraping code. This will take a very long time, upwards of one week total to scrape all the data. It is recommended *not* to run this. Be aware that this will only store data locally. |\\n|-c, --clean|bool|False|Use this flag to run all data cleaning code. It is expected that all files defined in the \"state\" key of data_params.json are present in the raw data folder.|\\n|-\\\\-data-config|str|\"config/data_params.json\"|The location at which data parameters can be found|\\n|-\\\\-web-config|str|\"config/web_params.json\"|The location at which web parameters can be found. These parameters simulate a user using the website.|\\n|-\\\\-top-pop|bool|False|Use this flag to print the top N most popular climbs. This does not use locally saved data, but rather uses saved data in MongoDB. Additionally, the exact climbs and number of climbs are determined by the web_params.json file.|\\n|-\\\\-cosine|bool|False|Use this flag to print the top N most similar climbs to the users favorite. This does not use locally saved data, but rather uses saved data in MongoDB. Additionally, the exact climbs and number of climbs are determined by the web_params.json file.|\\n|-\\\\-test|bool|False|Use this flag to run the two implemented models based on default config files. Using the --test flag will override all other present flags and is equivalent to running \"python run.py --top-pop --cosine --debug\".|\\n|-\\\\-delete|bool|False|Use this flag to wipe out all data from MongoDB. This will not do anything since the MongoDB login is set to read only.|\\n|-\\\\-upload|bool|False|Use this flag to upload cleaned data to MongoDB. This will not do anything since the MongoDB login is set to read only.|\\n|-\\\\-debug|bool|False|Use this flag activate various print statements throughout the project.|\\n\\n### Description of Parameters\\n\\n#### Data Parameters\\n\\n|Parameter Name|Type|Default Value|Description|\\n|-|-|-|-|\\n|raw_data_folder|str|\"data/raw/\"|The location at which raw data will be saved. Note that this path is relative to the project root.|\\n|clean_data_folder|str|\"data/cleaned/\"|The location at which clean data will be saved. Note that this path is relative to the project root.|\\n|states|dict|Too long to copy here...|Although the parameter is called states, this is really just the areas to scrape/clean and the urls at which they can be found. The file will be named based on the key string, and the area url to scrape is the value string. By default this contains all 50 states, with the state name as key and state area url as value.|\\n\\n#### Web Parameters\\nBe aware that all these parameters do is simulate a user using the website. Each of the parameters here refer to a form element on the website.\\n\\n|Parameter Name|Type|Default Value|Description|\\n|-|-|-|-|\\n|user_url|str|Too long to copy here...|The \"Mountain Project URL\" form element on the website. This value is only used if the user requests personalized recommendations. The default value is a user with a lot of climbs rated, about 600 in March 2021. You can find the actual default value in the config file.|\\n|location|[float, float]|[43.444918, -71.707888]|The \"Latitude\" and \"Longitude\" form elements on the website. This location is the center of the circle where climbs will be looked for. The default value is some random location in New Hampshire.|\\n|max_distance|int|50|The \"Max Distance (mi)\" form element on the website. This value is the radius of the circle where climbs will be looked for. The default value is 50 miles, which should be sufficient to encompass any climbing area.|\\n|recommender|str|\"top_pop\"|The \"Recommenders\" form element on the website. This is the recommender to use and will be any of \"top_pop\" or \"cosine_rec\". There is an additional hidden debug recommender that uses the string of \"debug\". The debug recommender is not accessible without modifying the \"mysite/bootstrap4/forms.py\" file|\\n|num_recs|int|10|The \"Number of Recommendations\" form element on the website. This is the maximum number of recommendations that will be displayed once the user hits submit.|\\n|difficulty_range|{\"boulder\": [int, int], \"route: [int, int]}|{\"boulder\": [0, 3], \"route\": [11, 16]}|The \"Boulder\", \"V_-V_\", \"Route\", and \"5.\\\\_-5.\\\\_\" form elements on the website. Due to the way data is cleaned, bouldering V grades and route 5. grades are converted to integers starting at 0 on different scales. You can find the scales defined in code. The two default difficulty ranges correspond to V0-V3 and 5.8-5.10d. Note that if the user does not want boulders or routes, the corresponding difficulty range will be [-1, -1]|\\n\\n',\n",
       "  'The project OnSight offers personalized recommendations for outdoor rock climbers. It provides recommendations based on climbs that users have done, but there is still a risk that the recommended route may be too difficult and dangerous. The project can be run on a website or customized by creating your own Heroku project or running it on the command line. The command line options include data scraping, data cleaning, printing popular climbs, printing similar climbs, running test models, deleting data from MongoDB, uploading cleaned data to MongoDB, and activating debug mode. The project also has data parameters for saving raw and clean data, and web parameters for simulating user inputs on the website.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_46': [\"# DSC180b-Capstone\\n\\nProject repository for Recommender Systems group 3\\n\\nThis project is focused on creating music recommendations for users and their parents.\\n\\n# HOW TO RUN\\n\\nOur project's current targets are: load-data, task0, task1, task2, all, test\\n\\nOur project's current config files are: test.json and run.json\\n\\n### Targets\\n\\nload-data: Pulls our training data from a S3 bucket where we store it and creates a new 'data'\\nrepository to store it.\\n\\ntask0: Generates a list of sample parent recommendations and saves them to data/recommendations as a csv file.\\n\\ntask1: Generates a list of parent-user recommendations and saves them to data/recommendations as a csv file.\\n\\ntask2: Generates a list of user-parent recommendations base on a user's listening history. Saves\\nthese recommendations to data/recommendations as a csv file.\\n\\nall: runs load-data, task0, task1, and task2 in succession\\n\\ntest: Runs through the same load-data, task0, task1, task2 pipeline but uses a pre-stored user access code.\\nThis allows us to 'test' our recommendation models without having to authenticate ourselves every\\nsingle time. The test data in this case is a user account that we have permission to read from.\\n\\n### Configs\\n\\nOur configuration files are relatively simple given our project's reliance on listening histories and\\notherwise limited user information. The values in each file are the same, but we have created two files\\nso that logic can be quickly tested without having to constantly change the configuration parameters during\\ndevelopment.\\n\\nusername: The username of the Spotify account that we are creating recommendations for\\nparent_age: The age of the user's parent\\ngenre: The parent's preferred genre of music\\nartist: The parent's preferred artist\\n\\nThis information would normally be provided by users through a form on our website, but for this situation\\nwe just reference configuration files.\\n\\n\\n### IN THE FUTURE\\n\\nWe plan on adding a clean-data target that isolates some of the small preprocessing that our code does: dropping na values\\nsmall transformations, etc.\\n\\nInstead of caching an auth_token for a specific user, we want to just directly load listening history. This would be a more\\nstraightforward procedure, the problem is that we inevitably have to authenticate regardless of if we have our user data predownloaded or not. Spotify has another authentication flow that would better suit itself to this situation, and we will be looking into that in the future.\\n\\n\",\n",
       "  \"This project focuses on creating music recommendations for users and their parents. The project's current targets include loading data, generating sample parent recommendations, generating parent-user recommendations, and generating user-parent recommendations based on a user's listening history. There are also configuration files for specifying the username, parent age, preferred genre of music, and preferred artist. In the future, the project plans to add a target for cleaning data and explore a more straightforward authentication procedure with Spotify.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_45': [\"## Asnapp\\n\\nAsnapp is a workout video recommender web application. \\n\\nAuthors: Amanda Shu, Peter Peng, Najeem Kanishka\\n\\n### Website URL\\nThe website is now live on: https://workout-recommender.herokuapp.com/\\n\\n### Video Demonstration\\nFor a demonstration of the project, visit: https://www.youtube.com/watch?v=QJFg0HguGuI\\n\\n### Data\\nThe data is scraped from https://www.fitnessblender.com/. We are using the data for academic purposes only.\\n\\n### Code Organization\\n\\n- `run.py`: Run to get data and model results.\\n- `app.py`: Runs flask web application.\\n- `workout_db.sql`: Contains sql statements for creation of tables in database.\\n- `requirements.txt`: Python packages required to run project\\n- `wsgi.py & Procfile & runtime.txt`: Entrypoint for Heroku, used in website deployment\\n\\n**Source**\\n- The `src/data` folder contains `scrape.py`, the web-scraping script that writes three data files into `data/raw` folder. `fbpreprocessing.py` takes these raw data files and outputs cleaned/transformed data files into `data/preprocessed` folder. `youtube.py` grabs youtube related data from the Youtube API. `model_preprocessing` reads in preprocessed data and transforms the data into what is needed for model inputs.\\n- `src/models` contains `run_models.py` which trains and evaluates the models. Models are implemented in `lightm_fm.py` and `top_popular.py`\\n- The `src/utils` folder has `clean.py` which implements the standard target `clean`.\\n- The `src/app` folder holds files for the web application. `forms.py` contains wtforms classes for registration/login pages. `recommendations.py` holds code for filtering user preferences and building recommendation lists. `register.py` contains helper functions to create the sql insertion/update statements for registering users and updating their workout preferences.\\n\\n**Static**:\\n- The`images` folder holds a gif ([source](https://www.pinterest.at/pin/512495632597411529/)) used for the loading page. No copyright intended.\\n- The `js` folder contains several javascript files. `overlay.js` is for the display of the popup videos on the recommendation page. `workout_info.js` is for registration and update preferences pages. `rec.js` is for the loading page and recommendation engine logic on the recommendations page.\\n- `libraries/slick` has several files for the carousel, `styles` has a css file, and `favicon.io` is the dumbbell icon\\n- `vendor` holds several javascript files (Bootstrap, JQuery) for styling/theming of the website\\n\\n**Config**: `data-params.json` has file paths outputs for data collection/preprocessing and `test-params.json` has the data paths for the test target. To webscrape, this folder should also include `chromedriver.json`. To gather Youtube data, `api_key.json` specifies the api key. To run the app, `db_config.json` has the database configurations.\\n\\n**Notebook**: `eda.ipynb` is a notebook with exploratory data analysis on scraped data. `param_comparision.ipynb` is a notebook reporting the recommendation models' performance across a couple parameters. `top_popular_extension.ipynb` is a notebook looking into adding Youtube API data into the top popular recommender. `KNN_collab.ipynb` contains results of a KNN collaborative filtering model from surprise package and a pure collaborative filtering from LightFM.\\n\\n**Templates**: Holds html files for the various endpoints.\\n\\n**Testdata/raw**: These are fake datasets meant to be used with the test target.\\n\\n**Docker**: Docker related files. See [here](https://github.com/amandashu/Workout_Recommender/blob/main/docker/README_DOCKER.md)\\n\\n**Materials**: Contains pdfs for presentation slides and a report detailing our methods/implementations.\\n\\n### Set Up Project Environment\\nThere are two ways to run this project: a) Docker (preferred) or b) Locally <br>\\na) To Run in Docker:<br>\\n  1) Pull the container with `docker pull nkanishka/workout-recommender`\\n  2) Run the container using:\\n  * General Use: `docker run -it -p 5000:5000 workout-recommender`\\n  * DSMLP Only: `launch.sh -i nkanishka/workout_recommender_dsmlp -c 4 -m 8` <br><br>`kubectl port-forward <Kubernates Cluster Name> 5000`<br><br> `ssh -N -L 5000:127.0.0.1:5000 <AD Name>@dsmlp-login.ucsd.edu`\\n  3) Inside container/cluster, type `cd Workout_Recommender`. Note that in the DSMLP environment, you will need to manually clone this repo.\\n  4) If using website, go to [localhost:5000](localhost:5000)\\n    <br>\\n\\nb) To run locally, install requiremnents.txt into a virtualenv. Make sure you have Python 3.8+ and Pip installed.\\n\\n### Run the Project Stages\\n- To get the data, run `python run.py data`. This scrapes the data and cleans the data and saves these files into `/data/raw` and `data/preprocessed` respectively.\\n  - Note: for scraping, this assumes that there is a file `config/chromedriver.json` that specifies where the path to the downloaded chromedriver.exe file for your Chrome version lies in the attribute `chromedriver_path`.\\n  - Note: for making requests to Youtube API, this assumes that there is a file `config/api_key.json` that specifies the api key in the `api_key` attribute.\\n- To run model results, run `python run.py model`. This takes in the preprocessed data, trains the models, and prints out the NDCG scores for each model.\\n- Standard target `clean` is implemented, and it will delete the `data` folder.\\n- Standard target `all` is implemented, and it equivalent to running `python run.py data model`.\\n- Standard target `test` is implemented, and runs the data preprocessing and modeling results on the test data. The purpose of this target is purely to check the implementation of the code.\\n- Use `python app.py` to run the app locally.\\n  - Note: this assumes that there is a file `config/db_config.json`, which has database host, user, password, and name information.\\n  - And a file `config/flask_keys.json` which has a Flask secret key\\n\",\n",
       "  'Asnapp is a workout video recommender web application created by Amanda Shu, Peter Peng, and Najeem Kanishka. The website is live at https://workout-recommender.herokuapp.com/ and there is a video demonstration available at https://www.youtube.com/watch?v=QJFg0HguGuI. The data for the application is scraped from https://www.fitnessblender.com/ for academic purposes only. The project code is organized into different files and folders, including source code for data scraping, model training, and web application functionality. The project environment can be set up using Docker or by running it locally. There are also instructions provided for running different stages of the project, such as getting the data and running model results.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_43': ['# Recipe-Recommendation\\n\\n## Introduction\\nThis repository contains the code and models needed to create a recommender system for recipe recommendations. Included in the repository are a few simple baselines as well as the final model utilized by our recommender system for recipe recommendation. The data for this project was pulled from Kaggle (https://www.kaggle.com/shuyangli94/food-com-recipes-and-user-interactions) and (https://www.kaggle.com/kaggle/recipe-ingredients-dataset/home).\\n\\n##  Files\\n\\nFor this project, we have files for running the code, retriving the data, and processing it into the desired output.\\n\\nThere are several files that will be obtained from Kaggle, but are not part of this repository. This is because they are data files and are too large to be version controlled.\\n\\nThe following files were created by us in order to create and run our baselines and final model.\\n\\n> run.py: Passes in the location of the data folder. Runs etl on the Kaggle data and stores the data in the data output folder. Runs baselines and model on the dataset. Evaluates models to see how well the baselines did in comparison to the final model.\\n\\n> mostPop.py: Baseline file that uses a top popular model to determine what users would rate certain recipes.\\n\\n> randomFor.py: Baseline file that uses a random forest model to determine what cuisine type each recipe would fall under.\\n\\n> conBased.py: Baseline file that uses a content based model with cosine similarity to determine what recipe to recommend based on what ingredients are listed by the user.\\n\\n> requirements.txt: Contains the amount of processing resources recommended to run the files within a few hours each, and the packages needed and the versions that were used to run all the processes.\\n\\n> Preliminary EDA.ipynb: Inside the notebook directory. Notebook containing the exploratory data analyses that was taken on the data to further understand and gain insight on the data we were using.\\n\\n##  Directories\\n\\nThe following directories were created by us in order to be able to store and retain the necessary information needed for different purposes.\\n\\n> config: Contains a list of all the config files that determines the parameters of each file. Use these files according to their use to change the parameters and change which subset of data you are running the processes on. Make sure you are changing the file paths correctly and throughout the entire config file.\\n\\n> data: Contains the data retrieved after running the repository, and the cleaned and processed data by us.\\n\\n> testData: Contains the randomly generated testData that we would run our models against to see how well they performed. Much smaller than full dataset, and allows for easy tracking to gain the most insight from how the model works.\\n\\n## Running the Code\\nPrior to running the code, make sure that you install all the packages listed in *requirements.txt* \\n\\nIn order to obtain the data, one can simply run the run.py python file with the command data or all. This will prompt the script to download the data and store it in the proper place for you.\\n\\n### Creating the Data\\n\\nTo create the processed data, run this following command on the command line terminal:\\n```\\npython run.py data\\n```\\nWhere the data will be returned into new files usable by our models in this project and placed in the data directory.\\n\\n### Running all the model targets\\n\\nIf you want to run all of these together, run this following command on the command line terminal:\\n```\\npython run.py all\\n```\\nWhere the all targets (excluding *test*) will run one after another.\\n\\n### Testing all the model targets\\n\\nTo test how if the repository and all the models and scripts are working, run this following command on the command line terminal\\n```\\npython run.py test\\n```\\nWhere the targets above will all be run one after another, but on small test data so that we can observe how the models and scripts are working.\\n\\n## Repository Organization\\n\\nTo ensure the code runs properly, it must have the same folders and files locations. etl.py must be inside src and data folders, and data must be in the data folder. mostPop.py, must be in the src and baselines folders.\\n',\n",
       "  'This repository contains code and models for a recipe recommender system. The data was obtained from Kaggle and there are several files for running the code, retrieving the data, and processing it. The repository includes baselines and a final model for recipe recommendation. There are also directories for config files, data, and test data. To run the code, install the required packages listed in requirements.txt. The data can be obtained by running run.py with the command \"data\" or \"all\". The processed data will be stored in the data directory. To test the models, run run.py with the command \"test\".'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_44': ['# Makeup Recommender\\n\\nThis project is a recommender that will provide a one-stop shop experience where a user will get recommended an array of products to create an entire makeup look based on similar products that the user enjoys, products that similar users have purchased, as well as products that are personalized to the user including skin type, skin tone, allergies, and budget. Our project aims to utilize collaborative filtering recommendations along with content-based filtering to ensure user satisfaction and success when creating their desired look.\\n\\n\\n## How To Run\\n\\nTo install all dependencies, run the following command from the root directory of the project:\\n> ```pip install -r requirements.txt```\\n\\n### Running the Project\\n\\nTo run the project, each command must start from the root directory of the project with:\\n> ```python run.py```\\n\\nThis base command can be modified with various different flags:\\n| Flag                | Type | Default Value             | Description                                                       |\\n|---------------------|------|---------------------------|-------------------------------------------------------------------|\\n| --item_data              |      |                      | Scrape item dataset from www.sephora.com.                              |\\n| --review_data              |      |                      | Scrape review dataset from www.sephora.com.                              |\\n| --features          |      |                           | Clean dataset and create features.                                |\\n| --model             |      |                           | Run model to create recommendations.                              |\\n| --accuracy          |      |                           | Evaluate accuracy of model.                                       |\\n| --test              |      |                           | Train model on a test dataset.                                    |\\n| --all               |      |                           | Train and evaluate accuracy of baseline model and our recommender model.      |\\n\\n\\n## Website\\n\\nTo visit the webpage copy and paste this URL into your browser: https://makeup-recommender.herokuapp.com/\\n\\n\\n## Document History\\n\\n**Project Proposal**: https://docs.google.com/document/d/1bAXSUrQHcss8uU_eeqIJ4ewX3N-I8RLAjGJi77Zqw6c/edit\\n\\n**Check-in**: https://docs.google.com/document/d/18rve8FbhRN8VXoO99ixB6nh96uWYtR-DNQMrjIwCr8Y/edit\\n\\n**Report Checkpoint**: https://docs.google.com/document/d/1Xh3Ddskyy4niA7ZbzBUc9hoaR0bsrLKWgni9YzXTtZY/edit#\\n\\n**Final Report**: https://docs.google.com/document/d/1WRj9ukUa-ozK3xtao-khCdradUQsR1k23tn4ELvQE3U/edit#\\n\\n**Presentation Slides**: https://docs.google.com/presentation/d/1WYmy2IKTuVGE193Pq5t9v2BRTVCKxFHXy5zfLZN1K_I/edit?usp=sharing\\n\\n\\n## Credits\\n\\n### For Usage of Scraping Script\\n\\nhttps://github.com/jjone36/Cosmetic\\n\\n\\n\\n### Responsibilities\\n\\n**Alex Kim**:\\n* Scraped eye products and reviews\\n* Designed website\\n* Cleaned dataset\\n* Encode ingredient data\\n* Write rough draft of report (Abstract, Description of Data, Method, Metrics, Results)\\n* Deploy website on Heroku\\n* Use Flask to connect model to website (not implemented in final product)\\n* Fix UI with Streamlit\\n* Filter recommendations\\n* Edit report\\n\\n**Justin Lee**:\\n* Fixed script to scrape data from Sephora\\n* Scraped lip products and reviews\\n* Coded data ingestion pipeline\\n* Incorporate run.py\\n* Code collaborative filtering model\\n* Create docker container\\n* Deploy website on Heroku\\n* Use Flask to connect model to website (not implemented in final product)\\n* Organize code and documentation\\n\\n**Shayal Singh**:\\n* Fixed script to scrape data from Sephora\\n* Scraped face and cheek products and reviews\\n* Coded website layout\\n* Top popular baseline\\n* Write rough draft of report (Website, Conclusion)\\n* Deploy website on Heroku\\n* Use Flask to connect model to website (not implemented in final product)\\n* Display recommendations\\n* Visual presentation slides\\n* Fix UI with Streamlit\\n* Filter recommendations\\n* Finalize website\\n',\n",
       "  \"The Makeup Recommender is a project that aims to provide users with personalized recommendations for creating a makeup look. The recommender takes into account the user's preferences, such as products they enjoy and similar users' purchases, as well as their skin type, skin tone, allergies, and budget. It utilizes collaborative filtering and content-based filtering techniques to ensure user satisfaction. The project includes instructions on how to run it and a website where users can access the recommender. The document also provides links to the project proposal, check-in, report checkpoint, final report, and presentation slides. The credits section acknowledges the usage of a scraping script and lists the responsibilities of each team member.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_71': ['# Opioid-Use Prevalance Analysis Project \\n```\\n### \\n* Author: Flory Huang\\n* Date: 03.19.2021\\n```\\n\\nThis repository contains code for extraction mentioned drug terms and emotions of drug use discussion in Reddit data. \\nThe code takes in reddit post that are discussing drugs and ontology list(RxNorm and Mesh),used to extract drug names. \\nThe emotion in reddit post will be analyzed.\\n\\nThe data_reddit.py takes care of loading data and formating data\\nThe analysis.py conduct the similar matching to extract drug terms.\\nThe emotion.py will analyze emotion in the reddit post\\nThe model.py procude a report of matching result\\n\\nThe code can be excuted by \\n    - python run.py test \\n    - python run.py data \\n    - python run.py analysis\\n\\nTarget explaination \\n    0: test: test the whole process of code\\n    1: data： read in, text, and ontology data. And process them into format (e.g.parse nouns from text and put into a dictionary with ontology terms) that can be used for following analysis steps.\\n    2: analysis: use similar matching to extract drug terms and perform emotion analysis and store result into output csv files. \\n\\n',\n",
       "  'This repository contains code for extracting drug terms and analyzing emotions in drug use discussions on Reddit. The code takes in Reddit posts discussing drugs and uses an ontology list to extract drug names. The emotion in the Reddit posts is also analyzed. The code can be executed for testing, loading data, or performing analysis.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_63': [\"# DSC180B-Capstone-Project\\n- dataset available for download at https://www.kaggle.com/crawford/gene-expression\\n  - #Make sure to unzip files into 'data/raw' folder#\\n- get data: in the command line enter `Rscript run-data.R data`\\n- analysis: in the command line enter `Rscript run-data.R analysis`\\n  - the resulting graphs will be in the data/out folder \\n\\n- data: contains the raw and cleaned versions of the datasets we're working with. Also will hold the graphs from analysis\\n- src: contains the analysis, cleaning, and data etl scripts.\\n  - analysis: golubAnalysis.R contains the script we used to do tests and generate plots\\n  - cleaning: golubCleaning.R contains the script we used to clean the raw datasets found in data/raw\\n  - data: etl.R contains the scirpt to extract the data for run-data.R\\n  \\nAcknowledgements\\n- Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression\\n\\n  - Science 286:531-537. (1999). Published: 1999.10.14\\n\\n  - T.R. Golub, D.K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J.P. Mesirov, H. Coller, M. Loh, J.R. Downing, M.A. Caligiuri, C.D. Bloomfield, and E.S. Lander\\n\",\n",
       "  \"This is a summary of the DSC180B-Capstone-Project. The dataset can be downloaded from the provided link and should be unzipped into the 'data/raw' folder. To get the data, enter `Rscript run-data.R data` in the command line. To perform analysis, enter `Rscript run-data.R analysis` in the command line. The resulting graphs will be saved in the 'data/out' folder. The project includes raw and cleaned datasets, as well as scripts for analysis, cleaning, and data extraction. The project acknowledges the Molecular Classification of Cancer study by Golub et al., published in Science in 1999.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_77': [\"# DSC180B_A07\\n\\nThis repository contains the completed codes for this project that focuses on multiple testing. The src folder contains the main python scripts, with all the functions inside, and the data folder, where all the datasets are held. \\n\\n\\nThe function will return the eda statistic, various plots that show the differences between the healthy people and cardial patients\\n\\nThe congfig folder contains one json files for the use of functions' parameters. \\n\\nURL to our webpage: https://larryzly.github.io/CardiovascularClassifier/\\n```\\n### Responsibilities\\n\\nAll the work and code are produced after our group discussion.\\n* Wentaon Chen developed the run.py and found out how to set the paths for the scripts\\n* Leyang Zhang developed some functions of model.py and upload the reports and datasets\\n* Zimin Dai developed functions in model.py and built the structure for the repository, docker image\\n```\\n\\n\",\n",
       "  'This repository contains completed codes for a project on multiple testing. It includes main python scripts in the src folder, with functions for analyzing data and generating plots. The data folder holds the datasets. The config folder contains a json file for function parameters. The webpage for this project can be found at https://larryzly.github.io/CardiovascularClassifier/. The responsibilities of the group members are also listed.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_29': ['\\n\\n# Using Epidemiology Model To Predict Case Numbers for COVID-19\\n\\n## Table of contents\\n* [General info](#general-info)\\n* [Technologies](#technologies)\\n* [Setup](#setup)\\n* [Directions](#directions)\\n* [Processing](#in_processing)\\n## General info\\n- Use covid-19 datasets provided by JHU to fit epidemiology model to U.S.. After figuring out the infection parameter, we can then predict \\n\\n## Introduction\\nFitting Epidemiology Model with Covid-19 JHU U.S. Data\\n## Technology\\nProject is created with:\\n* Image : https://hub.docker.com/repository/docker/caw062/test\\n## Setup\\n- Before running, use `pip install -r requirements.txt` to install all the required packages\\n- on terminal, run `python run.py data` to retrieve the most current data from JHU & Apple Data\\n\\n## Directions\\n`python run.py test` to first download test data, and then build epidemiology model on the test data. \\nIt will return the beta (infection rate), and D (infection duration) for the entire United States.\\nIt will also return a prediction for counties in Southern California on 1/22/2021 based on case counts on 1/21/2021 (previous day)\\n',\n",
       "  'This document provides information on using an epidemiology model to predict case numbers for COVID-19. It includes general information, technologies used, setup instructions, and directions for running the model. The model is fitted with COVID-19 datasets provided by JHU and can be used to predict case numbers for the United States and specific counties in Southern California.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_22': ['# Political Analysis on Senatorial Twitter Account Using Machine Learning\\n \\n### Project Description\\nThe modern American political landscape often seems void of bipartisanship. Nowhere is this stark divide between red and blue more evident than in the halls of the US Capitol, where the Senate and House of Representatives convene to carry out the duties of the legislative branch. While us average Americans rarely watch the daily proceedings of the Senate or House, Twitter has given us a unique window into the debates and discourse that shape our democracy. In fact, the 116th Congress, which served from January 3, 2019 to January 3, 2021 broke records by tweeting a total of 2.3 million tweets! As such, it is clear that Twitter is quickly becoming a digital public forum for American politicians. This surplus of tweets from the 116th Congress enables us to analyze the Twitter (following-follower) relationships between politicians on and across the two sides of the aisle. This project’s main inquiry is into whether there is a tangible difference in the way that Democrat members of Congress speak and interact on social media in comparison to Republican members of Congress. If there are such differences, this project will leverage them to train a suitable ML model on this data for node classification. That is to say, this project aims to determine a Senator’s political affiliation based off of a) their Twitter relationships to other Senators b) their speech patterns, and c) other mine-able features on Twitter. In order to truly utilize the complex implicit relationships hidden in the Twitter graph, we can use models such as Graph Convolutional Networks, which apply the concept of “convolutions” from CNNs to a graph network-oriented framework, and GraphSage model.\\n\\n### run.py\\nWe implement the GCN and GraphSage models as our main models for training and comparison.\\n\\n- parameters:\\n  - model: The choice of models. We only implement the GCN and GraphSage. \\n  - dataset: The choice of datasets. There are multiple datasets, including data_voting, data_voting_senti. The differences between these datasets are features. The adjacency matrix of each dataset stays the same.\\n  - output_path: The output of project will be stored in json file.\\n  - agg_func: The choice of aggregated function in the graphSage model. We only support MEAN aggregator. The default is MEAN.\\n  - num_neigh: The number of neighbors in the graphSage model. The default is 10.\\n  - n: The number of hidden layers in the GCN model. This can be tuned to reach higher accuracy.\\n  - self_weight: The weight of self-loop in the GCN model.\\n  - hidden_neurons: The number of hidden neurons in the GCN model. The default is 200 and it can be tuned to reach higher accuracy.\\n  - device: The device for training the model. We only support cuda.\\n  - epochs: The number of epochs for both models. The default is 200 epochs.\\n  - lr: The learning rate for both models. The default is 1e-4. This can be tuned for higher accuracy.\\n  - val_size: The size of testing data. The default is 0.3.\\n  - test: The parameter for running test data on models.\\n\\n- some examples for using the project:\\n  - python run.py\\n  - python run.py --test\\n  - python run.py --n_GCN\\n  - python run.py --dataset data_voting\\n  - python run.py --model n_GCN --n 2 --self_weight 20\\n\\n  \\n\\n# Our Project Website\\n\\nPlease view our website [here](https://anuragpamuru.github.io/dsc-180b-capstone-b03/)\\n\\n\\n### Contributers: \\nYimei Zhao, Anurag Pamuru, Yueting Wu\\n',\n",
       "  'This project aims to analyze the Twitter accounts of senators using machine learning techniques. The goal is to determine if there are differences in the way Democrat and Republican members of Congress speak and interact on social media. The project will use the Twitter relationships between politicians, their speech patterns, and other features to train a machine learning model for node classification. The models used in this project are Graph Convolutional Networks (GCN) and GraphSage. The \"run.py\" file provides information on how to run the models with different parameters. The project website can be accessed [here](https://anuragpamuru.github.io/dsc-180b-capstone-b03/). The contributors to this project are Yimei Zhao, Anurag Pamuru, and Yueting Wu.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_20': [\"# GNN-on-3d-points\\n\\n### Abstract:\\n   This research focuses on 3D shape classification. Our goal is to predict the category of shapes consisting of 3D data points. We aim to implement Graph Neural Network models and compare the performances with PointNet, a popular architecture for 3d points cloud classification tasks. Not only will we compare standard metrics such as accuracy and confusion matrix, we will also explore the model's resilience of data transformation. Besides, we also tried combining PointNet with graph pooling layers.\\n   \\n   \\nSee project website here: https://ctwayen.github.io/Graph-Neural-Network-on-3D-Points/\\n\\nDocker name: ctwayen/project_docker\\n\\nDocker web path: https://hub.docker.com/repository/docker/ctwayen/project_docker\\n\\n### Instruction:\\n\\n   If it is the first time you running this project, please download the data through python run.py all --mode download; You could also use the parameter --method to choose knn or fix-radius to construct the graph you like. Their corresponding parameters are --k and --r.The raw dataset would automatically download into the path 'data/modelnet/ModelNet40'. The points data is stored in 'data/modelnet/modelnet_points'. The consturcted graph trainning data is in 'data/modelnet/modelnet_(knn/fix_radius)(k/r){your param}'. Do not move those files. It may cause problems\\n   \\n   We support training Pointnet and GCN two models.Using --model to choose which one you want to train: GCN or pointNet.\\n   \\n   Shared paramaters are (default values are the best combination we found):\\n   \\n   --lr：Learning rate\\n   \\n   --bs: batch size\\n   \\n   --base: dataset path. If you are using default data, you do not need to specify this.Otherwise, write the graph dataset you just constructed\\n   \\n   --data: 10 or 40. Choose 10 to run 10 categories classfication and 40 for 40 categories\\n   \\n   --epoch: epoch\\n   \\n   --val_size: validation size. For example, 0.2 will have 20 % data as validation data\\n   \\n   --model_path: The path to store trained_model. Default is 'trained_models'\\n   \\n   --ouput_path: The path to store the ouput csv file\\n   \\n   Parameters that only used in GCN:\\n   \\n   --pool: Which pooling to use. SAG or ASA\\n   \\n   --ratio: The pooling ratio. For example, 0.4 will pool out 60% of data each time\\n   \\n#### Important! Training GCN will take about 7-10 minutes for one epoch. Training PointNet will take about 50s for one epoch. Please manage your time for training process.\\n\\n   \\n### Notebooks and results:\\n\\n   Besides training your own models, we also offered few trained models. You could check notebooks/Analyzing results to see our training results for different hyperparamters setting. You could also check how to use a trained_model there.\\n   \\n   You could also check the data augmentation's effects on models in the notebook/analyzing resistence file\\n   \\n   You could check how pooling layer affect result in the notebook/analyzing pooling file\\n\\nAuthor: @Xinrui Zhan.If you find any bug, contact me through the email: ctwayen@outlook.com \\n   \\n   \\n\",\n",
       "  'This research focuses on 3D shape classification using Graph Neural Network models. The goal is to predict the category of shapes consisting of 3D data points. The performance of these models will be compared with PointNet, a popular architecture for 3D point cloud classification tasks. The research also explores the resilience of the models to data transformation and combines PointNet with graph pooling layers. The project website and Docker information are provided for further reference. Instructions are given for downloading the data, choosing graph construction methods, and training the models. Notebooks and results are available for analyzing training results, data augmentation effects, and the impact of pooling layers on the results. Contact information is provided for any bug reports or inquiries.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_19': ['# Graph-based Product Recommendation\\nDSC180B Capstone Project on Graph Data Analysis\\n\\nProject Website: https://nhtsai.github.io/graph-rec/\\n\\n## Project\\nAmazon Product Recommendation using a graph neural network approach.\\n\\n### Requirements\\n- dask\\n- pandas\\n- torch\\n- torchtext\\n- dgl\\n\\n## Data\\n### Datasets\\nAmazon Product Dataset from Professor Julian McAuley ([link](http://jmcauley.ucsd.edu/data/amazon/links.html))\\n* Product Reviews (5-core)\\n* Product Metadata\\n* Product Image Features\\n\\n## GraphSAGE Model\\n\\n## PinSAGE\\n\\n### Graph & Features\\nThe graph is a heterogeneous, bipartite user-product graph, connected by reviews.\\n * Product Nodes (`ASIN`)\\n   * Features: `title`, `price`, image representation\\n * User Nodes (`reviewerID`)\\n * Edges (`user`, `reviewed`, `product`) and (`product`, `reviewed-by`, `user`)\\n   * Features: `helpful`, `overall`\\n\\n### Data Configuration (`config/data-params.json`)\\n\\n### Model\\nWe use an unsupervised PinSage model (adapted from [DGL](https://github.com/dmlc/dgl/tree/master/examples/pytorch/pinsage)).\\n\\n### Model Configuration (`config/pinsage-model-params.json`)\\n- `name`: model configuration name\\n- `random-walk-length`: maximum number traversals for a single random walk, `default: 2`\\n- `random-walk-restart-prob`: termination probability after each random walk traversal, `default: 0.5`\\n- `num-random-walks`:  number of random walks to try for each given node, `default: 10`\\n- `num-neighbors`: number of neighbors to select for each given node, `default: 3`\\n- `num-layers`: number of sampling layers, `default: 2`\\n- `hidden-dims`: dimension of product embedding, `default: 64 or 128`\\n- `batch-size`: batch size, `default: 64`\\n- `num-epochs`: number of training epochs, `default: 500`\\n- `batches-per-epoch`: number of batches per training epoch, `default: 512`\\n- `num-workers`: number of workers, `default: 3 or (#cores - 1)\\n- `lr`: learning rate, `default: 3e-4`\\n- `k`: number of recommendations, `default: 500`\\n- `model-dir`: directory of existing model to continue training\\n- `existing-model`: filename of existing model to continue training, `default: null`\\n- `id-as-features`: use id as features, makes model transductive\\n- `eval-freq`: evaluates model on validation set when `epoch % eval-freq == 0`, also evaluates model after last training epoch\\n- `save-freq`: saves model when `epoch % save-freq == 0`, also saves model after last training epoch\\n\\n## References\\n* [GraphSAGE Homepage](http://snap.stanford.edu/graphsage/)\\n* [GraphSAGE Research Paper](https://arxiv.org/abs/1706.02216)\\n* [PinSage Article](https://medium.com/pinterest-engineering/pinsage-a-new-graph-convolutional-neural-network-for-web-scale-recommender-systems-88795a107f48)\\n* [PinSage Research Paper](https://arxiv.org/abs/1806.01973)\\n',\n",
       "  'This project focuses on Amazon product recommendation using a graph neural network approach. The data used includes the Amazon Product Dataset, which consists of product reviews, product metadata, and product image features. The graph used in the model is a heterogeneous, bipartite user-product graph connected by reviews. The model used is an unsupervised PinSage model, and the configuration parameters for the model are provided. References to GraphSAGE and PinSage are also included for further reading.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_18': [\"# NBA-Game-Prediction\\nProject Group: MengYuan Shi, Austin Le\\n\\nThis repository contains a data science project that discover the NBA Game Prediction. We investigate the social network for individual NBA players and the relationship between each team. We will use team's statistics and players' statistics and analysis for predicting who wins the games by leveraging the team's statistics and players' statistics from 2015 season to 2019 season. We will use GraphSAGE which is a generalizable embedding framework to create a graph classification.\\n\\n\\n### Warning\\nOur group has altered and used the graphsage implementation that can be received from https://github.com/williamleif/GraphSAGE . We made minor changes within the model and inputs to align with the goals of our project, but we would like to cite them as a source for the main graphsage implementation.\\n\\n\\n### Running the project\\n- `python run.py` can be run from the command line to ingest data, train a model, and present relevant statistics for model performance to the shell\\n  - Reads in CSV file from eightthirtyfour for play by play data\\n  - Runs algorithm to create network between each player based on their playing time\\n  - Appends all player edges from all season onto a single graph \\n  - Embedd player categorical statistics onto each node \\n  - runs graphSage model to learn over features\\n\\n### Outputs\\n  - The outputs printed will be the corresponding accuracies obtained after the training\\n  - ~5min runtime\\n\\n### Responsibility \\n- Austin Le: Responsible for the data cleaning and data scraping and the coding part as well as the report.\\n- MengYuan Shi: Responsible for the paper researching and writing the report part as well as the visualization.\\n\",\n",
       "  'This repository contains a data science project focused on predicting NBA game outcomes. The project investigates the social network of individual NBA players and the relationship between teams. It uses team and player statistics from the 2015 to 2019 seasons to predict game winners. The project utilizes GraphSAGE, a graph classification framework, to create embeddings for analysis. The code for this project is based on an altered version of the GraphSAGE implementation from https://github.com/williamleif/GraphSAGE. Running the project involves ingesting data, training a model, and presenting performance statistics. The outputs include accuracy metrics obtained after training, with a runtime of approximately 5 minutes. Austin Le is responsible for data cleaning, scraping, coding, and report writing, while MengYuan Shi focuses on research, report writing, and visualization.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_21': [\"# DSC180B_Project\\n\\nURL: https://sdevinl.github.io/DSC180B_Project/\\n\\n**Disclaimer:**   \\n  We want to make it clear that the graphsage implentation found in this repo is not our own. We have made minor alterations to the code in order to better serve our overall project in regards to NBA team rankings. The original graphsage implementation can be found here https://github.com/williamleif/GraphSAGE . We would also like to cite their paper:\\n  \\n     @inproceedings{hamilton2017inductive,\\n\\t     author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},\\n\\t     title = {Inductive Representation Learning on Large Graphs},\\n\\t     booktitle = {NIPS},\\n\\t     year = {2017}\\n\\t   }\\n  For more information on how to run graphsage as well as the requirements for grapshage be sure to checkout the original graphsage's implementation.\\n\\n**About:**  \\n  This repository contains an implementation of a GraphSAGE for node classification on an NBA dataset. The goal being able to classify the ranks of NBA teams using player stats and a graph representation of matchups between teams in a season. \\n  \\n**Setting Up Docker Image**  \\n  The docker image that was created in order to have an environment able to run this project is found on the repo at aubarrio/graphsage . \\n    \\n**Model**  \\n  data: The data we use in this project is a compound of multiple webscraped data found on https://www.basketball-reference.com we used stats such as player and team stats, along with team schedules for the season. The seasons for which we collected data range from 2011 to the 2019 season.  \\n\\n**Basic Parameters**  \\n  Since we are in early stages of developing we only have one parameter of choice and that is [train, test]. This is to distinguish the data being input into the model.  \\n    train: Parameter train will train the model on all available features (181 different features)  \\n    test: Parameter test will train the model on 2 features (Rank and Id) which is meant to use as an evaluation of how our data is performing  \\n    \\n**Examples run.py**  \\n  python run.py test  \\n  python run.py  \\n  \\n  The run.py file will run a graphsage model with the mean aggregator on our NBA data. For more information on how to change the overall model or change the parameters of a model refer to the original graphsage implementation.\\n  \\n**Output**  \\n  Direct terminal output outlining the training, validation and test accuracies of our model.  \\n  In the sage_outputs folder you will find files that contain data on the model:\\n  \\tThe rawOutputs folder contains a json file that includes the raw output probabilites of our NBA test set.\\n\\tThe stats folder will contain all accuracies and losses captured whilst training your model.\\n\\t\\n**Acknowledgements**   \\n  As mentioned in the disclaimer the original version of this code can be found here https://github.com/williamleif/GraphSAGE. An appreciation to the original creators of this code and a thank you for allowing this code to be available and ready to use on projects such as ours. \\n\",\n",
       "  'This repository contains an implementation of GraphSAGE for node classification on an NBA dataset. The goal is to classify the ranks of NBA teams using player stats and a graph representation of matchups between teams in a season. The data used in this project is a combination of webscraped data from basketball-reference.com, including player and team stats, as well as team schedules for the season. The model can be trained on all available features or on a subset of features for evaluation purposes. The output includes terminal output with training, validation, and test accuracies, as well as files in the sage_outputs folder containing raw output probabilities and training statistics. The original code for GraphSAGE can be found at https://github.com/williamleif/GraphSAGE.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_76': ['Name: Jason Chau, Sung-Lin Chang, Dylan Loe\\n\\nWelcome to our Stock Predictor. \\n\\nIMPORTANT!\\nFor running actual models:\\n\\nIn order to run our stock predictor, just make sure that you are in the current directory and run the command \\n\\npython run.py all - this will let you scrape the data, as well as running our model\\n\\n\\npython run.py test - this will let you run our model on the data pulled from the webscraper, as well as predicting which stocks will\\nbe bullish or bearish in Dow Jones 30.\\n\\npython run.py fcn - this will let you run our Fully connected network model on the data pulled from the webscraper\\n\\npython run.py build - This command builds the test portion of the code. Please keep in mind this will pull in data from the yahoo finance api,\\nso it will require an internet connection to pull in the data and calculate it.\\n\\n\\n------ CONFIG FILE--------------\\n\\n\"NUM_EPOCHS\" : 100, ( this is the number of trials you want to use)\\n\"LEARNING_RATE\" : 0.001, ( this is the learning rate)\\n\"NUM_HIDDEN\" : 32, ( number of hidden features)\\n\"num_days\": 5, (number of lag days you will have, we chose 5 because there are 5 trading days)\\n\"nfeat\" : 20, ( this is 4 * num_days)\\n\"nclass\" : 1, \\n\"dataset\" : \"./data/dowJonescorrelation0.4graph.csv\", (this is the correlation graph we use for our model)\\n\"thresh\" : 0.4, ( the threshold for the node adjacency)\\n\"filepath\" : \"./data/12modowJonesData/\", (the dataset to use)\\n\"timeframe\" : \"12mo\" ( which time period, there is 12mo and 6 mo\\n\\nRequired packages\\n\\nyfinance == 0.1.55\\npandas-datareader = 0.9.0\\nbeautifulsoup4 4.9.3\\ntensorflow == 1.12.0\\nnetworkx == 2.1\\nnumpy == 1.14.3\\nscipy == 1.1.0\\nsklearn == 0.19.1\\nmatplotlib == 2.2.2\\n',\n",
       "  'This is a summary of the instructions and configuration details for running the Stock Predictor program:\\n\\n- To run the stock predictor, make sure you are in the current directory and run the command \"python run.py all\". This will scrape data and run the model.\\n- You can also use the commands \"python run.py test\" to run the model on data pulled from a web scraper, and \"python run.py fcn\" to run a fully connected network model.\\n- The command \"python run.py build\" builds the test portion of the code, which requires an internet connection to pull data from Yahoo Finance API.\\n- The configuration file includes parameters such as number of epochs, learning rate, number of hidden features, number of lag days, dataset file path, threshold for node adjacency, and timeframe.\\n- Required packages for running the program include yfinance, pandas-datareader, beautifulsoup4, tensorflow, networkx, numpy, scipy, sklearn, and matplotlib.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_0': ['# Political Popularity of Misinformation\\n- This project looks at the popularity and influence of politicians on Twitter by analyzing the engagement ratios as well as the rolling and cumulative maxes of likes and retweets over time.\\n\\n### Note\\n- To get the data necessary to replicate this project, access to the Twitter API is needed. \\n- You must have configured twarc using `twarc configure` in the terminal with your API credentials in order to run the data pipeline. Additional information on how to do so can be found here, https://github.com/DocNow/twarc.\\n- In addition, to run the data pipeline you must obtain a bearer token from Twitter’s API and store it in a config.py file in the root directory. Information on using and generating a bearer token can be found here, https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens. bearer_token = “...”\\n\\n### Obtaining the txt files\\n- We obtain the tweet IDs that compose our politicians’ timeline found in `src/data` from George Washington University’s TweetSets database found here, https://tweetsets.library.gwu.edu/datasets.\\n- We chose to focus on politicians who served in the 116th United States Congress, which corresponds to two datasets, Congress: Representatives of the 116th Congress and Congress: Senators of the 116th Congress.\\n- After identifying our politicians, we gathered the user IDs for their Twitter accounts using Tweepy, which are then used to query the two Congress datasets. The datasets also contain a file of the House and Senate members along with their user IDs which is an alternative way to obtain these IDs. The files can be found here, [Senate](https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/MBOJNS/8VQVWT&version=1.0) and [Representative](https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/MBOJNS/WXZE5N&version=1.0).\\n- To query the datasets, for each politician, we selected either the Representative or Senator dataset depending on their position and inputted their user ID in the “Contains any user id” box under the “Posted by” section. This process gives us a txt file of tweet IDs for each politician which is then stored in the `src/data/misinformation` and `src/data/scientific` folders depending on the group the politician is assigned to. \\n\\n### Using `run.py`\\n- To get the data, from the project root directory, run `python run.py data`\\n    * This downloads the data using the tweet IDs found in `src/data/misinformation` and `src/data/scientific`.\\n    * This also downloads the engagement metrics needed for ratio analysis using the same tweet IDs.\\n    * The politicians to analyze can be specified in `config/data-params.json`.\\n    * The name of the txt file containing the tweet IDs must match the name specified in `config/data-params.json`.\\n    * The output is a json file for each politician containing their collection of tweets as well as a csv file containing the engagement metrics and text of the tweet ID.\\n\\n- To calculate the ratios for the tweets, from the project root directory, run `python run.py ratio`\\n    * This calculates the ratios for the tweets found in the csv files in `src/data/misinformation` and `src/data/scientific`.\\n    * The politicians to analyze can be specified in `config/data-params.json`.\\n    * Output is an updated csv file for each politician containing the engagement metrics of their tweets and their ratio values.\\n\\n- To calculate the popularity estimate metrics, from the projecy root directory, run `python run.py metrics`\\n    * This will create a JSON file for each estimate metric you wish to analyze. \\n    * The outputs are stored in `src/out`.\\n    * These JSON files will then be used to create visualizations using the visualization target.\\n\\n- To create the visualizations, from the project root directory, run `python run.py visualization`\\n    * This creates visualizations from the JSON files created from the metrics target.\\n    * The outputs are stored in `src/out`.\\n\\n- To run the permutation test on our groups, from the project root directory, run `python run.py permute`\\n    * This will run a permutation test within our two groups as well as between our groups.\\n    * The outputs are stored in `src/out`.\\n\\n- To run all of the above targets, from the project root directory, run `python run.py all`\\n\\n- To run the test target, from the project root directory, run `python run.py test`\\n    * This runs most of the above targets on a small, fake dataset.',\n",
       "  \"This project analyzes the popularity and influence of politicians on Twitter by examining engagement ratios and the likes and retweets of their tweets over time. The data is obtained from the Twitter API and George Washington University's TweetSets database. The project includes various steps such as downloading data, calculating ratios, estimating popularity metrics, creating visualizations, running permutation tests, and running all targets. There is also a test target available for a small dataset.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_1': [\"# The Sentiment of U.S. Presidential Elections on Twitter\\nThis project investigates the public sentiment on Twitter regarding the 2016 and 2020 U.S. Presidential Elections. Political tensions in the United States came to a head in 2020 as people disputed President Donald Trump's handling of various major events that the year brought such as the COVID-19 pandemic and the killing of George Floyd and subsequent racial protests, and we aim to identify if this was quantifiably reflected in people's behavior on social media. To do this, we perform sentiment analysis on tweets related to the elections and conduct permutation testing to analyze how sentiment may differ between the two years and between and within politically left- and right-leaning groups of users. \\n\\n\\n### Running The Project\\n- All commands specified here are to be run from within this project's root directory\\n- To install necessary dependencies, run `pip install -r requirements.txt`\\n- Note: to get the data necessary to replicate this project, access to the Twitter API is needed\\n\\n### Using `run.py`\\n- This project uses publicly available 2016 and 2020 presidential election datasets located at https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/PDI7IN and https://github.com/echen102/us-pres-elections-2020. Given the 2016 dataset is not uniformly structured, you must manually download the txt files of tweet ids from the dataset's website to the directory `data/raw/2016`. The 2020 dataset can be downloaded programmatically using the `data` target, as follows.\\n\\n- To get hydrated tweets for the 2016 and 2020 tweet ids, run the command `python run.py data`\\n    * This samples from the 2016 tweet ids located in `data/raw/2016` and stores them in txt files in `data/temp/2020/tweet_ids/`\\n    * It also directly downloads tweets for the 2020 election from the dataset's GitHub page falling within the date range specified in `config/etl-params.json`, samples from them, and stores them in txt files in `data/temp/2020/tweet_ids/`\\n    * It then rehydrates the tweets using `twarc` and saves the them in jsonl format in the `hydrated_tweets/` directory within each year's respective data directory\\n\\n- To clean the rehydrated tweets, run the command `python run.py clean`\\n    * This takes the rehydrated tweets obtained from running `python run.py data` and creates a single csv file of tweets for each year with fields for the features of interest specified in `config/clean-params.json`\\n    * For the purpose of performing sentiment analysis later, tweets in languages other than English are filtered out.\\n    * The resulting csvs are stored as `clean/clean_tweets.csv` within each year's data directory.\\n\\n- To run the main analysis, run `python run.py compute` from the project root directory\\n    * For each year, this subsets the tweets into left and right leaning, classifies the the different types of dialogue, performs sentiment analysis on the various subsets of data, and conducts permutation testing on the subsets of the data between the two years, producing plots of the results.\\n    \\n- To run the `clean` and `compute` targets on fake test data, run the command `python run.py test`\\n\\n\",\n",
       "  'This project analyzes the sentiment of Twitter users during the 2016 and 2020 U.S. Presidential Elections. It aims to determine if public sentiment on social media reflected the political tensions and major events of those years. The project performs sentiment analysis on election-related tweets and conducts permutation testing to compare sentiment between the two years and among politically left- and right-leaning groups. The project provides instructions for running it, including installing dependencies, accessing necessary data, and executing different commands for data processing and analysis.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_2': ['# Data Science Capstone Project\\n\\n## Info\\n* The data being used here is Tweets from various news sources.\\n## Preqrequisites\\n* Ensure libraries are installed. (pandas, requests, os, gzip, shutil, json, flatten).\\n* Download repo: https://github.com/thepanacealab/covid19_twitter.\\n* Docker container id: tmpankaj/example-docker\\n## How to Run\\n* All parameters are of type str unless specified otherwise\\n* Set twitter API Keys in config/twitter-api-keys.json\\n#### Test\\n* run \\'python run.py test\\' in root directory of repo\\n* look in test/visualizations for the test targets\\n#### Data \\n* Go inside docker container\\n* Add .txt files with Tweet IDs from https://tweetsets.library.gwu.edu/ to some directory where preprocessed data will be stored. (E.g. cnn.txt in /data/preprocessed directory)\\n* Use this hydrator https://github.com/DocNow/hydrator to hydrate these tweets and make sure there is a .csv file in the same directory (E.g. cnn.csv in /data/preprocessed)\\n* Set data parameters in config/data-params.json\\n   * preprocessed_data_path: path to directory of preprocessed data\\n   * training_data_path: path to directory to output training data\\n   * dims (list of str): list of the names of the dimensions that polarities will be eventually calculated on (E.g. [\"moderacy\", \"misinformation\"])\\n   * labels (dict): dictionary with the keys including the news sources and each value being a list with a polarity for each dimension. Every news source that will be used in your data must have a label for every dimension. (E.g. {\"cnn\": [0, 1], \"fox\": [1, 0]})\\n   * user_data_path: path to directory to output user data\\n   * exclude_replies (bool): If true, will exclude replies when collecting user tweets.\\n   * include_rts (bool): If true, will include retweets when collecting user tweets.\\n   * max_recent_tweets (int): maximum recent number of tweets to obtain from a user\\n   * tweet_ids (list of str): list of tweet IDs to collect to analyze flagged vs unflagged retweeters\\n* Make sure paths to directories already exist\\n* run \\'python run.py data\\' in root directory of repo\\n* This will only collect the data\\n#### Train\\n* Go inside docker container and make sure data has been collected\\n* Set train parameters in config/train-params.json\\n   * training_data_path: path to directory of the training data (should be same as data-params)\\n   * model_path: path to directory to output models to be trained\\n   * dims (list of str): list of the names of the dimensions that polarities should be calculated on (E.g. [\"moderacy\", \"misinformation\"])\\n   * fit_priors (list of bools): hyperparemeter for Naive Bayes classifier (1 for each dimension) - Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\\n   * max_dfs (list of floats/ints): hyperparameter for CountVectorizer (1 for each dimension) - When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts.\\n   * min_dfs (list of floats/ints): hyperparameter for CountVectorizer (1 for each dimension) - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. \\n   * n_splits (int): number of folds to use for K-fold cross validation\\n   * outdir: path to directory to output a notebook of the results\\n* Make sure paths to directories already exist\\n* run \\'python run.py train\\' in root directory of repo \\n* Look in the outdir you specified for an html file of the results\\n#### Analysis\\n* Go inside docker container and make sure data has been collected and models have been trained\\n* Set analysis parameters in config/analysis-params.json\\n   * model_path: path to directory of trained models (should be same as train-params)\\n   * user_data_path: path to directory of user data (should be same as data-params)\\n   * dims (list of str): list of the names of the dimensions that polarities should be calculated on (E.g. [\"moderacy\", \"misinformation\"])\\n   * tweet_ids (list of str): list of tweet IDs to analyze\\n   * flagged (dict): dictionary should have a key for every tweet to be analyzed and a boolean for whether or not the tweet was flagged (E.g. {\"123\": true, \"456\": false})\\n   * outdir: path to directory to output a notebook of the results\\n* Make sure paths to directories already exist\\n* run \\'python run.py analysis\\' in root directory of repo\\n* Look in the outdir you specified for an html file of the results\\n#### Results\\n* Go inside docker container and make sure data has been collected, models have been trained, and analysis has been ran.\\n* Set results parameters in config/results-params.json\\n   * user_data_path: path to directory of user data (should be same as data-params)\\n   * dims (list of str): list of the names of the dimensions that results should be calculated on (E.g. [\"moderacy\", \"misinformation\"])\\n   * outdir: path to directory to output a notebook of the results\\n* Make sure paths to directories already exist\\n* run \\'python run.py results\\' in root directory of repo\\n* Look in the outdir you specified for an html file of the results\\n',\n",
       "  'This is a guide for running a Data Science Capstone Project. It provides information on the data being used, prerequisites, and step-by-step instructions on how to run the project. The guide includes sections on testing, data collection, training models, analysis, and viewing the results.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_3': ['## Where to begin?\\n### Begin by uploading your twitter API credentials into a json file as under a new .env director. The file path should look like this: .env/twitter_credentials.json\\n\\nThe json file should be structured as\\n\\n```json\\n{\\n   \"CONSUMER_KEY\":\"your-consumer-key-here\",\\n   \"CONSUMER_SECRET\":\"your-consumer-secret-here\",\\n   \"ACCESS_TOKEN\":\"your-access-token-here\",\\n   \"ACCESS_TOKEN_SECRET\":\"your-access-token-secret-here\"\\n}\\n```\\n\\nIf you do not have twitter API credentials, please visit https://developer.twitter.com/en/docs/twitter-api to apply for a developer account.\\n\\nTo install the dependencies, run the following command from the root directory of the project: pip install ```-r requirements.txt```\\n\\n## How to use run.py:\\nrun.py takes in one argument, a choice between *data*, *eda*, *test*\\n\\n## Directories\\n* A directory titled *data* will be created with 4 subdirectories: *graphs, raw, processed*\\n   * *graphs* will hold any charts from eda functions\\n   * *processed* will hold any statistic data from eda functions\\n   * *raw* will hold raw tweet data\\n* Each of the above directories will be split into two additional subdirectories, *news* and *election*\\n   * *news* will hold any data related to news stations\\n   * *election* will hold any data related to the election dataset\\n\\n## Description of arguments (targets)\\n\\n### data\\n* Your twitter API credentials for use in downloading data to be used in our project.\\n* The target will download all tweets as specified in the config file *news_params.json*\\n\\n### eda\\n* The eda target will generate statistics and visualizations after data has been gathered from the *data* target\\n* Currently we have built a wordcloud visualization that will be stored in *graphs* and a statistic of most common hashtags per news station stored in *processed*\\n\\n### compile and embed\\n* Performs graph embedding calculations as described in the methodology section of the report\\n\\n### test\\n* The test target is designed for grading functionality in the DSC180B capstone course and will test three functionalities:\\n   * *etl_news* checks that test data is available for use\\n   * *eda* generates visualizations and statistics based on the test data, stores in *test/testreport*\\n   * *similarity* will generate similarity hashtag vectors to be used in our main analysis *test/testreport*\\n\\n',\n",
       "  'This text provides instructions on how to begin using the run.py script. It explains how to upload Twitter API credentials, install dependencies, and use the different arguments of the script. It also describes the directory structure and the purpose of each directory. The arguments include \"data\" for downloading tweets, \"eda\" for generating statistics and visualizations, \"compile and embed\" for performing graph embedding calculations, and \"test\" for testing functionality.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_4': ['# Election-Sentiment\\nAn analysis of views towards the US 2020 Presidential election using Twitter data.\\n\\nContained in this repository are a few notebooks that contain our analyses in this investigation on Election Sentiment analysis as well as an investigation into how Twitter impacts election results.\\n\\n### Building the preoject using `run.py`\\n\\nRunning the test target via the command \"python run.py test\" will produce images related to the distribution of discussion levels of the two elections. In order to customize the data that this script is run on, replace the data in the test folder with data of your choice.\\n\\nProvided in the scripts folder, are scripts to donwload tweets from the github repository that we collected tweets on the 2020 election for. The links to the 2020 election and 2016 election tweet ID\\'s are below:\\n\\n2016:\\nhttps://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/PDI7IN\\n\\n2020:\\nhttps://github.com/echen102/us-pres-elections-2020\\n\\nIn order to run the data download scripts, you need to have twarc installed. However, building the project with the provided dockerfile in this repository will download all of the extra packages not native to most Machine Learning and Data Science platforms.\\n\\n\\n## Running the project\\n* To get the data from Twitter, create a developer account and get your developer keys\\n* Configure `twarc`\\n  - On the terminal, run `twarc configure`\\n  - Supply keys made earlier\\n\\n\\n## Groupmate Responsibilities\\n\\n### Chris\\n\\nChris was responsible for the EDA and sentiment analysis and those respective portions of the report. Her work was focused heavily on understanding the sentiment as time progressed and how that related to the individual elections. \\n\\n### Prem\\n\\nPrem was involved heavily in creating the scripts that could be ran by anyone in order to perform ETL on the data. In addition Prem worked on developing the discussion metric that was critical to this investigation.\\n',\n",
       "  'This repository contains notebooks analyzing views towards the US 2020 Presidential election using Twitter data. It also includes scripts for downloading tweets related to the 2020 and 2016 elections. The project can be run by configuring `twarc` with developer keys and running the provided scripts. Chris focused on EDA and sentiment analysis, while Prem worked on ETL scripts and developing the discussion metric.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_5': ['\\n# The Spread of Misinformation on Reddit\\nObserving how different forms of misinformation and conspiracies are spread through social media.\\n\\n## Overview\\nWith the amount of actively spread misinformation circulating popular social media platforms, our goal is to explore if various forms of misinformation follow varying patterns of diffusion. The scope of our project will be limited to two misinformation types -- myth and political misinformation, and focused on Reddit. Our data will be obtained from Reddit archive [pushshift.io](http://pushshift.io).\\n\\n## Contents\\n- `src` contains the source code of our project, including algorithms for data extraction, analysis, and modelling.\\n- `notebooks` contain some examples of the models this code will generate, detailing our findings under the circumstances in which we conducted our testing.\\n- `config` contains easily changable parameters to test the data under various circumstances or change directories as needed.\\n- `run.py` will build and run different the different parts of the source code, as needed by the user.\\n- `references` cite the sources we used to construct this project.\\n- `requirements.txt` lists the Python package dependencies of which the code relies on. \\n\\n## How to Run\\n- Install the dependencies by running `pip install -r requirements.txt` from the root directory of the project.\\n- Alternatively, you may reference our Docker image to recreate our environment, located [here](https://hub.docker.com/r/cindyhuynh/reddit-misinformation).\\n- Due to the open source nature of the PushShift archive, there is no need for any API use or developer account. \\n\\n### Building the project stages using `run.py`\\n- To download the data, run `python run.py data`\\n\\t- This downloads reddit comments from specified subreddits between a certain time period. The subreddits and time period are specified in `config/data_params.json`\\n- To create visualizations of EDA charts, run `python run.py eda`\\n\\t- This creates bar charts representing the dataset we have collected. It also shows visualizes statitics of one-time posters and average number of posts in each category and subreddit.\\n- To get user polarities, run `python run.py user_polarity`\\n\\t- This generates a metric for all users collected in the data, getting filepaths from `config/user_polarity_params.json`\\n- To generate common user matrices, run `python run.py matrices`\\n\\t- This creates two matrices demonstrating, for every possible pair of subreddits, the number and average user polarity of the users in that subset. Filepaths are specified in `config/matrix_params.json`. \\n\\t- NOTE: user_polarities should be run at least once before running `matrices`.\\n- To create visualizations of user polarities and matrices, run `python run.py visualize`\\n\\t- This creates bar charts representing the general user polarity spread, as well as charts showing how users of different types cross into other subreddits. These bar charts will be replaced by heatmaps in a future update for easier visualization. Filepaths are specified in `config/visualize_params.json`. \\n\\t- NOTE: `user_polarities` and `matrices` should be run at least once before running `visualize`.\\n- To run the full pipeline, run `python run.py all`\\n\\t- This will run all the targets. These targets include: `data`, `eda`, `user_polarity`, `matrices`, and `visualize`\\n- To run the full pipeline on test data, run `python run.py test`\\n\\t- This will run all the targets on test data. These targets include: `data`, `eda`, `user_polarity`, `matrices`, and `visualize`\\n',\n",
       "  'This project aims to explore the spread of misinformation on Reddit, specifically focusing on two types of misinformation - myth and political misinformation. The project utilizes data obtained from the Reddit archive pushshift.io. The source code, notebooks, and configuration files are provided for data extraction, analysis, and modeling. The project can be run by installing the necessary dependencies or using a Docker image. Various stages of the project can be executed using the `run.py` script, including downloading data, creating visualizations, generating user polarities and matrices, and visualizing the results. There is also an option to run the full pipeline or test it with test data.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_6': ['# DSC180B-Project\\n\\n## Table of Contents\\n\\n- [Introduction](#introduction)\\n- [Requirement](#requirement)\\n- [Steps for Building](#building)\\n- [Data](#data)\\n- [EDA](#EDA)\\n- [Features and Models](#features_and_models)\\n- [Contributors](#responsibilities)\\n\\n\\n## Introduction\\n\\nCovid-19 changed everyone, from the way we interact, to how we work, and our methods of communication, especially through social media. Under this pandemic period, social media becomes a huge and important part of people’s daily lives. It provides mobile users a convenient way to connect to each other around the world and acquire the updated and trending information about the topic of covid-19. Beside these, people can also express their thoughts and feelings toward certain topics by posting on social media. Throughout the studying of this quarter, we noticed that there are numbers of posts in our Twitter dataset that are related to the topic of covid-19 having some strong emotions and sentiments. In the meantime, a previous study has shown that more people are experiencing negative emotions such as anxiety and panic under this pandemic period. Therefore, we are interested in analyzing the posts that are related to the topic of covid-19 on social media and investigating the underlying causes of the negative emotions implied in these posts.\\n\\n\\n## Requirement\\n\\n- python 3\\n- install the used modules included in the requirements.txt:\\n```\\ncd references\\npip install -r requirements.txt\\n```\\n- In order to rehydrate the twitter data, you need to create a Twitter Developer Api account and get the API(costumer) key, API secret key, Access Token, and Access Token Secret.\\n- Also a Kaggle account with username and key to access the kaggle dataset.\\n- Save these keys into `.env` file in the project root directory. You can modify the `.env.example` with your own information and save it as `.env`.\\n\\n\\n## Building\\n\\nThere are six targets available in order to build the projects:\\n* data\\n    - run **`python run.py data`** for downloading and making datasets (three in total).\\n    - data will be saved in the paths `data/raw` and `data/inteirm`.\\n* analysis\\n    - run **`python run.py analysis`** for cleaning and analyzing the collected data\\n    - results (plots and tables) will be saved in the paths `data/analysis`.\\n    - you can directly view the EDA report in `notebooks/analysis.ipynb`.\\n* feature\\n    - run **`python run.py feature`** for building prediction models by using the pre-trained Kaggle dataset in order to label our own tweet dataset.\\n    - the mean sentiment scores per day will be saved in `data/final`.\\n* model\\n    - run **`python run.py model`** for using time series models to analyze the sentiment scores and daily new cases.\\n    - results will be saved in `data/final`.\\n* test\\n    - run **`python run.py test`** for building steps on our made-up test data.\\n    - this target is equivalent to **`python run.py test-data analysis feature model`**.\\n    - **IMPORTANT**: If you want to run any specific target on the test data, please include the target `test-data` in the command. For example, `python run.py test-data analysis` and `python run.py test-data feature`. And please make sure run the command in this order: test-data, analysis, feature, model.\\n* all\\n    - run **`python run.py all`** for building the complete project.\\n    - this command is equivalent to **`python run.py data analysis feature model`**.\\n\\n\\n## Data\\n\\nWe use data from [thepanacealab](https://github.com/thepanacealab) which gathers COVID-19 twitter data daily. Our data generation script enables us to input a certain date and automatically download the corresponding tweets on that date from the Panacea Lab, unzip the tab-delimited file (tsv), generate a list full of tweets IDs and rehydrate them using twarc, a command line tool and Python library that archives Twitter data.\\n\\nWe also obtain the Covid-19 daily cases dataset from [Our World in Data](https://github.com/owid/covid-19-data/tree/master/public/data) then sum up the daily cases numbers for all countries listed per day. In addition, in order to build efficient machine learning models, we use the tweet text dataset with sentiment labels from [Kaggle](https://www.kaggle.com/kazanova/sentiment140).\\n\\n```\\n.\\n├──src\\n│  ├── data              \\n│      ├── collect_data.sh  # changing directory, unzipping file, curl and twarc\\n│      ├── get_IDs.py   # table processing function that extracts the tweetID and output txt\\n       |—— extract_to_csv.py # convert all the json files into cleaner csv formats\\n       |—— clean_text.py # clean all the tweet text\\n       |—— case_download.py # dowanload and clean the daily cases dataset \\n       |—— train_dataset.py # dowanload and clean the labeled tweet sentiment dataset\\n└── ...\\n```\\n\\nIn order to successfully obtain data from Twitter and Kaggle, you need to have your Twitter and Kaggle Api information saved in the `.env` file. The script will download all of the tweetsIDs and hydrate them using `twarc`. The data will be saved under the `data/raw` folder with date being the subfolder name. Note that the `data/raw` folder is not being version controlled to avoid data corruption. The script will create the raw folder if it does not already exist. All cleaned datasets are saved in `data/interim`.\\n\\n\\n## EDA\\n\\nUse the command `python run.py analysis` to generate statistics and graphs of the twitter data. These files will be generated in the `data/analysis` folder. The `notebooks/report.ipynb` displays a analysis and results report alongside those statistics.\\n\\n\\n## Features and Models\\n\\nBy using the command `python run.py feature`, you can build machine learning models on the pre-labeled Kaggle dataset to label the collected tweet dataset. There are two models that can be used: svc and logreg. In order to change the model options, you can modify the configuration file `config/feature-params.json`: change the value of `model` to either `logreg` or `svc`. The results (mean sentiment score for each day from 03/22 to 11/30) will be saved in `data/final`.\\n\\n\\n## Test\\n\\nAll test data are saved in `test/testdata`. Our test data contains no personal information as all of the details have been replaced. Only some tweets in the test set has hashtags of COVID19 and specific words which ensures it to run efficiently.\\n\\n\\n## Responsibilities\\n```\\n* Jiawei Zheng developed\\n* Yunlin Tang developed\\n* Zhou Li developed\\n```\\n',\n",
       "  'This project is focused on analyzing posts related to COVID-19 on social media and investigating the underlying causes of negative emotions expressed in these posts. The project involves collecting Twitter data, cleaning and analyzing the data, building prediction models using pre-trained datasets, and using time series models to analyze sentiment scores and daily new cases. The project also includes an EDA report and test data for testing purposes. The responsibilities for the project were divided among Jiawei Zheng, Yunlin Tang, and Zhou Li.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_25': ['# DSC180B\\n\\nThere are “wars” going on every day online, but instead of cities, they are defending their options, and perspects. This phenomenon is especially common on the Wikipedia platform where users are free to edit others\\' revisions. In fact, there are “about 12% of discussions are devoted to reverts and vandalism, suggesting that the WP development process is highly contentious.” As Wikipedia has become a trusted source of information and knowledge which is freely accessible, It is important to investigate how editors collaborate and controvert each other in such a platform. This repository will show our coding methods to discuss a new method of measuring controvisality in Wikipedia articles. We have found out that controversiality is highly related to the number of revert edits, the sentiment level among one article comments, and the view counts of that article. Thus we developed a weighted sum formula, which combines those three factors to accurately measure the controversy level within articles in Wikipedia. \\n\\n\\n# Coding part\\nFrom the run.py file, you can notice that we have 2 targets, which is \"All\" and \"Test\". In the following part, We will discuss about the details of those two targets:\\n    \\nFor the \"ALL\" target, it uses datasets from the Wikimedia Data Archives and English Light Dump. Then it used the functions that are listed in the process to generate our final analysis results. There are the purpose for different functions:\\n\\n1. For the get_data.py and deal_withcomment.py, we use those coding files to download XML files from the Wikimedia Data Archives and then convert those raw XML file to dataframe, which is a better form to let us doing the analysis. \\n    \\n2. For the english_lighdump.py, the function for the python file is to download the English Light dump file from the WikiWarMonitor and convert this dataset to a dataframe. It also merge the English Lightdump Dataframe with the comment dataframe that is generated by get_data.py and deal_withcomment.py. \\n    \\n3. For the page_view.py, the function for this python file is to use the titles in the generated dataframe to find the raw description number of views on English Wikipedia of articles in the merged dataframe from those articles\\' start dates to 20210101. \\n    \\n4. For the sentiment_analysis.py, the function for this python file is to use the comments content in the generated dataframe and the Vader model to generate the sentiment score for each comment. Finally, in this python file we will generate a final dataframe that contains M score, page view count, article title, date, comment and sentiment score to use for the future analysis. \\n    \\n5. For the generatefinaldatf.py, the function for this python file is to generate two dataframes that we will use in our analysis part and generate some graphs from analysis. Those two dataframes are dataframe with M is zero and dataframe for all M. \\n    \\n6. For the Analysis.py, the function for this python file is to make some analysis. We generate four graphs in this analysis part:\\n  \\n    a. first one is analysis for corr between M and sentiment score\\n    b. second one is analysis for example of Wooster Ohio\\n    c. third one is relationship between pageview and sentiment score\\n    d. fourth one is view counts with M\\n   For each graph, we save as one figures and use those figures in our report. \\n\\n7. For the Weighted_sum_formula.py, the function for this python file is to generate our final weighed sum formula. And we will use the new scores which are generated by weighted sum formula to make some comparisons with the scores that are generated by M-statistics, and make some analysis on this comparison.  \\n\\n\\nFor the \\'Test\\' target, it mainly runs our test dataset, which means that the result that is generated by our \"test\" target is not representative. And there are some special function for this target, such as generatefinal_dataf_test.py and page_view_test.py, we generate those files because we need to use our test dataset. However, by using this \"test\" target, the analysis result will not be representative. \\n\\n# Notebook\\n\\nFor the content of the notebook, we put our analysis graphs which are generated by \\'ALL\\' target and we will use those graphs in our report. \\n\\n# Resource\\n\\nEnglish light dump data from WikiWarMonitor: http://wwm.phy.bme.hu/light.html\\n\\nXML file from the Wikimedia Data Archives: https://dumps.wikimedia.org/enwiki/20210220/\\n\\nAnd the pageview API from: https://github.com/Commonists/pageview-api\\n\\n# Responsibility\\n\\nCoding: Xingyu Jiang, Xiangchen Zhao\\nNotebook: Xingyu Jiang, Xiangchen Zhao\\nReport: Xingyu Jiang, Xiangchen Zhao and Hengyu Liu\\n',\n",
       "  'This summary discusses the phenomenon of online \"wars\" on platforms like Wikipedia, where users edit and defend their perspectives. The summary highlights the importance of investigating how editors collaborate and disagree on Wikipedia. The coding part of the project involves downloading XML files, converting them to dataframes, merging datasets, analyzing sentiment scores, generating graphs, and developing a weighted sum formula to measure controversy levels in Wikipedia articles. The \"Test\" target runs a test dataset but is not representative. The notebook contains analysis graphs used in the report. The resources used include English light dump data from WikiWarMonitor and XML files from the Wikimedia Data Archives. The responsibility for coding, notebook, and report is shared among Xingyu Jiang, Xiangchen Zhao, and Hengyu Liu.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_26': ['# The Large-Scale Collaborative Presence of Online Fandoms\\n\\nFan communities exist within every industry, and there has been little study on understanding their scale and how they influence the media and their industries. As technology and social media have made it easier than ever for fans to connect with their favorite influencers and find like-minded fans, we’ve seen a rise in fan culture or “fandom”. These individuals form fan groups and communities, which have become increasingly popular online and have rallied behind their favorite artists for different causes.<br><br>\\nThis repository contains library code to explore the similarities and differences in collaboration efforts among fans on two primary online social platforms, Twitter and Wikipedia. It contains methods to quantify the scale, strength, and influence of online fan communities—with a focus on the K-pop fanbase—and how this online collaboration affects outside audiences.\\n\\n## Materials\\n- [Website](https://kyleepeng.github.io/Fandom-Online-Collaboration/)\\n- [Report](https://raw.githubusercontent.com/dliu9999/artifact-directory-template/main/report.pdf)\\n\\n## Usage\\n\\n- Install dependencies (Navigate to the directory you cloned to)\\n`pip install -r requirements.txt`\\n\\n- Run (test) script:\\n`python run.py test` Runs the (all) script on test data found in `test/testdata`. Plots data and aggregate stats to `data/eda`',\n",
       "  'This summary discusses the presence and influence of fan communities in various industries. It highlights the rise of fan culture, facilitated by technology and social media, which has led to the formation of online fan groups and communities. The summary also mentions a repository containing code to explore collaboration efforts among fans on Twitter and Wikipedia, with a focus on the K-pop fanbase. The code helps quantify the scale, strength, and influence of online fan communities and examines how this collaboration affects outside audiences.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_27': ['# DSC180BProject: Wikipedia’s Response to the COVID-19 Pandemic \\n\\n\\nThis is the Wikipedia project working on its performance on providing COVID-19 pandemic information. Most of our data generated can be seen using certain targets, but \\nthere are also some analysis we made through notebook and we will specified those notebooks in the notebooks seciton.\\n\\n### Project Team Members:\\n- Yiheng Ye, yiy291@ucsd.edu\\n- Gabrielle Avila, ggavila@ucsd.edu\\n- Michael Lam, mel157@ucsd.edu\\n\\n### Requirements:\\n- python 3.8\\n- pandas 1.1.0\\n- wordcloud 1.8.1\\n- wikipedia 1.4.0\\n- sklearn 0.24.1\\n- gensim 3.8.3\\n- nltk 3.5\\n\\n### Code, Purpose, and Guideline:\\n\\n- run.py: If target=\\'data\\': Get top 1000 popular articles relating COVID-19 from Wikipedia. Get the pageview data for them in 2020.\\n          If target=\\'eda\\': Get top10 article with top average daily pageview and plot their daily views\\n          If target=\\'revision\": Get revision history for important pages and doing analysis with LDA model on them.\\n          if target=\\'word\\': Generates word cloud for Wikipedia, JHU, and WHO\\n          If target=\\'test\\': Runs test program about data: getting pageview on the test data and eda, getting revision data and doing LDA model on them, and \\n          generating word cloud.\\n- elt.py: the library for the data pipeline, see the documentation for detailed functions of every function writtened. Basically\\n          these functions are used to fulfill the job done in run.py.\\n- eda.py: the library for doing eda on data.\\n- revision.py: the library for analysis revision data\\n- word.py: the library for creating wordcloud\\n- config/data-params.json: it stores the links of the source data as well as the output path for raw data.\\n- code in src/data: the source code to fulfill the functions about processing data. The current usable files are get_data.py(getting top1000 articles\\'\\n  basic information) and get_apipageview.py(getting pageview from given article information csvs)\\n\\n### Notebooks\\nThe notebook file is primary serving as our original test base for code development. Additionally, it also has a notebook called Project EDA Single Webpage.ipynb which we investigate \"COVID-19 pandemic data\" page deeply.\\n\\nThere is also another notebook called \"Word Clouds.ipynb\" which produces word clouds on Wikipedia Coronavirus page, JHU page, and WHO page.\\n\\nThe \"top_model.ipynb\" generated LDA model for the LDA model on article \\'Coronavirus\", and this model needed to be open in a notebook to get visualization.\\n\\n## Responsibilities:\\n- Yiheng Ye set up the structure of the project and the structure of run.py. He also wrote get_data.py and get_apipageview.py and put them into the etl.py. He also \\n  wrote eda.py and eda_pageview.py\\n- Gabrielle Avila constructed our report and made deep analysis into the \"COVID-19 pandemic data\" page. She also made the \"Word Clouds.ipynb\"\\n- Michael Lam made LDA model analysis on the page \"Coronavirus\" and put them into the notebook \"top_model.ipynb\".',\n",
       "  'This is a Wikipedia project focused on providing information about the COVID-19 pandemic. The project team members are Yiheng Ye, Gabrielle Avila, and Michael Lam. The project requires Python 3.8 and various libraries such as pandas, wordcloud, wikipedia, sklearn, gensim, and nltk. The code includes different functionalities such as retrieving popular articles related to COVID-19, analyzing pageviews, analyzing revision history with LDA model, and generating word clouds. There are also notebooks for testing and specific analyses on COVID-19 data and word clouds. Each team member has specific responsibilities in the project.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_23': ['# DSC 180B Final Pipeline\\n---\\n## How to run\\n\\n```\\nusage: python run.py [-h] [-p] [-s] [-r]\\n\\noptional arguments:\\n  -h, --help       show help message and exit\\n  -p, --pages      obtain list of pages to analyze from Wikipedia\\n  -s, --sentiment  run sentiment analysis on pages from list\\n  -r, --results    obtain stats and visuals from sentiment analysis\\n  -t, --test       runs test suite\\n```\\n\\n## Purpose\\nThis program collects wikipedia data from URLs to analyze the sentiment of articles over time.\\n\\n## Config Formats\\nThe configuration .json files in the config folder can be used to change the program operation\\n### Pages\\n* language:      English, Spanish, or Chinese\\n* targets:       Wikipedia categories from which to analyze articles\\n* skip_cats:     Wikipedia categories to skip due to abundance of unnecessary articles\\n* output:        data file to write list of articles to\\n### Sentiment\\n* language:      English, Spanish, or Chinese\\n* infile:        data file to read in from\\n* outfile:       data file to write to\\n### Results\\n* language:      English, Spanish, or Chinese\\n* infile:        data file to read in from\\n* outfile:       data file to write to\\n### Test - Pages\\n* language:      English, Spanish, or Chinese\\n* targets:       Wikipedia categories from which to analyze articles\\n* skip_cats:     Wikipedia categories to skip due to abundance of unnecessary articles\\n* output:        data file to write list of articles to\\n### Test - Sentiment\\n* language:      English, Spanish, or Chinese\\n* infile:        data file to read in from\\n* outfile:       data file to write to\\n### Test - Results\\n* language:      English, Spanish, or Chinese\\n* infile:        data file to read in from\\n* outfile:       data file to write to\\n\\n---\\nYuanbo Shi\\n\\nHenry Lozada\\n\\nParth Patel\\n\\nEmma Logomasini\\n\\nUCSD Winter 2021\\n',\n",
       "  'This program is a final pipeline for DSC 180B. It collects Wikipedia data from URLs and analyzes the sentiment of articles over time. The program has different configurations for pages, sentiment analysis, and results. There are also test configurations for each category. The authors of this program are Yuanbo Shi, Henry Lozada, Parth Patel, and Emma Logomasini from UCSD Winter 2021.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_24': ['# Politics on Wikipedia\\nThis project is focused on detecting political controversy in online communities. We use a bag-of-words model and a party-embed model, trained on the ideological books corpus (Sim et al, 2013) as well as congressional record data (api.govinfo.gov), and attempt to generalize this to Wikipedia articles, validating it on edit comments which explicitly mention reverting bias.\\n\\n\\n## Usage\\n\\nThis code is intended to be run with the dockerfile vasyasha/pow_docker\\n\\nIt relies on data from the ideological books corpus (Sim et al., 2013) with sub-sentential annotations (Iyyer et al., 2014). To download this data please visit https://people.cs.umass.edu/~miyyer/ibc/index.html where you can send an email to the address in order to obtain the full dataset.\\n\\nOnce obtained, please extract the dataset to **/data/full_ibc/**\\n\\nOnce this is done, please alter the config in **/config/get_ibc_params** accordingly.\\n\\nTo run, in terminal type:\\n```\\npython run.py *target*\\n```\\n\\n## Description of Contents\\n\\n### `run.py`\\n\\nMain driver for running the project. The targets and their functions are:\\n* `scrape_anames` : scrapes political article names\\n* `retrieve_anames` : obtains political articles\\n* `ibc` : downloads test IBC data\\n* `interpret_ibc` : runs partyembed model on IBC data\\n* `revision_xmls` : downloads XML files for nine political Wikipedia articles\\n* `partyembed` : runs Rheault and Cochrane model on current-page Wikipedia articles\\n* `partyembed_time` : runs Rheault and Cochrane model on Wikipedia edit histories\\n* `all` : Runs the whole pipeline.\\n* `test`: Runs the pipeline with pre-loaded test data.\\n\\n### `config/`\\n\\n* `get_ibc_params.json` : Input parameters for running the ibc target.\\n\\n* `interpret_ibc_params.json` : Input parameters for running the interpret_ibc target.\\n\\n### `notebooks/`\\n\\n* `Partyembed+IBC_EDA.ipynb` : Jupyter notebook for the exploratory data analysis on Party_embed and IBC.\\n\\n### `src/`\\n\\n* `libcode.py` : Library code.\\n\\n### `src/etl/`\\n\\n* `bias.py` : Preliminary function for extracting bias from Rheault and Cochrane model.\\n* `get_anames.py` : Scrapes relevant article names.\\n* `get_atexts.py` : Scrapes article contents for the list gathered above.\\n* `get_ibc.py` : Downloads sample IBC data. For the full dataset, please see **Usage** above.\\n* `get_revision_xmls.py` : Downloads xml files using Wikipedia API for our time series analysis\\n\\n### `src/models/`\\n\\n* `difflib_bigrams.py` : Finds text difference between two article states\\n* `get_gns_scores.py` : Assigns scores to the article texts according to Gentzkow, Shapiro, Taddy 2019.\\n* `get_x2_scores.py` : Gets x2 scores based on the formula from Gentzkow and Shapiro 2010 from the IBC.\\n* `gns_histories.py` : Applies Gentzkow and Shapiro 2010 approach on edit histories\\n* `loadIBC.py` : This project uses code from (Sim et al., 2013) and (Iyyer et al., 2014). As this was written in a previous version of python, these updated versions replace downloads made during the building process.\\n* `partyembed_current_pages.py` : Applies partyembed model to get scores for the current pages of political Wikipedia articles\\n* `partyembed_ibc.py` : This file extracts from the partyembed .issue() function the ideological leanings of each word in each sentence of the ideological books corpus. After applying an aggregate function on this data, it writes this to a csv.\\n* `partyembed_revisions.py` : Applies partyembed model on edit histories to find change over time.\\n* `treeUtil.py` : This project uses code from (Sim et al., 2013) and (Iyyer et al., 2014). As this was written in a previous version of python, these updated versions replace downloads made during the building process.\\n\\n\\n## Sources\\n\\nPapers Referenced\\n* https://siepr.stanford.edu/sites/default/files/publications/16-028.pdf\\n\\n* https://www.cs.toronto.edu/~gh/2528/RheaultCochraneOct2018.pdf\\n\\nData\\n* https://people.cs.umass.edu/~miyyer/ibc/index.html\\n\\n* https://data.stanford.edu/congress_text\\n\\n* https://dumps.wikimedia.org\\n\\n',\n",
       "  'This project focuses on detecting political controversy in online communities, specifically on Wikipedia. It uses a bag-of-words model and a party-embed model trained on the ideological books corpus and congressional record data. The code provided can be run with the dockerfile vasyasha/pow_docker. The project includes various targets such as scraping political article names, obtaining political articles, downloading test IBC data, running the partyembed model on different datasets, and more. The code also includes configuration files and notebooks for exploratory data analysis. The sources referenced include papers and data used in the project.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_70': [\"Opioids Overdose Genome Analysis\\n==============================\\n\\n## Project Overview\\nOpioids are now one of the most common causes of accidental death in the US. According to statistics, two out of three drug overdose deaths in 2018 involved an opioid, so opioid abuse can not only affect people physically and mentally but can also deprive their lives (https://docs.google.com/document/d/1JXWb1Bla8iqvyKl3EUxJcGAWBWTvqfO6rRhn1kzrOr4/edit#bookmark=id.p3zzn76i4h3). Opioid addiction has a unique background in that a large reason for why people become addicted is that patients in hospitals are often prescribed opioids to treat pain, however these patients wind up misusing their prescriptions and become addicted.\\nThis is a data science project curated by Cathleen Peña, Zhaoyi Guo, and Dennis Wu. This github repo contains the codes that are essential to conduct the explicit visualization on the raw data gather from NCBI. \\n\\n## Website\\n\\nThe website that introduces the project is under https://genetics.denncc.com/\\n\\n## Running the Project \\n\\n### Pull the Docker Image\\nTo test on the project, in DSMLP, simply pull the image we've generated exclusively for this project by inputting:\\n\\n    $ launch-180.sh -i dencc/opioids-od:dw -G B04_Genetics\\n    \\n### Clone the Repository\\nIn the directory you want to run the project in, run\\n\\n    $ mkdir temp\\n    $ cd ./temp \\n    $ git clone https://github.com/denncc/opioids-od-genome-analysis\\n\\nThen you will be able to run the project the project after cloning\\n\\n### Test the project\\nTo test on the project, simply input\\n\\n    $ python run.py test\\n\\nYou will be able to see the testing procedure to run\\n\\n## Project Organization\\n```\\n📦opioids-od-genome-analysis\\n ┣ 📂config\\n ┃ ┣ 📜data_config.json\\n ┃ ┣ 📜feature_config.json\\n ┃ ┣ 📜model_config.json\\n ┃ ┣ 📜submission.json\\n ┃ ┗ 📜test_config.json\\n ┣ 📂data\\n ┃ ┣ 📂external\\n ┃ ┃ ┣ 📂bam\\n ┃ ┃ ┣ 📜.gitkeep\\n ┃ ┃ ┣ 📜GRCh38_latest_rna.fna\\n ┃ ┃ ┣ 📜Log.out\\n ┃ ┃ ┣ 📜SRA_case_table.csv\\n ┃ ┃ ┣ 📜chrLength.txt\\n ┃ ┃ ┣ 📜chrName.txt\\n ┃ ┃ ┣ 📜chrNameLength.txt\\n ┃ ┃ ┣ 📜chrStart.txt\\n ┃ ┃ ┣ 📜gencode.v24.annotation.gff3\\n ┃ ┃ ┣ 📜gencode.v24.annotation.gtf\\n ┃ ┃ ┣ 📜gencode.v24.annotation_mrna.gff\\n ┃ ┃ ┗ 📜genomeParameters.txt\\n ┃ ┣ 📂interim\\n ┃ ┃ ┣ 📜.gitkeep\\n ┃ ┃ ┣ 📜cts.tsv\\n ┃ ┃ ┗ 📜dds_res_before_filter.csv\\n ┃ ┣ 📂processed\\n ┃ ┃ ┣ 📂duplicates_removed\\n ┃ ┃ ┣ 📂htseq\\n ┃ ┃ ┣ 📂kallisto\\n ┃ ┃ ┣ 📂merged\\n ┃ ┃ ┣ 📂sorted\\n ┃ ┃ ┣ 📂temp\\n ┃ ┃ ┣ 📜.gitkeep\\n ┃ ┃ ┣ 📜htseq_cts.csv\\n ┃ ┃ ┣ 📜htseq_cts_1.csv\\n ┃ ┃ ┣ 📜htseq_cts_gene.csv\\n ┃ ┃ ┣ 📜htseq_cts_gene_filtered.csv\\n ┃ ┃ ┣ 📜kallisto_transcripts.idx\\n ┃ ┃ ┗ 📜test_gene_counts.csv\\n ┃ ┣ 📂raw\\n ┃ ┣ 📂test\\n ┃ ┃ ┣ 📂SRR7949794\\n ┃ ┃ ┃ ┣ 📜abundance.h5\\n ┃ ┃ ┃ ┣ 📜abundance.tsv\\n ┃ ┃ ┃ ┣ 📜pseudoalignments.bam\\n ┃ ┃ ┃ ┗ 📜run_info.json\\n ┃ ┃ ┣ 📜SRR7949794_1.fastq.gz\\n ┃ ┃ ┗ 📜SRR7949794_2.fastq.gz\\n ┃ ┗ 📜SRA_case_table.csv\\n ┣ 📂docs\\n ┃ ┣ 📜Makefile\\n ┃ ┣ 📜commands.rst\\n ┃ ┣ 📜conf.py\\n ┃ ┣ 📜getting-started.rst\\n ┃ ┣ 📜index.rst\\n ┃ ┗ 📜make.bat\\n ┣ 📂models\\n ┃ ┗ 📜.gitkeep\\n ┣ 📂notebooks\\n ┃ ┣ 📜.gitkeep\\n ┃ ┣ 📜EDA_python.ipynb\\n ┃ ┣ 📜EDA_r.ipynb\\n ┃ ┣ 📜HTSeq.ipynb\\n ┃ ┣ 📜SRA_eda.ipynb\\n ┃ ┗ 📜htseq_cts.py\\n ┣ 📂references\\n ┃ ┗ 📜.gitkeep\\n ┣ 📂reports\\n ┃ ┣ 📂figures\\n ┃ ┃ ┣ 📜.gitkeep\\n ┃ ┃ ┣ 📜Dist_of_Age.pdf\\n ┃ ┃ ┣ 📜Scatterplot_Matrix_All.pdf\\n ┃ ┃ ┣ 📜Scatterplot_Matrix_Users.pdf\\n ┃ ┃ ┣ 📜cocaine_use_diff_means.pdf\\n ┃ ┃ ┣ 📜cocaine_use_means.pdf\\n ┃ ┃ ┣ 📜diff_group_means.pdf\\n ┃ ┃ ┣ 📜drug_use_pie.pdf\\n ┃ ┃ ┣ 📜group_means.pdf\\n ┃ ┃ ┗ 📜race_pie.pdf\\n ┃ ┗ 📜.gitkeep\\n ┣ 📂src\\n ┃ ┣ 📂__pycache__\\n ┃ ┃ ┣ 📜__init__.cpython-36.pyc\\n ┃ ┃ ┗ 📜__init__.cpython-37.pyc\\n ┃ ┣ 📂data\\n ┃ ┃ ┣ 📂__pycache__\\n ┃ ┃ ┃ ┣ 📜__init__.cpython-36.pyc\\n ┃ ┃ ┃ ┣ 📜__init__.cpython-37.pyc\\n ┃ ┃ ┃ ┣ 📜import_data.cpython-36.pyc\\n ┃ ┃ ┃ ┗ 📜import_data.cpython-37.pyc\\n ┃ ┃ ┣ 📜.gitkeep\\n ┃ ┃ ┣ 📜__init__.py\\n ┃ ┃ ┣ 📜__init__.pyc\\n ┃ ┃ ┣ 📜import_data.py\\n ┃ ┃ ┗ 📜make_dataset.py\\n ┃ ┣ 📂features\\n ┃ ┃ ┣ 📂__pycache__\\n ┃ ┃ ┃ ┣ 📜__init__.cpython-36.pyc\\n ┃ ┃ ┃ ┣ 📜__init__.cpython-37.pyc\\n ┃ ┃ ┃ ┣ 📜build_features.cpython-36.pyc\\n ┃ ┃ ┃ ┗ 📜build_features.cpython-37.pyc\\n ┃ ┃ ┣ 📂r_scripts\\n ┃ ┃ ┃ ┗ 📜main.R\\n ┃ ┃ ┣ 📜.gitkeep\\n ┃ ┃ ┣ 📜__init__.py\\n ┃ ┃ ┗ 📜build_features.py\\n ┃ ┣ 📂models\\n ┃ ┃ ┣ 📂__pycache__\\n ┃ ┃ ┃ ┣ 📜__init__.cpython-37.pyc\\n ┃ ┃ ┃ ┣ 📜build_model.cpython-37.pyc\\n ┃ ┃ ┃ ┗ 📜htseq_cts.cpython-37.pyc\\n ┃ ┃ ┣ 📂r_scripts\\n ┃ ┃ ┃ ┣ 📜deseq2.R\\n ┃ ┃ ┃ ┣ 📜visualization.R\\n ┃ ┃ ┃ ┗ 📜wgcna.R\\n ┃ ┃ ┣ 📂sh_scripts\\n ┃ ┃ ┃ ┗ 📜samtools.sh\\n ┃ ┃ ┣ 📜.Rhistory\\n ┃ ┃ ┣ 📜.gitkeep\\n ┃ ┃ ┣ 📜__init__.py\\n ┃ ┃ ┣ 📜build_model.py\\n ┃ ┃ ┗ 📜htseq_cts.py\\n ┃ ┣ 📂visualization\\n ┃ ┃ ┣ 📜.gitkeep\\n ┃ ┃ ┣ 📜__init__.py\\n ┃ ┃ ┗ 📜visualize.py\\n ┃ ┣ 📜__init__.py\\n ┃ ┗ 📜__init__.pyc\\n ┣ 📜.gitignore\\n ┣ 📜Dockerfile\\n ┣ 📜LICENSE\\n ┣ 📜Makefile\\n ┣ 📜README.md\\n ┣ 📜command-line-htseq.txt\\n ┣ 📜r-bio.yaml\\n ┣ 📜requirements.txt\\n ┣ 📜run.py\\n ┣ 📜setup.py\\n ┣ 📜test_environment.py\\n ┗ 📜tox.ini\\n```\\n\",\n",
       "  'This project focuses on analyzing the genome of opioid overdose cases. Opioid addiction is a major problem in the US, leading to many accidental deaths. The project aims to visualize and analyze data gathered from the National Center for Biotechnology Information (NCBI). The project can be run using a Docker image, and the code is available on GitHub. The website for the project is https://genetics.denncc.com/.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_68': ['# antibiotic-resistance\\n Identifying the genetic basis of antibiotic resistance in E. Coli\\n',\n",
       "  'This article discusses the genetic basis of antibiotic resistance in E. Coli.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_66': ['# RNASeqToolComparison\\nData Science Senior Capstone Project: Comparing RNA Sequencing Differential Gene Expression Analysis Tools\\n\\nIn this project, we want to compare distinct differential gene expression analysis tools on simulated data created with different numbers of genes differentially expressed.\\n\\n## Running the project\\n* Use the command `launch.sh -i buijoseph21/rna-seq-tool-comparison:v1 -m 6 -P Always` in order to have the necessary software from `compcodeR` (e.g., `generateSyntheticData`, `runDiffExp`, `ABSSeq`, `PoissonSeq`, etc.) to generate the synthetic data & perform differential gene expression analysis. The `-m 6` specifies the number of RAM which is needed to run tools that require more memory. \\n\\n## Building the project using `run.py`\\n* Use the command `python run.py build` to generate the synthetic data in `data/data<N>.rds`, where N represents the dataset number, using `generateSyntheticData`\\n* Use the command `python run.py analysis` to perform `DESeq2`, `edgeR.exact`, `NOISeq`, `PoissonSeq`, `ttest`, `ABSSeq`, and `voom.limma` on the synthetic data created in `data` folder which returns the results in `out/data<N>_<tool>.rds`, where N represents the dataset number & tool represents the software. The output of each tool will be organized in its respective `<tool_name><synthetic_data_num>` folders in the `<tool_name>` folders. \\n* Use the command `python run.py graph` to build area under the curves (AUC), type I error rates, accuracy, sensitivity, specificity, and False Discovery Rates (FDR) graphs to compare how well the tools performed with each other. The output of each of the graphs will be stored in `rna_graphs`.\\n* Use the command `python run.py real` to run the whole pipeline mentioned above on the real life dataset: Post-Mortem Molecular Profiling of Schizophrenia, Bipolar Disorder, and Major Depressive Disorder (https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-017-0458-5). \\n\\n## Running the project on test data\\n* `ssh` into dsmlp and `git clone` the repository\\n* Use the command `launch.sh -i buijoseph21/rna-seq-tool-comparison:v1 -m 6` in order to have the necessary software from `compcodeR` (e.g., `generateSyntheticData`, `runDiffExp`, `PoissonSeq`, etc.) to generate the test synthetic data & perform differential gene expression analysis. The `-m 6` specifies the number of RAM which is needed to run tools that require more memory. \\n* Use the command `python run.py test` to create a test synthetic dataset of 100 genes differentially expressed where 50 are differentially expressed in condition 1 and 50 are differentially expressed in condition 2. This test dataset contains 5 samples per condition and is a baseline model with no outliers containing abnormal counts. When running `python run.py test`, it should first create a synthetic dataset which will be stored in the `data` folder and named as `test.rds`. Next, the 7 tools will be performed on the `test.rds` where the outputs will be stored in the `out/test` folder which is created when ran. Each tool will produce different results which is stored in `test_<tool>.rds`. For the next step of our pipeline, we want to produce the area under the curve for this test data, which will be stored in `/out/test/test_auc_plot.png` & the summary of all the metrics in `statistics.csv`.\\n\\n## Group Contributions\\n* Joseph built the dockerfile/container. He also created the starter code for building the synthetic datasets and performing differential expression analysis tools in `compcodeR` such as `DESeq2`, `edgeR`, `NOISeq`, `voom.limma`, and `ttest`. He was able to create an R script that read all the outputs from each tool mentioned previously to write out to a `num_expressed_by_tool.csv`. He created the notebook that illustrates the timings/duration for each of the tools performed on each synthetic dataset. As for the report, he helped write the Abstract, Background, Dataset (Figure 1), Methods (Creating the synthetic data, DESeq2, NOISeq, edgeR.exact), and briefly explained Figure 3. He also looked over other sections to add onto or revise. \\n* Brandon was responsible for creating the `random` outlier synthetic datasets and performing differential expression analysis tools not built-in `compcodeR`, including `ABSSeq` and `PoissonSeq`. Brandon helped write out to the `num_expressed_by_tool.csv` for his outputs produced by `ABSSeq` and `PoissonSeq`. As for the report, he helped write the Dataset and analysis of Figure 3. He also looked over other sections to add onto or revise. Brandon mainly focused on creating the graphs for the comparisons. \\n* Luigi was responsible for creating the `single` outlier and `poisson` synthetic datasets and helped perform `NOISeq` and `voom.limma` on the synthetic datasets. As for the report, Luigi created the citations, made revisions based on comments suggested by Shannon, and briefly described Figure 2. He wrote the Methods section for ABSSeq and PoissonSeq. Luigi was mainly responsible for the creation of the website and running/creating the pipeline for the real life dataset. \\n',\n",
       "  'This project compares different RNA sequencing differential gene expression analysis tools on simulated data with varying numbers of differentially expressed genes. The project includes instructions for running the project, building the project using `run.py`, running the project on test data, and group contributions.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_69': ['# Title: Genetic Overlap between Alzheimer\\'s, Parkinson’s, and healthy patients\\n\\n#### Capstone Project: Data Science DSC180B\\n\\n#### Section B04: Genetics\\n\\n#### Authors: Saroop Samra, Justin Lu, Xuanyu Wu\\n\\n#### Date : 2/2/2021\\n\\n### Overview\\n\\nThis repository code is for the replication project for the paper: Profiles of Extracellular miRNA in Cerebrospinal Fluid and Serum from Patients with Alzheimer’s and Parkinson’s Diseases Correlate with Disease Status and Features of Pathology (https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0094839). The data includes that miRNA sequences from tissues from two biofluids (serum and cerebrospinal fluid), and is from 69 patients with Alzheimer\\'s disease, 67 with Parkinson\\'s disease and 78 neurologically normal controls using next generation small RNA sequencing (NGS).\\n\\n\\n### Running the project\\n\\n•\\tTo install the dependencies, run the following command from the root directory of the project:\\xa0\\n\\n    pip install -r requirements.txt\\n\\n\\n### target: data\\n•\\tTo process the data, from the root project directory run the command:\\n\\n    python3 run.py data\\n\\n•   The data pipeline step takes the .fastq compressed files as input and then applies two transformations: process and align\\n\\n•\\tThis pipeline step also uses an additional CSV file that is the SRA run database, a sample looks like as follows:\\n\\n    Run expired_age    CONDITION    BIOFLUID     \\n    SRR1568567  40  Parkinson\\'s Disease Cerebrospinal \\n\\n\\n\\n•   The configuration files for the data step are stored in config/data-params.json. These include the parameters for the tools as well as the directories used for storing the raw, temporary and output files.\\n\\n    \"raw_data_directory\": \"./data/raw\",\\n    \"tmp_data_directory\": \"./data/tmp\",\\n    \"out_data_directory\": \"./data/out\",\\n\\n•   The configuration also includes an attribute to the SRA run input database (described above), and an attribute of where to store that in the data folder. Additional filter attributes are included for ease of use to avoid processing all patients, if this filter_enable is set it will only process a subset of SRA rows (filter_start_row to filter_start_row + filter_num_rows).\\n\\n    \"sra_runs\" : {\\n        \"input_database\" : \"/datasets/SRP046292/exRNA_Atlas_CORE_Results.csv\",\\n        \"input_database2\" : \"/datasets/SRP046292/SraRunTable.csv\",\\n        \"input_database3\" : \"/datasets/SRP046292/Table_S1.csv\",\\n        \"output_database\" : \"data/raw/exRNA_Atlas_CORE_Results.csv\",\\n        \"filter_enable\" : 0,\\n        \"filter_start_row\" : 120,\\n        \"filter_num_rows\" : 10   \\n    },\\n    \\n\\n•\\tAn optional transformation of the data is \"process\" that uses the following data configuration below that will invoke cutadapt which finds and removes adapter sequences. The attributes include the adapters (r1 and r2) to identify the start and end of pairs are a JSON array. The attribute enable allows to disable this cleaning step, instead it will simply copy the paired files from the source dataset. The arguments attribute allows flexible setting of any additional attribute to the cutadapt process. Finally, we have two wildcard paths that indicate the location of the SRA fastq pair files (fastq1 and fastq2).\\n\\n    \"process\" : {\\n        \"enable\" : 1,\\n        \"tool\" : \"/opt/conda/bin/cutadapt\",\\n        \"r1_adapters\" : [\"AAAAA\", \"GGGG\"],\\n        \"r2_adapters\" : [\"CCCCC\", \"TTTT\"],\\n        \"arguments\" : \"--pair-adapters --cores=4\",\\n        \"fastq1_path\" : \"/datasets/srp073813/%run_1.fastq.gz\", \\n        \"fastq2_path\" : \"/datasets/srp073813/%run_2.fastq.gz\"\\n    },\\n    \\n•   The second transformation of the data is \"aligncount\" that can be set to either use download, STAR or Kallisto. The choice is controlled by the aligncount attribute:\\n\\n    \"aligncount\" : \"download\",\\n\\n•   download step will use the ftp location of the gzip file in the Sra table and download using the curl command and unzips and the extracts the readCounts_gencode_sense.txt which represents thae gene counts for the sample. \\n\\n    \"download\" : {\\n        \"enable\" : 1,\\n        \"tool\" : \"curl\",\\n        \"arguments\" : \"-L -R\",\\n        \"read_counts_file\" : \"readCounts_gencode_sense.txt\"\\n    },\\n\\n•   kallisto uses the index_file attribute is the location of the directory of the reference genome, which for this replication project was GRCh37_E75. The arguments attribute allows flexible setting of any additional attribute to the kallisto process. Including the bootstaro samples.The attribute enable allows to disable this alignment step, this is useful for debugging the process prior step, for example, you can run quality checks on the processed fastq files before proceeding to alignment. \\n\\n    \"kallisto\" : {\\n        \"enable\" : 1,\\n        \"tool\" : \"/opt/kallisto_linux-v0.42.4/kallisto\",\\n        \"index_file\" : \"/datasets/srp073813/reference/kallisto_transcripts.idx\",\\n        \"arguments\" : \"quant -b 8 -t 8\"\\n    },\\n\\n•   STAR uses the gene_path attribute is the location of the directory of the reference genome, which for this replication project was GRCh37_E75 as described in the reference_gene attribute. The arguments attribute allows flexible setting of any additional attribute to the STAR process. Including TranscriptomeSAM in the quantMode arguments will also output bam files. Additionally, the log file gets outputted which has PRUA (percentage of reads uniquely aligned). The attribute enable allows to disable this alignment step, this is useful for debugging the process prior step, for example, you can run quality checks on the processed fastq files before proceeding to alignment. \\n\\n    \"STAR\" : {\\n        \"enable\" : 1,\\n        \"tool\" : \"/opt/STAR-2.5.2b/bin/Linux_x86_64_static/STAR\",\\n        \"reference_gene\" : \"GRCh37_E75\",\\n        \"gene_path\" : \"/path/to/genomeDir\",\\n        \"arguments\" : \"--runMode alignReads --quantMode GeneCounts --genomeLoad LoadAndKeep --readFilesCommand zcat --runThreadN 8\"\\n    },\\n\\n\\n\\n\\n•   The process and align transformation work on each of the samples. After each sample iteration, the temporary fastq files will be deleted to reduce storage requirements.\\n\\n\\n•   Example processing:\\n\\n    python3 run.py data\\n\\n    # ---------------------------------------------------\\n    # Process\\n    # ---------------------------------------------------\\n    # ---------------------------------------------------\\n    # Starting sample # 1 out of 1\\n    # ---------------------------------------------------\\n    # Starting sample # 1 out of 343\\n    curl-proxy -L -R -o ./data/tmp/SRR1568613.tgz ftp://ftp.genboree.org/exRNA-atlas/grp/Extracellular%20RNA%20Atlas/db/exRNA%20Repository%20-%20hg19/file/exRNA-atlas/exceRptPipeline_v4.6.2/KJENS1-Alzheimers_Parkinsons-2016-10-17/sample_SAMPLE_1022_CONTROL_SER_fastq/CORE_RESULTS/sample_SAMPLE_1022_CONTROL_SER_fastq_KJENS1-Alzheimers_Parkinsons-2016-10-17_CORE_RESULTS_v4.6.2.tgz\\n    sh: curl-proxy: command not found\\n    mkdir ./data/tmp/SRR1568613\\n    tar -C ./data/tmp/SRR1568613 -xzf ./data/tmp/SRR1568613.tgz\\n    cp ./data/tmp/SRR1568613/data/readCounts_gencode_sense.txt ./data/tmp/SRR1568613_ReadsPerGene.out.tab\\n    # ---------------------------------------------------\\n    # Starting sample # 2 out of 343\\n    curl-proxy -L -R -o ./data/tmp/SRR1568457.tgz ftp://ftp.genboree.org/exRNA-atlas/grp/Extracellular%20RNA%20Atlas/db/exRNA%20Repository%20-%20hg19/file/exRNA-atlas/exceRptPipeline_v4.6.2/KJENS1-Alzheimers_Parkinsons-2016-10-17/sample_SAMPLE_0427_PD_CSF_fastq/CORE_RESULTS/sample_SAMPLE_0427_PD_CSF_fastq_KJENS1-Alzheimers_Parkinsons-2016-10-17_CORE_RESULTS_v4.6.2.tgz\\n    sh: curl-proxy: command not found\\n    mkdir ./data/tmp/SRR1568457\\n    tar -C ./data/tmp/SRR1568457 -xzf ./data/tmp/SRR1568457.tgz\\n    cp ./data/tmp/SRR1568457/data/readCounts_gencode_sense.txt ./data/tmp/SRR1568457_ReadsPerGene.out.tab\\n    # ---------------------------------------------------\\n\\n\\n### target: merge\\n•   To merge gene count and/or BAM files generated from the data target, from the root project directory run the command:\\n\\n    python3 run.py merge\\n\\n•   The configuration files for the data step are stored in config/count-params.json. These include the parameters for the count merge and bam merge and it\\'s associated arguments.\\n\\n•   The format attrbute informs if to process downlload, kallisto (or STAR) files. The gene counts are merged into a TSV file and as well as a feature table based on the SRA run table. Additional STAR attributes in the JSON allow you to specify skiprows used when processing the  gene count files as well as identifying the column from the  gene matrix file to use as the column used to. There is an additional imputes attribute that allows you to impute any column with missing data. The attributes also include an optional \"filter_names\" gene table used to remove genes as well as removing false-positive genes. Finally, we can rename the feature columns before we save out the feature table.\\n\\n    \"count\" : {\\n        \"enable\" : 1,\\n        \"format\" : \"download\",\\n        \"skiprows\" : 4,\\n        \"column_count\" : 1,\\n        \"skip_samples\" : [\"SRR1568391\"],\\n        \"enable_filter\" : 0,\\n        \"filter_keep_genes\" : \"NM_\",\\n        \"filter_remove_genes\" : [\"chrX\", \"chrY\"],\\n        \"filter_names\" : \"/datasets/srp073813/reference/Gene_Naming.csv\",\\n        \"run_database\" : \"data/raw/exRNA_Atlas_CORE_Results.csv\",\\n        \"imputes\" : [\"TangleTotal\"],\\n        \"features\" : [\"Run\", \"CONDITION\", \"expired_age\", \"BIOFLUID\", \"sex\", \"PMI\", \"sn_depigmentation\", \"Braak score\", \"TangleTotal\", \"Plaque density\", \"PlaqueTotal\"],\\n        \"rename\" : {\"CONDITION\" : \"Disorder\", \"BIOFLUID\" : \"Biofluid\", \"Braak score\" : \"Braak_Score\", \"Plaque density\" : \"Plaque_density\"},\\n        \"replace\" : {\"from\":[\"Parkinson\\'s Disease\", \"Alzheimer\\'s Disease\", \"Cerebrospinal fluid\", \"Healthy Control\"], \"to\":[\"Parkinson\", \"Alzheimer\", \"Cerebrospinal\", \"Control\"]},\\n        \"output_matrix\" : \"data/out/gene_matrix.tsv\",\\n        \"output_features\" : \"data/out/features.tsv\"\\n    },\\n\\n•   For bam merging, which should not be enabled by default, we use the \"samtools\" merge feature that takes all the BAM files and combine them into one merged BAM file. \\n\\n\\n    \"bam\" : {\\n        \"enable\" : 0,\\n        \"output\" : \"data/tmp/merged.bam\",\\n        \"tool\" : \"/usr/local/bin/samtools\",\\n        \"arguments\" : \"merge --threads 8\"\\n    },\\n\\n\\n•   Example processing:\\n\\n    python3 run.py merge\\n\\n    # ---------------------------------------------------\\n    # Merge\\n    Input: SRR3438605_ReadsPerGene.out.tab\\n    Input: SRR3438604_ReadsPerGene.out.tab\\n    Output: data/out/gene_matrix.tsv data/out/features.tsv\\n    # Finished\\n    # ---------------------------------------------------\\n\\n\\n\\n### target: normalize\\n•   To normalize the aligned merge counts, from the root project directory run the command:\\n\\n    python3 run.py normalize\\n\\n•   The configuration files for the data step are stored in config/normalize-params.json. \\n\\n•   We use a custom R script which uses the DESeq2 module to take the input merged gene counts and the experiment features and outputs two normalized counts files. The analysis is done for all samples in the SRA run table. The output_dir sets the output location for the normalized count matrix files. One file is the standard normalized counts using the DESeq2 module, and the second normalized count file is after a Variable Stablization Transform (LRT). We also have a \"max_genes\" attribute that will filter the genes and removes ones that have little to no variance across disorder vesus control.\\n\\n•   The data JSON configuration file also holds an array of samples, a sample looks like as follows:\\n    \\n    {\\n        \"output_dir\" : \"data/out\",\\n        \"DESeq2\" : {\\n            \"Rscript\" : \"/opt/conda/envs/r-bio/bin/Rscript\",\\n            \"source\" : \"src/data/normalize.r\",\\n            \"input_counts\" : \"data/out/gene_matrix.tsv\",\\n            \"input_features\" : \"data/out/features.tsv\",\\n            \"max_genes\" : 8000\\n        },\\n        \"cleanup\" : 0,\\n        \"verbose\": 1\\n    }\\n\\n\\n•   Example processing:\\n\\n    python3 run.py normalize\\n\\n    # ---------------------------------------------------\\n    # Normalize\\n    Rscript  src/data/normalize.r data/out/gene_matrix.tsv data/out/features.tsv data/out/\\n    [1] \"Output data/out/normalized_counts.tsv data/out/vst_transformed_counts.tsv\"\\n    # Finished\\n    # ---------------------------------------------------\\n\\n\\n### target: analysis\\n•   To perform the analysis for the gene counts, from the root project directory run the command:\\n\\n    python3 run.py analysis\\n\\n•   The configuration files for the data step are stored in config/analysis-params.json. \\n\\n•   We use a custom R script which uses the DESeq2 module to take the input merged gene counts and the experiment features and outputs 2 sets of files for each biofluid region. Each biofluid region will compare a disorder versus Control. This will result in a total of 4 sets of files (2 biofluid regions x 2 disorder pair comparisons). Each output set includes a Likelihood Ratio Test (LRT) using the full and reduced model as specified in the attributes below as well as a MA-Plot and Heatmap. The additional attributes include the property of doing parallel processing for DESeq2.\\n    \\n    {\\n        \"output_prefix\" : \"data/out/%biofluid_region%\",\\n        \"DESeq2\" : {\\n            \"Rscript\" : \"/opt/conda/envs/r-bio/bin/Rscript\",\\n            \"biofluid_regions\" : [\"Cerebrospinal\", \"Serum\"],\\n            \"disorders\" : [\"Parkinson\", \"Alzheimer\"],\\n            \"control\" : \"Control\",\\n            \"input_counts\" : \"data/out/pca_normalized_counts.tsv\",\\n            \"input_features\" : \"data/out/features.tsv\",\\n            \"source\" : \"src/analysis/analysis.r\",\\n            \"full\" : \"expired_age+sex+PMI+sn_depigmentation+Braak_Score+TangleTotal+Plaque_density+PlaqueTotal+Disorder\",\\n            \"reduced\" : \"expired_age+sex+PMI+sn_depigmentation+Braak_Score+TangleTotal+Plaque_density+PlaqueTotal\",\\n            \"parallel\" : 0\\n        },\\n        \"cleanup\" : 0,\\n        \"verbose\": 1\\n    }\\n\\n\\n•   Example processing:\\n\\n    python3 run.py analysis\\n\\n    # ---------------------------------------------------\\n    # Analysis\\n    Cerebrospinal x Parkinson vs Control\\n    Rscript src/analysis/analysis.r data/out/Cerebrospinal/Parkinson/gene_matrix.tsv data/out/Cerebrospinal/Parkinson/features.tsv data/out/Cerebrospinal/Parkinson/ full=expired_age+sex+PMI+sn_depigmentation+Braak_Score+TangleTotal+Plaque_density+PlaqueTotal+Disorder reduced=expired_age+sex+PMI+sn_depigmentation+Braak_Score+TangleTotal+Plaque_density+PlaqueTotal charts=1 parallel=0\\n\\n\\n### target: visualize\\n\\n•   The visualize pipeline step can be invoked as follows:\\n\\n    python3 run.py visualize\\n\\n•   The configuration files for the data step are stored in config/visualize-params.json. The output will include multiple sets of charts: Gene Spread Variance Histogram, SRA Linear Correlation between SRA chart, MA-Plot 2x2 chart, Heat Map 2x2 chart, 2x2 Histogram, 4x4 Correlation Matrix and a Disorder Venn Diagram. Each chart type has flexible settings to control the input and layout for the charts as shown below:\\n\\n    \"gene_hist\" : {\\n        \"enable\" : 1,\\n        \"max_genes\" : 8000,\\n        \"nbins\" : 100,\\n        \"title\" : \"Distribution of Genes Based on Spread Metric: All vs Top Genes\"\\n    },\\n    \"missing_plot\" : {\\n        \"enable\" : 1,\\n        \"title\" : \"Percentage of Missing Genes over\"\\n    },\\n    \"sra_lm\" : {\\n        \"enable\" : 1,\\n        \"sra\" : [\"SRR1568567\", \"SRR1568584\"],\\n        \"normalized_counts\" : \"data/out/normalized_counts.tsv\",\\n        \"vst_counts\" : \"data/out/vst_transformed_counts.tsv\",\\n        \"title\" : \"%sra% Regression Log(Norm) v VST counts\"\\n    },\\n    \"ma_plot\" : {\\n        \"enable\" : 1,\\n        \"biofluid_regions\" : [\"Cerebrospinal\", \"Serum\"],\\n        \"disorders\" : [\"Parkinson\", \"Alzheimer\"],\\n        \"src_image\" : \"MAplot.png\",\\n        \"title\" : \"MA Plot: Biofluid Region vs Disorder\"\\n    },\\n    \"heat_map\" : {\\n        \"enable\" : 1,\\n        \"biofluid_regions\" : [\"Cerebrospinal\", \"Serum\"],\\n        \"disorders\" : [\"Parkinson\", \"Alzheimer\"],\\n        \"src_image\" : \"heatmap.png\",\\n        \"title\" : \"Heat Map: Biofluid Region vs Disorder\"\\n    },\\n    \"histogram\" : {\\n        \"enable\" : 1,\\n        \"biofluid_regions\" : [\"Cerebrospinal\", \"Serum\"],\\n        \"disorders\" : [\"Parkinson\", \"Alzheimer\"],\\n        \"title\" : \"Histograms Differential Gene Expression vs Control\",\\n        \"ylim\" : 55\\n    },\\n    \"corrmatrix\" : {\\n        \"enable\" : 1,\\n        \"title\" : \"Spearman Correlations of log2 fold gene expression\"\\n    },\\n    \"venn\" : {\\n        \"enable\" : 1,\\n        \"biofluid_regions\" : [\"Cerebrospinal\", \"Serum\"],\\n        \"disorders\" : [\"Parkinson\", \"Alzheimer\"],\\n        \"pvalue_cutoff\" : 0.05,\\n        \"title\" : \"Venn Diagram Disorders\"\\n    },\\n\\n\\n•   Example processing:\\n\\n    python3 run.py visualize\\n\\n    # ---------------------------------------------------\\n    # Visualize\\n    # Finished\\n    # ---------------------------------------------------\\n\\n\\n### target: qc\\n\\n•   The quality pipeline step can be invoked as follows:\\n\\n    python3 run.py qc\\n\\n•   The configuration files for the data step are stored in config/qc-params.json. These include the parameters for the output directory where the quality HTML reports will be outputted. \\n\\n    \"outdir\" : \"data/out\",\\n    \"inputs\" : \"data/tmp\",\\n\\n•   For fastq files, the quality tool attribute is set to fastqc and that includes attributes to extract reports or keep them in a zip file. To enable this quality check make sure you set the cleanup to 0 in the data configuration pipeline as well as to disable the STAR processing, this will retain the fastq.qz files after the data pipeline step is executed.\\n\\n    \"fastq\" : {\\n        \"enable\" : 1,\\n        \"tool\" : \"/opt/FastQC/fastqc\",\\n        \"extract\" : 1   \\n    },\\n\\n•   For bam files, the quality tool attribute is set to picard and that includes attributes such as collecting alignment summary metrics. To enable this quality check make sure you set the cleanup to 0 in the data configuration pipeline and add \\'TranscriptomeSAM\\' to the arguments for STAR which will then output BAM files that will be retained after the data pipeline step is executed.\\n\\n    \"bam\" : {\\n        \"enable\" : 1,\\n        \"tool\" : \"java\",\\n        \"jar\" : \"/opt/picard-tools-1.88/CollectAlignmentSummaryMetrics.jar\"\\n    },\\n    \\n\\n•   Example processing:\\n\\n    python3 run.py qc\\n\\n    # ---------------------------------------------------\\n    # Quality Check\\n    fastqc data/tmp/out.1.fastq.gz --outdir=data/out --extract\\n    fastqc data/tmp/out.2.fastq.gz --outdir=data/out --extract\\n    java -jar /opt/picard-tools-1.88/CollectAlignmentSummaryMetrics.jar INPUT=data/tmp/SRR3438604_Aligned.bam OUTPUT=data/out/SRR3438604_Aligned.bam.txt\\n    java -jar /opt/picard-tools-1.88/CollectAlignmentSummaryMetrics.jar INPUT=data/tmp/SRR3438605_Aligned.bam OUTPUT=data/out/SRR3438605_Aligned.bam.txt\\n    # Finished\\n    # ---------------------------------------------------\\n\\n\\n### target: report\\n•   To generate the report from the notebook, run this command:\\n\\n    python3 run.py report\\n\\n•   The configuration files for the data step are stored in config/report-params.json. \\n\\n    {\\n        \"tool\": \"jupyter\",\\n        \"args\": \"nbconvert --no-input --to html --output report.html notebooks/report.ipynb\",\\n        \"verbose\" : 1\\n    }\\n\\n\\n### target: clean \\n\\n•\\tTo clean the data (remove it from the working project), from the root project directory run the command:\\n\\npython3 run.py clean\\n\\n\\n### target: all \\n\\n•   The all target will execute the following steps in sequence: data, merge, normalize, analysis and visualize. It can be executed as follows:\\n\\npython3 run.py all\\n\\n\\n### Future Work\\n\\n•\\tNew pipeline step: predict. This step will use the model to predict the classification for a given miRNA sequences on the test data and reporting the classification errors\\n\\n\\n\\n### Major Change History\\n\\n\\nDate:  2/2/2021\\n\\nWork completed:\\n\\n- Created new visualization, Volcano Plot, wrote the code and implemented it into our pipeline in the visualize step \\n- Updated the code pipeline to make the correlation matrix more meaningful by adding color\\n- Finished descriptions for EDA plots\\n\\n\\nDate:  1/19/2021\\n\\nWork completed:\\n\\n- Got all steps in pipeline to work with new data (data, merge, normalize, analysis, visualize) \\n- Used LRT Hypothesis Testing and have updated all previous quarter visualizations to work for our new data set\\n- Compared the outputs of 2 samples that failed FastQC/ERCC quality check with 2 samples that passed\\n- Developed and organized EDA code for gene matrix (missingness, correlation between sequence count and numerical features of the samples) \\n\\n\\nDate:  1/12/2020\\n\\nWork completed:\\n\\n- Created repo, initial version from the DSC180A Genetics project\\n- Added new download step and modified data target to use new SRA\\n- Wrote out background information/introduction sections of the report, researched our diseases (Alzheimer’s/Parkinson’s) and data sources (miRNA, serum/CSF)\\n- Developed and organized EDA code for features in the SRA run table (box plots, histograms, bar plots, etc)\\n\\n\\n\\n### Responsibilities\\n\\n\\n* Saroop Samra, developed the original codebase based on the DSC180A genetics replication project. Saroop ported the code to support the new miRNA dataset including adding a new download step in the data target. She worked on modifying the code and configuration files for the merge, normalize, analysis and visualize targets to process and generate the visualizations from the DSC180A project. She got the new Volcano Plot to work for our dataset and wrote basic descriptions for the visualizations including what significant patterns exists (MA plot, heatmap, histogram, venn diagram, correlation matrix).\\n\\n* Justin Lu, wrote out background information/introduction sections of the report. Justin did the data quality control check with FastQC (focused on the FastQC report outputs that we acquired instead of actually running FastQC since we still do not have access to the raw .fastq data), wrote in descriptions for visualizations and some of the EDA in our final report notebook. He updated the code pipeline with colored correlation matrix.\\n\\n* Xuanyu Wu, generated around 20 EDA plots for features that describe our merged dataset (incl. box plots, histograms, bar plots, etc) Xuanyu created EDA plots to explore the missingness of the gene count matrix and the basic correlation of each sequence with the numerical features we have selected. She also finished the descriptions for EDA plots and analysis.\\n\\n\\n\\n\\n',\n",
       "  \"This repository code is for a replication project that focuses on the genetic overlap between Alzheimer's disease, Parkinson's disease, and healthy patients. The data includes miRNA sequences from tissues from two biofluids (serum and cerebrospinal fluid) and is obtained using next-generation small RNA sequencing. The project includes several pipeline steps such as data processing, merging gene counts, normalization, analysis, visualization, quality control check, and report generation. The code and configuration files are provided for each step of the pipeline. The project also includes future work plans and a change history section detailing the progress made by the authors. The responsibilities of each author are also mentioned.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_9': ['# Live_vs_Vod_Project\\n## Group Name: Live\\n\\nDue to the variety, affordability and convenience of online video streaming, there are more subscribers than ever to video streaming platforms. Moreover, the decreased operation of non-essential businesses and increase in the number of people working from home in this past year has further compounded this effect. More people are streaming live lectures, sports, news, and video calls via the internet at home today than we have ever seen before. Internet Service Providers, such as Viasat, are tasked with optimizing  internet connections and tailoring their allocation of resources to fit each unique customer’s needs. With this increase in internet activity, it would be especially beneficial for Viasat to understand what issues arise when customers stream various forms of video. In general, different internet activities require different resources to optimize the connection. For example, if a customer watches a lot of live video they may prefer a connection with lower latency and higher bandwidth. Although we are able to identify the genre of an activity when a user is not using a VPN, the challenge arises when a user chooses to surf the web through a VPN. When it comes to VPN use cases we can’t identify a user’s unique activity when they experience issues, thus making us unable to successfully troubleshoot  those problems. This is where a tool that could identify various internet activities, specifically live or uploaded video streaming, within a VPN tunnel would be extremely useful for an Internet Service Provider. \\n\\n## Project Report: \\nFound within the **references** folder. The file name is Final_Report.\\n\\n## Guide for Launching this Project:\\nNote: These instructions assume that the user has access to the DSMLP server to be able to run this project. Open terminal, run these commands in the associated order:\\n\\n1.) **ssh user@dsmlp-login.ucsd.edu** (user refers to your school username). Enter credentials.\\n\\n2.) **launch-180-gid.sh -G 100011655 -P Always -i apristin99/live_vs_vod_project**\\n\\n3.) **git clone https://github.com/pristinsky1/live_vs_video_on_demand_VPN_detection.git**\\n\\n4.) **cd live_vs_video_on_demand_VPN_detection**\\n\\nNow you are within the right directory with the environment already set up! Start running the files!\\n\\n5.) For files needed to be predict, you have to put them in the \"data/in\" directory. Acceptable files are files generated network-stats tool provided by Viasat.\\n\\n6.) If you wish to train a new classifier based on new data or new type of model and parameters, you have to change the location of training data and other settings in the \"config/train-parmas.json\". Otherwise, you can directly run **python run.py predict** using the trained model contained in the project.\\n\\n\\n## Guide for Pipeline Testing:\\nRun **python run.py test** and the results will be in **test/out**. Within that folder, it contains the output dataframes, model, and reports of accuracies for the test data.\\n\\n\\n## Contents:\\nThere are three parts of contents:\\n1. src folder - Contains all library code.\\n2. config folder - Contains directory of each target.\\n3. run.py - Main Program for this project.\\n4. notebooks folder - stores notebooks for this project.\\n5. data/out folder - stores results of this project.\\n\\nThe two files to look at for results under data/out:\\n\\ntraining_report.json: Json file contains the report of model\\'s basic information and its performance on validation set.\\npredictions.csv: DataFrame contains basic information of one record generated by network-stats and its prediction result.\\n\\n\\n\\n## How to run it?\\nWarning: The feature, train and predict has to be run in fixed order. If you want to run everything at once, you can use the all script.\\n\\nUse console to run **python run.py eda** as a script. This will run the eda notebook and store the html version for easy accessibility under \"/notebooks\". \\n\\nUse console to run **python run.py feature** as a script. This will create the features. The output dataframe will be stored in \"data/out\" directory in csv format.\\n\\nUse console to run **python run.py train** as a script. This will train the model. The output model and report will be stored in \"data/out\" directory in csv format.\\n\\nUse console to run **python run.py predict** as a script. This will classify the input dataset as live or streaming. The output dataframe will be stored in \"data/out\" directory in csv format.\\n\\nUse console to run **python run.py all** as a script. This will run everything listed above. The output dataframes and model and report will be stored in \"data/out\" directory in csv format.\\n\\n\\n## Description of Each Params Files\\n\"feature-params.json\" -- \"indir: the input directory of training set, outdir: the output directory of generated dataframe, output: 1 means output dataframe containing features information, 0 means only return it as a dataframe(Must be 1 in feature-params.json)\"\\n\\n\"train-params.json\" -- \\n    :param: indir: file directory where extracted features stored.\\n    :param: outdir: file directory where output of this funcition stored.\\n    :param: testsize: the portion of train dataset used for validation.\\n    :param: randomstate: the randomstate number to random split train and valid set.\\n    :param: method: the classifier name used for training.\\n    :param: method_parameters: the parameter used for training.\\n\\n\"predict-params.json\" -- \"indir: the input directory of stored model, indir2: the input directory of testset, outdir: the output directory of test result.\"\\n\\n\\n\\n```\\n### Responsibilities\\n\\n* Da Gong developed the structure of this project.\\n* Zishun Jin worked on the prediction model and the model features of this project.\\n* Tianran Qiu worked on the prediction model and the model features of this project.\\n* Andrey developed the environment and the model feature creation for this project.\\n* Mariam worked on the final report and model feature creation for this project. \\n```\\n\\n\\n\\n\\n### Website\\n\\nLink to the webpage: https://pristinsky1.github.io/live_vs_video_on_demand_VPN_detection/\\n',\n",
       "  'The Live_vs_Vod_Project focuses on understanding the issues that arise when customers stream various forms of video through a VPN tunnel. The project aims to develop a tool that can identify different internet activities, specifically live or uploaded video streaming, within a VPN tunnel. The project report can be found in the references folder with the file name Final_Report. To launch the project, follow the provided instructions assuming access to the DSMLP server. The project includes code files, notebooks, and data folders for testing and running different parts of the project. The website link for more information is https://pristinsky1.github.io/live_vs_video_on_demand_VPN_detection/.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_8': [\"# DANE - Data Automation and Network Emulation Tool\\n\\n<img align='right' src='docs/media/dane-transparent-small.png' height=248>\\n\\nDANE is a hackable dataset generation tool to collect network traffic in a variety of configurable network conditions.\\n\\nIt runs on Windows, Mac, and Linux.\\n\\n**Table of contents**\\n- [Why use DANE?](#why-use-dane)\\n- [Documentation](#documentation)\\n- [Contributing](#contributing)\\n- [Acknowledgements](#acknowledgements)\\n\\n\\n## Why use DANE?\\n\\nDANE provides two core functionalities:\\n\\n1. Automatically collect network traffic datasets in a parallelized manner\\n\\n   Manual data collection for network traffic datasets is a long and tedious process—run the tool and you can easily collect multiple hours of data in one hour of time (magic!) with one or many desired 'user' behaviors.\\n   \\n2. Emulate a diverse range of network conditions that are representative of the real world\\n\\n   Data representation is an increasingly relevant issue in all fields of data science, but generating a dataset while connected to a fixed network doesn't capture diversity in network conditions—in a single file, you can configure DANE to emulate a variety of network conditions, including latency and bandwidth.\\n\\nYou can easily hack the tool to run custom scripts, custom data collection tools, and other custom software dependencies which support your particular research interest.\\n\\n## Documentation\\n\\nFor all documentation, including a [quick start](https://dane-tool.github.io/dane/guide/quickstart.html), details about the [technical approach](https://dane-tool.github.io/dane/guide/approach.html), and [FAQs](https://dane-tool.github.io/dane/guide/faq.html), please consult the [**website 📖**](https://dane-tool.github.io/dane).  \\nhttps://dane-tool.github.io/dane\\n\\n## Contributing\\n\\nSee something you'd like improved? Better yet, have some improvements coded up locally you'd like to contribute?\\n\\nWe welcome you to **submit an Issue** or **make a Pull Request** detailing your ideas!\\n\\n## Acknowledgements\\n\\nThis project was originally created in affiliation with the **Halıcıoğlu Data Science Institute**'s data science program at UC San Diego.  \\nhttps://hdsi.ucsd.edu/, https://dsc-capstone.github.io/\\n\\nDANE was motivated and developed with the generous support of **Viasat**.  \\nhttps://viasat.com/\\n\",\n",
       "  \"DANE is a dataset generation tool that collects network traffic under various network conditions. It can run on Windows, Mac, and Linux. The tool provides automated data collection and the ability to emulate different network conditions. Users can customize the tool by running custom scripts and software dependencies. Documentation, including a quick start guide and FAQs, can be found on the website. Contributions are welcome through issue submissions or pull requests. The project was created in affiliation with the Halıcıoğlu Data Science Institute's program at UC San Diego and was supported by Viasat.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_7': ['# ResRecovery\\n\\nWebsite: https://stephdoan.github.io/ResRecovery/\\n\\n# Table of Contents\\n\\n1. [Abstract](#Abstract)\\n2. [Config Files](#config)\\n   - [`train-params.json`](#train)\\n   - [`model-params.json`](#model)\\n   - [`user-data.json`](#user)\\n   - [`generate-data.json`](#generate)\\n3. [Running the Project](#running)\\n\\n## Abstract\\n\\nVirtual private networks, or VPNs, have seen a growth in popularity as more of the general population has come to realize the importance of maintaining data privacy and security while browsing the Internet. In previous works, our domain developed robust classifiers that could identify when a user was streaming video. As an extension, our group has developed a Random Forest model that determines the resolution at the time of video streaming. Our final model has an overall accuracy of **87%**.\\n\\n<a name=\"config\"></a>\\n\\n## Configuration Files\\n\\n<a name=\"train\"></a>\\n\\n### `train-params.json`\\n\\nAllows users to adjust some parameters of the training data creation process. The main point of focus is the `{interval}` argument. This allows users to adjust how big of a chunk size they would like their model to be trained on. The default is 300 seconds as it allows replication of our project.\\n\\n| Parameter     | Description                                                                                                     |\\n| ------------- | --------------------------------------------------------------------------------------------------------------- |\\n| folder_path   | path to where all of the raw data is stored; please refer to the folder structure below to achieve best results |\\n| interval      | chunk size                                                                                                      |\\n| threshold     | minimum megabit value; used in peak feature creation                                                            |\\n| prominence_fs | sampling rate to find the max peak prominence                                                                   |\\n| binned_fs     | deprecated parameter                                                                                            |\\n\\n##### Data Folder Structure\\n\\nAll of the training data should be stored in an accessible `data` folder. CSV files should be categorized into folders according to their resolution below.\\n\\n```\\n+-- data\\n |\\n +-- 144p\\n +-- 240p\\n +-- 360p\\n +-- 480p\\n +-- 720p\\n +-- 1080p\\n```\\n\\n<a name=\"model\"></a>\\n\\n### `model-params.json`\\n\\nAllows users to adjust hyperparameters of the random forest classifier. The default values are the values we utilized in our original project.\\n\\n| Parameter         | Description                                                              |\\n| ----------------- | ------------------------------------------------------------------------ |\\n| training_data     | path to where training data is stored; data must be stored as a CSV file |\\n| n_estimators      | number of trees in the forest model                                      |\\n| max_depth         | max depth of the tree                                                    |\\n| min_samples_split | minimum number of samples required to split an internal node             |\\n\\n<a name=\"user\"></a>\\n\\n### `user-data.json`\\n\\nAllows users to input their own data to be classified by the model.\\n\\n| Parameter     | Description                                                                                                                     |\\n| ------------- | ------------------------------------------------------------------------------------------------------------------------------- |\\n| path          | path to where all of the raw user data is stored; must be an output of [network-stats](https://github.com/viasat/network-stats) |\\n| interval      | chunk size                                                                                                                      |\\n| threshold     | minimum megabit value; used in peak feature creation                                                                            |\\n| prominence_fs | sampling rate to find the max peak prominence                                                                                   |\\n| binned_fs     | deprecated parameter                                                                                                            |\\n\\n<a name=\"generate\"></a>\\n\\n### `generate-data.json`\\n\\nParameters used by the `generate_data.py` script. Please refer to [Selenium](https://www.selenium.dev/documentation/en/) documentation to install the appropriate `webdriver.exe`. For best use, please configure the `PATH` variable in the `generate-data.py` file to the correct file path of the webdriver. This script was developed using Google Chrome.\\n\\n| Parameter          | Description                                                                                                     |\\n| ------------------ | --------------------------------------------------------------------------------------------------------------- |\\n| network_stats_path | location of network-stats.py                                                                                    |\\n| interface          | user interface to collect from; refer to [network-stats](https://github.com/viasat/network-stats) documentation |\\n| playlist           | link to YouTube playlist                                                                                        |\\n| outdir             | to be implemented                                                                                               |\\n| resolutions        | list of resolutions to be collected                                                                             |\\n\\n<a name=\"running\"></a>\\n\\n## Running the Project\\n\\nThe project is current set to the assumption that users will collect their own training data. There is a repository of available training hosted on the DSMLP server located at `/teams/DSC180A_FA20_A00/b05vpnxray/personal_stdoan/data`. If not accessible, please refer to the [`generate_data.json`](#generate) configurations to automate collection of a training set.\\n\\n#### Running on the DSMLP Server\\n\\nThe project was mean to be run on the UCSD DSMLP server. Below are instructions if user has access to DSMLP resources.\\n\\n1. Open up a terminal and run the command below to log onto the server. Users will need to provide appropriate identification when asked.\\n\\n> `ssh [username]@dsmlp-login.ucsd.edu`\\n\\n2. Launch a docker container to ensure package dependencies are fulfilled by running the command:\\n\\n> `launch-180-gid.sh -G 100011652 -P Always stdoan/viasat-q1`\\n\\n3. Clone this repository.\\n\\n4. Adjust config files as necessary and then run the targets!\\n\\n#### Targets\\n\\n- `python run.py test` will test the various targets to ensure that all methods are running properly.\\n\\n- `python run.py clean` will delete files created from running various targets. The folder and files are deleted from the local machine.\\n\\n- `python run.py features` will create features from data specified in `train-params.json`.\\n\\n- `python run.py predict` will either create training data to create a model or utilize a Pickle\\'d model that we have included. Output is an array of resolution label for each chunk in the data.\\n',\n",
       "  'The ResRecovery project is focused on developing a Random Forest model that determines the resolution of video streaming. The project provides several configuration files (`train-params.json`, `model-params.json`, `user-data.json`, and `generate-data.json`) that allow users to adjust parameters and input their own data for classification. The project can be run on the UCSD DSMLP server, and it offers targets for testing, cleaning, creating features, and predicting video resolutions.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_10': [\"# Data Science Senior Capstone - Viasat VPN Analysis\\n\\n**Table of Contents**:\\n- [Link to Blog page](#blog-page)\\n- [Abstract](#abstract)\\n- [Approach](#approach)\\n- [Running](#running)\\n  - [Setup](#setup)\\n  - [Logging](#logging)\\n  - [Target `data`](#target-data)\\n  - [Target `features`](#target-features)\\n  - [Target `train`](#target-train)\\n- [Report](#report)\\n\\n## Link to Blog page\\nhttps://mhrowlan.github.io/streaming_provider_classifier_inside_vpn/\\n## Abstract\\n\\nWhether to access another country's Netflix library or for privacy, more people are using Virtual Private Networks (VPN) to stream videos than ever before. However, many of the different service providers offer different user experiences that can lead to differences in the network transmissions. This repository contains the implementation of our classifying model to determine what streaming service provider was being used over a VPN. The streaming providers that the model identifies are Amazon Prime, Youtube, Netflix, Youtube Live, Twitch, and an other category consiting of Disney+, Discovery+, and Hulu. This is valuable in understanding the differences in the network work patterns for the different streaming service providers. We achieve an average accuracy of 96.5% on our Random Forest model.\\n\\n## Approach\\n\\nWe utilize Viasat's [`network-stats`](https://github.com/Viasat/network-stats) to collect network traffic on a per-second, per-connection basis while we are connected to a VPN, then engage in either internet browsing or video streaming behavior.\\n\\nUtilizing the output of network-stats, we extract packet-level measurements and engineer features based on the packet sizes, arrival times, and directions.\\n\\nWe leverage these features in a classification model to determine whether or not a network-stats output contains video streaming activity.\\n\\n## Running\\n\\nThe following targets can be run by calling `python run.py <target_name>`. These targets perform various aspects of the data collection, cleaning, engineering, training, and predicting pipeline.\\n\\n### Setup\\n\\nTo leverage the existing dataset, you must be a member of DSMLP and have access to the shared /teams/ directory.\\n\\nLog on to DSMLP via `ssh <username>@dsmlp-login.ucsd.edu`\\n\\nLaunch a Docker container with the necessary components via `launch-180.sh -i jeq004/streaming_provider_classifier_inside_vpn -G B05_VPN_XRAY -c 8 -g 1 -m 64`\\n\\nClone this repository: `git clone https://github.com/mhrowlan/streaming_provider_classifier_inside_vpn.git`\\n\\nNavigate to this repository `cd streaming_provider_classifier_inside_vpn`\\n\\nNow, you are ready to configure targets for our project build. Details are specified below.\\n\\n### Logging\\n\\nLogging behavior can be configured in `config/logging.json`.\\n| Key | Description |\\n| --- | --- |\\n| produce_logs | Boolean. Whether or not to write to the log file. Default: `true` |\\n| log_file | Path to the log file. Default: `data/logs/project_run.log` |\\n\\n### Target `data`\\n\\nLoads data from a source directory then performs cleaning and preprocessing steps on each file. Saves the preprocessed data to a intermediate directory.\\n\\nIf on DSMLP with the proper group membership, this target will symlink existing data from the shared /teams/ directory.\\n\\nSee `config/data-params.json` for configuration:\\n| Key | Description |\\n| --- | --- |\\n| source_dir | Path to directory containing raw data. Default: `data/raw/` |\\n| out_dir | Path to store preprocessed data. Default: `data/preprocessed/` |\\n\\n### Target `features`\\n\\nEngineers features on the preprocessed data with configurable parameters and saves to an output directory.\\n\\nSee `config/features-params.json` for configuration:\\n| Key | Description |\\n| --- | --- |\\n| source_dir | Path to directory containing preprocessed data. Default: `data/preprocessed/` |\\n| out_dir | Path to directory to store feature engineered data. Default: `data/features/` |\\n| chunk_size | Milliseconds. We split our variable length data into smaller chunks with consistent time spans. Default: `90000` |\\n| rolling_window_1 | Milliseconds. We generate a smoothed mean using rolling windows of multiple lengths, this is the first length. Default `10000` |\\n| rolling_window_1 | Milliseconds. This is the second length for our smoothed means. Default `60000` |\\n| resample_rate | Time offset. We resample our packet measurements to produce a consistent sample-rate signal for spectral analysis. Default `500ms` |\\n| frequency | Hertz. We compute a power spectral density feature on a signal of this sample rate. Default `2.0` |\\n\\n### Target `train`\\n\\nTrains a classifier based on the new features and outputs the accuracy between the predicted and true labels. In other words, it prints out the percentage of cases that were correctly classified as streaming.\\n\\nSee `config/train-params.json` for configuration:\\n| Key | Description |\\n| --- | --- |\\n| source | Path to csv containing feature engineered data. Default: `data/features/features.csv` |\\n| out | Path to .pkl file which will save the trained model. Default: `data/out/model.pkl` |\\n| validation_size | Proportion. This amount of training data will be withheld to evaluate the performance of the trained classifier. Default: `0.3` |\\n| classifier | String name of scikit-learn classifier to use. One of 'RandomForest', 'KNN', or 'LogisticRegression'. Default: `RandomForest` |\\n| model_params | Scikit-Learn hyperparameters for the chosen model. See scikit-learn documentation. |\\n\\n### Target `all`\\n\\nRuns `data`, `features`, `train` in order.\\n\\n### Target `test`\\n\\nRuns `data`, `features`, `train` with configuration found in `test/config/`.\\n\\nCan optionally specify targets after test to only run that target. For example `python run.py test data` will only run the data target with the test config.\\n\\n## Report\\n\\nAn academic report on the exploration and model built in this repository can be found at [`SPICIVPN_report.pdf`](SPICIVPN_report.pdf)\\n\",\n",
       "  'This is a summary of the \"Data Science Senior Capstone - Viasat VPN Analysis\" project. The project focuses on analyzing Virtual Private Networks (VPN) used for streaming videos. The goal is to classify the streaming service provider being used over a VPN, such as Amazon Prime, Youtube, Netflix, Youtube Live, Twitch, and others. The project achieves an average accuracy of 96.5% using a Random Forest model. The approach involves collecting network traffic data using Viasat\\'s `network-stats` tool and extracting packet-level measurements to engineer features for classification. The project provides instructions for running different targets related to data collection, cleaning, engineering, training, and predicting. Additionally, an academic report on the project can be found in the provided link.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_65': ['## Interpreting Higgs Interaction Network with Layerwise Relevance Propagation\\n\\n#### Abstract\\n\\nWhile graph interaction networks achieve exceptional results in Higgs boson identification, GNN explainer methodology is still in its infancy. To introduce GNN interpretation to the particle physics domain, we apply layerwise relevance propagation (LRP) to our existing Higgs boson interaction network (HIN) to calculate relevance scores and reveal what features, nodes, and connections are most influential in prediction. We call this application HIN-LRP. The synergy between the LRP interpretation and the inherent structure of the HIN is such that HIN-LRP is able to illuminate which particles and particle features in a given jet are most significant in Higgs boson identification. The resulting interpretations are ultimately congruent with extant particle physics theory, with the model demonstrably learning the importance of concepts like the presence of muons, characteristics of secondary decay, and salient features such as impact parameter and momentum.\\n\\n#### [READ PAPER](report.pdf)\\n<hr>\\n\\n## Description of contents\\n\\n\\n* `notebooks`:\\n    * `relevance_heatmap.ipynb`: notebook that contains example plots of this project\\n* `src`:\\n    * `model`\\n        * `GraphDataset.py`\\n        * `InteractionNetwork.py`\\n    * `sanity_check`\\n        * `make_data.py`\\n    * `util`: utility functions such as I/O or plotting\\n        * `copy.py`\\n        * `data_io.py`\\n        * `model_io.py`\\n        * `plot.py`\\n    * `LRP.py`: core component of this project\\n\\n* `run.py`: Entry point for running different targets of this project\\n* `test`: directory for storing Dev data\\n    * `test.root`: the generated root file for testing purpose\\n* `data`\\n    * `model`: contains a trained IN state dictionary to start with\\n    * `definitions.yml`: contains metadata definition of the data used in this project\\n<hr>\\n\\n## Build Environment\\n* [Docker image](https://hub.docker.com/repository/docker/shiro0x19a/higgs-interaction-network) used for this project\\n\\n<hr>\\n\\n\\n## Usage\\nTo use `run.py`, a list of supported arguments are provided below\\n\\nFor sanity check of the explanation,\\n```\\npython run.py sc <arguments>\\n```\\n|arguments|purpose|\\n|-|-|\\n|`all`|build all targets, equivalent to using [`data` `train` `plot`] as argument|\\n|`data`| generate dummy data for sanity check|\\n|`train`| train a dummy IN on the sythesized data|\\n|`plot`|create static heatmap plots of precomptued relevance scores|\\n\\n\\n\\nFor explaining a pre-trained Higgs boson Interaction Network,\\n```\\npython run.py <arguments>\\n```\\n|arguments|purpose|\\n|-|-|\\n|`test`| build all targets, equivalent to using [`data` `train` `plot`] as argument on Dev data|\\n|`all`| similar to `test`, but build on actual data|\\n|`explain`| generate relevance score for given data|\\n|`plot`| create static heatmap plots of precomptued relevance scores|\\n\\n<br>\\n\\n',\n",
       "  'This paper introduces the application of layerwise relevance propagation (LRP) to interpret a Higgs boson interaction network (HIN) in particle physics. The method, called HIN-LRP, calculates relevance scores to identify the most influential features, nodes, and connections in predicting Higgs boson identification. The interpretations align with existing particle physics theory and highlight the importance of concepts like muons, secondary decay characteristics, impact parameter, and momentum. The paper provides code and instructions for running different targets of the project.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_64': ['# Particle Physics Result Replication\\n## Checkpoint #1\\n\\n#### Brief Description of Structure\\n\\n##### config\\n###### data-params.json\\nThis file includes the parameters for data extraction.\\n\\n##### src/data\\n###### etl.py\\nThis file contains the functions necessary for data extraction.\\n\\n##### run.py\\nThis file successfully extracts the data.\\n\\n#### In order to run the file run.py, execute the following command:\\n##### python run.py data',\n",
       "  'This is a summary of the structure and instructions for running the file \"run.py\" in the context of particle physics result replication. The file \"run.py\" successfully extracts data using the parameters specified in \"data-params.json\". To run \"run.py\", execute the command \"python run.py data\".'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_72': ['## Project MAI\\n### Project Description\\nMAI refers to Microservice-based Auto Infrastructure, a serverless architecture.\\nThis project aims to build a lightweight, effective, and robust system that helps with the automations of several workflows of the campus COVID detection team.\\n\\n### Underlying Architecture\\n![alt text](mai.png)\\n\\n### Supported Functions\\n#### AUM(auto-update microservice)\\n1. Automate excel parsing => cross-reference => update the sheet \\n2. All in one script(rewrote excel parse used to parse raw cases data)\\n3. One upload and a POST request\\n4. Improved code quality\\nfor more information, go to [repository](https://github.com/CrisZong/AUM)\\n\\n\\n#### statsTool\\n1. Prediction (autoregression model by buildings)\\n2. Autocorrelations (autocorrelation by building)\\n3. Stats(Cases by building sorted by number of cases)\\nfor more information, go to [repository](https://github.com/CrisZong/statsTool)\\n\\n#### Manhole Graph\\nthe project supports creating a graph of the manhole downstream that can be consumed by the campus notification team and running standard graph routines for paths searching or manipulations.\\n\\n#### Functions for graph\\nTo build a graph and see the resulting csv, please run `python run.py graph`\\n\\n### Dataset\\nThe data used was provided by the Knight Lab team. One data source is the daily wastewater data on the Google Spreadsheet. The other data source is the network folder which contains all the shape files about sewer and manhole connections on campus.\\n\\n',\n",
       "  'Project MAI is a serverless architecture called Microservice-based Auto Infrastructure. It aims to automate workflows for the campus COVID detection team. The project includes an auto-update microservice, a stats tool for prediction and analysis, and a manhole graph feature. The data used for the project is provided by the Knight Lab team, including daily wastewater data and shape files about sewer and manhole connections on campus.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_60': [\"# AutoBrick: A system for end-to-end automation of building point labels to Brick turtle files\\n\\n![alt text](https://github.com/Advitya17/AutoBrickify/blob/main/autobrick_workflow.png?raw=true)\\n\\n## Setup\\n\\nClone the repository and cd into the root directory.\\n\\n`git clone https://github.com/Advitya17/AutoBrickify` & `cd AutoBrickify`\\n\\nThen run the command below to setup the tool environment.\\n\\n`python run.py env-setup` (alternatively you can build with the Dockerfile!)\\n\\nThis'll print a message to the console at the end to confirm setup.\\n\\n#### `python run.py test` (Only for 180B Submission, can ignore otherwise)\\n\\n## Instructions\\n\\n### Step 1\\nSpecify your configurations in config/data-params.json. \\n\\nDetailed instructions are available in the `config/README.md` file in this repository.\\n\\n\\n### Step 2\\nRun the project from the root directory.\\n\\n`python run.py`\\n\\nYour Turtle object file (`output.ttl`) will be generated in the root directory!\\n\",\n",
       "  'AutoBrick is a system that automates the process of building point labels to Brick turtle files. To set up the tool environment, clone the repository and run the provided command. Then, specify your configurations in the `config/data-params.json` file. Finally, run the project using the command `python run.py` and your Turtle object file will be generated in the root directory.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_61': [\"## Infection Risk App\\nIn this project, We propose an application which estimates infection risk of COVID-19 in buildings. The application accepts building data and a set of parameters regarding occupants and infection rates of the surrounding community. Code and assumptions made in the algorithm are clearly explained to users for transparency, which those explainations are included in the project in our last quarter. \\n\\nOpening up the project\\nThe source codes for the calculator are located in /src/calculator. To see the notebook containing the underlying logic and sample runnings for the calculator, open presentation in /notebooks. Note there are actual codes in those notebooks, because the purpose of this project is to create an infection estimation algorithm that's clear to users and easily understanable by users. It's important to show what the algorithm does for each steps. To see our UI demo, open Website in /notebooks. To visit our website, visit https://hinanawits.github.io/DSC-180B-Presentation-Website/ Notice we are constantly updating our website with newest data so if the Website notebooks and the website itself are inconsistant it means we updated something. \\n\\nTo use run.py in command line, input python run.py [targets].\\n\\nWe currently have the following targets available:\\n\\ntest: which runs the calculator using sample parameters.\\nMore information about those sample runs can be found in the report notebooks mentioned above.\\n\\nResponsibilities\\n\\nEtienne Doidic built the structure and underlying logic of the calculator, and also the notebooks for walk through.\\n\\nNicholas Kho helped developing the application and migrated the codes to src and project structure.\\n\\nZhexu Li added features to the application migrating the codes, updated the project structure and developed configs and run.py.\\n\",\n",
       "  'This project proposes an application that estimates the risk of COVID-19 infection in buildings. The application takes building data and parameters related to occupants and infection rates into account. The algorithm used in the application is transparent, with clear explanations provided to users. The source codes for the calculator can be found in the /src/calculator directory, along with notebooks that explain the underlying logic and provide sample runs. A UI demo is also available in the /notebooks directory, and the website for the project can be accessed at https://hinanawits.github.io/DSC-180B-Presentation-Website/. The run.py file can be used in the command line with various targets, such as \"test\" which runs the calculator using sample parameters. The responsibilities of the team members involved are also mentioned.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_34': [\"# Project: User Wait\\n\\n## Introduction\\n\\nActivity monitor in the computer visualizes the system performance, but we don't know when our system gets slow or halted. If we are able to predict the mouse wait time, users could terminate their processes ahead of time to avoid waiting. Currently, there is little research conducted on the mouse wait prediction.\\n\\n## Running the project\\n\\n- To get the data run the following command line located inside run.py file.\\n  This will call specific dll files to collect the data regard to your laptop.\\n\\n- To get the data, from the project root dir, you should see the data folder.\\n  Inside the data folder, you should see a new database file being generated.\\n\\n## Type Of Data\\n\\nData provided by Intel includes 14,534,433 rows with 29,587 unique GUID within the 2020 interval. Since the complete dataset is fairly large, we sample 1/14 of the dataset, which leaves 27,014 GUID for our model. For the target, we divide the wait time into 0-3s, 3-5s, 5-7s, and 7+s as a preparation for the classification model. After exploring the correlation between potential features and the mouse wait time, we find that dynamic features, including CPU utilization, disk utilization, hard page faults, and static features, including the number of cores, RAM, model type, etc, could influence the mouse wait time. These features are then used in the model.\\n\\nThe data set we use is ”mousewaitall.csv001”,which is provided by the Intel teams. This data setrecords kinds of system usage before and after mousewait happens. Each feature in this data set consists ofprefix, infix and suffix. Prefix has ”before” and ”after”.It represent is this feature recorded before or after mousewait event. Infix has ”CPUUtil”, ”harddpf”, ”diskutil”and ”networkutil”. This represents what kind of systemusage this feature records. Suffix has ”min”, ”max” and”mean”. This represents the way this feature computesstatistics.\\n\\nThe second data set we use is ”system sysinfo uniquenormalized.csv000”, which is provided by the Intel teams. It contain 32 different features and 100,000unique systems.This data set provides informationabout the system hardware like CPU model, GPU,ram, etc.\\n\",\n",
       "  'This project focuses on predicting mouse wait time in order to allow users to terminate processes ahead of time and avoid waiting. The project uses data provided by Intel, including features such as CPU utilization, disk utilization, hard page faults, number of cores, RAM, and model type. The dataset used includes 14,534,433 rows with 29,587 unique GUID within the 2020 interval. The data is sampled to include 27,014 GUID for the model. Another dataset called \"system sysinfo uniquenormalized.csv000\" is also used to provide information about system hardware.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_33': ['# DSC180B-Capstone-Project\\nDSC Capstpne Project: A Prediction Model for Battery Remaining Time\\n## Usage Instructions\\nWe provided 4 targets for your usage. They are `data`, `eda`, `model`, and `test`.\\n\\n`test` would run our project on test data, which provides a miniature of what we have done on a smaller dataset. \\n\\nAn exmple for running our project through terminal is `python run.py data`, which will show our main dataset. To run other branches, just replace `data` with `eda`, `model` or`test`.\\n\\n\\n## Description of Contents\\n```\\nPROJECT\\n├── config\\n    └── data-params.json\\n    └── inputs.json\\n├── notebooks\\n    └── DSC180B_Presentation.ipynb\\n├── references\\n    └── README.md\\n└── src\\n    ├── EDA\\n        └──  feature_selection.py\\n    ├── data\\n        ├── Loading_Data.py\\n        ├── minimini_battery_event2.csv\\n        ├── minimini_battery_info2.csv\\n        ├── minimini_device_use1.csv\\n        ├── minimini_device_use2.csv\\n        ├── minimini_hw1.csv\\n        ├── minimini_hw2.csv\\n        ├── minimini_process1.csv\\n        └──minimini_process2.csv\\n    └── model\\n        └──hypothesis_testing.py\\n├── .gitignore\\n├── README.md\\n└── run.py\\n└── submission.json\\n```\\n\\n\\n### `config/`\\n* `data-params.json`: It contains the file path for our dataset\\n* `inputs.json`: It contains the argument inputs\\n\\n### `notebooks/`\\n* `DSC180B_Code.ipynb`: EDA, Hypothesis Testing and Visual Presentation on our project\\n\\n### `references/`\\n* `README.md`: External sources\\n\\n### `src/`\\n* `EDA/feature_selection.py`: code for selecting desired features for our prediction model\\n* `data/Loading_Data.py`: code for load our dataset\\n* `data/minimini_[DIFFERENT FILE NAME]_csv`: sample dataset\\n* `model/hypothesis_testing.py` : code for prediction model and hypothesis testing\\n\\n### `run.py`\\n* Main driver for this project.\\n',\n",
       "  'This is a description of the contents of the DSC180B-Capstone-Project. It includes information about the project structure, such as the `config/`, `notebooks/`, `references/`, and `src/` directories. It also provides details about specific files, such as `data-params.json`, `inputs.json`, `DSC180B_Presentation.ipynb`, and others. The main driver for the project is `run.py`.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_35': [\"# DSC180B_Project checkpoint_1\\n\\n# Background\\nIn the PC industry, there are different computer setups for omnifarious PC users. Different types of customers have different needs and budget for their computers. For instance, we think that gamers prefer desktops or laptops with high-end GPU and CPU while office users prefer the ones with decent CPU and long battery life. Hence, being aware of the different needs from different customers could help computer retailers dramatically with marketing and resource allocation. With this background, we decided to build a machine learning model that can predict a users’ persona based on the information of their computers.\\n\\n# How To Run: Classification of Your Own Audio Files\\n1. Clone this repository.\\n2. On the command line, navigate to this repository locally.\\n3. on the command line, use\\n\\n    *python run.py test*     runs the pipeline with the test-project target. This will run the test build classifier. THIS PROCESS WOULD LAST HOURS!!!\\n    \\n    *python run.py build-classifier*    to build all classifier. THIS PROCESS WOULD LAST HOURS!!!\\n    \\n    *python run.py hypo-test*      to run the hypothesis test on column 'ram' with other columns.\\n    \\n    *python run.py chi-square-test*     to run the hypothesis test on column 'ram' with other columns.\\n\",\n",
       "  \"This project aims to build a machine learning model that can predict a user's persona based on their computer information. The project provides instructions on how to run the classification of audio files and various tests using Python.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_36': ['# Prediction Task: Utilizing CPU Statistics and Application Usage to Predict a User’s Persona\\n\\n\\n\\n## Homepage\\nhttps://vlw003.github.io\\n\\n## Medium Blog\\nhttps://predicting-persona-b09group04.medium.com/\\n\\n## Usage\\n```\\ngit clone https://github.com/jonxsong/DSC180AB-Capstone.git\\ncd DSC180AB-Capstone\\npython run.py test\\n```\\n\\n\\n\\n## Files\\n\\n**./config/data-params.json** - directory where data should be output to\\n\\n**./config/hw-metric-histo-data-params.json** - description of the dataset and features we utilize\\n\\n**./config/systems-sysinfo-unique-normalized-data-params.json** - description of the dataset and features we utilize\\n\\n**./config/ucsd-apps-execlass-data-params.json** - description of the dataset and features we utilize\\n\\n**./config/frgnd_backgrnd_apps-data-params.json** - description of the dataset and features we utilize\\n\\n**./notebooks/eda.ipynb** - notebook containing data explorations from DSC180B\\n\\n**./notebooks/dsc180a-notebook.ipynb** - notebook containing data explorations from DSC180A\\n\\n**./src/data_exploration.py** - file containing relevant methods for data exploration\\n\\n**./src/model.py** - file containing relevant methods for data modelling\\n\\n**./requirements.txt** - required packages\\n\\n**./run.py** - call run.py to run data analysis\\n\\n\\n\\n## Data/Output Files\\n\\n**./data/out/...** - this location should hold all the outputted pictures generated from methods\\n\\n**./data/raw/...** - this location should hold all the datasets downloaded below\\n\\n\\n\\n## Link to download the datasets:\\nhttps://drive.google.com/drive/folders/1nNpwhzrbKUJd0ZwbCYLGQH49CKkKLTQ4?usp=sharing\\n\\nThe datasets we are using are too large for github. The datasets should be stored in /data/raw/.\\n\\n\\n\\n## Sources\\nLink to Project Report: https://docs.google.com/document/d/1IpWfuG2IxurT5LOMyudWpn3UOLsKYKdjbbwqNhPGlYk/edit?usp=sharing\\n\\n\\n\\n## Responsibilities:\\n\\nJon:\\n    - Report + main ideas\\n    - data analysis - code breakdown\\n    - repository structuring\\n    - notebook outlining\\n    - script writing\\n\\nVince:\\n    - data modeling\\n    - Report + targets\\n    - data cleaning\\n    - data explorations\\n    - classifications\\n    - Visual Presentation Checkpoint\\n    - Website\\n    - Final Report\\n    - Slides\\n\\nKeshan:\\n    - data preparation\\n    - tabled data\\n    - key notes all throughout notebook\\n    - graphs + graph analysis\\n    - ATL work\\n',\n",
       "  \"This document provides information about a project that aims to predict a user's persona using CPU statistics and application usage. It includes links to the project homepage and Medium blog, as well as instructions for cloning the project repository and running the code. The document also lists various files and their descriptions, including data exploration notebooks and relevant source code files. Additionally, it mentions the location of the data/output files and provides a link to download the datasets used in the project. Finally, it outlines the responsibilities of each team member involved in the project.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_32': [\"Scale model Group\\nThe purpose of this project is to provide a testing ground for the transmission code in SchoolABM. By scaling down the parameters to a single room of students, effects of elements such as distance and ventilation can be seen more easily.\\nThis also provides a testing environment that has a significantly faster runtime and has been useful in verifying our math for transmission both through droplets and aerosols\\n\\nTest Simulation can be run using: python -m run 'test'\\n\\nVisualization can be run using: python -m run 'visualize'\\n\\nVisualization includes:\\n-Color viz indicating which students are infected\\n-Distribution plot of transmission rates post-processing (for each 5 minute step and unique infected individual)\\n-Time series plot of when infection occurs\\n\\nTODO:\\nDocker Image (due thursday)\\nAiravata (due wednesday)\\n\\nWell-Mixed room (due thursday)\\n\",\n",
       "  'The scale model group project aims to test the transmission code in SchoolABM by scaling down parameters to a single room of students. This allows for easier observation of the effects of distance and ventilation. The project also provides a faster testing environment and helps verify the math for transmission through droplets and aerosols. The project includes test simulation and visualization options, such as color visualization indicating infected students, distribution plot of transmission rates, and time series plot of infection occurrences. There are also upcoming tasks including creating a Docker image, using Airavata, and implementing a well-mixed room by Thursday.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_31': [\"# Capstone Project dsc180B Covid-19 Transmission in Buses\\n\\nIn order to install mesa-geo and rtree, we have copied Johnny Lei's README file from our domain repo. Here it is:\\n\\n### Install MESA\\n#### Install ABM package MESA with:\\n\\n> pip install mesa\\n\\n#### Install MESA-geo\\n  If you are using a Mac Machine:\\n    Install rtree FIRST with conda (there seems to exist a distribution issue with pip specificly to Mac):\\n\\n> conda install rtree\\n\\n  Install geospatial-enabled MESA extension mesa-geo with:\\n\\n> pip install mesa-geo\\n\\nend of credit to J.L\\n\\n\\n### How to run\\n\\nOpen terminal, change to the directory of the code, then type in the following command \\n\\n> python run.py\\n\\nor if you want to run the code with your own parameters in config/test.json\\n\\n> python run.py test\\n\\n\\n## Logistics\\n\\nWe are using Agent-Based-Modeling to simulate covid-19 transmission in Buses.\\nIn order to run this simulation, you could change the parameters in config/test.json to simulate different scenarios.\\n\\nThis simulation start with an empty bus. The bus is scheduled to stop at a certain number of stops at certain times, picking up certain number of students. \\nAll these parameters are adjustable. As the bus continues, the sick students in the bus transmit the virus by normal activities like talking and breathing, and also coughing, sneezing, etc. This model stops the simulation at the end of a trip where the bus reaches the school (destination). Then this model creates a graph of the number of healthy and sick(if they received the virus) students every minute. There is also a gif of the simulation that shows the position of each student, the time since start, and the layout of the bus.\\n\",\n",
       "  'This text is about a capstone project on Covid-19 transmission in buses. It provides instructions on how to install the required packages and run the simulation. The simulation uses Agent-Based Modeling to simulate the transmission of the virus in buses. The parameters can be adjusted to simulate different scenarios, and the model creates a graph and a gif of the simulation.'],\n",
       " 'https://github.com/leonkuoDSC/Policing_and_Income': ['# Policing_and_Income\\nNotebooks include work on the car makes and models, using the San Diego Open Policing database, and getting the data by service area.\\n\\nUsing run.py with target test will add the car make_model, age and price to the tx_50.csv test dataset and create a new test.csv file with the added columns.\\n',\n",
       "  'The work on car makes and models using the San Diego Open Policing database is summarized in this notebook. The data is obtained by service area. By running the \"run.py\" script with the target \"test\", the car make_model, age, and price are added to the tx_50.csv test dataset. A new file called test.csv is created with the added columns.'],\n",
       " 'https://github.com/lizheng1226/DSC180B_Project_AutoFiNews': [\"# Model Analysis of Stock Price Trend Predictions based on Financial News\\n\\nThis is a data analysis project that aims to use different methods to predict the change of stock price from financial news and compare the performances of those methods.\\n\\nWe build following models:\\n- Baseline model: use Bag-of-Words to extract frequent words in news and determine their attitudes.\\n- AutoPhrase model: use AutoPhrase to extract high quality phrases and determine their attitudes.\\n- Doc2vec and LSTM model: use Doc2vec to create numberical representation of documents and use LSTM to predict the result.\\n- BERT model: use BERT to make prediction.\\n\\nThe code and results of different methods can be found in this repository.\\n\\n## Local Run\\n```\\n$ python run.py [test] [eda] [etl] [doc2vec_lstm_model] [doc2vec_lstm_model2] [baseline_model] [autophrase_model] [bert_model]\\n```\\n\\n### `test` target\\nThis target runs a subset of pre-trained AutoPhrase dataset on the Word2vec model using the data in `test/testdata` and test the configurations in `config`.\\n### `eda` target\\nThis target generate the EDA analysis and wordclouds of the positive and negative word bank and on the Apple stock price, and the results are saved to `data/eda_data`\\n### `etl` target\\nThis target cleans and outputs the training dataset from the Apple stock price and corresponding financial news release dates, and the results are saved to `data/etl_data`\\n### `doc2vec_lstm_model` target\\nThis target runs the doc2vec and lstm model to give prediction on Apple's stock price movement, and the model and results are saved to `data/d2v_lstm_model_data`\\n### `doc2vec_lstm_model2` target\\nThis target runs the doc2vec and lstm model to give the sentiment analysis labels prediction based on the financial news title. The model and results are saved to `data/d2v_lstm_model_data2`\\n### `baseline_model` target\\nThis target runs the baseline_model using the Bag-of-Words model.\\n### `autophrase_model` target\\nThis target runs the Autophrase model to predict the stock price changes.\\n### `bert_model` target\\nThis target runs the BERT model to predict the stock price changes.\\n\\n\\n## Docker\\n- The docker repository is `lizheng1226/dsc180_autofinews`.\\n- Please use the following command to run a DSMLP container using docker:\\n```\\nlaunch.sh -c 2 -m 4 -g 1 -i lizheng1226/dsc180_autofinews:latest\\n```\\n\\n## Webpage\\n* https://aponyua991.github.io/AutoFiNews/\\n\\n\\n## Group Members\\n- Liuyang Zheng\\n- Mingjia Zhu\\n- Yunhan Zhang\",\n",
       "  'This project aims to predict stock price trends based on financial news using various methods. The models built include a baseline model using Bag-of-Words, an AutoPhrase model, a Doc2vec and LSTM model, and a BERT model. The code and results for each method can be found in the repository. The project also includes targets for local run, such as testing the AutoPhrase dataset, generating EDA analysis and wordclouds, cleaning and outputting the training dataset, running the Doc2vec and LSTM models for stock price prediction, running the baseline model, running the AutoPhrase model, and running the BERT model. The project is also available as a Docker container and has a webpage for reference. The group members involved in this project are Liuyang Zheng, Mingjia Zhu, and Yunhan Zhang.'],\n",
       " 'https://github.com/jamesjaeyu/autophrase_over_time': [\"# Utilizing AutoPhrase on Computer Science papers over time\\n### DSC 180B Capstone Project\\n\\n### Group Members: Cameron Brody, Jason Lin, James Yu\\n\\n### Link to website: https://jamesjaeyu.github.io/autophrase_over_time/\\n\\n<br />\\n\\n[Link to DBLP v10 dataset download](https://lfs.aminer.cn/lab-datasets/citation/dblp.v10.zip)\\n\\n- Direct download link from [aminer.org/citation](https://www.aminer.org/citation)\\n- zip file size is 1.7 GB, 4.08 GB when extracted\\n\\nFile & folder descriptions:\\n- `config` folder: Contains configuration files for run.py\\n\\n- `data` folder: Directory for full data when running 'all' target on run.py\\n\\n- `docs` folder: Contains GitHub Pages files for the visual presentation website\\n\\n- `results` folder: Contains results of EDA figures and AutoPhrase\\n\\n- `src` folder: Contains .py files for processing datasets, performing EDA, and model generation\\n\\n- `test` folder: Contains `testdata` and `testresults` for running 'test' target on run.py\\n\\n- `run.py`: Run file for the project. Targets are 'data', 'eda', 'model', 'analysis'\\n    - 'all' will run all targets with full dataset\\n    - 'test' will run 'data', 'model', and 'analysis' with test data. 'eda' will run the same\\n\",\n",
       "  'This is a summary of the project \"Utilizing AutoPhrase on Computer Science papers over time\" by the DSC 180B Capstone Project group members Cameron Brody, Jason Lin, and James Yu. The project focuses on using AutoPhrase to analyze computer science papers. The project website can be found at https://jamesjaeyu.github.io/autophrase_over_time/. The project utilizes the DBLP v10 dataset, which can be downloaded from https://www.aminer.org/citation. The dataset is 1.7 GB in size when zipped and 4.08 GB when extracted. The project includes various folders such as \\'config\\', \\'data\\', \\'docs\\', \\'results\\', \\'src\\', and \\'test\\' which contain different files and scripts for data processing, EDA, and model generation. The \\'run.py\\' file is used to run different targets such as \\'data\\', \\'eda\\', \\'model\\', and \\'analysis\\'.'],\n",
       " 'https://github.com/YongqingLi14/codenames-ai-analysis': ['# codenames-ai-analysis\\n\\n## Introduction\\nThis project includes the experiments and analysis on the performance of Codenames AI, a system capable of replacing human efforts in the game Codenames.\\n* To play the game, please visit https://github.com/XueweiYan/codenames-game-ai\\n* For more details on the project, please visit https://xueweiyan.github.io/codenames-ai-website/\\n\\n\\n## Instructions\\n* Download the datasets from the below link and store them on the same level as this repository in a file named \"AI_dataset\":\\n  * https://drive.google.com/drive/folders/1FqEHYL_uTQDQ_MFw8T4gdD4VHaa2r0jv?usp=sharing\\n* Clone the game repo and place it on the same level as this repository:\\n  * https://github.com/XueweiYan/codenames-game-ai\\n* Resulting set up should look like this:\\n\\n<p align=\"center\">\\n  <img src=\"https://github.com/YongqingLi14/codenames-ai-analysis/blob/main/file_organization.png\" />\\n</p>\\n\\n\\n## Running the Repository\\n* Run the following docker image inside a container: \\n  * yongqingli/codenames_ai\\n* To test the pipeline of the project: \\n  * `python3 run.py test`\\n* To view full results (total time will take ~15 hours): \\n  * `python3 run.py all` \\n* To revert the repo back to its original state: \\n  * `python3 run.py clean` \\n* Results will be availale in Report.ipynb and Report.html\\n\\n\\n## Datasets\\nWe put 3 datasets to the test in this experiment:\\n* GloVe: \\n  * pretrained word embeddings from Wikipedia\\n  * cosine similarity\\n* Word2Vec\\n  * word embeddings trained from the English Simple Wiki using the gensim word2vec model\\n  * cosine similarity\\n* WordNet\\n  * word embeddings from the WordNet dataset\\n  * Wu-Palmer similiarity\\n\\n\\n## Testing Metrics\\n* Average turns taken to finish the game\\n* Number of assassins triggered\\n* Accuracy in correctly guessing the intended words\\n',\n",
       "  'This project analyzes the performance of Codenames AI, a system designed to replace human players in the game Codenames. The project includes instructions for setting up the necessary files and running the repository. The datasets used in the experiments are GloVe, Word2Vec, and WordNet. The testing metrics include average turns taken to finish the game, number of assassins triggered, and accuracy in correctly guessing the intended words.'],\n",
       " 'https://github.com/jonathantanoto/spam_detection_180B/': [\"# Spam Detection Using Natural Language Processing\\n\\nBuilding a spam detection algorithm by utilizing Natural Language Processing to extract features associated with spam emails. Deep Learning methods as well as word-to-vector transformation are used to create a spam email classifier.\\n\\n## Usage Instructions\\n\\nPotential run.py arguments:\\n* data: downloads and populates data folder from source.\\n* build: feature extraction and bi-LSTM model building, requires data to be run.\\n* predict: runs script to predict phrases whether it is a spam or not, requires data and build to be run.\\n\\n## Project Contents\\n\\n```\\nROOT FOLDER\\n├── .gitignore\\n├── .gitmodules\\n├── AutoPhrase (forked submodule repository)\\n├── run.py\\n├── README.md\\n├── data (populated by calling data argument of run.py)\\n├── models (populated by calling build argument of run.py)\\n│   ├── model\\n│   │   └── ...\\n│   └── tokenizer.pickle\\n├── notebooks\\n│   └── report.ipynb\\n└── src\\n    └── generate_dataset.py\\n    └── process_build.py\\n    └── spam_or_not.py\\n```\\n\\n### `src`\\n\\n* `generate_dataset.py`: Code that pulls dataset used for training from data source, and combines data into a dataframe.\\n* `process_build.py`: Code that extracts features from data, processes data to be used for training deep learning models. Trains Bidirectional Long Short-Term Memory model.\\n* `spam_or_not.py`: Code that loads model from process_build and runs a script to predict input text's probability of being a spam message.\\n\\n\\n### `notebooks`\\n\\n* Jupyter notebooks for Reports and line-by-line executed code.\\n* Final Report in PDF form.\\n\\n\\n## Links\\n\\n* Website: https://jonathantanoto.github.io/spamdetection/\\n* Presentation Video: https://youtu.be/wkc7R0J4_VM\\n\",\n",
       "  'This project focuses on building a spam detection algorithm using Natural Language Processing (NLP). The algorithm utilizes deep learning methods and word-to-vector transformation to create a spam email classifier. The project includes code for data generation, feature extraction, model building, and prediction. The project structure consists of a root folder with subfolders for data, models, notebooks, and source code. The \"src\" folder contains code for dataset generation, feature extraction, and model training. The \"notebooks\" folder contains Jupyter notebooks for reports and code execution. Links to the project website and presentation video are also provided.'],\n",
       " 'https://github.com/mcelz/MedCoin-Authorization-Smart-Contract': [\"# MedCoin Authorization Contract\\n## Overview\\nEHR(Electronic Health Records) was never intended to manage and preserve the complications of cross-institutional and lifelong medical records: Medical information for a patient comes from a variety of places, and different pieces of that information must be put together for clinicians to make efficient healthcare decisions. Because of storage constraints, EHRs frequently store health data at a single location for a few years rather than keeping all-time records for patients. EHR systems used by different hospitals are frequently incompatible. Patients who seek medical treatment at several locations must frequently retype their personal information and request data transfers across these health providers, and they encounter considerable issues accessing their reports, correcting incorrect information, and authorizing medical data. <br />\\n<br />\\nAnother concern in this area is the permission of medical records. To regulate the health industry, patient data protection procedure protocols such as HIPAA and EPHI were established, and different medical information sources have distinct authorization requirements that must be met before patient data can be shared with someone else. Sensitive data, such as the patient's gender, name, residence, zip code, and age, should not be leaked to a third party without authority; similarly, generally non-sensitive medical data should be examined with caution. No information can simply be aired or made available to the general public. Often, a physician will have all of the information they require, as well as others that they may not be aware of but are necessary to care for. <br />\\n<br />\\nWe propose to implement the authorization smart contracts for EHR-related medical blockchains. This auditing layer of medical blockchain has featured a security design that would prevent data breaches in the medical records and separate sensitive data from non-sensitive data: We separate sensitive and non-sensitive data when patients log their medical information and we would show legal statements (HIPAA, EPHI) to notify the patients when they are authorizing their private data to a third party. Then, the smart contracts would allow different levels of authorization of access to data. The smart contracts would also allow for the protection of private data while delivering useful medical data to health providers and doctors. \\n<br /><br />\\nThis authorization contract would therefore provide the medical record requesters(doctors, researchers, providers) with stratified access to medical information for research use, clinical use, or kept private; Patients could choose different authorization levels for people to access their medical records enabling a distributed system that provides layered and use for info users and info providers. This authorization smart contract would be the core construct in our medical data encryption, authorization stratification, and medical records transfer aggregation pipeline.\\n<br /><br />\\n\\n## Contributors\\nYifei Wang and Ruiwei Wan\\n\\n## Directory Structure\\n<pre>\\n├── README.md\\n├── notebooks/\\n│\\xa0\\xa0 ├── MedCoin White Paper.pdf\\n│\\xa0\\xa0 └── report.pdf\\n├── references/\\n│\\xa0\\xa0 └── README.md\\n├── report.pdf\\n└── src/\\n    ├── smart contract/\\n    │\\xa0\\xa0 ├── Patient_Registry.sol\\n    │\\xa0\\xa0 └── Relationship_Management.sol\\n    └── test/\\n        ├── Patient_Registry_Test.sol\\n        └── Relationship_Management_Test.sol\\n</pre>\\n\\n## Data Use Explanation\\nSince our project is not concerned with data ingestion and preprocessing,\\nwe are only concerned with authorization application in the smart contract,\\nwe therefore does not include the data in the data part.\\nFor testing purposes, we only rely on remix website, which is https://remix.ethereum.org/, \\nbecause implementing blockchain infrastructure would take months or even years.\\nSo if our test solidity file and smart contract solidity files were put on the remix and use the test resources there,\\nit would suffice the testing purpose of our contract.\\n\\n## Docker Use Explanation\\nWe also will not use docker, because it is hard to deploy docker for bloakchain project.\\n\\n## Testing website\\nFor testing purposes, we only rely on remix website, which is https://remix.ethereum.org/\\n\\nFirst we should download the solidity files in the src folder:\\n\\n- `Patient_Registry.sol`: the contract which is in charge of register new patients for authorization\\n- `Relationship_Management.sol`: the contract which manages the authorization pairs of patients and third parties\\n- `Patient_Registry_Test.sol`: the test contract for Patient_Registry.sol\\n- `Relationship_Management_Test.sol`: the test contract for Relationship_Management.sol\\n\\nThen we put the source files and test files onto remix website(https://remix.ethereum.org/) to test. \\nWe also connected to MetaMask Ropsten testnetwork, however, due to the limitations of blockchain infrastructure, our project stops here.\\n\\n## Project Report\\nWe also include the Project report in the notebook folder.\\nOur project website is: https://ellawan.github.io/.  \\n\",\n",
       "  'The MedCoin Authorization Contract aims to address the challenges of managing and preserving cross-institutional and lifelong medical records. The contract proposes the implementation of authorization smart contracts for EHR-related medical blockchains, which would ensure data security and separate sensitive data from non-sensitive data. The smart contracts would allow different levels of authorization for accessing medical data, providing layered access for doctors, researchers, and providers. The project includes source files and test contracts that can be tested on the Remix website. However, due to limitations in blockchain infrastructure, the project is currently not fully implemented.'],\n",
       " 'https://github.com/DSC180A-A04/spatiotemporal_analysis': ['# Spatiotemporal Analysis\\n\\n## Webpage\\nhttps://dsc180a-a04.github.io/spatiotemporal_analysis/\\n\\n\\nThe analysis in this repo uses uncertainty quantification feature that we have developed in [torchTS](https://github.com/Rose-STL-Lab/torchTS).\\n\\nExample output:\\n\\nUse conformal prediction to construct a conformal confidence band consisting of quantiles `[0.025, 0.5, 0.975]` for a 95% confidence level.\\n![uncertainty_quantification](./static/uncertainty_quantification.png)\\n\\n## Getting Started\\n\\n1. Create a virtual environment\\n\\n```bash\\npython3 -m venv venv\\n```\\n\\n2. Activate the virtual environment\\n\\n```bash\\nsource venv/bin/activate # for mac\\n```\\n```bash\\nvenv/Scripts/activate # for windows\\n```\\n3. Install dependencies\\n\\n```bash\\npip install -r requirements.txt\\n```\\n\\n4. Train models and make predictions. This will generate a `conformal_prediction.png` plot in the root directory.\\n\\n```bash\\npython run.py\\n```',\n",
       "  'This webpage provides information on spatiotemporal analysis using uncertainty quantification. The analysis utilizes a feature developed in torchTS. The example output demonstrates the use of conformal prediction to construct a conformal confidence band with quantiles [0.025, 0.5, 0.975] for a 95% confidence level. The webpage also includes instructions on getting started, such as creating a virtual environment, activating it, installing dependencies, and running the code to train models and make predictions.'],\n",
       " 'https://github.com/dhaar-data/DSC180B-project': [\"# DSC180B-project\\nThis is an exploration of the bootstrap post-prediction inference approach introduced by Wang et al. in [Methods for correcting inference based on outcomes predicted by machine learning](https://www.pnas.org/content/117/48/30266/tab-article-info) through a study of tweets and their corresponding political alignment in the US. When we look at a tweet, what are the kinds of words or phrases that most strongly indicate the tweet's alignment to Democrats or Republicans? In today's political climate, what topics or figures are most heavily scrutinized by one party or another? We seek to find these key figures, phrases, and topics through a statistical analyses of political tweets.\\n\\nAs said above, this statistical analyses also functions as an investigation of the bootstrap post-prediction inference approach. Post-prediction--or postpi, as Wang et al. calls it--is the use of predicted outcomes in lieu of observed outcomes during inferential analysis. If postpi is conducted without accounting for the use of predicted outcomes as is often the case, this leads to issues with bias and standard , among other things. While the aforementioned bootstrap post-prediction inference approach corrects these issues for a wide range of datasets, we seek to study its applicability specifically towards text data, particularly in political science. \\n\\n## Build Instructions\\nThis project has already provided pre-scraped Twitter data in `data/out/raw`.\\n* Data Collection: To clean and split the data into train-test-validation sets, run `python run.py data`. The datasets will be stored in `data/out/clean` in separate csvs for covariates and outcomes. \\n* EDA: To conduct EDA on the data, run `python run.py data eda`. The output will be three figures in `results/figures` as .png files.\\n* Prediction: To build the prediction model and predict the outcomes of the datasets, run `python run.py data predict`. The predicted values will be stored in `data/out/predicted` in txt files.\\n* Relationship: To build the relationship model based on the predicted and observed outcomes from the test dataset, run `python run.py data predict rel`.\\n* Inference/Bootstrapping: To conduct the bootstrap post-prediction inference on selected features, run `python run.py data predict rel inference`, equivalent to running all targets (see last bullet point). Features you want to conduct inference for can be adjusted in `config/inference-params.json`. This will output estimators, standard errors, and t-statistics of the features in a csv file located at `results/inference`.\\n* Run all:\\n    * To run the entire process on the dataset specified in `config/data-params.json`, run `python run.py all`\\n    * To run the entire process on test data in `test/testdata`, run `python run.py test`\\n    \\n## Running the Project\\n1. Clone this repo\\n    ```\\n    git clone https://github.com/dhaar-data/DSC180B-project.git\\n    ```\\n2. Build and run the docker image\\n    ```\\n    docker build -t ##\\n    docker run --rm -it ## /bin/bash.\\n    ```\\n3. Run the project according to build instructions above. As an example:\\n    ```\\n    python run.py all\\n    ```\\n    \\n## Project Website\\n```\\nhttps://dhaar-data.github.io/DSC180B-project/\\n```\\n\",\n",
       "  'This project explores the bootstrap post-prediction inference approach introduced by Wang et al. It focuses on analyzing tweets and their political alignment in the US to identify key words, phrases, and topics that indicate alignment to Democrats or Republicans. The project also investigates the applicability of the bootstrap post-prediction inference approach to text data in political science. The project provides instructions for data collection, exploratory data analysis, prediction modeling, relationship modeling, inference/bootstrapping, and running the entire process. The project repository and website are available for more information.'],\n",
       " 'https://github.com/duha-aldebakel/DSC180B-LDACode': ['# Overview & Setup\\n\\nThis repository serves as the central user code repository for Group A06\\'s DSC180B Capstone project.\\n\\n## Title:\\nExploration of Variational Inference and Monte Carlo Markov Chain Models for Latent Dirichlet Allocation of Wikipedia Corpus\\n## Abstract:\\nTopic modeling allows us to fulfill algorithmic needs to organize, understand, and annotate documents according to the discovered structure. Given the vast troves of data and the lack of specialized skillsets, it is helpful to extract topics in an unsupervised manner using Latent Dirichlet Allocation (LDA). LDA is a generative probabilistic topic model for discrete data, but unfortunately, solving for the posterior distribution of LDA is intractable, given the numerous latent variables that have cross dependencies. It is widely acknowledged that inference methods such Markov Chain Monte Carlo and Variational Inference are a good way forward to achieve suitable approximate solutions for LDA. In this report, we will explore both these methods to solve the LDA problem on the Wikipedia corpus. We find that better performance can be achieved via preprocessing the data to filter only certain parts-of-speech via lemmatization, and also exclude extremely rare or common words. We improved on the Expectations-Maximization (EM) Algorithm used for variational inference by limiting the number of iterations in the E step even if sub-optimal. This leads to benefit of faster runtimes and better convergences due to fewer iterations and avoidance of local minima. Finally, we explore early stopping runtimes on under-parameterized LDA models to infer the true dimensionality of the Wikipedia vocabulary to solve for topics. While the English language has around a million words, our findings are that it only takes around fifteen thousand words to infer around twenty major topics in the dataset.\\n\\n# To get it up and running:\\n## 1) Set up python environment\\n### Option 1: (Easiest) Pulling our Docker from Dockerhub\\n- Just run \"`docker run -it --rm daldebak/dsc180b bash`\"\\n- If the docker is not on your machine locally, the command should pull it from docker hub automatically. Here is how to force the action, \"`docker pull daldebak/dsc180b`\"\\n- Run the docker using \\'docker run -it --rm daldebak/dsc180b bash \\'\\n### Option 2: Rebuilding a Docker\\n- Build from Dockerfile using the docker CLI command\\n- Type \"`docker build -t <image-fullname> .`\" and hit \\\\<Enter\\\\>, notice the \"period/dot\" at the end of the command, which denotes the current directory. Docker will then build the image in the current directory\\'s context. The resulting image will be labeled `<image-fullname>`. Monitor the build process for errors.\\n- For example, a command could be \\'docker build -t daldebak/dsc180b .\\'\\n- Run the docker using \\'docker run -it --rm daldebak/dsc180b bash \\', or replace with your own <image-fullname>\\n### Option 3: If using DSMLP\\n- SSH to `dsmlp-login.ucsd.edu`. (Note if working outside the school, you would need to first connect via VPN)\\n- Run \"`launch.sh -i daldebak/dsc180b:latest`\"\\n### For local development:\\n- Make sure, preferably, you have python3.7+ installed and assuming you have configured the `PATH` and `PATHEXT` variables upon installation:\\n- `python3.7 -m venv env`\\n- `source env/bin/activate`\\n- `pip install -r requirements_pip.txt`\\n\\nTo interact with jupyter notebooks (make sure virtual environment is activated and requirements_pip.txt are installed):\\n- `cd DSC180B-LDACode`\\n- `ipython kernel install --user --name=env` (assuming you named your virtual environment `env`)\\n-  `jupyter notebook`\\n- When notebook server is running: Navigate to `Kernel` > `Change kernel` > select `env`\\n  \\n## 2) Getting the repository from Github\\n- \"`git clone https://github.com/duha-aldebakel/DSC180B-LDACode.git`\"\\n- \"`cd DSC180B-LDACode`\"\\n- \"`python run.py test-gensim`\" to run gensim on test data\\n- \"`python run.py gensim`\" to run gensim on production data \\n- \"`python run.py test-lda-cgs`\" to run gibbs on test data\\n- \"`python run.py lda-cgs`\" to run gibbs on production data \\n- \"`python run.py onlineldavb`\" to run blei MFVI LDA on production data   \\n',\n",
       "  \"This repository is for Group A06's DSC180B Capstone project on exploring Variational Inference and Monte Carlo Markov Chain Models for Latent Dirichlet Allocation (LDA) of the Wikipedia corpus. The report discusses the challenges of solving LDA and the use of inference methods such as Markov Chain Monte Carlo and Variational Inference. Preprocessing techniques, improved algorithms, and early stopping runtimes are explored to achieve better performance. The setup instructions include options for setting up the Python environment using Docker or locally, as well as getting the repository from GitHub. Various commands are provided to run different models on test or production data.\"],\n",
       " 'https://github.com/889884m/DSC180_Project': ['# Locating Sound with Machine Learning\\n\\nSite URL: https://889884m.github.io/DSC180_Project/\\n\\nMost of the project code was actually built using Jupyter Notebooks, so the latest working code would be found in the `Notebooks` folder. Here the latest figures and hyperparameter tuning can be found. Demonstrations of the Neural Net, Support Vector Machine, and Random Forest are here.\\n\\nThe code is run via the command `python run.py test`. This runs the baseline model on the test data, which is simply the normal data but randomized.\\n\\nProject code with working models can be found in `src` folder. Here the code for data generation and the test data can be found. This is also where the model code can be found which is in the `prediction` folder. Here, the code for the feed-forward Neural Net and the SVM can be found.\\n\\nTo build the `docker build -t <tag_name> .` which gives a local docker container with the libraries scipy, numpy, pandas, pytorch, and sklearn.',\n",
       "  'The project code for locating sound using machine learning is available on the provided site URL. The latest working code can be found in the \"Notebooks\" folder, which includes figures and hyperparameter tuning. Demonstrations of the Neural Net, Support Vector Machine, and Random Forest are also available there.\\n\\nTo run the code, use the command `python run.py test`, which runs the baseline model on randomized test data.\\n\\nThe source code for data generation, test data, and models can be found in the \"src\" folder. The model code is located in the \"prediction\" folder, where you can find the feed-forward Neural Net and SVM implementations.\\n\\nTo build a local docker container with necessary libraries (scipy, numpy, pandas, pytorch, and sklearn), use the command `docker build -t <tag_name> .`.'],\n",
       " 'https://github.com/sunqiaochen/NeurlPS_2022_DSC180': ['# NeurlPS_2022_DSC180\\nIn this project, dataset we used: https://drive.google.com/drive/u/0/folders/10gwW55-9xQyAvJh7KHR1uJqYSpDBW0iL\\nhttps://drive.google.com/file/d/1TmPrp74d2y77FCu3Fv0u_vlOP7HvLxkX/view?usp=sharing\\n\\nNowadays, human activities such as wildfires and hunting have become the largest factor that would have serious negative effects on biodiversity. In order to deeply understand how anthropogenic activities deeply affect wildlife populations, field biologists utilize automated image classification driven by neural networks to get relevant biodiversity information from the images. However, for some small animals such as insects or birds, the camera could not work very well because of the small size of these animals. It is extremely hard for cameras to capture the movement and activities of small animals. To effectively solve this problem, passive acoustic monitoring (PAM) has become one of the most popular methods. We could utilize sounds we collect from PAM to train certain machine learning models which could tell us the fluctuation of biodiversity of all these small animals. The goal of the whole program is to test the biodiversity of these small animals (most of them are birds). However, the whole program could be divided into plenty of small parts. I and Jinsong will pay attention to the intermediate step of the program.\\n\\nThe goal of our project is to generate subsets of audio recordings that have higher probability of vocalization of interest, which could help our labeling volunteer to save time and energy. The solutions could help us reduce down the amount of time and resources required to achieve enough training data for species-level classifiers. We perform the same thing with AID_NeurIPS_2021. Only the data is different between these two github. For this github, we use the peru data instead of Coastal_Reserve data.\\n\\n\\n',\n",
       "  'The project aims to understand how human activities impact wildlife populations, particularly small animals like insects and birds. To overcome the limitations of cameras in capturing their movements, passive acoustic monitoring (PAM) is used to collect sounds for training machine learning models. The project focuses on generating subsets of audio recordings with a higher probability of vocalization of interest, reducing the time and resources needed for species-level classifiers. The dataset used is different from a previous project, using Peru data instead of Coastal Reserve data.'],\n",
       " 'https://github.com/EdmundoZamora/TweetyNet_CUDA_GPU_Adaptation': ['### Binary classifies bird vocalizations in a wav files\\n\\n### Stores wav and csv data in data/raw/ it outputs results in data/out/\\n\\n### Takes in raw wave files, trains and outputs best weights and performance and data evaluation(labeling). \\n\\n### Run entire project with: python run.py data features model evaluate nips  : deletes data directory and recreates each time the above command is ran. To supress future dependent library version warnings : python -W ignore run.py data features model evaluate nips :\\n\\n### If data is already downloaded, spare your self the wait using : python run.py data skip features model evaluate nips: including skip in the targets skips the data downloading step. To supress future dependent library version warnings : python -W ignore run.py skip features model evaluate nips :\\n\\n\\nwebsite: [DSC180-A09-Eco-Acoustic-Event-Detection](https://edmundozamora.github.io/DSC180-A09-Eco-Acoustic-Detection)\\n\\nlink to CPU Model Repository: [CPU Model Repo](https://github.com/EdmundoZamora/Q1-Project-Code)\\n',\n",
       "  'This project is about binary classification of bird vocalizations in WAV files. It stores the WAV and CSV data in the \"data/raw/\" directory and outputs the results in the \"data/out/\" directory. The project takes raw wave files as input, trains a model, and outputs the best weights, performance, and data evaluation. The entire project can be run using the command \"python run.py data features model evaluate nips\". If the data is already downloaded, it can be skipped using the command \"python run.py data skip features model evaluate nips\". More information about this project can be found on the website [DSC180-A09-Eco-Acoustic-Event-Detection](https://edmundozamora.github.io/DSC180-A09-Eco-Acoustic-Detection). The CPU Model Repository can be accessed at [CPU Model Repo](https://github.com/EdmundoZamora/Q1-Project-Code).'],\n",
       " 'https://github.com/UCSD-E4E/Pyrenote': [\"## Pyrenote, The E4E Manual Audio Labeling System\\n\\nThis project, Pyrenote, creates moment to moment or strong labels for audio data. Pyrenote and much of this README are based on heavily on [Audino](https://github.com/midas-research/audino) as well as [Wavesurfer.js](https://github.com/katspaugh/wavesurfer.js). The name is a combination of Py, Lyrebird, and note (such as making a note on a label).\\n\\nIf you want to use Pyrenote, use the following to get started!\\n\\n**NOTE**: Before making any changes to the code, make sure to create a branch to safely make changes. Never commit directly to main or production branch.\\n**Read github_procedures.md for more detailed information before contributing to the repo.** \\n\\n## Usage\\n\\n*Note: Before getting the project set up, message project leads for env file. This file should be put in `/audino`. **Make sure the file is never pushed to the github***\\n\\nPlease install the following dependencies to run `Pyrenote` on your system:\\n\\n1. [git](https://git-scm.com/) *[tested on v2.23.0]*\\n2. [docker](https://www.docker.com/) *[tested on v19.03.8, build afacb8b]*\\n3. [docker-compose](https://docs.docker.com/compose/) *[tested on v1.25.5, build 8a1c60f6]*\\n\\n### Clone the repository\\n\\n```sh\\n$ git clone https://github.com/UCSD-E4E/Pyrenote.git\\n$ cd audino\\n```\\n\\n**Note for Windows users**: Please configure git to handle line endings correctly as services might throw an error and not come up. You can do this by cloning the project this way:\\n\\n```sh\\n$ git clone https://github.com/UCSD-E4E/Pyrenote.git --config core.autocrlf=input\\n```\\n\\n### For Development (Note this is the one we will test on and use)\\n\\nSimilar to `production` setup, you need to use development [configuration](./docker-compose.dev.yml) for working on the project, fixing bugs and making contributions.\\n**Note**: Before proceeding further, you might need to give docker `sudo` access or run the commands listed below as `sudo`.\\n\\n**To build the services (do this when you first start it), run:**  \\n**Note**: Remember to cd into audino before starting\\n```sh\\n$ docker-compose -f docker-compose.dev.yml build\\n```\\n\\n**To bring up the services, run:**\\n```sh\\n$ docker-compose -f docker-compose.dev.yml up\\n```\\nThen, in browser, go to [http://localhost:3000/](http://localhost:3000/) to view the application.\\n\\n**To bring down the services, run:**\\n\\n```sh\\n$ docker-compose -f docker-compose.dev.yml down\\n```\\n## Troubleshooting for starting docker\\n\\n1) Docker containers do not even get a chance to start\\n  - Make sure docker is set up properly\\n  - Make sure docker itself has started. On Windows, check the system tray and hover over the icon to see the current status. Restart it if necessary\\n2) Backend crashes\\n  - For this error, check the top of the log. It should be complaining about /r characters in the run-dev.sh files\\n  - The backend will crash if the endline characters are set to CRLF rather than LF\\n  - On VSCode, you can swap this locally via going into the file and changing the CRLF icon in the bottom right to LF\\n  - Do this for `frontend/scripts/run-dev.sh` and `backend/scripts/run-dev.sh`\\n3) Database migration issues\\n  - If the backend complains about compiler issues while the database migration is occurring go into `backend/scripts/run-dev.sh`\\n  - On line 25, check and make sure that the stamp command is pointing to the right migration for the database\\n      - Ask for help on this one\\n\\n## Getting Started\\n\\nAt this point, the docker should have gotten everything set up. After going to [http://localhost:3000/](http://localhost:3000/) you should be able to log into the docker\\n\\nTo access the site, sign in with the username of **admin** and password of **password**. On logging in, navigate to the admin-portal to create your first project. Make sure to make a label group and some labels for the project!\\n\\nAfter creating a project, get the API key by returning to the admin portal. You can use the API key to add data to a project. Create a new terminal (while docker is running the severs) and cd into `audino/backend/scripts`. Here use the following command:\\n\\n```\\npython upload_mass.py --username admin.test --is_marked_for_review True --audio_file C:\\\\REPLACE\\\\THIS\\\\WITH\\\\FOLDER\\\\PATH\\\\TO\\\\AUDIO\\\\DATA --host localhost --port 5000 --api_key REPLACE_THIS_WITH_API_KEY\\n```\\nMake sure to have a folder with the audio data ready to be added. For testing purposes, get a folder with about 20 clips. \\n\\nOnce that runs, you are ready to start testing!\\n\\n\\n### For Production (Don't use on windows)\\n\\nYou can either run the project on [default configuration](./docker-compose.prod.yml) or modify them to your need.  \\n**Note**: Before proceeding further, you might need to give docker `sudo` access or run the commands listed below as `sudo`.  \\n**Note**: Remember to cd into audino before starting  \\n\\n**To build the services, run:**\\n\\n```sh\\n$ docker-compose -f docker-compose.prod.yml build\\n```\\n\\n**To bring up the services, run:**\\n\\n```sh\\n$ docker-compose -f docker-compose.prod.yml up\\n```\\n\\nThen, in browser, go to [http://0.0.0.0/](http://0.0.0.0/) to view the application.\\n\\n**To bring down the services, run:**\\n\\n```sh\\n$ docker-compose -f docker-compose.prod.yml down\\n```\\n\\n### For Dev Team:\\nFeatures should be turned on and off by admins for individual projects. When adding a new feature to either a project's data page or\\nannotation page, make sure to do the following:\\n1) Go to `.\\\\audino\\\\frontend\\\\src\\\\containers\\\\forms\\\\featureForm.js`\\n2) Add a new item in the featuresEnabled directory. This will be the name of the `feature_toggle` variable. \\n3) Return to the page you are working on. \\n  - For example, if you are working on the annotation page, navigate to the `componentDidMount()` method\\n  - about 20 lines down in the `setState` callback, add to the list `SOME_VAR: response.data.features_list['VARIABLE_NAMED_IN_STEP_2']`.\\n\\n  \",\n",
       "  'Pyrenote is an audio labeling system that creates moment to moment or strong labels for audio data. It is based on Audino and Wavesurfer.js. To use Pyrenote, you need to install git, docker, and docker-compose. You can clone the repository and set up the project using the provided instructions. There are separate setups for development and production environments. Troubleshooting tips are also provided. Once the setup is complete, you can access the application in your browser and log in with the username \"admin\" and password \"password\". You can create projects, label groups, and labels in the admin portal. To add data to a project, you need to use an API key and run a Python script provided in the instructions. The summary also mentions how features can be enabled or disabled for individual projects by admins.'],\n",
       " 'https://github.com/RuojiaTao/Covid-Misinformation-Spread-on-Tweets': [\"# Covid-Misinformation-Spread-on-Tweets\\nAnalyze the misinformation about covid spreading in tweet\\n\\nOur website: https://ruojiatao.github.io/Covid-Misinformation-Spread-on-Tweets/\\n\\nAbstract: \\nSpread of misinformation over social media posts challenges to daily information intake and exchange. Especially under current covid 19 pandemic, the disperse of misinformation regarding to covid 19 diseases and vaccination posts threats to individuals' wellbeings and general publich health. This project seeks to invertigate the spread of misinformation over social media (Twitter) under covid 19 pademic. The first topic is the effect of bot users on the spread of misinformation, and the second topic is to examine users' attitude towards misinformation. These two topics are analyze under the social structure (connected social graphs) created through user's interactions on Twitter. This project also seeks to invertigate the change in proportion of bot users and users' attitude towards misinformation as it's approaching to the center of the social network. \\n\\n\\n\\n - The `run.py` file run all part of code\\n - `Data_Pre` folder contains: \\n     - `hydrate_tweets_twarc.py` : hydrating the data\\n     - `EDA.py`: explore the raw data and try to find interesting points\\n - `K_Core` folder contains:\\n     - `K_Core_Degree.py`: set up a graph and calculate the K-Core degree , assign K Core degree back to all data, and plot out the spread of K-Core.\\n     - `sample_dataset.py` : sample a small dataset for analzye\\n     - `Tests_result.py` : run a linear machine learning model to predict the negative sentiment based on different parapmeters. Base on the coefficient of ML model to find the relstionship between parpameters and negative sentiment.\\n - `Bot_User` folder contains: \\n     - `bot_detection.py`: use Botomerter to predict if a user is bot or not\\n     - `Bot_merge.py`: merge predictions to K Core results, generating graphs for percentage of bot useres in each K core degrees\\n - `NLP` folder contains:\\n     - `NLP.py`: get the sentiment scores of each tweets, and plot out overall trends\\n     - `NLP_ab_testing`: run a 2 sample t-test to see if there is an sigificant difference between negative sentiments in different pair of groups that has different K-Core degrees\\n - `data` folder contains:\\n     - `iffy.csv` used for checking misinformation links\\n\\n\\n\",\n",
       "  \"This project aims to analyze the spread of misinformation about COVID-19 on Twitter. It focuses on two main topics: the impact of bot users on the spread of misinformation and users' attitudes towards misinformation. The analysis is conducted using social structure data created from user interactions on Twitter. The project also investigates the change in proportion of bot users and users' attitudes as they move closer to the center of the social network. The code files in different folders perform various tasks such as data preprocessing, graph analysis, bot detection, sentiment analysis, and hypothesis testing.\"],\n",
       " 'https://github.com/davamini/DSC180B_Reddit_MisInfo_Capstone': ['# DSC180B Subreddit Misinformation Capstone Project\\nAnalyzing the spread of misinformation in the Reddit platform.<br>\\nWebsite URL: https://davamini.com/dsc180_project.github.io/index.html\\n<br>\\n### Build Instructions:\\n```sh\\npip install pandas praw gspread oauth2client\\n```\\n> Must create conf.json and google_sheets_creds.json in the <b>config directory</b>.<br>\\n> * conf.json must contain values for client_secret, and client_id, which are aquired from Reddit after creating an application.<br>\\n> * google_sheets_creds.json is aquired from Google Cloud after enabling APIs for google sheets.<br>\\n#### Run either:\\n```sh\\npython run.py\\n```\\n#### Or:\\n```sh\\npython run.py test\\n```\\n',\n",
       "  'This project analyzes the spread of misinformation on the Reddit platform. To build and run the project, you need to install pandas, praw, gspread, and oauth2client using pip. Additionally, you must create conf.json and google_sheets_creds.json files in the config directory. The conf.json file should contain values for client_secret and client_id obtained from Reddit after creating an application. The google_sheets_creds.json file is obtained from Google Cloud after enabling APIs for Google Sheets. To run the project, use either \"python run.py\" or \"python run.py test\".'],\n",
       " 'https://github.com/fieryashes/DSC180B_Misinformation_Project': ['# DSC180B Misinformation Project\\nProject Website: https://anaaamika.github.io/DSC180B-Misinformation/\\n\\n## Build Instructions\\nEnsure that you have twarckeys.py and youtubekeys.py files in your secrets folder. The twarckeys.py file should contain the variables consumer_key, consumer_secret, access_token, access_token_secret populated from the project keys and tokens from [the Developer Portal for the Twitter API](https://developer.twitter.com/en/portal/dashboard). The youtubekeys.py file should contain the variable api_key populated with a API key for [the YouTube Data API v3](https://developers.google.com/youtube/v3). \\n\\nTo build the project run the following commands: \\n`python run.py data` This will populate the *data* folder with tweet datasets, YouTube video ids dataset, and datasets with YouTube metadata. \\n\\nThen run `python run.py analysis` to view preliminary analysis on the YouTube captions and metadata.\\n\\nFinally, run `python run.py model` to build a misinformation detection model based on the YouTube transcripts. \\n',\n",
       "  'The DSC180B Misinformation Project provides a project website with build instructions. To build the project, ensure that you have the necessary files in your secrets folder and run specific commands using Python. The commands include populating the data folder with tweet datasets, YouTube video ids dataset, and datasets with YouTube metadata, viewing preliminary analysis on YouTube captions and metadata, and building a misinformation detection model based on YouTube transcripts.'],\n",
       " 'https://github.com/Bryan-Az/DSC-Capstone-Result-Replication': [\"## DSC-Capstone-Result-Replication\\n### DSC180A Quarter 1 and 2\\n\\nWebsite Link: http://www.bambriz.me/wp-content/uploads/2022/03/dsc180b.html\\n\\nWebsite code is included in the 'docs' directory of this repo.\\n\\nTo run the GENConv GNN model, create docker pod using jmduarte/capstone-particle-physics-domain:latest and group key to access data folder.\\n\\nThen clone this repo and cd into root folder.\\n\\nIn the root folder, you can use python run.py [train, test] to produce a dataframe with predictions and other data useful for visualization in the src/analysis/evaluation.ipynb notebook. \\n\\nTo visualize, you can simply run all cells in the eval. notebook.\\n\\nPls. ignore the loading bar in terminal it doesn't mean anything - but training it will take a while. \\n\\nThere is really no need to run the training since the testing script loads presaved weights (stored in github) to initialize the pre-trained model for testing.  \\n\",\n",
       "  'This is a summary of the instructions for replicating the results of the DSC-Capstone project. The website link provided contains the necessary code and documentation. To run the GENConv GNN model, a docker pod needs to be created using a specific image. After cloning the repository and navigating to the root folder, running a Python script will generate a dataframe with predictions and other data for visualization. The evaluation notebook can be used to visualize the results. It is mentioned that the loading bar in the terminal does not indicate progress, and training may take some time. However, running the training script is not necessary as pre-trained weights are already available for testing.'],\n",
       " 'https://github.com/UCSDJLEE/DSC180B-A11-Project': ['# DSC180B-A11-Project\\n## Particle Jet Mass Regression\\n\\n<img src=\\'https://raw.githubusercontent.com/isacmlee/particle-physics-visuals/main/images/cern_atlas.jpeg\\'>\\n\\n\\nWith jet data collected from proton-proton collision, the contents in this repository aim to predict the mass of particle jet, which is an important feature in classifying the types of those jets.\\nThe model is implemented based on PyTorch Neural Network APIs and gets fitted to training sets available on DSMLP server.\\n\\n#### Repository Structure:\\n\\n - `conf`: directory in which all necessary variables needed for configuration in development are stored\\n - `data`: temporary directory that explains the source of our data\\n - `notebooks`: directory in which .ipynb notebooks for EDA and model evaluation is stored\\n - `src`: source directory for all library codes used for this project\\n - `run.py`: main script to run within properly set-up environment using `python3 run.py train` for model training or `python3 run.py test` for model assessment\\n - `simplenetwork_best.pt`: PyTorch-based file that stores weights of optimized, or best \"fitting,\" model\\n',\n",
       "  'This repository aims to predict the mass of particle jets using jet data collected from proton-proton collision. The model is implemented using PyTorch Neural Network APIs and is fitted to training sets available on the DSMLP server. The repository structure includes directories for configuration variables, data sources, notebooks for exploratory data analysis and model evaluation, source code, and a main script for training and testing the model. The repository also includes a file that stores the weights of the optimized model.'],\n",
       " 'https://github.com/shonepatil/GNN-Spotify-Recommender-Project': [\"# Graph Neural Networks for Song Recommendation on Spotify Playlists\\n\\nThis project tackles the task of creating meaningful and accurate song recommendations to Spotify Playlists by using Graph Neural Networks. The goal is to better capture the characteristics of songs by analyzing co-occurence of song pairs across thousands of playlists in the form of a graph.\\n\\nTo obtain the spotify playlist data, visit https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge and put the files in `data/playlists`. You will have to create these subfolders. Confirm data paths for future files in `config/data-params`. To add song features and create graph from scratch if you only have the playlist data, set `create_graph_from_scratch` to be `True` in data-params. For obtaining song features using the Spotify API, put secret key and secret id in `spotifyAPI_script.py` within `/src/api`. Also within data-params, we suggest setting `playlist-num` to be 1000 such that the Spotify API requesting only takes 15-20 minutes. If you set it to 10000 for a larger dataset, the code may take 4 hours.\\n\\nTo run the GraphSAGE based model on the Spotify Playlist data, first use this command from the root folder: `python run.py data model`. You can leave out the `model` argument if you don't want to train the model from scratch. Next to see and create recommendations open and run through the notebook `Recommender Demonstration.ipynb` from within `src/dgl_graphsage/`.\\n\\nTo customize the model parameters, edit `config/model-params`.\\n\",\n",
       "  'This project focuses on using Graph Neural Networks to improve song recommendations on Spotify playlists. The approach involves analyzing the co-occurrence of song pairs across thousands of playlists in the form of a graph. To obtain the necessary data, visit a specific website and follow the instructions for downloading and organizing the files. Additionally, there are suggestions for setting certain parameters to optimize performance. To run the model, use a specific command from the root folder, and to customize the model parameters, edit a specific configuration file.'],\n",
       " 'https://github.com/yangshengaa/dynamic_stock_industry_classification': [\"# Dynamic Stock Industrial Classification\\n\\nUse graph-based analysis to re-classify stocks and experiment different re-classification methodologies to improve Markowitz portfolio optimization performance in the low-frequency quantitative trading context.\\n\\nNote that for strategy confidentiality, many files are hidden.\\n\\nTo accommodate speedy development, the current code structure simplicity is sacrificed. This will be addressed in later versions.\\n\\nProject Website: [Dynamic Stock Industrial Classification](https://yangshengaa.github.io/dynamic_stock_industry_classification/)\\n\\n## Module Breakdown\\n\\nThis project contains the following six modules:\\n\\n- [data ingestion](src/data_ingestion): address finance data I/O and handle storage of intermediate results;\\n- [factor generation](src/factor_generation): compute and store factors alpha factors and risk factors for low-frequency trading;\\n- [backtest](src/backtest): low-frequency backtest framework (both factors and signals). Factors have continuous values on each cross section whereas signals have only -1, 0, and 1 overall;\\n- [factor combination](src/factor_combination): combine factors using ML models;\\n- [portfolio optimization](src/portfolio_optimization): Markowitz portfolio optimization, with turnover, industrial exposure, style exposure, and various other constraints.\\n- [graph cluster](src/graph_cluster): experiment different graph-based clustering on stocks.\\n\\n## Data\\n\\nChina A-Share stocks, the corresponding major index data (sz50, hs300, zz500, zz1000), and the member stock weights from 20150101 to 20211231, provided by [Shanghai Probability Quantitative Investment](http://www.probquant.cn/).\\n\\n## Experiment Results\\n\\nWith a fixed predicted ML results, we go through the optimization pipeline to optimize each trained classification.\\n\\n**Stock Pool**: zz1000 member stocks  \\n**Benchmark**: zz1000 index  \\n**Time Period**: 20170701 - 20211231  \\n\\n| Model | AlphaReturn (cumsum) | AlphaSharpe | AlphaDrawdown | Turnover |\\n| ----- | :---------------------: | :----------: | :-----------: | :------: |\\n| LinearRegressor | 71.58 | 1.92 | -19.84 | **1.01** |\\n| LgbmRegressor | 145.64 | **3.65** | **-11.58** | 1.21 |\\n| LgbmRegressor-opt | **146.73** | 2.96 | -29.79 | 1.11 |\\n| .. | .. | .. | .. | .. | .. |\\n| 40-cluster PMFG Unfiltered Spectral | 154.45 | 3.15 | **-22.69** | 1.11 |\\n| 10-cluster PMFG Filtered Average Linkage | 160.95 | **3.32** | -26.77 | 1.11 |\\n| 30-cluster AG Unfiltered Sub2Vec | 160.96 | 3.24 | -23.05 | 1.10 |\\n| 5-cluster MST Unfiltered Sub2Vec | 163.26 | 3.27 | -27.39 | 1.11 |\\n| **20-cluster PMFG Filtered Node2Vec** | **164.68** | 3.30 | -27.06 | 1.11 |\\n\\nCompared to the original optimization result, we observe a 12.23% improvement in excess return and 12.16% improvement in excess Sharpe ratio.\\n\\nSince factors based on price and volume lost their predictive power staring from 20200701, we also look at the performances before that time.\\n\\n**Time Period**: 20170701 - 20200701\\n\\n| Model | AlphaReturn (cumsum) | AlphaSharpe | AlphaDrawdown | Turnover |\\n| ----- | :---------------------: | :----------: | :-----------: | :------: |\\n| LgbmRegressor | 150.64 | 6.06 | **-4.59** | 1.23 |\\n| LgbmRegressor-opt | **170.31** | 5.43 | -6.76 | 1.12 |\\n| .. | .. | .. | .. | .. | .. |\\n| 10-cluster PMFG Filtered Sub2Vec | 173.10 | 5.49 | **-5.51** | 1.12 |\\n| 5-cluster MST Filtered Sub2Vec | 182.89 | 5.78 | -7.14 | 1.12 |\\n| 10-cluster AG Filtered Sub2Vec | 181.50 | 5.64 | -7.40 | 1.12 |\\n| **20-cluster PMFG Filtered Node2Vec** | **184.21** | **5.85** | -6.42 | 1.12 |\\n\\nIn this period, we observe a 8.16% improvement in excess return and a 7.73 improvement in excess Sharpe ratio, compared to the original optimization result.\\n\\nFor a complete list of results, check out [summary_20170701_20211231.csv](out/res/signal_test_file_20220305_long_experiment/summary.csv) and [summary_20170701_20200701.csv](out/res/signal_test_file_20220305_short_experiment/summary.csv). And more details are discussed on the project website listed above.\\n\\n## Environment\\n\\nTo run codes in this project, it is recommended to create an environment listed in the [environment.yml](environment.yml). If conda is installed, run:\\n\\n```bash\\nconda env create -f environment.yml\\nconda activate finance-base\\n```\\n\\nAlternatively, one could also pull the corresponding docker image from [yangshengaa/finance-base](https://hub.docker.com/repository/docker/yangshengaa/finance-base) and then activate the finance-base environment using the latter conda command.\\n\\n## Quick Start\\n\\nIt's very easy to use this platform!\\n\\nTips:\\n\\n- run each module at a time, and run the following command sequentially;\\n- change config for corresponding module in respective files (file location indicated inside [run.py](run.py));\\n- detailed running instructions, including a walkthrough of parameters in each modules, are in README of each module.\\n\\nTo run each module, in current directory:\\n\\nFactor Generation:\\n\\n- factor generation: `python run.py gen`\\n\\nBacktest:\\n\\n- backtest factor: `python run.py backtest_factor`\\n- backtest signal: `python run.py backtest_signal`\\n\\nFactor Combination:\\n\\n- factor combination: `python run.py comb`\\n\\nPortfolio Optimization:\\n\\n- generate factor returns: `python run.py opt_fac_ret`\\n- estimate covariance matrices: `python run.py opt_cov_est`\\n- adjust weight: `python run.py opt_weight`\\n\\nGraph Clustering:\\n\\n- train graph clustering: `python run.py cluster_train`\\n\\nTo run each submodules, in current directory:\\n\\n- generate pairs factors: `python run.py pairs`\\n- generate risk factors: `python run.py gen_risk`\\n\\nCurrently risk attribution module is very slow and suboptimal. To be addressed later.\\n\\n## Acknowledgement\\n\\nSpecial thanks to coworkers and my best friends at [Shanghai Probability Quantitative Investment](http://www.probquant.cn/): Beilei Xu, Zhongyuan Wang, Zhenghang Xie, Cong Chen, Yihao Zhou, Weilin Chen, Yuhan Tao, Wan Zheng, and many others. This project would be impossible without their data, insights, and experiences.\\n\\n## For Developer\\n\\nLog known issues here:\\n\\n- signals given by factor test could not give the same alpha returns (slightly less) as in signal test\\n  - examine output holding stats\\n- plain risk attribution\\n\",\n",
       "  'The Dynamic Stock Industrial Classification project aims to improve the performance of Markowitz portfolio optimization in low-frequency quantitative trading. It uses graph-based analysis to re-classify stocks and experiment with different re-classification methodologies. The project consists of six modules, including data ingestion, factor generation, backtesting, factor combination, portfolio optimization, and graph clustering. The data used in the project includes China A-Share stocks, major index data, and member stock weights. The experiment results show improvements in excess return and excess Sharpe ratio compared to the original optimization results. The project provides an environment setup guide and quick start instructions for running the modules. Acknowledgements are given to coworkers and friends who contributed to the project.'],\n",
       " 'https://github.com/MarthaY01/hdsi_faculty_tool/tree/main': ['# HDSI Faculty Exploration Tool\\n\\nThis repository contains project code for experimenting with LDA for Faculty Information Retrieval System.\\n\\nThis tool is now deployed @ https://hdsi-faulty-tool.herokuapp.com/\\n\\n## Running the Project\\n* All of the below lines should be run within a terminal:\\n\\n* Before running any of the below commands, launch the docker image by running `launch.sh -i duxiang/dsc180a:latest`\\n\\n* To get the preprocessed data file, run `python run.py process_data`\\n* To get the fitted sklearn.lda model, run `python run.py model`\\n* To prepare/update the dashboard, run `python run.py prepare_sankey`\\n* To run the live dashboard, run `python run.py run_dashboard`\\n\\n## Using the Dashboard\\n* When executing `run_dashboard`, it will launch dash with a locally hosted port.\\n* It would require port-forwarding on a remote server.\\n\\n# Web Link\\nWebsite: https://marthay01.github.io/hdsi_faculty_tool/\\n',\n",
       "  'This repository contains project code for experimenting with LDA for a Faculty Information Retrieval System. The tool is deployed at https://hdsi-faulty-tool.herokuapp.com/. To run the project, launch the docker image using `launch.sh -i duxiang/dsc180a:latest` and then run various commands such as `python run.py process_data` to get the preprocessed data file, `python run.py model` to get the fitted sklearn.lda model, `python run.py prepare_sankey` to prepare/update the dashboard, and `python run.py run_dashboard` to run the live dashboard. The dashboard can be accessed by executing `run_dashboard`, which will launch dash with a locally hosted port. Port-forwarding may be required on a remote server. The website for this tool can be found at https://marthay01.github.io/hdsi_faculty_tool/.'],\n",
       " 'https://github.com/amuamushu/adv_avod_ssn': ['# AVOD for Single Source Robustness Against Adversarial Attacks.\\nThis project is worked on by Amy Nguyen ([@amuamushu](https://github.com/amuamushu)) and Ayush More ([@ayushmore](https://github.com/ayushmore)) over the course of 20 weeks under the mentorship of Lily Weng. \\n\\nVisual Presentation: https://ayushmore.github.io/2022-03-07-improving-robustness-via-adversarial-training/\\n\\nOral Presentation Slides: https://docs.google.com/presentation/d/1DSoFa1vUQBcjghLfds6ce4sIG0qkBvtL1c2ixlEs7qc/edit?usp=sharing\\n\\n## Setup\\n### Container Setup\\nTo mimic our environment, please build a docker container using the image `amytn/avod-adv:latest`. To prevent memory errors, please ensure your container has **at least 16 GB of RAM** available. For faster runtime, including a GPU may be useful.\\n\\n### Cloning the repository\\nSince this reposity contains a submodule, cloning would require an additional commandline argument. \\n\\nMake sure to clone the reposity in your home directory so the Python paths in the Docker image match up. If the repository is cloned elsewhere, please set up the python path yourself (see [Setting up Necessary Python Paths](#setting-up-necessary-python-paths)).\\n\\n```\\ngit clone --recurse-submodules https://github.com/amuamushu/adv_avod_ssn.git\\n```\\n\\n### The dataset\\nThe dataset we will be using is the [KITTI dataset](http://www.cvlibs.net/datasets/kitti/). For the dataset and mini-batch setup, please follow the download steps listed in the [AVOD repository](https://github.com/kujason/avod#dataset).\\n\\nFor the dataset, we will be follow a slightly different setup to the one on the AVOD repository.\\n\\nIn your home directory, the layout should look like this:\\n\\n```\\nhome\\n..\\\\avod_data\\n....\\\\Kitti\\n......\\\\object\\n........\\\\testing\\n........\\\\training\\n..........\\\\calib # camera calibration (can ignore)\\n..........\\\\image_2 # the 2D images\\n..........\\\\label_2 # true labels and bounding boxes\\n..........\\\\planes \\n..........\\\\velodyne # the lidar point clouds\\n....... train.txt # list of sample names to use for training\\n....... val.txt # list of sample names to use for validation\\n..\\\\adv_avod_ssn # this repository!\\n  \\n```\\nMore information about the true labels can be found here: https://github.com/kujason/avod/wiki/Data-Formats\\n\\n## Run\\n```\\npython3 run.py [clean] [test] [clean-model] [adv-model] [ssn-model]\\n```\\n\\nThese targets can be run one-by-one in the order provided.\\n\\n### `clean` target: \\nDeletes all files in the `outputs` folder.\\n\\n### `test` target: \\nRuns training and inference on test data found under `test/testdata` and writes the predictions and AP scores to the `outputs/<checkpoint_name>` directory. For this target, the checkpoint name is `test_data`.\\n- Note: If you plan on only using the test data and not downloading the full dataset, in `scripts/offline_eval/kitti_native/eval/run_eval.sh`, please update `$prev/avod_data/Kitti/object/training/label_2/` to be `$repo/test/testdata/Kitti/object/training/label_2/`. `run_eval.sh` is used for computing the AP Scores after running inference. \\n\\n### `clean-model` target:\\nRuns training for a clean model and adversarial inference on the full dataset found at `home/avod_data` and writes the predictions and AP scores to the `outputs/<checkpoint_name>` directory. For this target, the checkpoint name is `pyramid_cars_with_aug_simple`.\\n\\n### `adv-model` target:\\nRuns training for an adversarial model and all three types of inference (clean, adversarial, SSN) on the full dataset found at `home/avod_data` and writes the predictions and AP scores to the `outputs/<checkpoint_name>` directory. For this target, the checkpoint name is `test_adv`.\\n- **NOTE**: Since the adversarial model only fine-tunes the clean model, running this target requires the clean model to be completely trained.\\n\\n### `ssn-model` target:\\nRuns training for a clean model and adversarial inference on the full dataset found at `home/avod_data` and writes the predictions and AP scores to the `outputs/<checkpoint_name>` directory. For this target, the checkpoint name is `trainsin_pyramid_cars_with_aug_simple_rand_5`.\\n- **NOTE**: Since the single-source-noise model only fine-tunes the clean model, running this target requires the clean model to be completely trained.\\n\\nNote: `pyramid_cars_with_aug_simple` and `trainsin_pyramid_cars_with_aug_simple_rand_5` are the same checkpoint names that our previous work author Taewan Kim had so we kept it for ease of comparison.\\n\\n## The Shell Script\\nEach shell script is responsible for running the entire experiment for one model. Details about the specific experiments are covered in the paper.\\n\\n**Arguments for training, inference, and evaluation:**\\n\\n`pipeline_config`: Specifies the path to the experiment configurations (batch size, number of steps, learning rate, number of iterations, dataset path, etc).\\n\\n`data_split`: Can be `train`, `val`, or `test`. Whichever keyword is specified determines which samples are used.\\n\\n`output_dir`: Path to where the results and AP scores are written to.\\n\\n`ckpt_indices`: This value specifies which model checkpoint to use for inference. \\n  - Every couple of steps during training, the current trained model is saved to a checkpoint. \\n\\n#### Configuration Files: \\n\\n\\n## Experimental Configurations\\nEach shell script references multiple `.config` files: one for training the model and one or more for inference. These are found under `./avod/configs`.\\n\\n### `model_config`:\\nContains configurations for the object detection model. Notable configurations are:\\n- `model_name`: Should be `avod_model` for all the experiments we run.\\n- `checkpoint_name`: The name of the experiment checkpoint. This determines what folder the outputs are saved to.\\n- `is_adversarial`: A boolean value defaulted to False. Train the model adversarially and runs adversarial inference if True, otherwise train the model normally.\\n- `adv_epsilon`: The epsilon to use for our perturbation. Only used if the model is being trained or running inference adversarially.\\n\\n### `train_config`: \\n- `pretrained_ckpt`: What checkpoint to continue training from. This would be used and should be updated as the model is being fine-tuned.\\n\\n### `eval_config`:\\n- `dataset_dir`: The dataset used for training and infernece. \\n- `pretrained_chkpt`: What trained model checkpoint to use for inference. This is useful if the inference `.config` file is different from the training `.config` file. \\n\\n\\n### Viewing Results\\n#### AP Scores: \\nThe AP scores can be found under `outputs/<checkpoint_name>/offline_eval/results` where the 3 values provided for each test corresponds to easy, medium, and hard respectively.\\n\\n**Here is an exmaple of what the AP Scores look like**:\\n```\\ncar_detection AP: 89.973267 87.620293 80.301704\\ncar_detection_BEV AP: 89.292168 86.383148 79.538277\\ncar_heading_BEV AP: 89.177887 85.891479 78.938744\\ncar_detection_3D AP: 77.332970 67.945251 66.929985\\ncar_heading_3D AP: 77.288345 67.749474 66.543098\\n```\\n\\nAfter inference, if the AP scores are not saved properly, they can be manually calculated and saved again using this command:\\n```\\nbash scripts/offline_eval/kitti_native_eval/run_eval.sh ./outputs/<checkpoint_name>/<prediction_type>/kitti_native_eval/ 0.1_val <training_step> <checkpoint_name>\\n```\\n- `checkpoint_name`: name of the checkpoint as specified in the `.config` file\\n- `prediction_type`: would be ...\\n   - `prediction` if inference results on clean data is wanted\\n   - `prediction_adv` if inference results on adversarial data is wanted\\n   - `predictions_sin_rand_5.0_5` if inference results on SSN data is wanted\\n\\n\\n#### Visualization with bounding boxes, IoU scores, and confidence level: \\nRun the below code to generate bounding boxes on top of the images used during inference. Images will be saved to `outputs/<checkpoint_name>/predictions/images_2d`\\n\\n```\\npython3 demos/show_predictions_2d.py <checkpoint_name>\\n```\\n\\n**Here is an example of a generated image**:\\n![000152](https://user-images.githubusercontent.com/35519361/152208158-833ca90f-911a-4ab5-a846-e167cfc2e1a3.png)\\n\\n\\n\\n## References\\nThis project builds off of Kim, Taewan and Ghosh, Joydeep\\'s work on \"Single Source Robustness in Deep Fusion Models.\" In their GitHub repository (https://github.com/twankim/avod_ssn), they incorporate single source noise into the inputs of the AVOD 3D object detection model. We expand on their work and code by incorporating adversarial noise, rather than Gaussian noise, into the input images.\\n```\\n@inproceedings{kim2019single,\\n  title={On Single Source Robustness in Deep Fusion Models},\\n  author={Kim, Taewan and Ghosh, Joydeep},\\n  booktitle={Advances in Neural Information Processing Systems},\\n  pages={4815--4826},\\n  year={2019}\\n}\\n```\\n\\nWe also relied on the documentation of the original AVOD code (https://github.com/kujason/avod) for setting up the model and data as well as understanding our results.\\n```\\n@article{ku2018joint, \\n  title={Joint 3D Proposal Generation and Object Detection from View Aggregation}, \\n  author={Ku, Jason and Mozifian, Melissa and Lee, Jungwook and Harakeh, Ali and Waslander, Steven}, \\n  journal={IROS}, \\n  year={2018}\\n}\\n```\\n\\nWe referred to the KITTI dataset website (http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d) and its related papers to better understand our dataset.\\n```\\nA. Geiger, P. Lenz and R. Urtasun, \"Are we ready for autonomous driving? The KITTI vision benchmark suite,\" \\n2012 IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 3354-3361, doi: 10.1109/CVPR.2012.6248074. \\nhttp://www.cvlibs.net/publications/Geiger2012CVPR.pdf\\n```\\n\\n\\n## Appendix\\n### Setting Up Necessary Python Paths\\nRun these two commands to set the Python paths for `avod_ssn` and `wavedata`:\\n```\\nexport PYTHONPATH=$PYTHONPATH:\\'<path to avod>\\'\\nexport PYTHONPATH=$PYTHONPATH:\\'<path to wavedata>\\'\\n```\\n\\n### Pretrained Models:\\nSince training takes many hours, we included our pretrained clean model here:\\nhttps://drive.google.com/drive/folders/18U7t-4gU4sXvAD33GEuVnwSwUSItHdPX?usp=sharing\\n',\n",
       "  'This project, called \"AVOD for Single Source Robustness Against Adversarial Attacks,\" was worked on by Amy Nguyen and Ayush More over the course of 20 weeks under the mentorship of Lily Weng. The project aims to improve the robustness of object detection models against adversarial attacks. The code and documentation for the project can be found on their respective GitHub repositories. The project utilizes the KITTI dataset and provides instructions on how to set up the dataset and run the code. It also includes pretrained models that can be used for inference. The project builds upon previous work by Kim and Ghosh on single source robustness in deep fusion models. The paper and code of their work are referenced in this project.'],\n",
       " 'https://github.com/Maderlime/DSC180_Q1_Code': [\"# STEPS TO RUN CODE ON ADVERSARIAL ROBUST TRAINING (QUARTER 2 - DSC 180B)\\n\\n## How to SSH into the DSMLP server\\nssh [user]@dsmlp-login.ucsd.edu\\n\\n## How to build the Docker file\\ndocker build -t test .\\ndocker run -it --rm test /bin/bash\\n  \\n## Deploy a pod with GPU support\\nlaunch-scipy-ml-gpu.sh\\n\\n# LOADING IN DATA\\n\\nLoad in the data from the following source: https://www.dropbox.com/sh/tg6xij9hhfzgio9/AADqu6BMq3Rko7U7-q6vwmMFa?dl=0\\n\\nWe will use the following files for each dataset:\\n- val_test_x_preprocess.npy\\n- val_test_y.npy\\n\\nMake a folder in for the test and train data for each dataset. Within each of these folders, create two subfolders titled as '0' and '1'. \\n\\nFrom here, go to the file at DSC180_Q1_Code/patch_attacks/data/cxr/make_fast_adversarial_documents.ipynb and run the code in these cells. This will load in the images as Numpy files and partition them into training and test sets. Set the output writing dirctories to the folders you created above. Use a 70/30 split in the ranges in the code based on the size of the dataset. \\n\\n# MAKING ADJUSTMENTS TO THE CODE\\n\\nYou can edit hyperparameters for the FGSM training model in the train_fgsm.py file in src/test. You can edit attack parameters in the evaluate_pgd method on the utils.py file in src/model. You can select the datasets you want to load in from the ones you created above. \\n\\n\\n# Command line prompt to run the code\\npython run.py test\\n\\n# STEPS TO RUN CODE ON ADVERSARIAL ATTACKS (QUARTER 1 - DSC 180A)\\n\\n# Build Overview\\n### Command line prompt to run the code\\n#### python run.py test\\n### Run the larger test/analysis code with\\n#### python run.py analysis\\n\\n\\n## How to SSH into the DSMLP server\\nssh <user>@dsmlp-login.ucsd.edu\\n\\n## How to build the Docker file\\ndocker build -t test .\\ndocker run -it --rm test /bin/bash\\n\\n<!-- docker run -it --rm mjtjoa/dsc180a_quarter1_code bash -->\\n\\n\\ndocker tag test mjtjoa/dsc180a_quarter1_code\\ndocker push mjtjoa/dsc180a_quarter1_code:latest\\n\\n# Cleaning docker\\ndocker system prune\\ndocker system prune -a\\nsudo rm -rf /var/lib/docker\\n\\n### Launching the Docker File  \\nlaunch-scipy-ml-gpu.sh -i mjtjoa/dsc180a_quarter1_code:latest\\n  (if this doesn't work, add -P Always)\\n### Relevant Links\\n  Project Report: https://docs.google.com/document/d/1iQ0lZ_wpxqQXRwwjwKANrwNn6FRDwvasISHf9vpNIHw/edit?usp=sharing\\n  \\n  Project Proposal for Q2: https://docs.google.com/document/d/1d4Z4yS0aSyCMxht0NaaEf_mMxH5KFPg2B8b3rKXyRwk/edit?usp=sharing\\n  \\n  Source Report: https://arxiv.org/pdf/1804.05296.pdf\\n  \\n  Source Repository: https://github.com/sgfin/adversarial-medicine\\n\",\n",
       "  'This document provides steps to run code on adversarial robust training for Quarter 2 of DSC 180B. It includes instructions on how to SSH into the DSMLP server, build the Docker file, deploy a pod with GPU support, and load in data from a Dropbox source. It also explains how to make adjustments to the code, edit hyperparameters and attack parameters, and select datasets. The command line prompt to run the code is provided as well.\\n\\nAdditionally, it briefly mentions steps to run code on adversarial attacks for Quarter 1 of DSC 180A. This includes instructions on how to SSH into the DSMLP server, build the Docker file, and launch the Docker file. Relevant links to project reports and repositories are also provided.'],\n",
       " 'https://github.com/Actionable-Recourse/recourse-api': ['# Recourse API\\n\\nhttps://recourse-api.herokuapp.com/recourse\\n\\nFrom the link above, you can check a list of actions a person can take to be accepted by a Credit Scoring algorithm.\\n\\n## How to set up the environment\\n\\n```pip install virtualenv```\\n```virtualenv venv``` to create your new environment\\n```. venv/bin/activate```\\n```pip install -r requirements.txt``` to install all the required packages\\n\\n## How to run\\n\\nActivate the python environment.\\n```\\n. venv/bin/activate\\n```\\n\\n```\\npython app.py\\n```\\n\\n**To run this in WSL, you might need to run ```wsl --shutdown``` in Powershell. Otherwise, localhost will not be accessible.**',\n",
       "  'The Recourse API provides a list of actions that can help a person be accepted by a Credit Scoring algorithm. To set up the environment, you need to install virtualenv and create a new environment. Then, activate the environment and install the required packages. To run the API, activate the Python environment and run the app.py file. If running in WSL, you may need to run \"wsl --shutdown\" in Powershell for localhost to be accessible.'],\n",
       " 'https://github.com/freebreadstix/capstone_B02': ['# Group B02 Capstone project repo\\n\\nTo run:\\n\\n\\nIf running from DSMLP cluster:\\n```\\nssh user@dsmlp-login.ucsd.edu\\nlaunch-scipy-ml.sh -i freebreadstix/q1-replication\\n```\\nElse be sure to run in container: https://hub.docker.com/repository/docker/freebreadstix/q1-replication\\n\\nThen:\\n```\\ngit clone https://github.com/freebreadstix/capstone_B02.git\\ncd capstone_B02\\n```\\nIf not merged to main, make sure to switch to branch with run.py\\n```\\ngit checkout lucas-runpy\\n```\\n\\nConfigure config yaml with appropriate parameters. You can make your own .yml using config.yml as reference, just pass it as the argument on CLI\\n\\nRun run.py w/ config yaml corresponding to configuration you are running. For testing this is test_config.yml\\n```\\npython3 run.py test_config.yml\\n```\\nLink to Presentation Website\\n```\\nhttps://micmiccitymax.github.io/dsc180b02-site/\\n```\\n\\nExplainations of Config.yml output options\\n```\\nnum_words: how many words are in the \"important words\" for the models\\nsave_predictions: saves output of predictions to a file\\nprint_results: prints results of evaluations to terminal\\nprint words: Prints important words of each model in terminal\\nintersections: computes the important words similarity of all combinations of model and topics, USE ONLY WHEN YOU HAVE ALL MODELS MADE\\ndecision_tree_model: outputs a plotting of decision tree to a figure\\nwordcloud: outputs an important word wordcloud to a figure in the figures folder\\n```\\n\\n**Note**: if you are using intersections, decision_tree_model, or wordcloud options, make sure data is saved as \\'data/processed/general.csv\\' and has columns \\'Original Article Text\\' as the document text, \\'Verdict\\' as \\'TRUE\\' or \\'FALSE\\', \\'Category\\' as category, or change code within old_utils.py\\n\\nProject Organization\\n------------\\n\\n    ├── LICENSE\\n    ├── Makefile           <- Makefile with commands like `make data` or `make train`\\n    ├── README.md          <- The top-level README for developers using this project.\\n    ├── data\\n    │\\xa0\\xa0 ├── external       <- Data from third party sources.\\n    │\\xa0\\xa0 ├── interim        <- Intermediate data that has been transformed.\\n    │\\xa0\\xa0 ├── processed      <- The final, canonical data sets for modeling.\\n    │\\xa0\\xa0 └── raw            <- The original, immutable data dump.\\n    │\\n    ├── docs               <- A default Sphinx project; see sphinx-doc.org for details\\n    │\\n    ├── models             <- Trained and serialized models, model predictions, or model summaries\\n    │\\n    ├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\\n    │                         the creator\\'s initials, and a short `-` delimited description, e.g.\\n    │                         `1.0-jqp-initial-data-exploration`.\\n    │\\n    ├── references         <- Data dictionaries, manuals, and all other explanatory materials.\\n    │\\n    ├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\\n    │\\xa0\\xa0 └── figures        <- Generated graphics and figures to be used in reporting\\n    │\\n    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\\n    │                         generated with `pip freeze > requirements.txt`\\n    │\\n    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported\\n    ├── src                <- Source code for use in this project.\\n    │\\xa0\\xa0 ├── __init__.py    <- Makes src a Python module\\n    │   │\\n    │\\xa0\\xa0 ├── data           <- Scripts to download or generate data\\n    │\\xa0\\xa0 │\\xa0\\xa0 └── make_dataset.py\\n    │   │\\n    │\\xa0\\xa0 ├── features       <- Scripts to turn raw data into features for modeling\\n    │\\xa0\\xa0 │\\xa0\\xa0 └── build_features.py\\n    │   │\\n    │\\xa0\\xa0 ├── models         <- Scripts to train models and then use trained models to make\\n    │   │   │                 predictions\\n    │\\xa0\\xa0 │\\xa0\\xa0 ├── predict_model.py\\n    │\\xa0\\xa0 │\\xa0\\xa0 └── train_model.py\\n    │   │\\n    │\\xa0\\xa0 └── visualization  <- Scripts to create exploratory and results oriented visualizations\\n    │\\xa0\\xa0     └── visualize.py\\n    │\\n    └── tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io\\n\\n\\n--------\\n\\n<p><small>Project based on the <a target=\"_blank\" href=\"https://drivendata.github.io/cookiecutter-data-science/\">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>\\n',\n",
       "  'This is a summary of the Group B02 Capstone project repo:\\n\\n- To run the project, you can either run it from the DSMLP cluster or in a container.\\n- If running from the DSMLP cluster, use the provided command to launch the project.\\n- If running in a container, use the provided Docker image.\\n- Clone the project repository and navigate to it.\\n- If not merged to main, switch to the branch with run.py.\\n- Configure the config.yml file with appropriate parameters.\\n- Run run.py with the corresponding config yaml file.\\n- The presentation website can be accessed at a provided link.\\n- The config.yml file has options for output settings and explanations are provided for each option.\\n- The project organization is described in detail in the README file.'],\n",
       " 'https://github.com/aavelasq/dsc180-Q2sentiment': ['# The Effect of Cancel Culture on Sentiment Over Time\\n\\nThis project analyzes the change in public sentiment towards musicians\\nwho were cancelled on English-speaking Twitter due to socially unacceptable behavior.\\nSpecifically, we looked at how the \\ntype of issue, the background of the artist, and the strength of their\\nparasocial relationship with their fans affected sentiment towards them over time. \\nFor our analysis, we chose to focus on music artists from three different genres: \\nK-Pop, Hip-Hop, and Western Pop. \\nTo measure sentiment, we utilized the \\n[Google Perspective API](https://www.perspectiveapi.com/). \\n\\n## Running the Project\\n- Install dependencies using `pip install -r requirements.txt`\\n\\n- To run the project using test data: run `python run.py test`\\n\\n- To scrape Twitter data: run `python getTweets.py`\\n    - In order to run this script, must obtain valid Twitter API keys and save to a \\n    file named `twitterkeys.py`\\n    - To change artist and timeframe of tweets, change the query attribute in the `query_params` variable\\n    - Saves scraped Twitter data with columns `id, text, author_id, created_at` to `data\\\\raw`\\n\\n- To run the project using real data: run `python run.py data`\\n    - This calls `etl.py` and retrieves data stored in `data\\\\temp` folders. The directory where data is stored can be changed in `data-params.json`\\n\\n- The different sentiment API and library scripts are found in `run.py`.\\n    - To run Google Perpsective API script on Twitter data: run \\n    `python run.py data toxicity`\\n        - Before runnning, must obtain Google Developer API keys to run API script.\\n        - Outputs a dataframe containing toxicity, severe toxicity, insult, \\n        and profanity probability scores and saves to `data\\\\temp`\\n    - To run TextBlob library script on Twitter data: run \\n    `python run.py data polarity`\\n        - Outputs a dataframe with sentiment polarity values and saves to `data\\\\out`\\n    - To run Vader library script on Twitter data: run \\n    `python run.py data vader`\\n        - Outputs a dataframe with sentiment polarity values and saves to `data\\\\out`\\n\\n- To calculate some exploratory statistics and visualizations: run\\n    `python run.py data eda`\\n    - Runs on each individual artist, both canceled and control\\n        - Saves a dataframe containing the number of tweets collected per day \\n        to `data\\\\out`\\n        - Saves visualizations of user activity, toxicity, and polarity over time to `data\\\\out`\\n\\n- To smooth out short-term trends and compute rolling average of sentiment data:\\n    run `python run.py data preprocessing`\\n    - Saves rolling average dataframe to `data\\\\temp`\\n\\n- To generate results for first sub-question (type of issue): \\n    - run `python run.py data typefOfIssue`\\n        - Saves dataframes used later to generate visuals to `data\\\\temp\\\\rq1_type`\\n    - run `python run.py visuals_ti`\\n        - Saves visualizations to `data\\\\out\\\\rq1_type`\\n\\n- To generate results for second sub-question (background of artist): run `python run.py data background`\\n    - Saves dataframes used later to generate visuals to `data\\\\temp\\\\rq_bg2`\\n    - Saves visualizations to `data\\\\out\\\\rq_bg2`\\n\\n- To generate results for third sub-question (parasocial relationships): \\n    - run `python run.py data parasocial`\\n        - Saves dataframes used later to generate visuals to `data\\\\temp\\\\rq3_ps`\\n    - run `python run.py ps_visuals`\\n        - Saves visualizations to `data\\\\out\\\\rq3_ps`',\n",
       "  'This project analyzes the change in public sentiment towards musicians who were cancelled on English-speaking Twitter due to socially unacceptable behavior. The analysis focuses on the type of issue, the background of the artist, and the strength of their parasocial relationship with their fans. The project utilizes the Google Perspective API to measure sentiment and includes steps for scraping Twitter data and running different sentiment analysis scripts. It also generates exploratory statistics, visualizations, and results for specific sub-questions related to the type of issue, background of the artist, and parasocial relationships.'],\n",
       " 'https://github.com/18anguyen9/Single_Cell_Coupled_Autoencoders': ['# Single-Cell-Coupled-Autoencoders\\n\\n&emsp; &emsp; In this project, we implement a coupled autoencoder for working with single-cell data, which includes data sets on DNA, mRNA and protein data.\\n\\n## About\\n\\n&emsp; &emsp; Historically, analysis on single-cell data has been difficult to perform, due to data collection methods often resulting in the destruction of the cell in the process of collecting information. However, an ongoing endeavor of biological data science has recently been to analyze different modalities, or forms, of the genetic information within a cell. Doing so will allow modern medicine a greater understanding of cellular functions and how cells work in the context of illnesses. The information collected on the three modalities of DNA, RNA, and protein can be done safely and because it is known that they are same information in different forms, analysis done on them can be extrapolated understand the cell as a whole. Previous research has been conducted by Gala, R., Budzillo, A., Baftizadeh, F. et al. to capture gene expression in neuron cells with a neural network called a coupled autoencoder. This autoencoder framework is able to reconstruct the inputs, allowing the prediction of one input to another, as well as align the multiple inputs in the same low dimensional representation. In our paper, we build upon this coupled autoencoder on a data set of cells taken from several sites of the human body, predicting from RNA information to protein. We find that the autoencoder is able to adequately cluster the cell types in its lower dimensional representation, as well as perform decently at the prediction task. We show that the autoencoder is a powerful tool for analyzing single-cell data analysis and may prove to be a valuable asset in single-cell data analysis.\\n\\n## How to run this project\\n\\n1. Clone this repository onto your local machine with `git clone https://github.com/18anguyen9/Single_Cell_Coupled_Autoencoders.git` and change into the directory.\\n\\n2. Launch the Docker image for the project with the following line: `launch.sh -i alandnin/method3:latest`\\n\\n3. The code is ran with command line arguments:\\n\\n    * `test`: (`python3 run.py test`) Due to the size of the full data set, this will perform a test run on a much smaller subset of our data sets to simulate the output of our project. \\n    \\n    *  `test-full`: (`python3 run.py test-full`) This a full run of training the coupled autoencoder with the entire data set. This will take much longer (expect 20-30 minutes) than `test`, but will contain meaningful outputs compared to the simulated `test`. However, you will first need to download the entire data set using the following command line argument `aws s3 sync s3://openproblems-bio/public/ $HOME/data/ --no-sign-request` and moving the `cite_gex_processed_training.h5ad` and `cite_adt_processed_training.h5ad` files into the `/data` file of this directory.\\n    \\n    *  `clear-cache`: The training computation is memory expensive. `python3 run.py clear-cache` can be run in the case that your machine runs out of memory. This will only happen when `test-full` is ran.\\n\\n## Related\\n\\nWe based our project off of this NeurIPS competition: https://openproblems.bio/neurips_docs/about/about/\\n\\nOur website: https://18anguyen9.github.io/DSC_180_website/\\n    \\n    \\n    \\n',\n",
       "  'This project implements a coupled autoencoder for analyzing single-cell data, specifically DNA, mRNA, and protein data. The goal is to gain a better understanding of cellular functions and their relationship to illnesses. The autoencoder framework used in this project can reconstruct inputs and align multiple inputs in a low-dimensional representation. The project successfully clusters cell types and performs well in the prediction task. The code can be run using command line arguments for different levels of testing or full training with the entire dataset. The project is based on a NeurIPS competition and more information can be found on their website.'],\n",
       " 'https://github.com/zwcolin/T5_SQuAD_Prompt_Tuning': [\"# On Evaluating the Robustness of Language Models with Tuning\\nAuthors: Colin Wang, Lechuan Wang, Yutong Luo\\n\\nWebsite: https://rachelluoyt.github.io/T5_SQuAD_Prompt_Tuning/\\n## Pipeline for DSC 180B (not normally used by us, but for DSC 180B which asserts a certain format)\\nBuild a container using `zwcolin/180_method5:latest` docker. Clone the repo, then at the root folder, run `python run.py test`. Warning: lots of time may be spent on downloading the data, pretrained model, tokenizer, and preparation. The testing itself may take ~30 minutes (not including downloading and building the dataset) to output evaluation metrics (we've modified the script for you so it just measures the first 10 examples, which may take around 30 seconds to intialize and process). If you do want to see some results, you may want to wait for quite a bit. Alternatively, some existing train/testing logging has been provided inside the the `prompt_tuning` folder. You can take a look at that instead of actually running the code.\\n\\n## Internal Pipeline\\n### Manipulating Model in `run.py`\\n#### Training & Testing\\n- Simply do `bash experiment.sh` and modify any experiment meta info as well as hyperparameter as necessary. The pipeline has been built to suit single/multi GPU configurations under a single server instance.\\n\\n## Deployment\\nA `Dockerfile` has been provided in the root folder to set up a docker environment. Note that this dockerfile has only been experimented at UCSD's DataHub. Use it with caution.\\n\\n## DSC 180B Specific Instructions\\nWe don't strictly follow the structure of the given suggestions, with a `test` folder and a `testdata` folder inside it. It's too rigid. Instead, all the training and test data will be store inside the `data` folder and `experiment.sh` contains all the necessary code to build the model or to test the model for running the model. We don't like the way that you need to run `test.py` with some arguments in the command line because it's obviously not suitable for a deep learning project where there are way many possible arguments (i.e. you will likely have to type `test.py -xx -xx -xx -xx`, repeating `-` for dozens of times).\\n\\n## Reference\\nThe script is based on the following paper:\\n@misc{lester2021power,\\n      title={The Power of Scale for Parameter-Efficient Prompt Tuning}, \\n      author={Brian Lester and Rami Al-Rfou and Noah Constant},\\n      year={2021},\\n      eprint={2104.08691},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n\",\n",
       "  'The authors discuss the evaluation of the robustness of language models with tuning. They provide instructions for building a container and manipulating the model for training and testing. They also mention specific instructions for DSC 180B and provide a reference to the paper they based their script on.'],\n",
       " 'https://github.com/zhw005/DSC180B-Project': ['# DSC180B: Explainable AI\\nThis is a repository that contains code for DSC180B section B06\\'s Q2 Project: Explainable AI.\\n\\n\"build-script\": \"zhw005/dsc180b-project\"\\n\\n## Authors\\n- [Jerry (Yung-Chieh) Chan](https://github.com/JerryYC)\\n- [Apoorv Pochiraju](https://github.com/apochira)\\n- [Zhendong Wang](https://github.com/zhw005)\\n- [Yujie Zhang](https://github.com/yujiezhang0914)\\n\\n## Introduction\\nIn our project, we will be focusing on using different techniques from causal inferences and explainable AI to interpret various machine learning models across various domains. In particular, we are interested in three domains - healthcare, banking, and the housing market. Within each domain, we are going to train several machine learning models first:XGBoost, LightGBM, TabNet, and SVM. And we have four goals in general: \\n1) Explaining black-box models both globally and locally with various XAI methods; \\n2) Assessing the fairness of each learning algorithm with regard to different sensitive attributes; \\n3) Explaining False Negative and False Positive predictions using Causal Inference;\\n4) Generating recourse for individuals - a set of minimal actions to change the prediction of those black-box models.\\n\\n## Running the project\\n\\n target | config | experiment |\\n| :---: | :---: | :---: |\\n| airbnb_features | \\'config/FeatureEng-params-airbnb.json\\' | Do feature engineering for airbnb dataset |\\n| loan_features | \\'config/FeatureEng-params-loan.json\\' | Do feature engineering for loan dataset |\\n| diabetes_features | \\'config/FeatureEng-params-diabetes.json\\' | Do feature engineering for diabetes dataset |\\n| fairness | \\'config/Fairness-example.json\\' | Do fairness evaluation |\\n| FN_FP | \\'config/FN_FP-example.json\\' | Do False Negative and False Positive explanation |\\n| model_explanations | \\'config/Model_Explanations_Example_loan.json\\'| Do model explanations - loan data example|\\n| recourse | \\'config/Recourse-example.json\\'| Generate recourse explanation - loan data example|\\n',\n",
       "  \"This repository contains code for the DSC180B section B06's Q2 Project on Explainable AI. The project focuses on using techniques from causal inferences and explainable AI to interpret machine learning models in healthcare, banking, and the housing market domains. The goals include explaining black-box models, assessing fairness, explaining false negative and false positive predictions, and generating recourse for individuals. The project can be run with different configurations for feature engineering, fairness evaluation, model explanations, and generating recourse explanations.\"],\n",
       " 'https://github.com/TanveerMittal/Feature_Type_Inference_Capstone': [\"# Feature Type Inference Capstone\\n\\n### Team Members: Tanveer Mittal & Andrew Shen\\n### Mentor: Arun Kumar\\n\\n## Resources:\\n- [Torch Hub Release of Pretrained Models](https://github.com/TanveerMittal/BERT-Feature-Type-Inference)\\n    - Allows anyone to load our models in a single line of code using the th PyTorch Hub API\\n- [Tech Report](https://tanveermittal.github.io/capstone/)\\n    - Provides detailed methodology and results of our experiments\\n- [ML Data Prep Zoo](https://github.com/pvn25/ML-Data-Prep-Zoo)\\n    - Provides benchmark data and pretrained models for Feature Type Inference\\n- [Project Sortinghat](https://adalabucsd.github.io/sortinghat.html)\\n\\n## Overview:\\n\\nOne of the first steps in automated data prepration in AutoML platforms is to identify the feature types of individual columns in input data. This information then allows the software to understand the data and then preprocess it to allow machine learning algorithms to run on it. Project Sortinghat frames this task of Feature Type Inference as a machine learning multiclass classification problem. As an extension of Project SortingHat, we worked on applying Bidirectional Encoding Representation Transformer(BERT) models to this task and did further investigations on the effects of adjusting the feature set for input with a random forest model. Our BERT CNN models currently outperform all existing tools currently benchmarked against SortingHat's ML Data Prep Zoo.\\n\\nThis repository includes code for architecture and feature experiments for the transformer models. The results of our 2 released models can be seen in the tables below:\\n\\n- BERT CNN with Descriptive Statistics:\\n    - 9 Class Test Accuracy: **0.934**\\n\\n| Data Type | numeric | categorical | datetime | sentence | url   | embedded-number | list  | not-generalizable | context-specific |\\n|-----------|---------|-------------|----------|----------|-------|-----------------|-------|-------------------|------------------|\\n| **Accuracy**  |   0.983 |       0.972 |        1 |    0.986 | 0.999 |           0.997 | 0.994 |             0.968 |            0.967 |\\n| **Precision** |   0.959 |       0.935 |        1 |    0.849 | 0.969 |           0.989 |  0.96 |             0.848 |             0.87 |\\n| **Recall**    |   0.996 |       0.943 |        1 |    0.859 | 0.969 |           0.949 | 0.842 |             0.856 |            0.762 |\\n\\n- BERT CNN without Descriptive Statistics:\\n    - 9 Class Test Accuracy: **0.929**\\n\\n| Data Type | numeric | categorical | datetime | sentence | url   | embedded-number | list  | not-generalizable | context-specific |\\n|-----------|---------|-------------|----------|----------|-------|-----------------|-------|-------------------|------------------|\\n| Accuracy  |   0.981 |       0.967 |    0.999 |    0.987 | 0.999 |           0.997 | 0.994 |             0.966 |            0.968 |\\n| Precision |   0.958 |       0.917 |    0.993 |    0.853 | 0.969 |            0.99 | 0.959 |             0.869 |            0.854 |\\n| Recall    |   0.992 |       0.941 |        1 |     0.88 | 0.969 |            0.96 | 0.825 |             0.805 |            0.789 |\\n\",\n",
       "  'The team members for the Feature Type Inference Capstone project are Tanveer Mittal and Andrew Shen, with Arun Kumar as their mentor. They have developed a set of resources including pretrained models, a tech report, benchmark data, and a project called Sortinghat. The goal of the project is to identify the feature types of columns in input data for automated data preparation in AutoML platforms. They have applied Bidirectional Encoding Representation Transformer (BERT) models to this task and experimented with adjusting the feature set using a random forest model. Their BERT CNN models outperform existing tools in terms of accuracy. The results of their released models show high accuracy, precision, and recall for various data types.'],\n",
       " 'https://github.com/amelia-kawasaki/dsc_capstone': ['# DSC Capstone: Group B08\\n## Exploring Noise in Data: Applications to ML Models\\n### Models Supported:\\nKernel machines, Random Forests, k-Nearest Neighbor Classification\\n### Building the Project:\\nPlease build the project using the Docker container located at the DockerHub repo in submission.json\\n### Running the Project:\\nTo run on all data:\\n> python3 run.py\\n\\nTo run on all data with a custom config file:\\n> python3 run.py all [json config file]\\n\\nTo run the code on test section of data:\\n> python3 run.py test\\n\\nTo clean all output files:\\n> python3 run.py clean\\n\\nWebsite for an introduction to our project:\\nhttps://amelia-kawasaki.github.io/dsc_capstone/\\n',\n",
       "  'This is a summary of the DSC Capstone project by Group B08. The project explores noise in data and its applications to machine learning models. The supported models include Kernel machines, Random Forests, and k-Nearest Neighbor Classification. The project can be built using the Docker container located at the DockerHub repo mentioned in submission.json. To run the project, you can use the provided commands in the summary. Additionally, there is a website available for an introduction to the project: https://amelia-kawasaki.github.io/dsc_capstone/'],\n",
       " 'https://github.com/edinhluo/DSC180-Capstone-Project': ['# COVID-19 Group Testing Strategies\\n\\n## Abstract\\n\\nThe COVID-19 pandemic that has persisted for more than two years has been combated by efficient testing strategies that reliably identifies positive individuals to slow the spread of the pandemic. Opposed to other pooling strategies within the domain, the methods described in this paper prioritize true negative samples over overall accuracy. In the Monte Carlo simulations, both nonadaptive and adaptive testing strategies with random pool sampling resulted in high accuracy approaching at least 95% with varying pooling sizes and population sizes to decrease the number of tests given. A split tensor rank 2 method attempts to identify all infected samples within 961 samples, converging the number of tests to 99 as the prevalence of infection converges to 1%.\\n',\n",
       "  'This paper discusses COVID-19 group testing strategies that prioritize true negative samples and aim for high accuracy. Monte Carlo simulations show that both nonadaptive and adaptive testing strategies with random pool sampling can achieve at least 95% accuracy. A split tensor rank 2 method is also proposed to identify infected samples within a large population, reducing the number of tests needed.'],\n",
       " 'https://github.com/pnair7/ml-fairness': ['# Patterns of Fairness in Machine Learning\\n\\n[Website Link](https://annemxu.github.io/ml-fairness/)\\n\\n[Report Link](https://raw.githubusercontent.com/pnair7/artifact-directory-template/main/report.pdf)\\n\\nAn empirical analysis of machine learning fairness using a variety of metrics, models, and datasets.\\n\\n## Instructions\\n`python run.py` for full output matrix\\n\\n`python run.py test` to run on just test data\\n\\n### Adding your own data\\nRaw datasets sit in the `rawDatasets/` folder, and can be processed by a script in the `preprocessing/` folder. The output should add a folder to `cleanedDatasets/` containing two elements: a JSON config file (see folder for example format) and a CSV file of the cleaned dataset. Input columns should all be numerical, and output columns should be 0/1 for binary classification. (If your data is already cleaned to these specifications, it can be placed directly in the `cleanedDatasets` folder, no need to upload the raw dataset or use a preprocessing script.)\\n\\n#### Example Config File\\n\\n```\\n{\\n    \"y_col\": \"refer\",                                    # which column are you predicting?\\n    \"X_cols\": [                                          # which columns are the predictor variables?\\n        \"dem_age_band_18-24_tm1\",\\n        \"dem_age_band_25-34_tm1\",\\n        \"dem_age_band_35-44_tm1\",\\n        ...\\n        \"trig_max-high_tm1\",\\n        \"trig_max-normal_tm1\",\\n        \"gagne_sum_tm1\"\\n    ],\\n    \"group_cols\": [                                      # which column is the protected attribute? is a list for consistency, but just one element\\n        \"race\"\\n    ],\\n    \"prediction_type\": \"binary\",                         # only binary is implemented (not used)\\n    \"dataset_name\": \"Obermeyer Health Dataset\",          # display name for dataset\\n    \"data_path\": \"rawDatasets/obermeyer_data.csv\",       # path to raw dataset (not used)\\n    \"data_script\": \"preprocessing/obermeyer.py\"          # script to preprocess raw data (not used)\\n}\\n```\\n\\n### Adding your own models or metrics\\nAdding your own models or metrics is a little more involved, but still quite simple. \\n\\nCurrently, models are contained in `models/sklearn_models.py`. First, you must write a model function that takes in the same parameters as the other models, which can be located in the same file, or in a different file in the `models` directory. Next, in the `utils/utils.py` file, you must import the model, and add the model to the `run_models` function, assigning it a string name for the model to map to the function. Finally, you must add the string name of the model to the list of models at the top of `run.py`.\\n\\nThe process is nearly identical for metrics. Create a metric function that takes in the same parameters as the other metric functions, add the metric to the `apply_metric` function in `utils/utils.py` with its own string name, and add that string name to the list of metrics in `run.py`.\\n',\n",
       "  'This is a summary of the information provided:\\n\\nThe article discusses an empirical analysis of machine learning fairness using various metrics, models, and datasets. It provides instructions on how to run the analysis and add your own data, models, or metrics. The article also includes an example configuration file for preprocessing raw datasets.'],\n",
       " 'https://github.com/mglevitt/Medical-Disparity-Causal-Analysis': ['## Quality of Life Causal Analysis Project\\n\\nIn this project, we aim to establish causality between various socioeconomic variables and life expectancy outcomes in  roughly 166 different countries, noting the strongest connections between economic and political factors with the length of life expectancy. \\n\\n### How to Use\\n\\nTo run our project just run the following two lines of code in a Unix shell, we utilized Unbuntu 20.04 LTS but it should work for others.\\n```\\ndocker pull mglevitt/world_happiness_project:run_project\\ndocker run mglevitt/world_happiness_project:run_project\\n```\\nThe pull should not take too long, but to run the code may take upwards of half an hour as a result of the many computations being made with PC. The end output should be a dictionary of the relations we found that will be printed to your terminal and all the graphs of the relations we found will pop up on your device as the code is run. Feel free to consult us if you run into any difficulties with getting our code to run properly.\\n\\n#### Run PC on Your Own Data\\n\\nThe pipeline we developed is flexible to work with any data in the correct format to find causal relations present in the data. You can follow the step by step instructions below to run PC on your own datasets. \\n\\n1. In a local terminal, navigate to where you would like to place the repository of our code\\n2. Clone this repository on to your local machine in the destination of your choice and navigate into the repository of our code with: \\n```\\ngit clone https://github.com/mglevitt/Medical-Disparity-Causal-Analysis.git\\ncd .\\\\Medical-Disparity-Causal-Analysis\\\\\\n```\\n3. Install pipenv on your local machine and use it to to install the dependencies needed to run our code with: \\n```\\npip install pipenv\\npipenv install\\n```\\n4. Save the data you would like to reformat to src/data as a csv or xlsx file. \\n5. When adding in your own datasets there are a few prior cleaning steps that may have to be done. For any of our provided datasets that you want to use, this step can be skipped. First, the data must be global time series data identified by country names or country name and year. All datasets must have a column with country names that has \"country\" in the column name. The country names also must follow the same naming conventions as other data that you are merging your data with. If you wish to have an included column with years, then \"year\" must be in the column\\'s name. When combining datasets with a year column, the pipeline will only include years that are in both datasets, so make sure the years are overlapping in their span. \\n6. Open the code of src/scripts/download_data.py with whatever method works from you local terminal or file exploror. Edit the 2 variables after the line \"# Add your own data here\" towards the bottom of the script to have the correct names for your file names in the list for variable name \"new_file_names\", including the extension .csv or .xlsx, and the name for your output file before .csv. Once these variable are edited, delete the \"#\" from before the last line of code then to save the changes to this file.\\n7. Run the code to have your new data file added to src/final_data with your inputted file name with: \\n```\\npipenv run .\\\\src\\\\scripts\\\\download_data.py\\n```\\n8. After all these steps, a table of relations with the most common relations at the top should be outputted in your terminal. \\n9. This step is not neccesary, but for further exploration into the causal relations present you can adjust the signifigance level of PC. Higher signifigance will lead to more relations being present and vice versa. The default signifigance is .2 and this value must be between 0 and 1. The signifigance can be adjusted by editing the value of alpha in the last line of code. \\n',\n",
       "  'This project aims to establish causality between socioeconomic variables and life expectancy outcomes in approximately 166 countries. The strongest connections between economic and political factors with life expectancy are noted. To run the project, two lines of code need to be executed in a Unix shell. The code may take some time to run due to the computations involved. The output will be a dictionary of the found relations, and graphs will be displayed. The pipeline developed for this project can also be used with other datasets by following the provided instructions.'],\n",
       " 'https://github.com/GogoHYX/DSC180_sleep_apnea': [\"# DSC180 Capstone Project\\n## [Project Website](https://gogohyx.github.io/DSC180_sleep_apnea/)\\n### Build instructions\\n\\nIn the home directory, running `python run.py --targets` builds the requisite files.\\n\\n#### Targets:\\n\\n1. `features`: reads raw data files, builds features, joins the files, and stores the columns of resulting dataframe in different files under `data/out`.\\n2. `model`: trains the voting models and saves them under `results/` directory. **RUNNING THIS TAKES A WHILE FOR THE FIRST TIME BECAUSE OF THE IMPORT STATEMENT. AFTER THAT IT'S INSTANTANEOUS.**\\n3. `predict`: loads the saved models, makes predictions, stores them under `results/` directory, and also stores the model recall values in a `.txt` file.\\n4. `test_rnn` test the rnn model on a subset of test files and save the output under `data/out`.\\n5. `test`: provides the functionality of `1` `2` `3` and `4` targets combined, on the test raw data.\\n6. `all`: provides the functionality of `1` `2` `3` and `4` targets combined, on the real raw data\\n\\n### Directory Map\\n\\n1. `config/`: contains the configuration `.json` files.\\n\\n    a. `create-test-data-params.json`: config file to build test-data.\\n    \\n    b. `data-params.json`: config file for cleaning data in the future.\\n    \\n    c. `eda-params.json`: config file for any EDA figures that will be generated.\\n    \\n    d. `features-params.json`: config file for building features from raw data.\\n\\n    e. `test-features-params.json`: config file for building features from test raw data.\\n    \\n    f. `model-params.json`: config file for model parameters.\\n\\n    g. `test-model-params.json`: config file for test model parameters.\\n    \\n    h. `test-rnn-params.json`: config file for testing rnn model.\\n\\n2. `data/`: contains the raw data files and data files after feature engineering. \\n\\n    a. `raw/`: contains the raw data files downloaded from source. _Does not include anything in the repo because raw data is confidential_. \\n    \\n    b. `out/`: contains the data after feature generation.\\n    \\n3. `notebooks/`: notebooks with some EDA and experimentation.\\n4. `references/`: this contains acknowledgement for any models or results that we use to build our project off of.\\n5. `results/`: running the build script populates this directory with the trained models (`.pkl`) and a `.txt` file which outlines the model performance. The `actual/` directory has the results of our actual model. The 'test/' directory will be created on running the `test` target and will contain results of our model on the test data.\\n6. `src/`: contains all the script `.py` files.\\n    \\n    a. `features/`: `build_features.py` performs feature engineering on clean data and populates `data/out` with files that can be used to train and test the models.\\n    \\n    b. `models/`: `train_model.py` trains the voting models and saves them in a `.pkl` file ; `test_model.py` makes predictions using the saved models and saves the performance metric in a `.txt` file. Both files are stored in `results/` directory.\\n    \\n    c. `helper_functions.py`: library of functions that are used to perform common tasks.\\n    \\n7. `test/`: contains `testdata/` which has the artificially generated test data.\\n8. `dockerfile`: creates a container with the necessary libraries and packages to run all the scripts.\\n9. `run.py`: running this script builds the requisite files.\\n10. `submission.json`: contains the dockerhub-id for building the container and build-script command to build the targets.\\n\",\n",
       "  'The DSC180 Capstone Project is a project focused on sleep apnea. The project website provides build instructions for running the project. The build instructions include different targets such as building features, training models, making predictions, and testing the models. The directory map of the project includes folders for configuration files, raw data files, notebooks, references, results, source code files, test data, and a Dockerfile for creating a container with the necessary libraries and packages. The run.py script is used to build the requisite files.'],\n",
       " 'https://github.com/chinkevin/DSC180_sleep_apnea': [\"# DSC180 Capstone Project\\n\\nSleep apnea is a sleep disorder where breathing starts and stops intermittenly. It can cause many issues while sleeping and even increases the risk of strokes and heart attacks. Traditionally, sleep research relies on human visual scoring. However with the advancement of machine learning, sleep research can be become a highly automated process. The purpose of this respoitory is to automatically classify sleep stages specifically for people with sleep apnea. Using signals from polysomnography data, such as EEG, EMG, EOG, and ECG, we can score sleep records using a Light Gradient Boosted Machine classifier into five stages: wake state, REM, N1, N2, and N3.\\n\\n### Building the project stages using `run.py`\\n\\n* To get the data, from the project root dir, run `python run.py data features`\\n  - This fetches the data, then creates features (defined in\\n    `src/features.py`) and saves them in the location specified in\\n    `features-params.json`.\\n* To include ECG features, run 'python run.py data features_ecg'\\n  - This builds the same features as before with additional ECG features.\\n* To build a model, from the project root dir, run `python run.py data\\n  features model`\\n  - This fetches the data, creates the features, then trains a lgbm classifier\\n    (with parameters specified in `config`).\\n* To predict and validate a model, from the project root dir, run `python run.py predict validate`\\n  - This runs the model on validation data, analyzes its performance, and creates visualizations.\\n\",\n",
       "  \"This project focuses on using machine learning to automatically classify sleep stages for people with sleep apnea. The project involves using signals from polysomnography data and a Light Gradient Boosted Machine classifier to score sleep records into five stages: wake state, REM, N1, N2, and N3. The project stages are built using the `run.py` script, which fetches the data, creates features, builds a model, and predicts and validates the model's performance.\"],\n",
       " 'https://github.com/a2lu/CAPSTONE_WILDFIRE': ['# CAPSTONE_WILDFIRE\\n\\n## Usage\\n```\\ngit clone https://github.com/a2lu/CAPSTONE_WILDFIRE.git\\ncd CAPSTONE_WILDFIRE\\npython run.py test\\n```',\n",
       "  'The given code is for a project called CAPSTONE_WILDFIRE. It can be used by cloning the repository and running the `run.py` file with the `test` argument.'],\n",
       " 'https://github.com/LauraDiao/Anomaly_Detectives': ['# Anomaly Detectives\\nAn in depth approach to detecting significant real-time shifts in network performance indicating network degradation. Building on the data generation process behind [DANE](https://github.com/dane-tool/dane) and Viasat\\'s [network stats](https://github.com/Viasat/network-stats), we build a classification system that determines if there are substantial changes to packet loss rate and degree of latency. Please visit [our webpage](https://lauradiao.github.io/Anomaly_Detectives) for a more comprehensive view of this project.\\n\\n<br>\\n\\n# Quick Links\\n- [Modified DANE](https://github.com/jenna-my/modified_dane)\\n- [network-stats](https://github.com/Viasat/network-stats)\\n\\n<br>\\n\\n## To generate data for this project:\\n\\n1. Generate data using our [modified fork of DANE](https://github.com/jenna-my/modified_dane)\\n    - ```make```, ```docker.io```, and ```docker-compose``` are required on your machine to run modified_dane properly.\\n    - a recursive flag is required to properly install modified_dane: <br>```git clone https://github.com/jenna-my/modified_dane --recursive```\\n\\n2. Clone this branch of the repository\\n   ```\\n   git clone https://github.com/LauraDiao/Anomaly_Detectives\\n   ```\\n\\n3. Place all raw DANE csv files within the directory ```data/raw``` of this repository. If the directory has not been created, run the command ```run.py``` once to generate all relevant directories.\\n\\n<br>\\n\\n## To use this repository: \\nEach of these targets implements a core feature of the repository within ```run.py```. All code can be executed with the run.py according to various targets specified below. <br>\\nExample call: ```python run.py data inference```\\n### Target List:\\n- ```data```: generates features from unseen and seen data\\n- ```eda```: Generates visualizations used in exploring which features to use for the model\\n- ```train```: prints results of model performance tested on training (\"seen\") data with four different models with varying architectures: decision tree, random forest, extra trees, and gradient boost\\n- ```inference```: (deprecated) prints results of model performance tested on testing (\"unseen\") data with the same exact models.\\n- ```clean```: Removes files generated by targets in commonly used output directories\\n- ```test```: Verifies target functionality by running the targets ```data```,```eda```, ```train```, and ```inference``` with a subset of the original model training data.\\n- ```all```: runs all targets except ```test```\\n\\n<br><br>\\n\\nOur modified version of DANE creates csv files with a naming scheme in the following format: \\n> *datevalue*_*latency*-*loss*-*deterministic*-*laterlatency*-*laterloss*-iperf.csv\\n\\ne.g. ```20220117T015822_200-100-true-200-10000-iperf.csv```\\n\\nthis format is crucial for the model to train on the proper labels.\\n\\n## Configuration Files\\n### eda.json\\n\\n- `lst`: [1, 2], # list of runs to compare side by side made by plottogether() inside of eda.py\\n- `filen1`: \"combined_subset_latency.csv\", - subset of the processed data to make eda\\n- `filen2`: \"combined_t_latency.csv\", - features generated from processed data\\n- `filen3`: \"combined_all_latency.csv\" - all processed - \\n\\n### model.json\\n\\n- `n_jobs`: -1 - number of cores the model training is done on\\n- `train_window`: 20 - number of seconds that the model will aggregate on for training window size\\n- `pca_components`: 4 - number of components for PCA, we determined 4 was optimal for our model\\n- `test_size`: 0.005 - model validation set size (train _test_ split)\\n- `threshold`: -0.15 - threshold for loss anomaly detection\\n- `emplosswindow`: 25 - rolling window aggregation of empirical loss, set at 25 seconds\\n- `pct_change_window`: 2 - how many seconds the anomaly detection system looks back for determining change.\\n- `verbose`: \"True\" - whether terminal output should be verbose or not. For debugging purposes.\\n \\n\\n',\n",
       "  'The text describes a project called \"Anomaly Detectives\" that focuses on detecting real-time shifts in network performance indicating network degradation. The project builds on the data generation process of DANE and Viasat\\'s network stats. It includes a classification system that determines substantial changes in packet loss rate and latency. The text provides links to the project webpage and related repositories. It also provides instructions for generating data and using the repository, along with information about configuration files used in the project.'],\n",
       " 'https://github.com/tatummaston/anomaly_network_detection': ['# anomoly_network_detection',\n",
       "  'Anomaly network detection refers to the process of identifying and flagging unusual or abnormal behavior within a network.'],\n",
       " 'https://github.com/arjunsawhney1/intel-capstone-project': ['# Intel Telemetry: Data Collection & Time-Series Prediction of App Usage\\n## Abstract\\nDespite advancements in hardware technology, PC users continue to face frustrating app launch times, especially on lower end Windows machines. The desktop experience differs vastly from the instantaneous app launches and optimized experience we have come to expect even from low end smartphones. We propose a solution to preemptively run Windows apps in the background based on the app usage patterns of the user. \\n\\nOur solution is two-step. First, we built telemetry collector modules in C/C++ to collect real-world app usage data from two of our personal Windows 10 devices. Next, we developed neural network models, trained on the collected data, to predict app usage times and corresponding launch sequences in python. We achieved impressive results on selected evaluation metrics across different user profiles. \\n\\n## Usage\\nDue to the nature of our project, we have two distinct predictive tasks.\\n\\nThe project pipeline for our HMM model may be run as follows:\\n```\\nlaunch-scipy-ml.sh -i arjunsawhney1/intel-telemetry:latest\\ngit clone git@github.com:arjunsawhney1/intel-capstone-project.git\\ncd intel-capstone-project/src/models/HMM\\npython run.py\\n```\\n\\nThe project pipeline for our LSTM model may be run as follows:\\n```\\nlaunch-scipy-ml.sh -i arjunsawhney1/intel-telemetry:latest\\ngit clone git@github.com:arjunsawhney1/intel-capstone-project.git\\ncd intel-capstone-project/src/models/LSTM\\npython run.py\\n```\\n\\nOutputs for both models can be located in the outputs folder.\\n\\n## Project Website\\nhttps://arjunsawhney1.github.io/intel-capstone-project/\\n\\n## Docker Image\\narjunsawhney1/intel-telemetry:latest\\n',\n",
       "  'This article discusses Intel Telemetry, a solution to improve app launch times on lower-end Windows machines. The solution involves collecting real-world app usage data and using neural network models to predict app usage times and launch sequences. The article provides instructions for running the project pipeline for both the HMM and LSTM models, as well as a link to the project website and the Docker image.'],\n",
       " 'https://github.com/andydo1998/dsc180-data-analysis': ['# DSC 180 Section B14 Project\\n\\n## Project Introduction\\nIn an effort to reduce app wait time, the time it takes for an application to launch, we collected data on application use and app wait time for a single user over several weeks. With this data we plan to build a series of models to predict which application a user will open with an emphasis on when and for how long. The focus is currently on our foreground app data which shows us which app is in the foreground of the user’s computer with timestamp. Using this data, we created a single chain Hidden Markov Model (HMM) to predict the next app the user opens based on their current one. With not enough amount of layers, we implemented a Long Short-Term Memory (LSTM) model to predict the amount of time a user will use an application.\\n\\n## Overview\\nWe collected foreground windows with our data collection library for 2 months on a Windows laptop. The Jupyter Notebook contains the code to read in and combine each database file, data cleaning/preprocessing, the HMM model, and the LSTM model. The run.py file is a streamline version of our notebook that reads in the files, creates a Hidden Markov Model, and outputs prediction on every unique application that was present in our data collection.\\n\\n## How to Use\\n1. Pull the repo to obtain all necessary files to run test\\n2. With a terminal, navigate onto overarching folder (dsc180-data-analysis)\\n3. Run the command: \\n```\\npython run.py test\\n```\\n4. When finished, the terminal should report an accuracy of the model and outputs all possible predictions onto outputs/outputs.txt\\n\\n## More Information\\nFor more information, please read our report, the pdf file on the repository, for a more in depth explanation of the process.\\n\\nA visual presentation can also be viewed here: https://www.youtube.com/watch?v=2h5k6alz3WU\\n\\n(note that to retrieve the most amount of information, please view the report, as the video only provides a summary of our process)\\n',\n",
       "  'The project aims to reduce app wait time by predicting which application a user will open and for how long. The data collected includes application use and app wait time for a single user over several weeks. The project uses a Hidden Markov Model (HMM) to predict the next app based on the current one, and a Long Short-Term Memory (LSTM) model to predict the amount of time a user will use an application. The code is available in a Jupyter Notebook and can be run using the provided run.py file. The accuracy of the model is reported in the terminal, and all predictions are outputted to outputs/outputs.txt. More information can be found in the report and a visual presentation is also available.'],\n",
       " 'https://github.com/cgorlla/intel-sur': [\"# INTELli*next*: A Fully Integrated LSTM and HMM-Based Solution for Next-App Prediction With Intel SUR SDK Data Collection\\n# Intel DCA x HDSI UCSD System Usage Reporting Research\\n\\nCyril Gorlla, Jared Thach, Hiroki Hoshida. [INTELli*next*: A Fully Integrated LSTM and HMM-Based Solution for Next-App Prediction With Intel SUR SDK Data Collection.](https://github.com/cgorlla/intel-capstone-submission/blob/main/report.pdf) *Halıcıoğlu Data Science Institute Capstone Showcase, March 11, 2022*\\n\\nAs the power of modern computing devices increases, so too do user expectations for them. Despite advancements in technology, computer users are often faced with the dreaded spinning icon waiting for an application to load. Building upon our previous work developing data collectors with the Intel System Usage Reporting (SUR) SDK, we introduce INTELli*next*, a comprehensive solution for next-app prediction for application preload to improve perceived system fluidity. We develop a Hidden Markov Model (HMM) for prediction of the k most likely next apps, achieving an accuracy of 70% when k = 3. We then implement a long short-term memory (LSTM) model to predict the total duration that applications will be used. After hyperparameter optimization leading to an optimal lookback value of 5 previous applications, we are able to predict the usage time of a given application with a mean absolute error of ~45 seconds. Our work constitutes a promising comprehensive application preload solution with data collection based on the Intel SUR SDK and prediction with machine learning.\\n\\n\\nThis repository contains the code for our research at UCSD on predicting PC user behavior in collaboration with Intel Corporation.\\n\\nYou can read about the development of the Input Libraries that collected the data used to predict in the paper below.\\n\\nCyril Gorlla, Jared Thach, Hiroki Hoshida. Development of Input Libraries With Intel XLSDK to Capture Data for App Start Prediction. 2022. ⟨[hal-03527679](https://hal.archives-ouvertes.fr/hal-03527679)⟩\\n\\n## Repository Overview\\n- `config\\\\`: contains configuration files for various scripts, such as data and output locations\\n- `notebooks\\\\`: contains EDA with visualizations and other helpful Jupyter Notebooks to better understand the data\\n- `src\\\\`: contains the main data loading, analysis, and model building scripts\\n- `main.py`: Python script to execute data parsing, data cleaning, training, and testing\\n\\n## `run.py`\\nThis Python file contains the necessary code to parse and clean data from the Input Libraries detailed in the above paper, as well as to build the models in the project. These include:\\n- First Order Hidden Markov Model for Next-App Prediction\\n- LSTM for Next-App Prediction\\n- LSTM for App Duration Prediction\\n\\n### Building `run.py`\\nTo run: `python run.py {data} {analysis} {model}`\\n\\nTo just build the model: `python run.py data model`\\n\\nTo test: `python run.py test` \\n\\nThis will load in test data in `test\\\\testdata` and build the HMM and LSTM prediction models off of it. The predictions of the test model will be stored in `data\\\\out\\\\test_{model}.csv`, which you may verify against the provided files named `prov_{model}.csv` to ensure the model is functioning as expected. Note that due to the inherently probabilistic nature of the models your outputs may not be the same as the provided files, but they provide a sanity check.\\n\\n## `src\\\\model\\\\model.py`\\n\\nThis file contains the:\\n\\n- First order Hidden Markov model classfor predicting future foreground applications. After splitting the data and fitting the training set to a `first_order_HMM` instance using `fit`, the model keeps track of the prior and posterior probabilities of the training set's foreground applications. When inputting an observation, `X`, to `predict`, the function returns a list of foregrounds, (of size `n_foregrounds`, with default value of 1) with the highest conditional probability given `X`'s inputted foreground application and the trained model's posterior probabilities. `accuracy` returns the accuracy of the `y_test` on `y_pred` by taking each true foreground application in `y_test` and checking whether or not it appears in its respective list of foregrounds in `y_pred`.\\n\\n- The next-app prediction LSTM model using a “look-back” value of one previous foreground application in order to predict one future foreground application, where a “look-back” is defined as the number of previous events a single input will use in order to generate the next output\\n\\n- The duration prediction LSTM using a look-back value of five. In other words, the model uses the previous five data points to predict the next. \\n\\nBoth LSTM models' architecture is similar, with the four layers in the same order. \\n\\n\\n\\n## Docker\\nA dockerfile is included and will create a Docker environment that allows for the successful execution of all code in this repository.\\n\",\n",
       "  'The research paper titled \"INTELli*next*: A Fully Integrated LSTM and HMM-Based Solution for Next-App Prediction With Intel SUR SDK Data Collection\" introduces a comprehensive solution for next-app prediction to improve system fluidity. The paper discusses the development of a Hidden Markov Model (HMM) for predicting the next apps with an accuracy of 70% when considering the top 3 likely apps. It also presents a long short-term memory (LSTM) model for predicting the duration of app usage with a mean absolute error of approximately 45 seconds. The research utilizes data collected with the Intel System Usage Reporting (SUR) SDK and applies machine learning techniques for prediction.\\n\\nThe repository contains code for predicting PC user behavior in collaboration with Intel Corporation. It includes configuration files, notebooks for exploratory data analysis, and scripts for data loading, analysis, and model building. The main script, `run.py`, allows for parsing and cleaning data from input libraries and building the HMM and LSTM models. The `model.py` file contains classes for the first-order Hidden Markov Model and LSTM models used in the project.\\n\\nA Docker environment is also provided to facilitate the execution of the code in the repository.'],\n",
       " 'https://github.com/wolftossH/DSC--180AB-escrow': ['# DSC--180AB-escrow\\n<!-- Improved compatibility of back to top link: See: https://github.com/othneildrew/Best-README-Template/pull/73 -->\\n<a name=\"readme-top\"></a>\\n<!--\\n*** Thanks for checking out the Best-README-Template. If you have a suggestion\\n*** that would make this better, please fork the repo and create a pull request\\n*** or simply open an issue with the tag \"enhancement\".\\n*** Don\\'t forget to give the project a star!\\n*** Thanks again! Now go create something AMAZING! :D\\n-->\\n\\n\\n\\n<!-- PROJECT SHIELDS -->\\n<!--\\n*** I\\'m using markdown \"reference style\" links for readability.\\n*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).\\n*** See the bottom of this document for the declaration of the reference variables\\n*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.\\n*** https://www.markdownguide.org/basic-syntax/#reference-style-links\\n-->\\n[![Contributors][contributors-shield]][contributors-url]\\n[![MIT License][license-shield]][license-url]\\n<!-- [![Forks][forks-shield]][forks-url]\\n[![Stargazers][stars-shield]][stars-url]\\n[![Issues][issues-shield]][issues-url] -->\\n\\n\\n\\n<!-- PROJECT LOGO -->\\n<br />\\n<div align=\"center\">\\n  <a href=\"https://github.com/othneildrew/Best-README-Template\">\\n    <img src=\"new_client/images/final_logo.png\" alt=\"Logo\" width=\"200\" height=\"200\">\\n  </a>\\n\\n  <h3 align=\"center\">Best-README-Template</h3>\\n\\n  <p align=\"center\">\\n    An awesome README template to jumpstart your projects!\\n    <br />\\n    <a href=\"https://github.com/othneildrew/Best-README-Template\"><strong>Explore the docs »</strong></a>\\n    <br />\\n    <br />\\n    <a href=\"https://escryptow.net/\">View Webiste</a>\\n\\n  </p>\\n</div>\\n\\n\\n\\n<!-- TABLE OF CONTENTS -->\\n<details>\\n  <summary>Table of Contents</summary>\\n  <ol>\\n    <li>\\n      <a href=\"#about-the-project\">About The Project</a>\\n      <ul>\\n        <li><a href=\"#built-with\">Built With</a></li>\\n      </ul>\\n    </li>\\n    <li>\\n      <a href=\"#getting-started\">Getting Started</a>\\n      <ul>\\n        <li><a href=\"#prerequisites\">Prerequisites</a></li>\\n        <li><a href=\"#installation\">Installation</a></li>\\n      </ul>\\n    </li>\\n    <li><a href=\"#usage\">Usage</a></li>\\n    <li><a href=\"#roadmap\">Roadmap</a></li>\\n    <li><a href=\"#contributors-and-contact\">Contributors and Contact</a></li>\\n    <li><a href=\"#acknowledgments\">Acknowledgments</a></li>\\n  </ol>\\n</details>\\n\\n\\n\\n<!-- ABOUT THE PROJECT -->\\n## About The Project\\n\\n[![Product Name Screen Shot][product-screenshot]](https://example.com)\\n\\n\\n\\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\\n\\n\\n\\n### Built With\\n<p>\\n\\n[![React][React.js]][React-url] \\\\\\n[![Node][Node.js]][Node-url] \\\\\\n[![Solidity][solidity]][solidity-url] \\\\\\n[![Vite][vite]][vite-url] \\\\\\n[![Tailwind][tailwind]][tailwind-url] \\\\\\n[![Hardhat][hardhat]][hardhat-url] \\\\\\n![HTML][html] \\\\\\n![CSS][css]\\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\\n\\n<!-- GETTING STARTED -->\\n## Getting Started\\n\\nThis is an example of how you can use the website\\n\\n### Prerequisites\\n\\nThis is an example of how to list things you need to use the software and how to install them.\\n\\n\\n<!-- USAGE EXAMPLES -->\\n## Usage\\n\\nEscrow shop for all users\\n<div align=\"center\">\\n  <a href=\"https://github.com/othneildrew/Best-README-Template\">\\n    <img src=\"images/cart.png\" alt=\"Logo\" width=\"500\" height=\"300\">\\n  </a>\\n</div>\\n\\n\\n\\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\\n\\n\\n\\n<!-- ROADMAP -->\\n## Roadmap\\n\\n- [x] Design logo\\n- [x] Added ipfs\\n\\n\\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\\n\\n\\n\\n<!-- LICENSE -->\\n## License\\n\\nDistributed under the ....... License. See `LICENSE.md` for more information.\\n\\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\\n\\n\\n\\n<!-- CONTACT -->\\n## Contributors and Contact\\n\\nHuy Trinh - [![LinkedIn][linkedin-shield]][linkedin-url-huy]\\n\\nAntoni Liria-Sala - [![LinkedIn][linkedin-shield]][linkedin-url-antoni]\\n\\nWilliam Li - [![LinkedIn][linkedin-shield]][linkedin-url-william] \\n\\nGuangyu Yang - [![LinkedIn][linkedin-shield]][linkedin-url-irvin] \\n\\n\\n\\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\\n\\n\\n\\n<!-- ACKNOWLEDGMENTS -->\\n## Acknowledgments\\n\\nUse this space to list resources you find helpful and would like to give credit to. I\\'ve included a few of my favorites to kick things off!\\n\\n* [Choose an Open Source License](https://choosealicense.com)\\n* [Img Shields](https://shields.io)\\n* [Font Awesome](https://fontawesome.com)\\n* [React Icons](https://react-icons.github.io/react-icons/search)\\n\\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\\n\\n\\n\\n<!-- MARKDOWN LINKS & IMAGES -->\\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\\n[contributors-shield]: https://img.shields.io/github/contributors/wolftossH/DSC--180AB-escrow.svg?style=for-the-badge\\n[contributors-url]: https://github.com/wolftossH/DSC--180AB-escrow/graphs/contributors\\n[forks-shield]: https://img.shields.io/github/forks/othneildrew/Best-README-Template.svg?style=for-the-badge\\n[forks-url]: https://github.com/othneildrew/Best-README-Template/network/members\\n[stars-shield]: https://img.shields.io/github/stars/othneildrew/Best-README-Template.svg?style=for-the-badge\\n[stars-url]: https://github.com/othneildrew/Best-README-Template/stargazers\\n[issues-shield]: https://img.shields.io/github/issues/othneildrew/Best-README-Template.svg?style=for-the-badge\\n[issues-url]: https://github.com/othneildrew/Best-README-Template/issues\\n[license-shield]: https://img.shields.io/github/license/othneildrew/Best-README-Template.svg?style=for-the-badge\\n[license-url]: https://github.com/othneildrew/Best-README-Template/blob/master/LICENSE.txt\\n[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555\\n\\n[linkedin-url]: https://www.linkedin.com/feed/\\n[linkedin-url-huy]: https://www.linkedin.com/in/huy-trinh-9868ba194\\n[linkedin-url-antoni]: https://www.linkedin.com/in/antoniliriasala/\\n[linkedin-url-william]: https://www.linkedin.com/in/tianyangwillli/\\n[linkedin-url-irvin]: https://www.linkedin.com/in/irvinyang/\\n\\n[product-screenshot]: images/website_main_pic.jpg\\n[Next.js]: https://img.shields.io/badge/next.js-000000?style=for-the-badge&logo=nextdotjs&logoColor=white\\n[Next-url]: https://nextjs.org/\\n[React.js]: https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB\\n[React-url]: https://reactjs.org/\\n[Vue.js]: https://img.shields.io/badge/Vue.js-35495E?style=for-the-badge&logo=vuedotjs&logoColor=4FC08D\\n[Vue-url]: https://vuejs.org/\\n[Angular.io]: https://img.shields.io/badge/Angular-DD0031?style=for-the-badge&logo=angular&logoColor=white\\n[Angular-url]: https://angular.io/\\n[Svelte.dev]: https://img.shields.io/badge/Svelte-4A4A55?style=for-the-badge&logo=svelte&logoColor=FF3E00\\n[Svelte-url]: https://svelte.dev/\\n[Laravel.com]: https://img.shields.io/badge/Laravel-FF2D20?style=for-the-badge&logo=laravel&logoColor=white\\n[Laravel-url]: https://laravel.com\\n[Bootstrap.com]: https://img.shields.io/badge/Bootstrap-563D7C?style=for-the-badge&logo=bootstrap&logoColor=white\\n[Bootstrap-url]: https://getbootstrap.com\\n[JQuery.com]: https://img.shields.io/badge/jQuery-0769AD?style=for-the-badge&logo=jquery&logoColor=white\\n[JQuery-url]: https://jquery.com \\n[html]: \\thttps://img.shields.io/badge/HTML5-E34F26?style=for-the-badge&logo=html5&logoColor=white\\n[css]: https://img.shields.io/badge/CSS3-1572B6?style=for-the-badge&logo=css3&logoColor=white\\n[solidity]: https://img.shields.io/badge/Solidity-e6e6e6?style=for-the-badge&logo=solidity&logoColor=black\\n[solidity-url]: https://soliditylang.org/\\n[vite]: https://img.shields.io/badge/Vite-B73BFE?style=for-the-badge&logo=vite&logoColor=FFD62E\\n[vite-url]: https://vitejs.dev/\\n\\n[tailwind]: https://img.shields.io/badge/Tailwind_CSS-38B2AC?style=for-the-badge&logo=tailwind-css&logoColor=white\\n[tailwind-url]: https://tailwindcss.com/\\n\\n[Node.js]: https://img.shields.io/badge/Node.js-339933?style=for-the-badge&logo=nodedotjs&logoColor=white\\n[node-url]: https://nodejs.org/en/\\n\\n[hardhat]: https://hardhat.org/_next/static/media/hardhat-logo.5c5f687b.svg\\n[hardhat-url]: https://hardhat.org/\\n',\n",
       "  'This is a README template for a project called \"DSC--180AB-escrow\". It provides information about the project, including its purpose, built technologies, how to get started, usage examples, roadmap, contributors and contact information, and acknowledgments. The template also includes various badges and links to relevant resources.'],\n",
       " 'https://github.com/matin-g/Q2-DSC180B-A02': ['# Most up to date web app code in this folder --> front-end-webApp\\n# Most up to date contract code is final_purchase.sol\\n\\n<br>\\n\\n# Q2 capstone project \\nA decentralized exchange via ethereum smart contracts intergrated within a website (Dapp) in order to create a decentralized peer-to-peer ecommerce platform. Please refer to our [report](https://github.com/matin-g/DSC180a-Q1-final-code/blob/main/report.pdf) (note: the future work section explains what we are doing now in this repository)\\n\\n# Please see Q1 code and report here: \\nhttps://github.com/matin-g/DSC180a-Q1-final-code\\n\\n\\n# Instruction to run on local machine\\nAfter pulling all the codes within front-end folder to your local machine ---\\n\\nMake sure you have node.js (npm) installed (https://nodejs.org/en/download/)\\ninstall express package: open up terminal, input \"npm install express --save\"\\ncd into the front-end folder, and input \"node server.js\", and you will see \"port is open on 8082\"\\nopen chrome - \"http://127.0.0.1:8082/\"\\n',\n",
       "  'The folder \"front-end-webApp\" contains the most up-to-date web app code. The file \"final_purchase.sol\" contains the most up-to-date contract code. The Q2 capstone project aims to create a decentralized exchange using Ethereum smart contracts integrated within a website. More information can be found in the provided report and code links. Instructions for running the project on a local machine are also provided.'],\n",
       " 'https://github.com/crvander/capstoneproj2023': ['<h1 align=\"center\">\\n  Transformers for Sentiment Analysis on Financial Text\\n</h1>\\n\\n<h4 align=\"center\">\\n  Fine-tuned Models based on pretrained Hugging Face Transformers\\n</h4>\\n\\n## Usage\\n\\n### From Command Line\\n```bash\\n# this will install necessary packages\\npip install -r requirements.txt\\n\\n# this will run the whole pipeline consists of downloading full dataset, generate data, \\n# downloading our finetuned models from google drive, unzip model folders,\\n# predict sentiments on testing dataset\\n\\npython run.py generate_data download_models test\\n\\n# trainning process based on pretrained models from HuggingFace\\npython run.py generate_data train test\\n\\n# for predict based on tweets data from twitter API\\npython run.py download_models predict\\n\\n# for default testing run (submission for Quater1), \\n# test run will download, unzip our finetuned models from google drive,\\n# predict on dummy testdata(3 samples) and output predicted labels\\n\\npython run.py testing\\n```\\n\\n### File structure and configuration\\nAll configuration will be read from config folder, **data-params.yml** will be read when generating data, **model_config.yml** will be read when downloading, unzipping finetuned models, **train-params.yml** will be read as training hyperparameters as well as io path, **test-params.yml** consists io for test dataset and testrun dummy dataset.\\n\\nRaw dataset will be scraped from data sources and saved in data/raw, processed data will be saved in data/temp predictions will be saved in data/out, all datafiles will be saved in .csv file.\\n\\nFinetuned models, no matter directly output from training process, or download from google shared drive, all will be saved in results folder **train.py** will download pretrained models from Hugging Face hub, and read data from data/temp, finally save finetuned model to result folder.\\n\\n**test.py** will take two argument, test_target and test_lines. test_target can be specified as testing, which will generate prediction on testing data\\nor default as test to predict on testrun dummy data. All prediction will be saved in data/out.\\n\\n```\\nFinTech Project                      //\\n├─ config                            //\\n│  ├─ config.json                    //\\n│  ├─ data-params.yml                //\\n│  ├─ model-config.yml               //\\n│  ├─ test-params.yml                //\\n│  └─ train-params.yml               //\\n├─ data                                                            //\\n│  ├─ kaggle.json                                                  //\\n│  ├─ out                                                          //\\n│  │  ├─ model.joblib                                              //\\n│  │  └─ preds.csv                                                 //\\n│  ├─ raw                                                          //\\n│  ├─ temp                                                         //\\n│  │  ├─ test.csv                                                  //\\n│  │  └─ train.csv                                                 //\\n├─ myapp.log                                                       //\\n├─ README.md                                                       //\\n├─ run.py                                                          //\\n├─ spacy                                                           //\\n│  ├─ .DS_Store                                                    //\\n│  └─ create_model.py                                              //\\n├─ reports                           //\\n│  ├─ abstract.md                    //\\n│  ├─ demo.md                        //\\n│  ├─ discussion.md                  //\\n│  ├─ figures                        //\\n│  │  └─ logo_png.png                //\\n│  ├─ intro.md                       //\\n│  ├─ introduction.md                //\\n│  ├─ methods.md                     //\\n│  ├─ requirements_jb.txt            //\\n│  ├─ results.md                     //\\n│  ├─ _config.yml                    //\\n│  └─ _toc.yml                       //\\n├─ src                                                             //\\n│  ├─ data                                                         //\\n│  │  └─ make_dataset.py                                           //\\n│  ├─ test.py                                                      //\\n│  ├─ train.py                                                     //\\n│  ├─ utils                                                        //\\n│  │     └─ download_models.py                                     //\\n├─ submission.json                                                 //\\n├─ test                                                            //\\n│  └─ testdata                                                     //\\n│     └─ test.csv                                                  //\\n├─ twitter                                                         //\\n│  ├─ pull_tweets.py                                               //\\n│  └─ twitter_credentials.py                                       //\\n├─ _requirements.txt                                               //\\n└─ _run.py                                                         //\\n```\\n\\n',\n",
       "  'This is a project that focuses on using Transformers for sentiment analysis on financial text. The project uses fine-tuned models based on pretrained Hugging Face Transformers. The usage of the project can be done from the command line, with various options available such as generating data, downloading models, training, testing, and predicting sentiments. The file structure and configuration of the project are also explained.'],\n",
       " 'https://github.com/nathansng/fintech_library': ['# FinDL - Financial Deep Learning Library\\n\\n## DSC 180 Project\\n\\nThe goal of this project is to create a deep learning and machine learning library that allows users to easily create and deploy machine learning models for finance related tasks, such as future stock forecasting. This repo contains a data loader, data preprocessing functions, time series forecasting models, and loss visualization functions to provide an end-to-end machine learning and visualization pipeline. This project is made in parallel with the finance library\\'s NLP group. \\n\\n#### Project Website\\nLink to FinDL\\'s project website: [https://nathansng.github.io/fintech_library](https://nathansng.github.io/fintech_library/)\\n\\nThe project website code can be found in the finDL_website branch of this repository.\\n\\nLink to FinDL\\'s documentation: [https://fintech-library.readthedocs.io/en/latest/code/overview.html](https://fintech-library.readthedocs.io/en/latest/code/overview.html)\\n\\nThe project documentation website code can be found in the docs folder of the main branch. \\n\\n### Downloading Data\\n\\nOur data is downloaded from Kaggle.com. The dataset that we used for our experiment is the Stock Exchange Data created by Cody in 2018. The dataset is available at: https://www.kaggle.com/datasets/mattiuzc/stock-exchange-data. The dataset we used from this Kaggle dataset is \"indexProcessed.csv\". Save the csv file in the path `./data/raw/`. \\n\\n### Models \\n\\nThe following models have been implemented in the current implementation of the library. \\n\\n- CNN\\n- LSTM\\n- GRU\\n- TreNet\\n\\nThe Convolutional Neural Network (CNN) takes raw data points as input, extracts and learns the local feature information, and outputs the predicted local feature. The tuneable parameters of CNN are as follows:\\n\\n- number of layers\\n- convolutional layer size \\n- filter size\\n- dropout\\n- output size\\n\\nThe Long Short Term Memory takes the trends\\' slope and duration, which are extracted by using linear approximation approach on the raw data points, learns the trend dependencies, and outputs the predicted slope and duration of the next trend. The tunable parameters of LSTM are as follows:\\n\\n- input size\\n- hidden size\\n- number of layers\\n- output size\\n\\nThe Gated Recurrent Network works similar to LSTM and uses an update and reset gate. It is also used to extract information from the trends\\' slope and duration. The tunable parameters of GRU are as follows: \\n\\n- input size\\n- hidden size\\n- number of layers\\n- output size\\n\\nTreNet takes the predicted results from both CNN and LSTM, and combines them using a fully connected layer to generate the predicted output. The tunable parameters of TreNet are as follows: \\n\\n- Hyperparameters of LSTM \\n- Hyperparameters of CNN\\n- size of feature fusion layer\\n- output size \\n\\n\\n### Running Code\\n\\n*Note*: Running the code on a gpu will make the program run significantly faster than only cpu. The code also benefits with more RAM, as too low of memory will kill the process. \\n\\nTo run the code, run `python run.py [target]` to run the corresponding target. The available targets and their description are listed below: \\n\\n- `all`: Runs all targets using actual data, data path can be specified in `./config/data_params.json` file\\n\\n- `test`: Runs all targets using test data, test data can be found in `./test/testdata/testdata.csv`, test path can be specified in `./config/test_data_params.json` file\\n\\n- `data`: Runs the data and feature loading code, which opens a dataset and converts the dataset into trend durations and slopes\\n  - Configure parameters in `./config/data_params.json`\\n\\n- `features`: Runs the preprocessing code for the trends and stock data, normalizes all data, and splits data into training and testing sets for machine learning model use \\n  - Configure parameters in `./config/feature_params.json`\\n\\n- `model`: Runs the machine learning model training code \\n  - Configure model parameters in `./config/model_params.json`\\n  - Configure training parameters in `./config/training_params.json`\\n\\n- `visual`: Runs the loss plot visualization code which stores a line plot of the loss per epoch to a path \\n  - Configure parameters in `./config/visual_params.json`\\n\\nYou can also specify the model to run by specifying the model name in addition to any targets listed above by using `python run.py [targets] [model]`. The default model used if no model is specified is TreNet. The available models and their description are listed below: \\n\\n- `trenet`: Runs the TreNet model. Processes 1 dimensional time series data into a sequence of linear regressions encoded by the regressions slope and duration. Uses trend slope and duration data to predict future trends. \\n\\n- `lstm`: Runs an LSTM model. Trains on 1 dimensional time series data to predict future time series data. \\n\\n- `cnn`: Runs the CNN stack from the TreNet model. Trains on 1 dimensional time series data to predict future time series data. \\n\\n\\n',\n",
       "  'The FinDL project is a deep learning and machine learning library for finance-related tasks, such as stock forecasting. It includes data loading, preprocessing functions, time series forecasting models, and loss visualization functions. The project also has a website and documentation. The library supports models like CNN, LSTM, GRU, and TreNet. The code can be run with different targets to perform various tasks like data loading, feature preprocessing, model training, and loss visualization. The default model used is TreNet, but other models like LSTM and CNN can also be specified.'],\n",
       " 'https://github.com/vineettalla/Servicechain.io': ['This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\\n\\n## Getting Started\\nFirst, Make sure to install the dependencies in the package.json file by running:\\n\\n```bash\\nnpm install\\n```\\n\\n\\nNext(get it?), run the development server:\\n\\n```bash\\nnpm run dev\\n# or\\nyarn dev\\n# or\\npnpm dev\\n```\\n\\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\\n\\nYou can start editing the page by modifying `pages/index.js`. The page auto-updates as you edit the file.\\n\\nThe `pages/api` directory is mapped to `/api/*`. Files in this directory are treated as [API routes](https://nextjs.org/docs/api-routes/introduction) instead of React pages.\\n\\nYou now should be able to work with our current codebase!\\n\\n### Editing Smart Contract \\nIf you wish to make changes to the smart contract \\n```bash\\ncd ./ethereum/contracts\\n```\\nIn here you will see the solidity file that has the smart contract logic.<br>\\nOnce you have made changes you will need to compile and deploy the factory contract once again in order to get the ABI and Byte Code<br>\\nWe can do this by running:\\n```bash\\nnode compile.js\\nnode deploy.js\\n```\\nAfter the deploy command you will be give an address in the terminal window similar to the image below:\\n\\n![deployment](https://user-images.githubusercontent.com/80795080/225056996-8a1e5df9-6f87-4a60-aa08-c4a0bf53ee1d.png)\\n<br>Copy the contract address and paste it into the factory.js file located at \\n```bash\\ncd ./ethereum/factory.js\\n```\\nYou will replace the preexisting \"addressOfDeployedFactory\" with the one you pasted at the location shown below:\\n![image](https://user-images.githubusercontent.com/80795080/225057653-dcd111f9-0691-4b3a-9a37-d2b95f58ea92.png)\\n<br>\\nNow you have a fresh new smart contract that is connected to the frontend of ServiceChain.io! \\n\\n### Changing Backend\\nIf you are editing the smart contract you must also change the configuration of the firebase backend to connect to your own! In order to do so go to:\\n```bash\\ncd ./config/firebase.js\\n```\\nAll you have to do is login to [firebase](https://firebase.google.com/), create a project, enable authentication and firestore, then just change the firebase config variable with your own! \\n\\n## Deployed Version \\nYou can also checkout the actual deployed webpage [here](https://servicechain-io.vercel.app/).\\n\\n## Using the App\\n\\nCurrently our app only works if you have a metamask account since this is a way to interact with the ethereum blockchain with little to no work.\\n1. Download [Metamask](https://metamask.io/download/) \\n2. Create an Account\\n3. You should be prompted to a screen like this.\\n<br><img src =\"https://user-images.githubusercontent.com/80795080/225046985-9b79bf0b-86fd-4da8-9023-0908b620ea22.png\" width =\\'200\\' height =\\'300\\'><br> \\n**Make sure to switch your network to the Goerli Test Network**\\n<br>\\n5. Optionally if you wish to use actions in our app like sending ratings, tips, etc. You must load your account with test ether. In order to do so go to to a [faucet](https://goerlifaucet.com/) and paste your public address from metamask which is the highlighted value in the image below.\\n<br> <br><img src =\"https://user-images.githubusercontent.com/80795080/225049758-e570310c-452a-4a9b-98ce-92e9aa570ba1.png\"><br> \\n6. Head to our [website](https://servicechain-io.vercel.app/) and click signup to create your own account and use the app! \\n\\n## Test Accounts\\nIf you wish to just browse the site, we have an account for each user type. Note that functionalities like tipping **will not work** unless you have completed step 5.\\n<br>\\n<br>\\nUser Type: Employee<br>\\nUsername: je@gmail.com<br>\\nPassword: password<br>\\n<br>\\nUser Type: Manager<br>\\nUsername: jm@gmail.com  <br>\\nPassword: password<br>\\n<br>\\nUser Type: Customer<br>\\nUsername: sr@gmail.com  <br>\\nPassword: password<br>\\n<br>\\n## Extras\\nI hope you enjoy playing around and improving our dapp. Feel free to point out any flaws or inefficiencies that can be improved upon. After all the beauty of a DAPP is that its all open source!\\n\\n\\n\\n',\n",
       "  'This is a Next.js project bootstrapped with create-next-app. To get started, install the dependencies in the package.json file by running \"npm install\". Then, run the development server using \"npm run dev\". The project can be accessed at http://localhost:3000. The page can be edited by modifying pages/index.js. The pages/api directory is used for API routes instead of React pages. If you want to make changes to the smart contract, navigate to ./ethereum/contracts and modify the solidity file. After making changes, compile and deploy the factory contract using \"node compile.js\" and \"node deploy.js\". Replace the addressOfDeployedFactory in ./ethereum/factory.js with the new contract address. To change the backend configuration, go to ./config/firebase.js and update the firebase config variable with your own credentials. The deployed version of the webpage can be found at https://servicechain-io.vercel.app/. The app requires a Metamask account to interact with the Ethereum blockchain. Download Metamask, create an account, switch to the Goerli Test Network, and load your account with test ether from a faucet if needed. Test accounts are available for each user type on the website.'],\n",
       " 'https://github.com/scottasut/dsc180b-project': ['<h1 align=\"center\">\\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/18/UCSD_Seal.png\", width=150, height=150>\\n<img src=\"https://avatars.githubusercontent.com/u/71526309?s=280&v=4\", width=150, height=150>\\n<img src=\"https://logodownload.org/wp-content/uploads/2018/02/reddit-logo-16.png\", width=150, height=150>\\n\\nInteraction Graph-Based Community Recommendation on Reddit\\n</h1>\\n\\n#### Group Members\\n\\n- Scott Sutherland (sasuther@ucsd.edu)\\n- Ryan Don (rdon@ucsd.edu)\\n- Felicia Chan (f4chan@ucsd.edu)\\n- Pravar Bhandari (psbhanda@ucsd.edu)\\n\\n## Overview:\\n<br/>\\n<div align=\"center\">\\n<img src=\"https://user-images.githubusercontent.com/55766484/224842781-a9657aef-54d5-4305-8a09-fce7112693a1.png\"  width=\"600\" height=\"300\">\\n</div><br/>\\n\\nThis capstone project focuses on graph-based recommender systems for the social media platform Reddit. Users can choose to comment, subscribe, or otherwise interact in different online communities within Reddit called subreddits. Utilizing the graph database and analytics software TigerGraph, we create a recommendation model that recommends subreddits to users based on a variety of different interaction-related features.\\n\\nThe source code for the project is broken up as follows:\\n- `src/dataset`: files which handle data downloading and parsing it into a heterogeneous graph representation.\\n- `src/features`: files which handles the non-graph feature generation process for our graph data (users/subreddits).\\n- `src/models`: our baseline and final models which actually make recommendations for users as well as an evaluation handler class. `src/models/baselines.py` contains the non-graph baseline models while `src/models/models.py` contains the graph-based final models. `src/models/evaluator.py` handles evaluation of recommendations via precision@k calculation given a testing interaction set.\\n\\nThe website associated to this project can be found [here](https://scottasut.github.io/dsc180b-project/).\\n\\nDue to this project\\'s reliance on TigerGraph\\'s tools, our models cannot be run via a test target without access to a cluster. For a quick demo that our code which makes recommendations for a user, please refer to [this video](https://www.youtube.com/watch?v=fD63_7fDCcM). Additionally, you may refer to `notebooks/model_testing.ipynb` to see the evaluation of the models.\\n\\n## Project Structure:\\n```\\ndsc180b-project/\\n├─ docs/\\n│  ├─ css/\\n│  ├─ images/\\n│  ├─ _config.yml\\n│  ├─ index.html\\n├─ notebooks/\\n│  ├─ eda.ipynb\\n│  ├─ model_testing.ipynb\\n│  ├─ network_stats.ipynb\\n│  ├─ test.ipynb\\n├─ src/\\n│  ├─ dataset/\\n│  │  ├─ create_dataset.py\\n│  │  ├─ generate_dataset.py\\n│  │  ├─ make_dataset.py\\n│  ├─ features/\\n│  │  ├─ build_features.py\\n│  ├─ models/\\n│  │  ├─ baselines.py\\n│  │  ├─ evaluator.py\\n│  │  ├─ model.py\\n│  │  ├─ models.py\\n│  ├─ util/\\n│  │  ├─ logger_util.py\\n│  │  ├─ tigergraph_util.py\\n├─ .gitignore\\n├─ Dockerfile\\n├─ README.md\\n├─ poster.pdf\\n├─ report.pdf\\n├─ run.py\\n├─ submission.json\\n```\\n\\n## Prerequisites:\\n\\nBeyond the packages outlines in `requirements.txt`, there are a few tools needed for this project. Namely:\\n- [wget](https://www.gnu.org/software/wget/): In order to download the data, you will need wget installed on your system.\\n  - If you do not meet this requirement, [here](https://www.jcchouinard.com/wget/) is a useful guide on how you can get it.\\n- [TigerGraph](https://www.tigergraph.com/): To get this setup for this project there are quite a few steps. Let\\'s walk through them:\\n\\n### Working with TigerGraph:<a name=\"workingwithtigergraph\"></a>\\n\\n#### Setting up a TGCloud account\\nIn order to leverage graph-based machine learning techniques and TigerGraph\\'s suite of tools in particular, we need to set up a TGCloud instance to work from:\\n\\n1. Got to https://tgcloud.io/.\\n2. Select \\'sign up\\'.\\n3. Fill in the requested information on the sign up page. The organization name can be anything you like, but you will need it to log in.\\n4. Log in using the information you just provided.\\n5. Select \\'Clusters\\' on the left hand side menu bar, and then select \\'Create Cluster\\' in the upper right of the interface.\\n6. From here, you can choose how to configure your cluster. We were able to achieve all the goals of this project using a free cluster with the following specifications: Version: `3.8`, Instance type: `4 vCPU, 7.5GB Memory`, Storage: `50GB`, Number of nodes: `1 Node, Partition Factor 1, Replication Factor 1`.\\n\\n#### Defining a Graph and Graph Schema\\nNext, within the GraphStudio tool, we need to create a graph schema to hold our data:\\n\\n1. Nagivate to \\'Tools\\' > \\'GraphStudio\\' and select the cluster you created.\\n2. Follow [these](https://youtu.be/Z48cjYuJXX4) steps to create the required schema:\\n\\n\\n#### What is a graph schema?\\n\\nA graph schema is a kind of blueprint that defines the types of nodes and edges in the graph data structure, as well as the relationships and constraints between them. TigerGraph has a graphical user interface called GraphStudio that can be used to set up the initial schema and data mapping/loading. [Here](https://docs.tigergraph.com/gsql-ref/current/ddl-and-loading/defining-a-graph-schema#:~:text=A%20graph%20schema%20is%20a,(properties)%20associated%20with%20it) is a useful link that goes more in depth in terms of defining and loading a graph using TigerGraph. [This](https://www.youtube.com/watch?v=Q0JUkiU0lbs) is another short video demonstration showing how to create a schema in GraphStudio.\\n\\n#### Loading our Data:\\n*This step requires that the data has been downloaded and processed, please refer to the [Usage](#usage) section*\\n\\nWithin GraphStudio, you can follow [these](https://www.youtube.com/watch?v=7sg6Cw7BuWw) steps\\n\\nOnce you have TigerGraph up and running, you need to be able to authenticate yourself when using it. In this project, you can do so by creating `configs/tigergraph_config.json` in this directory which contains the following: \\n```\\n{\\n    \"host\": \"<The Host for your TigerGraph Cluster>\",\\n    \"graphname\": \"<The Name of Your Graph>\",\\n    \"username\": \"<Your TigerGraph Username>\",\\n    \"password\": \"<Your TigerGraph Password>\",\\n    \"gsqlSecret\": \"<Your Secret Key>\",\\n    \"certPath\": \"<The location of your my-cert.txt>\"\\n}\\n``` \\nWhile we worked in TigerGraph, we needed to have a file `my-cert.txt` located in our local machine\\'s root directory `~`. Please refer to [this](https://dev.tigergraph.com/forum/t/tigergraph-python-connection-issue/2776) thread for information on how to get that file.\\n\\n## Usage:<a name=\"usage\"></a>\\nIn order to run the different components of the project, you will interact with the `run.py` file. There are two main \\'targets\\' or arguments you can pass to the script when running it to work with the project: `data`, `features`. Due to the nature of the project and reliance on running TGCloud instance, testing targets are not available out of the box.\\n\\n- `data`: downloads the raw data and parses it into a heterogeneous graph format\\n- `features`: generates necessary features for final model from raw data. Depends on the `data` target.\\n\\nTargets can be called as follows `python run.py data features`.\\n\\n\\n#### Important Usage Notes:\\n- Your TigerGraph cluster must be on when calling any of the functions here which use `pyTigerGraph` otherwise a connection will not be able to be established. If you are experiencing connection errors, ensure that the cluster you are using is indeed turned on.\\n- The `data` and `feature` target processes can be configured in a couple of ways via a mandatory file `configs/setup.json` which contains the following where `year`, `month`, `test_year`, `test_month` specify the years and months which training and testing data should be pulled from Reddit respectively and `keywords` specifies the number of keywords we save from a user\\'s comment history (and by extension the size of their keyword embeddings). *Note that more recent data within Reddit is larger and will increase the computational needs for almost every aspect of the project. To see where the data is pulled from and see the file sizes, please refer [here](https://files.pushshift.io/reddit/comments/)*.\\n```\\n{\\n    \"year\": \"2010\",\\n    \"month\": \"12\",\\n    \"test_year\": \"2011\",\\n    \"test_month\": \"03\",\\n    \"keywords\": 25\\n}\\n```\\n\\n## Resources:\\n- [Course Site](https://dsc-capstone.github.io/)\\n- [Project Specifications](https://dsc-capstone.github.io/assignments/projects/q2/)\\n- [TigerGraph](https://www.tigergraph.com/)\\n- [TigerGraph Cloud](https://tgcloud.io/)\\n- [Reddit Comment Datasets](https://files.pushshift.io/reddit/comments/)\\n- [TigerGraph Community ML Algos](https://docs.tigergraph.com/graph-ml/current/community-algorithms/)\\n',\n",
       "  'This project focuses on graph-based recommender systems for the social media platform Reddit. The goal is to recommend subreddits to users based on their interactions. The project utilizes the graph database and analytics software TigerGraph. The source code is organized into different directories for data handling, feature generation, and models. The project also includes a website and a video demonstration of the recommendation model. To run the project, prerequisites include installing wget and setting up a TigerGraph cluster. The usage involves running the `run.py` file with the `data` and `features` targets.'],\n",
       " 'https://github.com/KazumaYamamoto2023/DSC180B-Q2-Project': ['# Graph-Based Deep Learning for Fraud Detection in Ethereum Transaction Networks\\n\\nThis project aims to compare graph-based to non-graph based algorithms for fraud detection in Ethereum transaction networks. We will predict whether a given Ethereum wallet in the transaction graph is fraudulent or non-fraudulent, given the wallet\\'s transaction history in the network.\\n\\nGraph exploration, analysis, and model building will be conducted using [TigerGraph](https://tgcloud.io/), an enterprise-scale graph data platform for advanced analytics and machine learning. \\n\\nModel performance was determined by taking the average classification accuracy on the testing set over 10 model runs. The resulting classifier performance for this prediction task are as follows:\\n\\n* Support Vector Machine (~60.5%)\\n* K-Nearest Neighbors (~74.6%)\\n* XGBoost (~81.6%)\\n* Graph Convolutional Network (~79.6%)\\n* Graph Attention Network (~78.5%)\\n* GraphSAGE (~81.9%)\\n* Node2Vec (~76.6%)\\n* Topology Adaptive Graph Convolutional Network (~82.2%)\\n\\n## Getting Started\\n1. Launch a docker container with the following command\\n```bash\\ndocker run -it srgelinas/dsc180b_eth_fraud:latest\\n```\\n2. Clone the repository and `cd` to the project directory:\\n```bash\\ngit clone https://github.com/KazumaYamamoto2023/DSC180B-Q2-Project.git\\n```\\n\\n3. Create a [TigerGraph](https://tgcloud.io/) account and launch an \"ML Bundle\" database cluster. Save the cluster\\'s domain name in `config/tigergraph.json`.\\n\\n4. Open [GraphStudio](https://tgcloud.io/app/tools/GraphStudio/) and create a new graph named \\'Ethereum\\'\\n\\n5. Open [AdminPortal](https://tgcloud.io/app/tools/Admin%20Portal/) and navigate to the \"Management\" tab and select \"Users.\" Generate a secret alias and secret value, and save the secret value in `config/tigergraph.json`.\\n\\n6. Run the following command to connect to the TigerGraph database instance, build the graph schema, load the dataset, and evaluate the models\\n    * This process is detailed in `notebooks/tg_data_loading.ipynb`\\n```bash\\npython run.py eth\\n```\\n\\n## Project Structure \\n```bash\\n├── config\\n│   └── tigergraph.json\\n├── data\\n│   └── visuals \\n│       ├── tagcn_feat_imp.png\\n│       └── tagcn_subgraph.png\\n│   ├── edges.csv\\n│   └── nodes_train_test_split.csv\\n├── gsql\\n│   ├── build_schema.gsql\\n│   ├── get_degrees.gsql\\n│   ├── load_data.gsql\\n│   └── summarize_ammounts.gsql\\n├── notebooks\\n│   ├── tagcn_model_validation.ipynb\\n│   └── tg_data_loading.ipynb\\n├── src\\n│   ├── baseline.py\\n│   ├── connect.py\\n│   ├── gnn_models.py\\n│   ├── node2vec.py\\n│   ├── ta_gcn.py\\n│   └── visualize.py\\n├── Dockerfile\\n├── README.md\\n└── run.py\\n```\\n\\n## File Descriptions\\n\\n`root`\\n* `run.py:` Python file with main method to run the project code\\n* `Dockerfile:` Dockerfile with dependencies to bulid docker image to deploy containerized environment\\n\\n`gsql`\\n* `build_schema.gsql:` GSQL query to create transaction network graph schema in TigerGraph\\n* `get_degrees.gsql:` GSQL query to add indegree/outdegree as node features\\n* `load_data.gsql:` GSQL query to load dataset into TigerGraph database\\n* `summarize_amounts.gsql:` GSQL query to add summary statistics of sent/received ETH as node features\\n\\n`notebooks`\\n* `tagcn_model_validation.ipynb:` Notebook comparing models, visualizing node feature importance and fraudulent wallet subgraphs\\n* `tg_data_loading.ipynb:` Notebook documenting graph schema design and data upload to TigerGraph\\n\\n`src`\\n* `baseline.py:` Python file containing baseline models: Support Vector Machine, K-Nearest Neighbors, XGBoost\\n* `connect.py:` Python file to connect to TigerGraph database, add node features, upload and retreive data\\n* `gnn_models.py:` Python file containing Graph Neural Network models (GCN, GAT, GraphSAGE)\\n* `node2vec.py:` Python file containing Node2Vec model\\n* `ta_gcn.py:` Python file containing TAGCN model\\n* `visualize.py:` Python file to generate visualizations for node feature importance and fraudulent wallet subgraphs\\n\\n\\n## Data Source\\nThis dataset contains transaction records of 445 phishing accounts and 445 non-phishing accounts of Ethereum. We obtain 445 phishing accounts labeled by [Etherscan](etherscan.io) and the same number of randomly selected unlabeled accounts as our objective nodes. The dataset can be used to conduct node classification of financial transaction networks. \\n\\nWe collect the transaction records based on an assumption that for a typical money transfer flow centered on a phishing node, the previous node of the phishing node may be a victim, and the next one to three nodes may be the bridge nodes with money laundering behaviors, as figure shows. Therefore, we collect subgraphs by [K-order sampling](https://ieeexplore.ieee.org/document/8964468) with K-in = 1, K-out = 3 for each of the 890 objective nodes and then splice them into a large-scale network with 86,623 nodes and 106,083 edges. \\n\\n![A schematic illustration of a directed K-order subgraph for phishing node classification.](https://s1.ax1x.com/2020/03/27/GCZGmd.md.jpg)\\n\\n[XBlock](http://xblock.pro/#/dataset/6) collects the current mainstream blockchain data and is one of the blockchain data platforms with the largest amount of data and the widest coverage in the academic community.\\n```\\n@article{ wu2019tedge,\\n  author = \"Jiajing Wu and Dan Lin and Qi Yuan and Zibin Zheng\",\\n  title = \"T-EDGE: Temporal WEighted MultiDiGraph Embedding for Ethereum Transaction Network Analysis\",\\n  journal = \"arXiv preprint arXiv:1905.08038\",\\n  year = \"2019\",\\n  URL = \"https://arxiv.org/abs/1905.08038\"\\n}\\n```\\n---\\n[Project Website](https://srgelinas.github.io/dsc180b_eth_fraud/)\\n\\n[Demo Video](https://youtu.be/WStx_VLHuNk)',\n",
       "  'This project focuses on fraud detection in Ethereum transaction networks using graph-based deep learning algorithms. The goal is to predict whether a given Ethereum wallet is fraudulent or non-fraudulent based on its transaction history. The project compares the performance of various algorithms, including Support Vector Machine, K-Nearest Neighbors, XGBoost, Graph Convolutional Network, Graph Attention Network, GraphSAGE, Node2Vec, and Topology Adaptive Graph Convolutional Network. The highest performing algorithm achieved an accuracy of 82.2%. The project provides instructions for getting started and includes a detailed project structure and file descriptions. The dataset used in the project contains transaction records of phishing and non-phishing accounts in Ethereum.'],\n",
       " 'https://github.com/Barry0121/graph-neural-net-benchmark': ['# DSC180 - Graph Neural Network\\n\\nAuthor: Barry Xue\\n\\nThis repository contains the materials and codes from the exploration of the topic of Graph Neural Network.\\n\\n# Option 1: Train WGAN network with GraphRNN, Discriminator, and Inverter with `main.py`\\n`main.py` will call the training function, and then it will call the visualization function to generate the graphs.\\n* All user defined parameters can be modified in `src/models/args.py` file.\\n\\n# Option 2: Test GCN and GCN-AE with `run.py`\\n`run.py` can run node classification task and edge prediction task with GCN Models (Multi-layer GCN for node classification, and GCN-AE for edge prediction).\\n* Use the `--task` flag to choose between the two tasks.\\n    - Example: python run.py --name \\'expt_test\\' --dataset \\'Cora\\' --task \\'Edge Prediction\\'\\n\\nThere are also two dataset to run either task upon: Cora, CiteSeer, and PubMed.\\n* Use the `--dataset` flag to choose between the two datasets.\\n    - Example: python run.py --name \\'expt_test\\' --dataset \\'CiteSeer\\' --task \\'Node Classification\\'\\n\\nOther options:\\n1. `--epochs`: change the number of training epochs.\\n2. `--hidden_size`: change the first layer hidden layer size.\\n3. `--encode_size`: change the encoding size/final hidden layer size.\\n4. `--train`/`--validation`/`--test`: specify number of training sample per classes, validation size, and testing data size.\\n\\n## Note on installing `pytorch_geometric`\\n* Normally, the command: `conda install -c pyg pyg` will work on MacOS, Windows, and any Linux distro with anaconda/miniconda installed.\\n* There are two known cases where this wouldn\\'t work:\\n1. If the pytorch isn\\'t installed, or the installed version in the environment is <1.12.x, y9ou might need to look into alternative installation method (ex: building from source, use pip, etc).\\n2. The newest MacOS Ventura (v13.0.1) has installation issue, due to the lack of support for M1 Macbooks (pytorch scatter doesn\\'t support \\'mps\\' device yet). One way to get around this issue is to follow this post: https://github.com/rusty1s/pytorch_scatter/issues/241.\\n\\n### Note on our codebase \\n* If you are interested in the \"Discovering Continuous Latent Space Representation of Graph\", everything is in the `src` directory. \\n* If you are interested in simple GNNs, everything is in `legacy_code/model` and `legacy_code/features`. \\n',\n",
       "  'This repository contains materials and codes related to the exploration of Graph Neural Networks. There are two options available for training and testing different models. Option 1 involves training a WGAN network with GraphRNN, Discriminator, and Inverter using the `main.py` file. Option 2 allows testing GCN and GCN-AE models using the `run.py` file, with the ability to choose between node classification and edge prediction tasks. The codebase is organized into different directories depending on the specific area of interest.'],\n",
       " 'https://github.com/hblyx/CommunityDetection': [\"# DSC180 Project 2 - Performance Evaluation of Community Detection on Neural Networks\\n#### Yaoxin Li, Justin Nguyen, Vivek Rayalu\\n\\n### Introduction\\nAs our previous work, we explored the traditional solutions of community detection, including Louvain, Girvan-Newman, and etc, we are also explring the solutions with neural networks. For this project, we explore the community detection solutions with machine learning, deep learning in particular. We specifically explore the performance of neural networks on task of community detection by implementing and training different neural networks, including Multiple Layers Perceptrons and Graph Neural Networks, to test whether neural networks can be a solution for community detection.\\n\\nThis repository contains all code for all findings and attempt related to this project.\\n\\n### Files\\n* `checkpoints` contains best models' stats in format of PyTorch's `.pt`.\\n* `config` contains parameters used for models.\\n* `notebooks` contains notebooks which illustrates data analysis, the result,  and training process.\\n* `outputs` contains the training plots including loss and score plots of models.\\n* `references` contains all reference.\\n* `src` contains all source code used for data analysis, models, and training. \\n    * `data` contains source code used for generate random and test data, data analysis, loading data for models, and some code for read specific format of data.\\n    * `features` contains code for feature engineering.\\n    * `models` contains the code for models, algorithms, and training.\\n* `test` contains the test data for `run.py`. Specifically, the test data are stored in `/test/testdata/`.\\n* `requiremetnts.txt` speficy the requirements of running this project.\\n* `run.py` can run a simple test for this project.\\n* `submission.json` contains information of Docker image for this project.\\n\\n### Requirements\\n`submission.json` contains the Docker image which have all packages needed. Meanwhile, the specific requirements are in `requirements.txt`. The Docker image contains all needed environment exclude `torch` and `torch_geometric`. Specifically, since `torch` and `torch_geometric` requires specific version according to the device, CUDA version, we choose to leave them. Therefore, to reproduce our results, `torch` and `torch_geometric` needed to be installed correctly according to CUDA version or CPU only version. The details of installing `torch` and `torch_geometric` can be found in https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html and https://pytorch.org/get-started/locally/ .\\n\\n### Run\\nTo run the project, we left the test with `run.py`. However, since this project is  more about to explore the solutions for community detection, the content is relatively mass. It is really difficult to re-run all analysis, model training, and algorithms implemented in a short period of time. Therefore, in `test` of `run.py`, it will run our naive traditional community detection algorithm which depends on the number of common neighbors on the test dataset. It will just make sure the graph/network enviroment has been set correctly with test data. In addition, since the entire environment of the running models of Graph Neural Network depends on the hardware environment. Specifically, since the training process requires to specify the GPU/CPU, it must corporate with the correct version of PyTorch, `torch`, and `torch_geometric` which need to specify whether use CPU only or specific CUDA version. Since the device run these code might have different hardware environment, we leave this part free to change. However, all training results and process are reproducible in the notebooks and code.\\n\\nInstead of presenting our results and code in `run.py`, we choose notebooks to show the results and process. Specifically `/notebooks` folder contains all attempts, models, and results we did, built, and ran. \\n\\n### Website\\n\\nThe project [website](https://hblyx.github.io/CommunityDetection/) and its source code is under the [gh-pages branch](https://github.com/hblyx/CommunityDetection/tree/gh-pages) of the same repository.\\n\",\n",
       "  'This project focuses on the performance evaluation of community detection on neural networks. The researchers explore traditional solutions like Louvain and Girvan-Newman, as well as solutions using machine learning and deep learning. They implement and train different neural networks, such as Multiple Layers Perceptrons and Graph Neural Networks, to test their effectiveness in community detection. The repository contains code, notebooks, and data related to the project. The requirements for running the project are specified in the `requirements.txt` file. The project can be run using the `run.py` script, but it mainly serves as a test for graph/network environment setup. The results and process are presented in the notebooks found in the `/notebooks` folder. Additionally, there is a website associated with the project that showcases its findings and includes the source code.'],\n",
       " 'https://github.com/stassinopoulosari/dsc180b-a15-q2project': ['# DSC 180B WI23-A15-2: Community Detection on Twitter\\n\\n## Abstract\\n\\nRecent work examined the vast unfolding of communities in large networks, in which it was shown that the Louvain Algorithm was the most effective at identifying and dividing communities into clusters. The growth of social media networks in the modern world nurtures the growth and identification of similarities between groups of people. These similarities between groups can be identified more formally as communities. While the number and types of communities grow, the identification and classification of these communities becomes more challenging. To define the scope of the project, we will be utilizing public data from the social media network Twitter. Specifically, we will look at the followers and followings of users throughout twitter. In this paper, we utilize the Louvain Algorithm to explore communities within the social media platform twitter. The results of the study allowed us to uncover and analyze distinct communities based on our seed account of a modern day rap musician named Dessa (@DessaDarling). Further research is needed to determine the potential applications of these algorithms in the field of community detection in social media since we only used one platform’s data.\\n\\n## Running our code\\n\\nWe recommend using [this docker repository](https://hub.docker.com/repository/docker/stassinopoulosari/dsc180b-wi23-a15-2/general). To start the code, use the following entry point command:\\n\\n`python run.py data test`',\n",
       "  'This paper discusses the use of the Louvain Algorithm to identify and analyze communities on Twitter. The study focuses on the followers and followings of a rap musician named Dessa (@DessaDarling). The results reveal distinct communities within the platform. Further research is needed to explore the potential applications of these algorithms in community detection on social media. To run the code, it is recommended to use a specific docker repository and execute a command.'],\n",
       " 'https://github.com/darehunt/DSC180B-Project2': ['# DSC 180B [WI 23] Project 2:<br> Community Detection of Music Genres\\n*** Classifying Spotify Artists through Community Detection and Clustering ***\\n\\n<!--This site was built using [GitHub Pages](https://pages.github.com/).-->\\n\\n## Data Background\\n<!-- TODO -->\\nOur primary source of data is a dataset of Spotify playlists collected by Andrew Maranhão, which is freely available on Kaggle [(link)](https://www.kaggle.com/datasets/andrewmvd/spotify-playlists). This dataset was collected using a subset of users who published their #nowplaying tweets via Spotify. This tabular dataset lists a row for each song that was tweeted out, containing the name of the song, the artist of the song, and the playlist that song was playing from. While the data also contained the user IDs of each person who tweeted the track they were listening to, our model does not take personal information as input.\\n\\n## Deployment\\n\\nClone the project\\n\\n```bash\\n  git clone https://github.com/darehunt/DSC180B-Project2\\n```\\n\\nAfter running Docker and logging into your account, pull and launch the docker image: (note that this requires more RAM than the usual to run)\\n```bash\\n  launch.sh -i anmokhta/DSC180B-proj2:latest -m 32\\n```\\n\\nFrom there, copy in the directory, change directories into the project and run commands from bash using run.py:\\n\\n```bash\\n  cp DSC180B-Project2 .\\n  cd DSC180B-Project2\\n  python run.py all\\n```\\n## Commands\\n\\nThe build script can be run directly from bash `python run.py`\\n\\n| Command | Description |\\n| --- | --- |\\n| `clean`  | Clears reminents of previous networks or community detection  |\\n| `data`  | Downloads, extracts, and prepares data network from Kaggle dataset  |\\n| `model`  | Runs community detection to attempt to group artists by genre |\\n| `all`  | equivalent of running `data model`  |\\n\\n## Authors\\n\\n- [@darehunt](https://www.github.com/darehunt)\\n- [@anmokhta](https://www.github.com/anmokhta)\\n- [@Btran206](https://www.github.com/Btran206)\\n- [@jul016](https://www.github.com/jul016)\\n\\n',\n",
       "  'This project focuses on community detection and clustering of Spotify artists to classify music genres. The primary source of data is a dataset of Spotify playlists collected from #nowplaying tweets via Spotify. The deployment process involves cloning the project, running Docker, and executing commands using run.py. The available commands include cleaning previous networks or community detection, downloading and preparing data network from the Kaggle dataset, running community detection to group artists by genre, and running all commands. The authors of this project are darehunt, anmokhta, Btran206, and jul016.'],\n",
       " 'https://github.com/gordonhu608/Revisit_CLIP': ['# Revist CLIP: Multi-perspective improvements on Vision-Language Model\\n\\n\\n> [Wenbo Hu](https://gordonhu608.github.io/), [Johnny Liu](https://github.com/jawkneeLoo)\\n\\n[![Website](https://img.shields.io/badge/Project-Website-87CEEB)](https://gordonhu608.github.io/Revisit_CLIP/)\\n[![Temporary paper](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://gordonhu608.github.io/files/revisitclip.pdf)\\n\\n\\n<hr />\\n\\n# :rocket: Highlights\\n\\n![main figure](docs/main_figure.png)\\n> **<p align=\"justify\"> Abstract:** *Large-scale contrastive vision-language pre-training\\n> has shown significant progress in visual representation\\n> learning. Unlike traditional visual systems trained by a\\n> fixed set of discrete labels, a new paradigm was introduced\\n> in CLIP to directly learn to align images with raw texts in\\n> an open-vocabulary setting. On downstream tasks, a carefully designed text prompt is employed to make zero-shot\\n> predictions. To avoid non-trivial prompt engineering, context optimization has been proposed to learn continuous vectors\\n> as task-specific prompts with few-shot training  examples. Instead of learning the input prompt token,\\n> an orthogonal way is learning the weight distributions of\\n> prompt, which is also very effective. An alternative\\n> path is fine-tuning with a light-weight feature adapter\\n> on the visual branch The most recent work introduces multimodal prompt learning, which uses a synergy function\\n> to simultaneously adapt language and vision branches for\\n> improved generalization. In our work, we revisit recent improvements in CLIP from different perspectives and propose\\n> an optimal way of combining the model’s architecture. We\\n> demonstrate that Data Augmentation (DA) and Test-Time\\n> Augmentation (TTA) are important for few-shot learning\\n> (FSL). We propose an end-to-end few-shot learning pipeline\\n> (DA + MaPLe + Adapters + TTA) that can be referenced for\\n> all downstream tasks. Compared with the state-of-the-art\\n> method ProDA  in FSL, our model achieves an absolute\\n> gain of 6.33% on the 1-shot learning setting and 4.43% on\\n> the 16-shot setting, averaged over 10 diverse image recognition datasets.* </p>\\n\\n## Main Contributions\\n\\n1) **Standard workflow for few-shot learning:** We combined current state-of-the-art models from different perspectives and achieved better performance.\\nWe gained an average of 5.28% absolute improvement for [1,2,4,8,16] shots learning over 10 diverse image\\nrecognition datasets than the best baseline model.\\n2) **Data and Test Time Augmentation:** We employed optimal Data Augmentation and Test Time Augmentation (TTA) and demonstrates TTA’s\\nimportance in few-shot learning and thus should be used as a convention in future few-shot learning tasks\\n\\n\\n## :ballot_box_with_check: Supported Methods\\n\\n| Method                    | Paper                                         |                             Configs                             |          Training Scripts          |\\n|---------------------------|:----------------------------------------------|:---------------------------------------------------------------:|:----------------------------------:|\\n| MaPLe                     | [arXiv](https://arxiv.org/abs/2210.03117)    | [link](configs/trainers/MaPLe/vit_b16_c2_ep5_batch4_2ctx.yaml)  |       [link](scripts/maple)        |\\n| CoOp                      | [IJCV 2022](https://arxiv.org/abs/2109.01134) |                  [link](configs/trainers/CoOp)                  |        [link](scripts/coop)        |\\n| Co-CoOp                   | [CVPR 2022](https://arxiv.org/abs/2203.05557) |                 [link](configs/trainers/CoCoOp)                 |       [link](scripts/cocoop)       |\\n| Ours                    | [arXiv](https://gordonhu608.github.io/files/revisitclip.pdf) |                  [link](configs/trainers/Ours)                  |        [link](scripts/ours)        |\\n<hr />\\n\\n## Results\\n### Our method in comparison with existing methods\\nResults reported below show accuracy for few shot training on [1,2,4,8,16] for across 10 recognition datasets averaged over 3 seeds.\\n\\n| Name                                                      | 1-shot | 2-shot |   4-shot     | 8-shot |  16-shot | \\n|-----------------------------------------------------------|:---------:|:----------:|:---------:|:------:|:------:|\\n| [CLIP](https://arxiv.org/abs/2103.00020)                  |   36.13   |   47.83    |   58.52  |   66.24   |   72.03    |   \\n| [CoOp](https://arxiv.org/abs/2109.01134)                  | 59.95 |   63.74    |    67.18  |  70.52  |  74.03  | \\n| [CLIP-Adapter](https://arxiv.org/abs/2110.04544)                |   62.13   |   65.64    |   69.07   |   72.55  |   76.  |  \\n| [ProDA](https://arxiv.org/abs/2205.03340)                |   65.19   |   68.59    |   71.4   |   74.21   |   76.78   | \\n| [Ours](https://gordonhu608.github.io/files/revisitclip.pdf)          |   **71.94** | **74.12**  | **78.48** |   **80.2**    |   **82.64**   | \\n\\n## Installation \\nFor installation and other package requirements, please follow the instructions detailed in [INSTALL.md](docs/INSTALL.md). \\n\\n## Data preparation\\nPlease follow the instructions at [DATASETS.md](docs/DATASETS.md) to prepare all datasets.\\n\\n## Training and Evaluation\\nPlease refer to the [RUN.md](docs/RUN.md) for detailed instructions on training, evaluating and reproducing the results using our pre-trained models.\\n\\n\\n<hr />\\n\\n## Contact\\nIf you have any questions, please create an issue on this repository or contact at w1hu@ucsd.edu\\n\\n\\n## Acknowledgements\\n\\nOur code is based on [Co-CoOp, CoOp](https://github.com/KaiyangZhou/CoOp) and [MaPLe](https://github.com/muzairkhattak/multimodal-prompt-learning) repository. We thank the authors for releasing their code. If you use our model and code, please consider citing these works as well.\\n\\n',\n",
       "  \"The paper discusses improvements on the CLIP (Contrastive Language-Image Pre-training) model from different perspectives. The authors propose an optimal way of combining the model's architecture and demonstrate the importance of Data Augmentation (DA) and Test-Time Augmentation (TTA) in few-shot learning. They also provide a standard workflow for few-shot learning and achieve better performance compared to baseline models. The results show that their method outperforms existing methods on various recognition datasets. The paper provides installation instructions, data preparation guidelines, and training/evaluation details for reproducing the results.\"],\n",
       " 'https://github.com/GSam789/DSC180B-Capstone-Network-Dissection': ['# Improving Network Accuracy through Network Manipulation\\n\\n\\n## To run our code:\\nSimply run ```python3 test.py``` to find the accuracies of our 3 trained VGG16 models on the first 100 test data points of CIFAR-100.\\n\\n## Project Background & Context\\nDeep learning has been growing rapidly over the past couple decades due to its ability in solving extremely complex problems. However, this machine learning\\nmethod is often considered as a \"black box\" since it is unclear how the neurons of a deep learning model work together to arrive at the final output. A recently found\\nmethod called Network Dissection has solved this interpretability issue by coming up with a visual that shows what each neuron looks for and why. Given that we have \\ninformation regarding neuron activities, we want to investigate if we can use this information to further improve our network\\'s performance.\\n\\nThus, in this project, we explored 3 methods:\\n1. Network Dissection Intervention\\n2. FocusedDropout\\n3. Input Gradient Regularization\\n\\nWe implemented these network dissection intervention on VGG16 that was pre-trained on Places365.\\nWe implemented FocusedDropout and Input Gradient Regularization separately on VGG16 on CIFAR-100 dataset. This was done due to our limited resources since training on \\nPlaces365 would take 50 days.\\n\\n## Rundown of Folders & Files\\nIn this Github repository, you will find the following folders and files:\\n\\n- Folders:\\n  - models: Contains all the state dictionaries of the trained models\\n  - utils: Contains all the model skeletons and FocusedDropout implementation\\n\\n- Files:\\n  - dataloader.py: Loads the CIFAR-100 dataset into a DataLoader\\n  - plotting.py: Plots the resulting accuracies and losses over epochs\\n  - test_data.pt: The first 100 test data points of CIFAR-100\\n  - test.py: Runs our 3 VGG16 on CIFAR-100 models (Baseline, VGG16 with FocusedDropout, VGG16 with Input Gradient Regularization) on test_data.pt \\n  - train_baseline_focuseddropout.py: When run, trains either baseline model (Plain VGG16) or VGG16 with FocusedDropout from scratch based on selected model in file and \\nsaves final model\\'s state dictionary into the models/ folder\\n  - train_input_grad.py: When run, trains VGG16 with Input Gradient Regularization and saves final model\\'s state dictionary into the models/ folder\\n\\n',\n",
       "  'This project aims to improve the accuracy of deep learning networks through network manipulation. Three methods were explored: Network Dissection Intervention, FocusedDropout, and Input Gradient Regularization. The implementation was done on VGG16 models trained on CIFAR-100 dataset. The code can be run using `python3 test.py` to obtain accuracies for the first 100 test data points of CIFAR-100. The repository contains folders for models and utils, as well as files for data loading, plotting, and training different models.'],\n",
       " 'https://github.com/agupta01/ml-theory-capstone': ['# Benchmarking Kernel Machines on Text Datasets\\nData Science Capstone Project advised by Mikhail Belkin.\\n\\n## Scaling Tests\\nIf you would like to run the expriments used in the report \"On Feature Scaling of Recursive Feature Machines\", follow the steps below:\\n\\n1. Have a GPU and a GPU-enabled Pytorch v1.13 environment (feel free to use the environment.yml file in the repo, although this contains a lot of other stuff).\\n2. Navigate to `src` within the project root (`cd src`)\\n3. Run the following:\\n```shell\\npython scaling.py --name=<PROVIDE A NAME HERE> \\\\\\n\\t--noise=<specify noise to add to dataset here> \\\\\\n\\t--N_runs=<set to 100 for 100 trials> \\\\\\n\\t--N=<number of examples in dataset> \\\\\\n\\t--target_fn=<cubic for the default function, randmat for the random matrix function> \\\\\\n\\t--baseline=<True to run baseline (Laplacian) kernel, False to train full RFM)\\n```\\n4. After the experiment is run (100 trials takes about 30 minutes when N=1000 on a RTX 2060), you can find result files in `<project root>/results/arrays/scaling_results`. Every run will generate two numpy arrays, named `train_MSEs_<name>.npy` and `test_MSEs_<name>.npy`, appended with \"\\\\_baseline\" if a baseline run was used. Each array has shape (N_runs, len(d_range)), where d_range are the feature sizes attempted ([5, 6, 7, ..., 99] + [100, 110, 120, ..., 2000] in the base experiment in the original paper).\\n\\n## How to use\\n```shell\\nusage: run.py [-h] [--verbose] {test,test-data,mnist,cifar10,fashionmnist}\\n\\npositional arguments:\\n  {test,test-data,mnist,cifar10,fashionmnist}\\n                        task to run\\n\\noptional arguments:\\n  -h, --help            show this help message and exit\\n  --verbose             Set logging to DEBUG\\n```\\n\\nExample:\\n\\n```shell\\n$ python run.py mnist --verbose\\nDec 04 2022 11:38PM [DEBUG] \\t Logging set to DEBUG\\nDec 04 2022 11:38PM [DEBUG] \\t Using 1259 training samples and 210 test samples\\nDec 04 2022 11:38PM [DEBUG] \\t x_train shape: (1259, 784)\\nDec 04 2022 11:38PM [DEBUG] \\t y_train shape: (1259,)\\nDec 04 2022 11:39PM [INFO] \\t TRAIN MSE: 0.000 | Accuracy: 1.000\\n TEST MSE: 0.019 | Accuracy: 0.995 | Precision: 0.995 | Recall: 0.992\\n```\\n\\nThe corresponding config files for each dataset can be found in `config/<dataset>.json`. Below is an explanation of the options:\\n\\n```json\\n{\\n    \"kernel_type\": \"laplace\", # can be either laplace or gaussian\\n    \"print_result\": true, # to print result to log\\n    \"data\": {\\n        \"dataset\": \"mnist\", # dataset to use, can be \"mnist\", \"cifar10\", or \"fashionmnist\"\\n        \"subset\": 0.1, # subset of dataset to use. See note below.\\n        \"pos_class\": 1, # class label for positive class (1)\\n        \"neg_class\": 8 # class label for negative class (-1)\\n    },\\n    \"model\": {\\n        \"gamma\": 0.00128, # kernel bandwidth, see src/utils.py for further details\\n        \"return_metrics\": true # return metrics rather than predictions after training kernel\\n    }\\n}\\n```\\n\\n## A note on not breaking your computer\\nThis code produces pairwise distance kernels for use in kernel machines for binary classification. If your dataset\\nhas $n$ examples, your kernel will be $n \\\\times n$! My 2018 Macbook Pro with 16GB RAM can only handle $n \\\\approx 1000$ \\nbefore it starts to freeze up on itself. Use the `subset` parameter in the config file, do the math, and you may \\navoid bricking your laptop for 5 minutes.\\n\\nIf you\\'re using DSMLP, subset = 0.01 should work for most datasets.\\n',\n",
       "  'This text provides instructions and information about benchmarking kernel machines on text datasets. It includes steps for running scaling tests, usage instructions for the code, and configuration options for different datasets. The text also includes a note on avoiding computer performance issues when working with large datasets.'],\n",
       " 'https://github.com/jimzers/DSC180B-A08': [\"# DSC180B A08: Imitating Behavior to Understand the Brain\\n\\nScott Yang, Daniel Son, Akshay Murali, Adam Lee, Eric Leonardis, Talmo Pereira\\n\\n#### Repos combined:\\n\\n- https://github.com/danielcson/dsc_capstone_q1\\n- https://github.com/scott-yj-yang/180A-codebase\\n- https://github.com/akdec00/DSC-180A\\n- https://github.com/jimzers/dsc180-ma5\\n\\n#### DSMLP Spawning Script\\n\\nTo train the model using UC San Diego's Data Science & Machine Learning Platform (DSMLP), you can setup a training\\nenvironment in the following commands\\n\\n```bash\\n# on dsmlp's bash\\nIDENTITY_PROXY_PORTS=1 launch-scipy-ml.sh -b -j -g 1 -m 32 -c 10 -i scottyang17/dm:latest\\n```\\n\\nExplanation: `IDENTITY_PROXY_PORTS=1` allow DSMLP's proxy port forwarding another empty port for the sake of record\\nkeeping interfaces such as `tensorboard`. `-b` means run the pod in background mode, `-j` means launch Jupyter notebook\\nserver within container (default), `-i` means custom docker image name.\\n\\n#### Training Procedures\\n\\nEntrypoint: Train expert SAC agent with `train_cheetah.py`\\n\\n```bash\\npython train_cheetah.py --automatic_entropy_tuning=True\\n```\\n\\nExtract activations\\n\\n```bash\\npython src/run/extract_activations.py --model_path data/models/sac_checkpoint_cheetah_123456_10000 --env_name HalfCheetah-v4 --num_episodes 1000 --save_path data/activations/cheetah_123456_10000\\n```\\n\\nCollect expert data\\n\\n```bash\\npython src/run/collect_expert.py --model_path data/models/sac_checkpoint_cheetah_123456_10000 --env_name HalfCheetah-v4 --num_episodes 15 --save_path data/rollouts/cheetah_123456_10000\\n```\\n\\nTrain behavioral cloning agent\\n\\n```bash\\npython src/run/train_bc.py --rollout_path data/rollouts/cheetah_123456_10000/rollouts.pkl --save_path data/bc_model/cheetah_123456_10000 --epochs 10 --batch_size 32 --lr 3e-4\\n```\\n\\nTODO: Run analysis\\n\\n```bash\\npython run_analysis --policy=path/to/policy --analysis_path=path/to/analysis \\n```\\n\\n\\n### Fire and forget bash scripts\\n\\n```bash\\nbash scripts/collect_expert.sh\\nbash scripts/train_bc.sh\\nbash scripts/collect_activations.sh\\nbash scripts/collect_activations_bc.sh\\n```\\n\",\n",
       "  \"This is a summary of the DSC180B A08 project on imitating behavior to understand the brain. The project involves multiple contributors and repositories. The DSMLP Spawning Script is provided to set up a training environment using UC San Diego's Data Science & Machine Learning Platform. The training procedures include training an expert SAC agent, extracting activations, collecting expert data, and training a behavioral cloning agent. There are also fire and forget bash scripts available for convenience.\"],\n",
       " 'https://github.com/jmryan19/DSC180': ['# DSC180\\nHelpers contatins python files of helper code written to make experimentation faster\\nMost exploring is done in 1-3_NAME.ipynb.\\n\\nUnfortunately, github cannot hold the weights of the final model, as the file is too large. \\n\\nIn order to run:\\nMust be run with a GPU!\\npython run.py [test or train]\\n\\nTrain will go through the training process of the four omodels utilized in the paper.\\nTest will run all statistics on models used on paper and regenerate figures.\\n\\nThe website corresponding to this project is located at https://jmryan19.github.io/DSC180.io/\\n',\n",
       "  'The DSC180 project includes helper code in Python files to speed up experimentation. Most exploration is done in the 1-3_NAME.ipynb notebook. However, the weights of the final model cannot be stored on GitHub due to their large size. To run the project, a GPU is required. The \"run.py\" script can be used with either \"test\" or \"train\" as arguments. Running \"train\" will go through the training process of the four models used in the paper, while running \"test\" will generate statistics and figures for all models used in the paper. The corresponding website for this project can be found at https://jmryan19.github.io/DSC180.io/.'],\n",
       " 'https://github.com/ddav118/DSC-180B': [\"# UC San Diego, DSC-180B, Winter 2023 <br>\\nPredicting Pulmonary Edema Using Deep Learning and Image Segmentation <br>\\nTeam Members: David Davila-Garcia, Marco Morocho, Yash Potdar\\n\\nNote: All data was deidentified but is not publicly available\\n\\n├── README.md          <- The top-level README for developers using this project.<br>\\n├── Final_Report.pdf<br>\\n├── Final_Poster.pdf<br>\\n├── models             <- Contains the outputs from trained models: Losses and Test Set Predictions<br>\\n│   ├── Losses         <- Training and Validation MAE Losses by Epoch.<br>\\n│   ├── Test Set Preds <- NT-proBNP predictions on test set using the best model (minimize MAE valid loss).<br>\\n├── 1 - Preprocessing.ipynb                               <- Cleaning the original x-rays + clinical data, excluding rows with missing data/no image available<br>\\n├── 2 - Transfer Learning Training & Evaluation.ipynb     <- (Not used in project) Provided by UCSD AIDA Lab, shows training of U-Net segmentation model.<br>\\n├── 3 - Predicting Unannotated.ipynb                      <- Used the U-Net segmentation model from the UCSD AIDA Lab to create binary masks (lungs, heart, clavicles, spinal column) for each radiograph in our dataset. Saved the segmentations to an hdf5 file. <br>\\n├── 4 - Creating Masks.ipynb                              <- Uses the binary masks created in '3 - Predicting Unannotated.ipynb' to produce the segmentation inputs for our model <br>\\n├── 5 - CNN Models.ipynb                                  <- Contains all code for training and testing models. <br>\\n├── model.py           <- Contains modified ResNet152 architectures, extends the Pytorch ResNet152 implementation <br>\\n├── train.py           <- Contains model training and testing functions; different inputs called for different architectures<br>\\n\\n\\nAcknowledgements: Thank you to our incredible mentor Albert Hsiao, MD, PhD for his guidance, and Amin Mahmoodi for providing the U-Net segmentation network.\\n\",\n",
       "  'This is a summary of a project conducted by a team at UC San Diego. The project focuses on predicting pulmonary edema using deep learning and image segmentation. The team members are David Davila-Garcia, Marco Morocho, and Yash Potdar. The project includes various files such as a final report, final poster, and trained models. The team used preprocessing techniques, transfer learning, and CNN models for training and testing. They also utilized a U-Net segmentation model provided by the UCSD AIDA Lab. The project was guided by mentor Albert Hsiao, MD, PhD, and the U-Net segmentation network was provided by Amin Mahmoodi.'],\n",
       " 'https://github.com/shivsakthivel/CNN-Multilabel-Classification': ['# Exploring the viability of Convolutional Neural Networks (CNNs) on a multi-label classification task to detect radiographic outliers\\n\\n## Task\\nAn implementation of a Convolutional Neural Network (CNN) multi-label classifier that takes in chest radiograph images as and outputs their corresponding predicted labels for detecting pulmonary edema and pleural effusion.\\n\\n## Retrieving the Data for this project\\nThe data available for this project came in the form of DICOM files stored on a Google Cloud instance (credentialed access only), with the entire database being of size 4 TB. The required credentialing can be obtained [here](https://physionet.org/content/mimic-cxr/2.0.0/). For the purposes of this project and accessing the DSMLP resources, the source radiograph images had to be manually downloaded in batches and transferred onto the teams drive on DSMLP. \\n\\nThe scripts associated with this repository, therefore, assume that the user has the required access to the data files, with the required filepaths relative to the directory in which they were developed. However, this GitHub repository contains exploratory notebooks, walking through the data access and model training process, and covers examples of the obtained results. Specifically, the notebook `Single-Var-Model-Edema.ipynb` is a comprehensive exploration of one of the single label binary classifiers developed for this project. The model build and evaluation techniques for the other models developed in this project largely follow a similar process.\\n\\n## Build and Run\\n- To run the single label Pulmonary Edema classifier run `python main.py edema`.\\n- To run the single label Pleural Effusion classifier run `python main.py effusion`.\\n- To run the multi-label classifier run `python main.py multilabel`.\\n- To run the multi-class classifier run `python main.py multiclass`.\\n\\n## Requirements\\nThe dependencies required for this project can be installed by running `pip install -r requirements.txt`.\\n\\n## Other Notes\\nIf running the code on this repository, the DSMLP instance should be launched with GPU to ensure that the files run efficiently (The project was developed using tensorflow).\\n',\n",
       "  'This project explores the use of Convolutional Neural Networks (CNNs) for a multi-label classification task to detect radiographic outliers. The goal is to develop a CNN classifier that can predict labels for pulmonary edema and pleural effusion in chest radiograph images. The data for this project is stored as DICOM files on a Google Cloud instance, with a total size of 4 TB. The required credentials can be obtained from the provided link. The scripts in this repository assume that the user has access to the data files and have the necessary filepaths relative to the development directory. The repository also includes exploratory notebooks and examples of model training and evaluation. To run the classifiers, specific commands are provided in the \"Build and Run\" section. The project requires certain dependencies, which can be installed using the provided requirements.txt file. It is recommended to run the code on a DSMLP instance with GPU for efficient execution, as TensorFlow was used for development.'],\n",
       " 'https://github.com/Angela-Wang111/Pneumothorax_classification': ['# DSC-180B-Project\\n**Project Topic:** classification of penumothorax dataset [CANDID-PTX](https://pubs.rsna.org/doi/10.1148/ryai.2021210136) using classification models, segmentation models, and cascade models.\\n\\nGROUP NAME: AC/DS :metal: (Angela + Cecilia -> AC, Data Science -> DS, AC/DC -> AC/DS) :fist_right::fist_left:\\n\\n**Brave Angela Not Afraid of Error :partying_face:**\\n\\n**Be Calm and Write Code Cecilia :innocent:**\\n\\n## Goal :pray:\\n- [x] Birthday (Angela) :birthday:\\n- [x] Classification Final Results\\n- [x] Segmentation Final Results\\n- [x] Cascade Final Results\\n- [x] Poster [03/09 DDL]\\n- [x] Website/Report/Code [03/14 DDL]\\n- [ ] Presentation/Birthday (Cecilia) :birthday: [03/15 DDL]\\n\\n## Website\\nIf you just want to have an idea of what this project is about without seeing all these codes (which I understand :stuck_out_tongue_winking_eye:), click here :point_right: https://angela-wang111.github.io/Pneumothorax_classification/\\n\\n## Documentation\\nThe useful files for checkpoint testing phase are: run.py, submission.json, config.json, src(folder), test(folder), and outpout(folder). \\n\\n:heavy_exclamation_mark:Execution instruction (in terminal):\\n1. `ssh <username>@dsmlp-login.ucsd.edu`\\n2. `launch.sh -i angela010101/pneumothorax:latest -c 8 -m 64 -g 1` at least 1 GPU and 64 GB memory is needed\\n3. `git clone https://github.com/Angela-Wang111/Pneumothorax_classification` (first time execution only)\\n4. `cd Pneumothorax_classification`\\n5. `python run.py <model type>` model type has to be one of \"classification\", \"segmentation\", or \"cascade\"\\n6. :crossed_fingers:\\n### submission.json\\nContains the URLs for this Github Repository and the DockerHub Repository used for building a docker image for this pipeline.\\n### config.json\\nContains the hyperparameters used for model training.\\n### run.py\\nThis is the main .py file for executing the whole pipeline from data preprocessing to training and test classification models, segmentation models, cascade models. To run it, just run `python run.py <model type>` in the terminal (and hope everything goes fine :crossed_fingers: ). Model type has to be in the following three formats: \"classification\", \"segmentation\", \"cascade\" (all in lowercase). Example of the full terminal command: `python run.py classification`.\\n### src\\n#### data_preprocessing.py\\nThis file contains functions to decode the RLE encoded pixels from the source \"test/testdata/Pneumothorax_reports_small.csv\" file and to save both positive and negative masks into test/testdata/masks, so they could be used for the segmentation model training/testing.\\n#### generate_train_val_test_csv.py\\nThis file contains functions to generate \"test/testdata/train.csv\", \"test/testdata/train_pos.csv\", \"test/testdata/train_neg.csv\", \"test/testdata/validation.csv\", and \"test/testdata/test.csv\" for model training/validation/test.\\n#### create_dataloader.py\\nThis file contains functions to create dataframe from .csv file like \"test/testdata/validation.csv\", create custermized Dataset, and create DataLoader. The function to create customized Dataset is modified to read .png formatted original images. The full version code is written to read DICOM format images stored in the team group folder.\\n#### build_model.py\\nThis file contains functions to build customized pytorch pretrained ResNet 34 model and pretrained EfficientNet-B3 model, and train/validate classification models & segmentation models & cascade models.\\n#### evaluate_test.py\\nThis file contains fucntions to plot, print, and save metrics based on the test set to evaluate all models.\\n#### save_model_imgs.py\\nThis file contains functions to save predicted masks from pre-trained segmentation models. Mainly used for preparing images to be input to the classification models during the cascade model training.\\n#### run_model.py\\nThis file contains functions to execute the entire pipeline to run classification, segmentation, and cascade models. It automatically run all models within the same structure.\\n- We currently disabled save_model() function to avoid saving large files on github. To enable it, please uncomment the lines `save_model(cla_model, file_name)` in the run_class() function and `save_model(seg_model, file_name)` in the run_seg() function.\\n\\n### test\\nAll the data here are just a small portion of CANDID-PTX (100/19237) for pipeline testing purpose only since the original data size is ~30GB.\\n*All empty folders currently store the outputs after test trials.*\\n#### testdata\\n- *Pneumothorax_reports_small.csv*: source test .csv file. Includes 100 penumothorax samples (15 positive, 85 negative) with **SOPInstanceUID** to identify each sample, and **EncodedPixels** to specify the penumothorax region (in RLE encoded format if positive, -1 if negative).\\n- *images*: folder contains the original X-Ray images (1024x1024). Named in the format \"\\\\<SOPInstanceUID>.png\". The original images are in DICOM format, but are changed to .png format for the pipeline testing purpose. \\n- *masks*: empty folder to store the decoded binary masks.\\n- *intermediate_data*: empty folder to store the intermediate images generated by the segmentation models. These images will be used as input for classification models in the cascade structure.\\n\\nAfter runing the pipeline, the following files would be created :point_down:\\n- *masks*: folder contains the decoded binary masks (1024x1024). Named in the format \"\\\\<SOPInstanceUID>.png\" if positive, \"negative_mask.png\" if negative.\\n- *train.csv*: training set with 80 samples (12 postive, 68 negative).\\n- *train_pos.csv*: all positive samples in the training set\\n- *train_neg.csv*: all negative samples in the training set\\n- *validation.csv*: validation set with 10 samples (2 positive, 8 negative).\\n- *test.csv*: test set with 10 samples (1 positive, 9 negative).\\n\\n#### saved_model\\nThis is the folder where saved models will be located if the function is enabled.\\n\\n### output\\nThis should be an empty folder before executing the pipeline. After executing the pipeline, the following metrics plots will be created :point_down:\\n- auc-roc plot inside *auc-roc* folder.\\n- train/val losses inside *both_loss* folder.\\n- confusion matrix inside *confusion_matrix* folder.\\n',\n",
       "  'The project topic is the classification of the penumothorax dataset using classification models, segmentation models, and cascade models. The group name is AC/DS, consisting of Angela and Cecilia. The goal includes completing various tasks such as final results for classification, segmentation, and cascade models, creating a poster and website/report/code, and giving a presentation. The documentation provides instructions for executing the project pipeline and describes the various files and folders involved.'],\n",
       " 'https://github.com/styyxofficial/DSC180B-Quarter-2-Project': ['# Processing Electrophysiology Data to Extract Neural Trajectories\\n\\nRaw electrophysiology data is very high dimensional and contains a lot of noisy, spiky, activity. Due to this, it must be heavily processed before the accurate neural trajectories can be extracted.\\nWe utilized Variational Latent Gaussian Process in our study to reduce its dimensions and smooth our data.\\n\\nUsing this dimensionality reduced smooth data, we created a classifier to predict mouse behavior.\\n\\n## Variational Latent Gaussian Process\\n\\nIn a variational latent Gaussian process (VLGP), the observed data, y, is modeled as a Gaussian process, with mean function, f(x), and covariance function, k(x, x\\'). The underlying structure in the data is captured by latent variables, z, which are treated as random variables. The prior distribution over the latent variables is modeled as a Gaussian distribution.\\n\\nThe goal of the VLGP is to infer the posterior distribution, q(z|x), over the latent variables given the observed data. This is done using variational inference by minimizing the objective function, also known as the evidence lower bound (ELBO), given by:\\n$ELBO = -D_{KL}(q(z|x) || p(z)) + E_{q(z|x)}[log(p(y|z,x))]$\\nwhere $D_{KL}$ is the Kullback-Leibler divergence, which measures the difference between two distributions, and E is the expected value. The first term in the ELBO encourages the approximate posterior, q(z|x), to be close to the prior, p(z), while the second term represents the negative log-likelihood of the data given the latent variables. [1]\\n\\nThe optimization problem can be solved using gradient-based optimization algorithms, such as gradient descent or conjugate gradient. The solution provides estimates of the latent variables, which can be used to reconstruct the hidden patterns in the data. For the purposes of our project, vLGP is used to extract neural trajectories, which are the underlying patterns in neural activity that reflect how the brain processes information.\\n# To Run\\n`python run.py <config_name>.json`\\n\\nConfig files are .json files stored in \"config/\". They contain the hyperparameters of the model, as well as the PID, EID, and probe of the mouse, which determines what data will be analyzed. You can go to [the IBL website](https://viz.internationalbrainlab.org/app) to get different data. If the data you want is not already in \"data/raw/ONE/\", then in run.py remove the `mode=\\'local\\'` flag when instantiating ONE.\\n\\nOutputs of run.py will be stored in \"output/<exp_name>/\"\\n',\n",
       "  'This text discusses the processing of electrophysiology data to extract neural trajectories. The data is high dimensional and noisy, so it needs to be processed before accurate trajectories can be obtained. The authors used a method called Variational Latent Gaussian Process (VLGP) to reduce the dimensions and smooth the data. They then created a classifier to predict mouse behavior using this processed data. The VLGP model infers the posterior distribution over latent variables given the observed data using variational inference. The optimization problem is solved using gradient-based optimization algorithms. The solution provides estimates of the latent variables, which can be used to reconstruct hidden patterns in the data. To run the code, a config file with hyperparameters and mouse information needs to be provided. The outputs will be stored in a designated folder.'],\n",
       " 'https://github.com/somet3000/1kgp-coverage-analysis': ['# 1KGP Coverage Analysis\\nExpression quantitative trait loci (eQTL) and fine-mapping analysis of a cohort from the 1000 Genomes Project using both lower coverage and higher coverage (30x) data.\\n\\nThe raw VCF data can be obtained from the 1000 Genomes Project: https://www.internationalgenome.org/data\\n\\nThe gene expression data can be obtained from the BioStudies website for the RNA-sequencing 1KGP paper: https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-GEUV-1/sdrf?full=true\\n\\nThe analysis is a little bit different when running on at-scale versus test data, leading to different files between them. To run this code on the test data: ```python run.py test```. The test code will produce the QQ-plots and fine-mapping plots from the test data in the repository directory. The QQ plot will be in ```output_qqplot_test.pdf``` and the fine-mapping plots will be in the ```Rplots.pdf``` file. To run this code on the real data: ```python run.py all```\\n\\nThis repository uses plink (https://www.cog-genomics.org/plink/1.9/), Matrix eQTL (http://www.bios.unc.edu/research/genomic_software/Matrix_eQTL/), susieR (https://github.com/stephenslab/susieR), and UCSC LiftOver (https://genome.ucsc.edu/cgi-bin/hgLiftOver). Check them out! \\n\\nThanks for stopping by this repository! :)\\n\\n',\n",
       "  \"This is a summary of the #1KGP Coverage Analysis. It involves the analysis of expression quantitative trait loci (eQTL) and fine-mapping using data from the 1000 Genomes Project. The raw VCF data can be obtained from the project's website, while gene expression data can be obtained from the BioStudies website. The analysis can be run on both test data and real data, with different files being generated for each. The code uses various tools such as plink, Matrix eQTL, susieR, and UCSC LiftOver.\"],\n",
       " 'https://github.com/jacquelinekclee/twas-dsc180-a17': [\"# Application of Transcriptome-Wide Association Studies for Identifying Genes Associated with Inflammatory Bowel Disease\\n\\nFind the capston project website, including the full report and summary of the project's background, analysis, and findings, [here](https://notsamzhou.github.io/twas/).\\n\\n## Running the analysis\\n\\nThis repository provides a pipeline to perform a TWAS analysis.\\n\\nThis analysis requires the DockerHub repository at `notsamzhou/twas:latest`\\n\\nTo run the analysis, run `python run.py all`\\n\\nIf running another analysis with the same gene expression and variant data but different GWAS summary statistics, we do not need to recompute weights for each gene. Just run  `python run.py assoc` with an updated data-params.json\\n\\nTo run the respository on test data, run `python run.py test`\\n\\n## Obtaining raw data\\n\\nThe primary vcfs used in the analysis can be downloaded from [here](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20110521/ALL.chr22.phase1_release_v3.20101123.snps_indels_svs.genotypes.vcf.gz) and [here](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20110521/ALL.chr22.phase1_release_v3.20101123.snps_indels_svs.genotypes.vcf.gz.tbi). This analysis used the Chromosome 22 vcfs from the 1000 Genomes Project.\\n\\nThe gene expression data can downloaded from [here](https://www.ebi.ac.uk/biostudies/files/E-GEUV-1/E-GEUV-1/analysis_results/GD462.GeneQuantRPKM.50FN.samplename.resk10.txt.gz).\\n\\nThe population data can be downloaded from [here](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20110521/phase1_integrated_calls.20101123.ALL.panel).\\n\\nVarious summary statistic files can be downloaded from [here](https://github.com/TiffanyAmariuta/TCSC/tree/main/sumstats) based on a disease of interest.\\n\\n[This file](https://drive.google.com/uc?export=download&id=1gd6FP4qlteo1dBoAH8zGkXzbZvs2PPt4), which provides IDs and locations for various genes, is also required for plotting purposes.\\n\\nAll of these files should be placed directly in data/raw\\n\",\n",
       "  'The provided text is a set of instructions and links related to the application of Transcriptome-Wide Association Studies (TWAS) for identifying genes associated with Inflammatory Bowel Disease. It includes information on running the analysis, obtaining raw data, and accessing additional files needed for plotting purposes.'],\n",
       " 'https://github.com/Med-Dash/Med-Dash.github.io': ['# Medical Dashboarding - Quarter 2 Project\\n',\n",
       "  'The summary of the project titled \"Medical Dashboarding - Quarter 2\" is not provided.'],\n",
       " 'https://github.com/BradPowell23/First-and-Second-Level-Brain-Analysis': ['Dataset Link:\\nhttps://openneuro.org/datasets/ds003338/versions/1.1.0\\n\\nLink to download the pre-processed data needed for analysis:\\nhttps://app.globus.org/file-manager?origin_id=dc43f461-0ca7-4203-848c-33a9fc00a464&origin_path=%2Fr8b8-k094%2F\\n\\n`notebooks`: code and analysis in the form of Jupyter notebooks stored\\n<br>\\n`references`: nilearn tutorials\\n',\n",
       "  'The dataset link provided is for a specific dataset on OpenNeuro. The pre-processed data needed for analysis can be downloaded from the given link. The dataset includes Jupyter notebooks with code and analysis, as well as references to nilearn tutorials.'],\n",
       " 'https://github.com/mzh4ng/DSC180B_Q2_Project': ['# Evaluating Fungal Feature Importance in Predicting Life Expectancy for Cancer Patients\\nThis is the repository for DSC180B Section B18-1\\'s Project consisting of Benjamin Sacks, Ethan Chan, and Mark Zheng.\\nThis project is an extension of a study on the classification of cancer types using fungal mycobiome counts which can\\nbe found here: https://www.cell.com/cell/fulltext/S0092-8674(22)01127-8.\\n\\nThis project consists of two main machine learning models based upon the data presented in the previously mentioned\\nstudy as well as additional metadata collected about each sample that was not used in prior models. The first is a \\nregression model to predict the \"days to death\" continuous metadata variable measuring when the patient died in days\\nafter their sample was taken. The second is a classification model which aims to distinguish between different cancer\\nstages(I-IV) as opposed to cancer types in the original study.\\n\\n\\nINSTRUCTIONS:\\n\\nTo run these models, run the run.py file with 1 argument, the name of the config file for the desired model. \\nEx. \"run.py default-cancer-stage.json\".\\nAdditionally, there is a notebook in the path notebooks/run.ipynb that can be used to run this program in \\nJupyter Notebook if desired.\\n\\nDifferent models can be selected and run using the config files. Config files are json files in the \"config\" directory. \\nThey can be edited to change the parameters of the experiment as well as the type of experiment run. Each experiment\\nonly has 1 config file that it uses to increase the customization of experiments without flooding the folder with \\ntoo many config files.\\n\\nIn each config file, there are 3 subcategories: dataset, preprocessing, and model.\\n<br /> Dataset specifies information about the raw feature tables including which column is the target variable.\\n<br /> Preprocessing specifies the parameters of the preprocessing including what transformations to apply to each column. \\n    Preprocessing can also be turned off if data is already preprocessed with \"do_preprocessing\".\\n<br /> Model specifies the parameters of the model as well as cross validation. These are model specific and will vary\\n    based upon which type of model is being used.\\n\\nAdditionally, these are some important keys in the config file:\\n<br /> experiment_name: Specifies the unique id of the experiment. This is important for separating plots in figures.\\n<br /> experiment_title: Title of the experiment that will be displayed on the graphs\\n<br /> experiment_type: internal parameter telling the pipeline which class of model to use (classification or regression)',\n",
       "  'This project is an extension of a study on the classification of cancer types using fungal mycobiome counts. It consists of two main machine learning models: a regression model to predict the \"days to death\" variable measuring when the patient died after their sample was taken, and a classification model to distinguish between different cancer stages. To run these models, the \"run.py\" file should be executed with the name of the config file for the desired model as an argument. The config files can be edited to customize the parameters and type of experiment. The important keys in the config file include experiment_name, experiment_title, and experiment_type.'],\n",
       " 'https://github.com/Amandoj/DSC180-Q2-Project': ['# MULTI-LABEL DISEASE PREDICTION BASED ON GUT MICROBIOME\\nAbstract: In this study, we will be exploring the gut microbiome of Latin American immigrants to determine what factors of their gut microbiome affect metabolic diseases. The goal of our project is to determine what metabolic diseases/disorders an individual has based on their gut microbiome and other supporting information on the individual. To achieve our goal, we will be exploring machine learning and data analysis techniques to summarize the key points of the data and understand the patterns and relationships in the data.\\n\\n\\n## Retrieving the data locally:\\n(1) Download the data files from the following Google Drive: https://drive.google.com/drive/folders/1cpUvpXbh3YEHHaW4jmeKhL8DYfE7tE5V?usp=sharing\\n\\n(2) Place files in `data/raw` directory\\n\\n## Activating Qiime2\\nAfter launching container, open terminal and type in the following command before running `run.py`:\\n\\n`conda activate qiime2-2022.11`\\n\\nTo use within jupyter notebook also type in the following commands: \\n\\n`pip install -–user ipykernel`\\n\\n`python -m ipykernel install -–user -–name=qiime2-2022.11`\\n\\nthen refresh jupyter hub\\n\\nand select the qiime2 kernel\\n\\n## Running the Project:\\n* To revert to a clean repository, from the project root dir, run `python run.py clean`\\n  * This deletes all built files\\n* To run the entire project on test data, from the project root dir, run `python run.py test`\\n  * This fetches the test data, creates features, cleans the data, performs permanova tests, creates pcoa plots, creates machine learning model and model performance graphs\\n  for given disease types\\n* To run the entire project on the real data, from the project root dir, run `python run.py all`\\n  * This fetches the original data, creates features, cleans the data, performs permanova tests, creates pcoa plots, performs UMAP, creates machine learning model and model performance graphs\\n  for given disease types\\n  \\n## Model Performance\\nTo view model performance graphs, permanova tests, and pcoa plots after running `run.py`, download `.qzv` files from `data/out` and upload to https://view.qiime2.org/\\n\\nCollaborator: Amando Jimenez, Emerson Chao, Renaldy Herlim\\n\\nFor more information visit: https://renaldyh27.github.io/Capstone-Website/\\n',\n",
       "  'This study aims to explore the gut microbiome of Latin American immigrants and identify the factors that contribute to metabolic diseases. The researchers will use machine learning and data analysis techniques to analyze the data and determine the relationship between the gut microbiome and metabolic diseases. The project includes steps for retrieving the data, activating Qiime2, and running the project on test or real data. Model performance graphs, permanova tests, and pcoa plots can be viewed by downloading `.qzv` files from `data/out` and uploading them to https://view.qiime2.org/. Collaborators for this project are Amando Jimenez, Emerson Chao, and Renaldy Herlim. More information can be found at https://renaldyh27.github.io/Capstone-Website/.'],\n",
       " 'https://github.com/ZixinMa27/DSC180-Aerosol-Flow-Modeling-and-Simulation-in-a-Classroom-with-Mobile-Sensors': [\"## Modeling and Simulation of Aerosol Flow in a Classroom Environment with Mobile Sensors\\n#### Team Members: Zixin Ma, Jiali Qian, Yidan Wang \\n#### Mentors: Professor Tauhidur Rahman, PhD Tanjid Hasan Tonmoy\\n\\n### Overview:\\nIn light of the significant impact of COVID-19, it is crucial for individuals to assess the safety of indoor environments effectively and accurately. Although there are existing apps that monitor factors such as air quality and temperature, they fail to consider the concentrations of respiratory aerosols or other contaminants. To address this issue, we aim to develop a mobile application that utilizes built-in sensor data  and machine learning models to simulate aerosol flow and forecast the safety of indoor environments. Our app will not only serve as a tool for assessing COVID-19 safety, but also for other illnesses and purposes. \\n\\n**This project is also related to an ongoing doctorate research project, thus some of the repos are kept private per professor's request.**\\n**Based on the condition above, this repo is created for the purpose of showing our current progress, so the commits are a little compact. Further proof can be provided to show that our effort on the project is consistent.**\\n### Resources:\\n1. Data Collection APP https://github.com/tanjidt/hdsi-capstone-project [private]\\n2. https://github.com/tanjidt/aerosol-models [private]\\n3. Thermal Image Analysis: https://github.com/kavetinaveen/Thermal_Image_Processing \\n\\n### Build instruction:\\nTo avoid path conflicts, run.py is not located in the root directory.\\nPlease run `python src/models/compartment_model/run.py test` instead.\\n\",\n",
       "  'The team aims to develop a mobile application that uses sensor data and machine learning models to simulate aerosol flow and predict the safety of indoor environments. This app will be useful for assessing COVID-19 safety as well as other illnesses. The project is related to an ongoing doctorate research project and some repositories are kept private. The build instructions suggest running a specific Python file instead of the root directory file to avoid path conflicts.'],\n",
       " 'https://github.com/Brian96086/STNP_RL': ['## Accelerating STNP with Reinforcement Learning \\n\\nOverview: This repository implements the improvement of brute-force parameter search with DeepQ Network(DQN). In the repository, there will \\n\\n\\n### Notes to DSC180A TA\\'s\\n\\n### Instructions - Conda Virtual Environment\\nIn this section, you\\'ll execute the code with the below steps:\\n1. Create a conda environment with python version 3.9 `conda create --name placeholder_name python=3.9`. Note the \"placeholder_name\" is the environment name that you desire\\n2. Activate the conda environment `conda activate placeholder_name`. \\n3. Within the environment, install the python packages by running `pip install -r requirements.txt`\\n4. By this stage, the conda environment should contain all of the required packages. To execute the code, run `python main.py` (or `python3 main.py`)\\n\\n## Repository Structure\\n- The repository currently contains config folder, models folders, and utils folder. \\n- The config folder will store all of the hardcoded constants and allows one to tune and perform the experiments/hyperparameters. In particular, make sure to change the snapshot parameter to the location you want output files to be stored. \\n- The models folder contain the original source code from the STNP model. It is seperated into two files - seir(the actual simulation) and dcrnn (the surrogate model)\\n- The utils folder contains the core components that assist the STNP model to select parameters, namely the reinforcement-related files. It is further categorized into agents, env, exploration_strategies, trainer, which are standard RL modules/abstractions.\\n- engine.py provides helper methods that run the training procedure and wraps complicated logic into each method \\n- main.py performs the execution of the code. Therefore, you\\'ll be compiling on main.py\\n\\nREADME update date: March 14th, 2023',\n",
       "  'This repository implements the improvement of brute-force parameter search with DeepQ Network (DQN). It contains config, models, and utils folders. The config folder stores constants for tuning experiments/hyperparameters. The models folder contains the original source code for the STNP model. The utils folder includes reinforcement-related files for parameter selection. The engine.py file provides helper methods for running the training procedure, and main.py executes the code.'],\n",
       " 'https://github.com/3XiangyiKong3/DSC180AB_code': ['# Optimization of DeepGLEAM on Flu Forecasting Time-Series Data\\nThe current COVID-19 pandemic and common flu highlight the importance of time-sensitive information in biomedical institutions, politics, and economics. The application of data science in creating real-time predictive models is crucial to help researchers and world leaders better understand disease spread and take preventative measures.\\n\\n## GLEAM Prediction before and after Interpolation\\n![GLEAM Before Interpolation](./references/beforeinterp.png)\\n![GLEAM After Interpolation](./references/afterinterp.png)\\n\\n## ARIMA and ETS\\n![ARIMA](./references/arima_validation_plot1.png)\\n![ETS](./references/ets_validation_plot1.png)\\n## Prediction\\nFour weeks ahead Flu prediction residual between groundtruth and prediction for 10 states\\n![uncertainty_quantification_flu_residual_washingtion](./references/10_states_4_weeks_prediction.png)\\n\\n## Result Comparison \\n![MAE result](./references/Combied_result.png)\\n\\n## Setup, Model training and Model Testing\\n \\n1. Requirements\\n```bash\\n>>> pip install -r requirements.txt\\n```\\n2. Train models and make prediction (Model already trainned in the submission)\\n```bash\\n>>> python3 run.py --config_filename=data/model/dcrnn_cov.yaml\\n```\\n3. For Test, run the following command\\n```bash\\n>>> ./test.sh\\n```\\n- Visualization \\n\\n  - After running the command for test, a new folder named plot_weeknumber_result will appear containing [0.025, 0.5, 0.975] residual predictions the .npz files \\n  - Select the one with lowest MAE score \\n  - Run the flu_forecast_result_plot notebook\\n\\n## Docker\\n\\n```bash\\n>>> docker build -f ./Dockerfile -t Dockerfile .\\n>>> docker run --rm -it Dockerfile /bin/bash\\n>>> launch.sh -i xiangyikong/dsc180a:latest #Use this command below to launch the image in DSMLP\\n```\\n',\n",
       "  'This document discusses the optimization of DeepGLEAM for flu forecasting using time-series data. It emphasizes the importance of real-time predictive models in understanding disease spread and taking preventative measures. The document includes visualizations of GLEAM prediction before and after interpolation, as well as ARIMA and ETS models. It also presents a comparison of results and provides instructions for setup, model training, testing, and visualization. Docker instructions are also provided.'],\n",
       " 'https://github.com/apatankar22/hier-neural-proc': [\"\\n# Capstone Project (DSC 180B): Active Learning with Neural Processes for Epidemiology Modeling\\n## [Report](https://drive.google.com/file/d/1Mk2uujYlSpMKpOzAgYlZWoz1AOed6XPl/view), [Poster](https://drive.google.com/file/d/1m3Gy5ldjGqiTkYX6XV3meAU44MSHP9dL/view), [Website](http://apatankar22.github.io/hier-neural-proc/) <br>\\nAuthors: Amogh Patankar <br>\\nMentors: Rose Yu, Yian Ma\\n\\n## [SIR Neural Process and Gaussian Process Data](https://drive.google.com/drive/folders/1osXBkuDuzSmB8__2r3lLoOLHIXqju3G2)\\nSIR_GP: Save <code>nargp_data</code> and <code>sfgp_data</code> on the same level as <code>src</code>. \\n\\nSIR_NP: Save <code>mfnp_nested, nested, and nonnested</code> on the same level as <code>src/sir_np/</code>. \\n\\n## Packages\\nInstalled packages through pip. To replicate, run <code> pip install -r requirements.txt.</code> <br>\\nAlternatively install along as you go, the Gaussian processes don't require all the same packages as the neural processes. \\n\\n## How to Run\\nFor Gaussian Processes, simply change to <code>src/sir_gp</code> and run the two jupyter notebooks.<br>\\nFor Neural Processes, simply change to <code>src/sir_np/MA</code>, then go into the desired process and run train.py. This should train and also test + evaluate the model.\\n\",\n",
       "  'This is a summary of the instructions for the Capstone Project (DSC 180B) on Active Learning with Neural Processes for Epidemiology Modeling. The project report, poster, and website are available. The authors are Amogh Patankar and the mentors are Rose Yu and Yian Ma. The project involves SIR Neural Process and Gaussian Process Data. There are instructions on how to save data for SIR_GP and SIR_NP. The required packages can be installed using pip or installed as needed. To run Gaussian Processes, go to src/sir_gp and run the jupyter notebooks. To run Neural Processes, go to src/sir_np/MA, choose the desired process, and run train.py to train, test, and evaluate the model.'],\n",
       " 'https://github.com/Grizlucks/DSC180B-CapstoneFinalProject': ['# DSC180B-CapstoneFinalProject\\n\\nEvaluating gender bias within images generated by DallE-2.\\n\\nRequirements:\\n\\nThis project makes use of OPENAI and requires a \\'config.py\\' file\\nwith the below line. \\n\\nOPENAI_API_KEY = \"YOUR_API_KEY_HERE\"\\n\\nIf running the test target with a valid OpenAI key, it will generate ten images\\n(256x256) on that account to be charged (or with the user\\'s remaining free\\ncredits if any).\\n\\n## Instructions and Project Guide\\n\\n### run.py\\n\\nCall `python run.py test` to run the project using test data. \\n\\nTo run the project on the data used for the actual report, call `python run.py` however this will take a bit of time.\\n\\nThe run.py is setup to work with seperated targets that can be called by using the format `python run.py target1 target2 ...`:\\n\\n- `power` Performs analysis on BLS like data to give information on the sample size necessary at 1% statistical power for our statistical testing.\\n- `images` Generates images based on the occupations specified in the main or test `params.json` files. Requires a `config.py` file to be setup as specified.\\n- `analysis` Performs the statstical tests for the report while generating informative visualizations. Requires labeled image data in the format found under `test/raw/labeled_results.csv`. \\n\\n## notebooks/\\n\\nIncluded for reference, these notebooks were used in the hand-labeling \\nprocess. \\n\\n## src/ \\n\\nContains all source code in generating images and selecting occupations\\nwithin a specified range (of a 50/50 gender split). Currently, only tester\\nimages can be generated. ',\n",
       "  'This project aims to evaluate gender bias in images generated by DallE-2. It requires an OpenAI API key and provides instructions on how to run the project using test data or actual report data. The project includes targets for performing analysis on BLS-like data, generating images based on specified occupations, and performing statistical tests. The \"notebooks\" folder contains notebooks used in the hand-labeling process, while the \"src\" folder contains the source code for generating images and selecting occupations.'],\n",
       " 'https://github.com/ptse8204/airlinedatabias': [\"# Welcome to the Airline Pricing Model Bias Repositary\\n\\n## Introduction\\nThis repositary showcase how we investigate on how airline price discriminate on certain protected groups, including but not limit to:\\n* Race\\n* Income\\n* Geo areas\\n\\nWe understand that airfare pricing is a business decision that was driven by revenues. However, by investigating such factors, it may also drives airline's bottom line as the result could be useful for more attractive pricing for passengers.\\n\\n## Methodology\\n### Dataset\\nWe uses the Airline Origin and Destination Survey from the USDOT. The main reason why we chose to use such a dataset, instead of the advertising fare of the flight is because the data point represent a fare that is actually purchase by a customers. \\n\\nThe detail descrption and the data of the dataset is available on: https://www.transtats.bts.gov/tables.asp?QO_VQ=EFI&QO_anzr=Nv4yv0r\\n\\n### How does finding bias work?\\nWe aim our investigation (mainly) in 2 directions:\\n* Investigate whether there is price discrepency in protected groups on existing dataset\\n* Feed the data on to our custom build models, and see whether the model would generate results that showcase strong bias. In especially models that are extremely accuracte, and has a hard time to correctly identity areas that has a strong influence with protected groups.\\n\\n## Expected Goals and Outcomes\\n### Goals\\n* Discover bias, if any\\n* Creating an accurate model, that is both accurate and unbias, using various accuracy measurments, and bias mitogation techniques.\\n* Discover any trend shift of airfare pre-pandemic and post-pandemic\\n\\n### Outcomes\\n* An unbias model for extimatting the fair price that customers pay\\n* An indicator allow consumers to know whether they are price dscriminated and whether they are paying the fair price\\n\\n## Credits\\nTba :)\\n\\n## Notes\\nWe will keep updating the repo alongside with our progress.\\nreadme file last update: Feb 12, 2023\\n\",\n",
       "  'This repository investigates how airlines discriminate against certain protected groups, such as race, income, and geographic areas, in their pricing models. The dataset used is the Airline Origin and Destination Survey from the USDOT. The investigation aims to discover any bias in pricing and develop an accurate and unbiased model using various accuracy measurements and bias mitigation techniques. The expected outcomes include an unbiased model for estimating fair prices and an indicator for consumers to determine if they are being price discriminated against. The repository will be regularly updated with progress.'],\n",
       " 'https://github.com/NicoloWX/CausalTreeInference': ['# DSC180 Capstone Project\\n- `run.R`: a demo of the tree-based method\\n\\nFor models in `src/models/`:\\n\\nTo replicate the table[2][3][4] in the report\\n- `ageTest.R`\\n- `yearTest.R`\\n- `genderTest.R`\\n\\nTo replicate the tree graphs in the report\\n- `ageTest.R`\\n- `yearTest.R`\\n- `genderTest.R`\\n\\nTo replicate the boxplots in the report\\n- `boxplotGenAge.R`\\n- `boxplotGenYear.R`\\n- `boxplotGenGender.R`\\n\\nTo replicate table[5][6][7] in the report\\n- `forest-TestAge.R`\\n- `forest-TestYear.R`\\n- `forest-TestGender.R`\\n',\n",
       "  'The `run.R` file is a demo of the tree-based method for the DSC180 Capstone Project. There are several files in the `src/models/` directory that can be used to replicate different parts of the report. \\n\\nTo replicate the table[2][3][4] in the report, you can use the `ageTest.R`, `yearTest.R`, and `genderTest.R` files. \\n\\nTo replicate the tree graphs in the report, you can also use the `ageTest.R`, `yearTest.R`, and `genderTest.R` files. \\n\\nTo replicate the boxplots in the report, you can use the `boxplotGenAge.R`, `boxplotGenYear.R`, and `boxplotGenGender.R` files. \\n\\nLastly, to replicate table[5][6][7] in the report, you can use the `forest-TestAge.R`, `forest-TestYear.R`, and `forest-TestGender.R` files.'],\n",
       " 'https://github.com/alecpanattoni/MissingnessFairnessAnalysis': ['When in the repo directory, in order to produce the test results, one can use target \"test\". In order to produce results with the downloaded data, target name \"all\" can be used. To make use of these targets, simply cd into the repo\\'s director and input \"python3 run.py <target>\"\\n\\nThe project directory contains the following:\\n\\ndata folder contains test data, the notebook for generating the test data, as well as the complete allegations data\\n\\nDockerfile from which necessary packages will be installed to run project\\n\\nreport folder contains the overleaf report pdf\\n\\nrun.py is the code in which results are produced (main coding file)\\n\\nsrc folder contains python code for data cleaning and generation of missing data (methods called in run.py). It also contains the models for ensuring that the model is fair and producing predictions so that fairness notion measurements can be produced. ',\n",
       "  'To produce test results in the repository directory, use the \"test\" target. To produce results with downloaded data, use the \"all\" target. To utilize these targets, navigate to the repository directory and run \"python3 run.py <target>\". The project directory includes a data folder with test data and a notebook for generating it, as well as complete allegations data. The Dockerfile installs necessary packages. The report folder contains the Overleaf report PDF. The main coding file is run.py, which produces results. The src folder contains Python code for data cleaning, generating missing data, ensuring model fairness, and producing predictions for fairness measurements.'],\n",
       " 'https://github.com/ejsong37/Trustworthy-Recommender-Systems-Capstone': ['# Trustworthy Recommender Systems via Bayesian Bandits Capsone\\n\\n### Team Members: Eric Song, Xiqiang Liu, Hien Bui, Vivek Saravanan\\n\\n### Mentor: Yuhua Zhu\\n\\n## About\\nRecommender systems have emerged as a simple yet powerful framework for the suggestion of relevant items to users. However, a potential issue arises when recommender systems overly recommend or spam undesired products to users in which the model loses the trust of the user. We propose a constrained bandit-based recommender system. We show this model outperforms Upper Confidence Bound (UCB) and Thompson sampling in terms of expected regret and does not lose the trust of the users. This work was presented at the Halıcıoğlu Data Science Institute Capstone Showcase on March 15th, 2023 at UC San Diego.\\n\\n## Resources\\n- [Website](https://hi3nb1.github.io/capstone/)\\n- [Poster](https://drive.google.com/file/d/1BjS6ZcwmB4TsGctyS56vNxZsTev8F0zF/view)\\n- [Report](https://drive.google.com/file/d/10VEKJZ_TWxqBKimkeTWUmmYjivagMGZJ/view)\\n\\n## Running Experiments\\n```bash\\npython run.py all  # run all experiments\\n\\npython run.py etc  # run Explore-Then-Commit (ETC) experiments\\npython run.py ucb  # run UCB experiments\\npython run.py ts  # run Thompson Sampling experiments\\npython run.py optimal  # run Bayesian Optimal Policy experiments\\n\\npython run.py linucb  # run LinUCB experiments\\npython run.py lints  # run Linear Thompson Sampling experiments\\n```\\n\\nTo run experiments related to Trustworthy Recommender Systems, run code in experiments.ipynb in TrustworthyMAB folder.\\n\\nAll the results are going to be saved in `results/` sub-directory.\\n\\n## Visualize Results\\n\\nNotebooks to visualize collected results could be found in `notebooks/` sub-directory.\\n',\n",
       "  'This project is about a trustworthy recommender system that addresses the issue of recommending undesired products to users. The proposed model outperforms other methods in terms of expected regret and does not lose the trust of the users. The project was presented at the Halıcıoğlu Data Science Institute Capstone Showcase at UC San Diego. You can find more information, including a website, poster, and report, in the provided links. There are also instructions for running experiments and visualizing results in the code.'],\n",
       " 'https://github.com/abhianish0105/DSC180B-Q2-Project': ['# DSC180B-Q2-Project\\n\\n# Twitter Sentiment Analysis on Gun Control\\n\\n## Introduction\\nThis project is on the sentiment analysis of tweets regarding the topic of gun control. In recent years in the United States, there have been an outburst of several horrific events as a result of guns getting in the hands of the wrong people. In 2023, the number of mass shootings in the US has already reached triple digits. With these incidents, we believe it would be an interesting study to do further research into the sentiment and beliefs that people have toward these issues and study this using automation and machine learning. Twitter is a very vocal platform and with the use of our new knowledge regarding sentiment analysis, there are possibly very many interesting discoveries to be made, such as how user sentiment toward one topic may differ from another. We hypothesize that because of the many gruesome events that have occurred in recent years, we will observe a mostly positive sentiment toward gun control, in that people support more regulation of weapon distribution rather than less. Our proposed project period of 10 weeks can be attributed to the fact that we are looking to improve upon our previous work by expanding our knowledge on the Astra Streaming features, and working on more efficient implementation of our architecture to ensure that we receive enough data for sufficient analysis.  We will also look to go further by integrating new features into our project, including displaying results from our visualizations on a website in addition to streaming data to a feature store or database. As we work towards these goals, we recognize that we may come across technical issues that may require us to be more flexible and adjust our project structure, so we would like to stay open minded in regards to what additional features will be added.\\n\\n\\n## Running the Repository\\n* The code in this repository runs our sentiment analysis code on a small test set of test tweets, the same code that was ran on our collected database of tweets retrieved from the Twitter API.\\n* Run the following docker image inside a container: \\n  * tepatel/test_q2\\n* To test the model on the test data: \\n  * `python3 run.py test`\\n\\n\\n\\n',\n",
       "  \"This project focuses on analyzing the sentiment of tweets related to gun control. The goal is to understand people's beliefs and attitudes towards this issue using automation and machine learning. The project aims to explore how user sentiment may differ on different aspects of gun control. The project period is 10 weeks, during which the team plans to improve their previous work, expand their knowledge on Astra Streaming features, and implement a more efficient architecture. They also plan to integrate new features such as visualizations on a website and streaming data to a feature store or database. The repository provides code for running sentiment analysis on a small test set of tweets.\"],\n",
       " 'https://github.com/DSC-180A/spam.detector.github.io': ['# Q2-Project\\n\\n## How Spam Affects the Sentiment of Tweets\\n\\n## Contributors\\nLucas Lee, Tyson Tran, Yi (Skylar) Li\\n\\n\\n## Objective\\nThe objective of our research is to develop a pipeline that filters spam content to model noise-reduced sentiments towards abortion on Twitter in real-time and analyze how spam content affects said sentiments. We compared the use of both Naive Bayes and a transfer learning model based on BERT to do spam filtration, and analyzed the impact of spam on sentiment distribution results to gain a deeper understanding of the role it plays in shaping public opinion on social media platforms.\\n\\n\\n## Pipeline\\nThis project is produced as part of the DSC180B Capstone at UCSD, working with mentors from DataStax. In this project, we utilized Apache Pulsar through Astra Streaming to create a pipeline that ingests live stream of abortion related tweets, incoporate spam-filtering ML models, and performs sentiment analysis. This is primarily done through the following steps:\\n<img src=\"visuals/Untitled drawing (2).jpg\" width=350 height=600> \\n  \\n1. A producer makes Twitter API calls to request a stream of tweets through the FilteredStreamV2 endpoint \\n2. The producer then publishes each incoming tweet (stringified Json) to a pulsar topic — Raw Tweet Topic.\\n3. Pulsar consumers subscribes to the Raw Tweet Topic and\\n   a) Consumer 1 performs sentiment analysis directly.\\n   b) Consumer 2 deploys Naive Bayes Model to label spam tweets, then performs SA.\\n   c) Consumer 3 deploys BERT Model to label spam tweets, then performs SA.\\n4. Consumers then update data to a data source.\\n5. Grafana visualizes the finding on <a href=\"https://skylar1013.grafana.net/d/_ztsas0Vz/capstone?orgId=1&from=1675065600000&to=1676188799000\">dashboards</a>\\n\\n\\nWith the above described pipeline, we now have a real-time stream of tweets being classified. We used Google Spreadsheet <a href=\"https://docs.google.com/spreadsheets/d/1fZ6MsCqtPXHWekonx2QGst0-eGei9ABzMG5LFDMEFbA/edit#gid=0\">Google Spreadsheet</a> to collect real-time tweets with producer running.\\n\\n## Requirements\\n- `requirements.txt` provided with all dependencies needed to run the code below\\n- `capstone_googlesheet_key.json` provided necessary credentials to update tweets to database.\\n- Astra Streaming account\\n- Twitter developer account\\n\\n### Required Envronmental Variables\\nRunning the producers and consumers require Astra Streaming topic keys. Setting up topics and creating an account is free, and more information can be found at the tutorial [here](https://docs.google.com/document/d/1VS31dXTIAmEkIh9o_9FcAhD-rVvcmnTo_Zm1zSMgCmY/edit).\\n\\nThe shell envrionmental variables are used within the `Producer` and `Consumer` classes within `producer.py` and `consumer.py`.\\n- `ASTRA_STREAMING_TOKEN`\\n- `ASTRA_STREAMING_URL`\\n- `ASTRA_TOPIC`\\n\\n\\n## How to run\\n`run.py test`. This runs a test pipeline on test data.\\nThis will then run:\\n`python src/producer.py`. This makes requests to the Twitter API, publishing remote-work related tweets to a pulsar topic. In its current test tag, it will run the `src/producer_offline.py`, which is encouraged to be used to test.\\n`python src/consumer_*.py`. This captures the cleaned tweets, utilizes spam detection ML models, performs sentiment analysis, alters it so that it feeds your needs for downstream analysis. Note that there are three such consumers that will be simultanouesly run in the background. Please use `ps` to check the list of processes and `kill [pid]` to end the processes. This is fully intended as producer and consumers are long running jobs in the background.\\n\\n\\n\\n## Usage\\n* Since the publishing time of the tweet is currently calculated by when the consumer receives the tweet from the topic, it\\'s recommended to use concurrently run both `producer.py` and `consumer.py` simultaneously\\n* Keep in mind that this requires the setup of the Astra Streaming dashboard, a tutorial is available [here](https://docs.google.com/document/d/1VS31dXTIAmEkIh9o_9FcAhD-rVvcmnTo_Zm1zSMgCmY/edit#heading=h.3znysh7)\\n* With the above setup, the only thing left to change is the topic that the `Consumer` subscribes to, which is in its constructor.\\n* The path and nameof the generated CSV is modifiable in the constructor of a `Consumer` in `consumer.py`\\n\\n## Files\\n- `src/producer.py`: Main driver class for fetching tweets\\n- `src/consumer_*.py`: Main driver class for filtering spams and performing sentiment analysis.\\n- `requirements.txt`: Required dependencies in Python\\n\\n',\n",
       "  'This project aims to develop a pipeline that filters spam content in real-time and analyzes how spam affects sentiments towards abortion on Twitter. The pipeline uses Naive Bayes and BERT models for spam filtration and performs sentiment analysis. The project also visualizes the findings using Grafana dashboards. The code requires certain environmental variables and dependencies to run. There are producer and consumer classes for fetching tweets, filtering spams, and performing sentiment analysis.'],\n",
       " 'https://github.com/DSC180A/transaction_categorization': ['# transaction_categorization\\n\\n## Rethinking Credit Scores: Ensuring Fair Lending through NLP for Transaction Categorization\\n\\n### By Kyle Nero, Chung En (Shawn) Pan, Nathan Van Lingen, Koosha Jadbabaei\\n\\n#### Industry Mentor: Brian Duke (Petal)\\n#### Faculty Mentor: Berk Ustun\\n\\nThe invention of credit has reshaped modern personal finance. For most, a credit card provides an opportunity to buy something you can\\'t afford in full, such as a car or a home. However, for others, credit is seen as a barrier. Because of the nature of how credit scores are calculated, certain groups, often referred to as the \"credit invisible\" are neglected. These individuals may be credit invisible for any number of reasons– they may be a young adult, or may have recently immigrated to a new country. Regardless, this group of \"credit invisible\" individuals often struggle to be approved for loans because they have no credit history.\\n\\nIn our project, we aim to cater to this \"credit invisible\" group by creating a tool that is able to classify transaction memos from an individual\\'s checking history. We are working with industry partner Petal, a financial services company who uses an alternative metric, called the \"CashScore\", as opposed to a conventional credit score. Being able to accurately classify transaction memos is important because it allows Petal to more precisely calculate a CashScore for each credit applicant, which will over time increase the amount of people who qualify for credit– especially those with no credit history.\\n\\nTo reproduce this analysis, simply run the run.py file using the included docker image.\\n',\n",
       "  'The article discusses the importance of credit scores and how they can exclude certain groups of people, known as the \"credit invisible.\" The authors propose a tool that can classify transaction memos to help financial services company Petal calculate a more accurate metric called the \"CashScore\" for credit applicants. The article also provides instructions on how to reproduce the analysis.'],\n",
       " 'https://github.com/kkw002/Quarter2Project': [\"# DSC180B Quarter 2 Project: Prediction of Transaction Types using NLP analysis.\\nThis project attempts to identify which phrases, among other features that are generated from given data from Petal are used to predict the categorization of transaction made by a user. Currently, the model in the files only represents the 'base' model in which the features are relatively basic, as well as the model that was used to generate the accuracy.\\n\\n## Accessing Data\\nThe data needs to be accessed through ``` https://drive.google.com/file/d/10JH-rN5c1cMXIEXgPkPGImWSGOzC19Kx/view?usp=share_link ```.\\n1) After downloading the data, replace the ``` testdata.parquet ``` file with the downloaded file.\\n2) In the run.py file, replace ```getData('data/testdata.parquet')``` with ```getData('data/DSC180B.parquet')```\\n\\n## Viewing Results\\nTo see the accuracy score of the model on the dataset run ``` python run.py test ```\\n\",\n",
       "  'This project aims to predict transaction types using NLP analysis. The model uses phrases and other features generated from Petal data to categorize user transactions. The data can be accessed through a specific link, and the accuracy score of the model on the dataset can be viewed by running a specific command.'],\n",
       " 'https://github.com/NJMIXI98/DSC180_Q2PROJECT': ['# Auditing race inequality based on gender in data science related job market\\n\\nData Science Capstone Project advised under Stuart Geiger\\n\\n# Authors\\n\\nNancy Jiang and Sahil Wadhwa\\n\\n# Overview\\n\\nIn the twentieth century, one of sociology’s findings is that race and gender matter in the job market. Jobs were segregated by race and gender with whites earning more than other people of color and men earning more than women. Race inequality in the job market has been a long-standing interest of scholars. Notably, some research indicates that racial gaps are more significant for women than for men. Women face the unique challenges of lower wages and lower rewards in the global workforce. However, as women’s relative share in occupations grows nowadays, the gender inequality gap narrows in most job markets(Stier et al.,2014). Besides, a finding shows that race-based discrimination is weaker in high-paid jobs. Back in 2012, the Harvard Business Review acclaimed data science as “the sexiest job of the 21st century”. Some may pose the question of whether the statement still holds today. According to the U.S. BUREAU of Labor Statistics, employment in data science is projected to grow 36% from 2021 to 2031, much faster than the average for all occupations, which means employers will create more than 13,500 new data science related job opportunities each year on average, over the decade(Bureau of Labor Statistics 2022). Thus, we are curious about the gender-based race inequality in the data science related job market. \\n\\n# Code Instruction\\nUse Docker image: \"ucsdets/datahub-base-notebook:2022.3-stable\"\\nRunning python run.py test\\n',\n",
       "  \"The authors of this project, Nancy Jiang and Sahil Wadhwa, are auditing race inequality based on gender in the data science job market. They discuss how race and gender have historically affected job opportunities and wages, with women facing unique challenges. However, as women's representation in occupations increases, the gender inequality gap is narrowing. The authors also mention that race-based discrimination is weaker in high-paid jobs. They are interested in exploring the gender-based race inequality specifically in the data science job market, which is projected to grow significantly in the coming years. The code instruction suggests using a specific Docker image and running a Python script for testing purposes.\"],\n",
       " 'https://github.com/brianjhuang/CryptoWho': ['# CryptoWho\\nData Science Capstone Project advised under Stuart Geiger\\n\\n## Authors\\n#### Brian Huang and Lily Yu\\n\\n## Overview\\nIn 2020, Bitcoin, other Cryptocurrencies and blockchain investments (NFTs) experienced a major boom. With the endorsements of many major companies and what seemed to be a large scale adoption on the horizon, the Crypto craze had started kicking off.\\n\\nFor many, cryptocurrency and NFTs were the first time they encountered an \\'investment\\'. Many proclaimed crypto/NFT traders had no prior experience with any investing (stock market, retirement accounts, etc.) and looked to crypto/NFT as a get rich quick path. However, with the influx of new and young traders it was only a matter of time before the crypto/NFT scams began to pray on it\\'s new consumers.\\n\\nWhile crypto/NFT is not inherently a scam, many bad actors began to manipulate the influx of young and inexperienced investors. Many schemes akin to pump and dumps began to appear and with the popularity of crypto/NFT throughout social media, many inexperienced investors were quick to turn a blind eye to scams that seasoned investors may recognize immediately.\\n\\nCases like this have occured with traditional markets as well, where investors pour money into an asset based off of hype with no sound reasoning ($GME or GameStop). Crypto/NFTts however were especially susceptible to this with the combination of a relatively new asset, large demographic of young investors, and many social media influencers promoting these assets (Logan Paul, Doja Cat, etc)\\n\\nWith the most recent scandals in the cryptocurrency/NFT world (FTX, Logan Paul\\'s CryptoZoo), it\\'s more important than ever to investigate the platforms that many of these assets are promoted on. While platforms like YouTube and TikTok may not be intentionally promoting this content, it\\'s important that they\\'re aware of if their algorithms do indeed promote this type of content. To clarify, we do not provide an opinion on whether these scams are run by the founders of these products (FTX\\'s Sam Bankman-Fried and CryptoZoo\\'s Logal Paul) but rather emphasize that the scams have occured.\\n\\n![crypo_losses](references/figures/crypto_losses.png)\\n\\nAccording to the FTC, over a billion dollars has been lost to these types of scams since 2021. Half of which have orginated directly from social media. The most susceptible group of people? Young individuals.\\n\\nThe goal of this project is to investigate YouTube\\'s recommendation algorithm to provide insight on the types of investment recommendations being provided to users across a variety of age groups. We assume that all individuals should be receiving the same proportion of recommendations based on their search trends (within a margin of error) regardless of age. This implies that a user who is younger and searching for general investment advice should not be receiving more crypto/NFT recommendations than someone who is older with similar watch history. By conducting audits on YouTube, we hope to gain valuable insight on YouTube and it\\'s role in propogating this type of content on their platform (whether intentional or not).\\n\\n### What does this repository offer?\\n\\nThe repository offers all tools used to conduct the audit. Helper functions and classes help download video and metadata from YouTube, run headless browsers to watch seed videos, and query the GPT-3 API for video sentiment classification.\\n\\nAll code can be found in the `src` folder and imported respectively. For any changes in filepath or settings, please look through the `config` folder.\\n\\n## Table of Contents\\n\\n- [Overview](#overview)\\n- [Methodology](#methodlogy)\\n- [Installation](#installation)\\n- [Downloading YouTube Data](#downloading-youtube-video-data)\\n- [GPT Prompting and Fine-Tuning](#gpt-prompting-and-fine-tuning)\\n- [Conducting the Audit](#conducting-the-audit)\\n- [Inference](#running-inference)\\n- [Analyzing Audit Results](#analyzing-audit-results)\\n- [Acknowledgements](#acknowledgements)\\n\\n## Methodology\\n\\nOur project is split into three parts:\\n\\n**Part 1: Collecting Seed Videos and Creating Persona Users**\\n\\nSeed videos function as a way for us to evaluate our GPT model and the videos that users watch to build watch history. Given the time we had, we could only collect and label 140 videos (20 for each age, 40 for traditional investments, 40 for blockchain investments, 5 for mixed, and 15 for edge cases where the video discusses money but is actually unrelated).\\n\\nThese seed videos fall into one of four labels:\\n\\nBlockchain, Traditional, Mixed, and Unrelated.\\n\\nUsing our seed videos, we create six persona users with different watch behaviors.\\n```\\nA young individual (18-23) who:\\n\\nWatches blockchain\\n\\nWatches traditional\\n\\nWatches mixed\\n```\\n```\\nA old individual (55-60) who:\\n\\nWatches blockchain\\n\\nWatches traditional\\n\\nWatches mixed\\n```\\nBy comparing the recommendations across these users, we can determine if YouTube is fairly and evenly recommending this content to all age groups. \\n\\n**Part 2: Creating a Prompt that peforms the best for our task at hand**\\n\\nCreating a prompt is important when using GPT as a classifier. More info about our prompt can be found in the sections that follow.\\n\\n**Part 3: Running the audit**\\n\\nUsing selenium, we can mimic the behavior of the six persona users above. By having a headless browser click and watch each video, we can collect the recommendations of each user. \\n\\nEach user watches 50% of the video by default. This and other config can be changed in the `config/Audit.py` file. \\n\\nThe diagrams below illustrate the general pipeline for our audit:\\n\\n![method](references/figures/methods.png)\\n![audit](references/figures/audit.png)\\n\\n## Installation\\n\\nWe are using Python version [3.8.5](https://www.python.org/downloads/release/python-385/).\\n\\nPlease install this version of Python, as gensim summarization is still supported.\\n\\nVersions between 3.8 and 3.9 should work, however we recommend you install the same version we use.\\n\\nWe recommend using [Anaconda](https://www.anaconda.com/) environments to do so.\\n\\n**Once you\\'ve installed Anaconda, please run the following to create your environment.**\\n\\n```bash\\nconda create --name <env_name> python=3.8.5\\n```\\n\\n**Activate your conda environment like so:**\\n```bash\\n# Linux/Mac\\nsource activate <env_name>\\n\\n# Windows\\nactivate <env_name>\\n```\\n\\n**Install required packages**\\n\\n```bash\\npip install -r requirements.txt\\n```\\n\\n**If you have Python 3.8.5 working outside of Conda (or any other version of Python that works with gensim) you can create a normal environment if you prefer**\\n\\n```bash\\npython3 -m venv .venv\\n\\nsource .venv/bin/activate\\n```\\n\\n**Install required packages**\\n\\n```bash\\npip install -r requirements.txt\\n```\\n\\n### Headless Browser Setup\\n\\nWe use Selenium with the Firefox Webdriver to conduct our audit and gather YouTube video recommendations.\\n\\nNote: More recent versions of FireFox will just launch if you have the browser installed. Please install the Firefox browser. If it does not work, install the driver.\\n\\nTo run the scraper, you will need to install the Firefox Webdriver, which can be downloaded [here](https://github.com/mozilla/geckodriver/releases).\\n\\nTo install, place your OS-appropriate executable in a directory locatable by your PATH.\\n\\n### API Key Setup\\n\\nOur codebase uses the YouTube Data API to download video metadata, comments, and for many other purposes like searching YouTube and grabbing recommendations. We use the OpenAI API to provide snippets and retrieve classification labels for our downloaded videos.\\n\\nYou can enable the YouTube Data API for your Google account and obtain an API key following the steps <a href=\"https://developers.google.com/youtube/v3/getting-started\">here</a>.\\n\\nThe key can be found after you set up your cloud console.\\n\\nYou can fetch your OpenAI API key from <a href=\"https://platform.openai.com/\">here</a>.\\n\\nThe key can be found in your profile under ***View API Keys***.\\n\\nOnce you have both API keys, please set the ```YOUTUBE_DATA_API_KEY``` and ```OPENAI_API_KEY``` variable in your environment:\\n\\nYou can do so by going to your home directory and running the following command:\\n\\n**Mac OS and Linux**\\n\\n```\\nnano .bash_profile\\n\\n# Note Mac Users using zsh shell users should also set their keys in their zsh_profile\\nnano .zsh_profile\\n```\\n\\nInside your bash profile, you can go ahead and set this at the top:\\n\\n```\\n# YOUTUBE API KEY\\nexport YOUTUBE_DATA_API_KEY=\"YOUR_API_KEY\"\\nexport OPENAI_API_KEY=\"YOUR_API_KEY\"\\n```\\n\\nClose out of your terminal and your code editor to see changes occur.\\n\\n**Check that updates have been made**\\n```\\necho $YOUTUBE_DATA_API_KEY\\necho $OPENAI_API_KEY\\n```\\n\\nThe following tutorials cover how to do this as well:\\n\\nhttps://www.youtube.com/watch?v=5iWhQWVXosU&t=1s (Mac/Linux)\\n\\nhttps://www.youtube.com/watch?v=IolxqkL7cD8 (Windows)\\n\\nIf you are not seeing updates, your `bash_profile` may not be sourced. To resolve this, add the following line to your `.bashrc`:\\n\\n```\\n. ~/.bash_profile\\n\\n# Note Mac Users using zsh shell users should do this in .zshrc\\n. ~/.zsh_profile\\n```\\n\\nThis can be anywhere, but we\\'ve put ours at the very bottom. Use the following command to enter your `.bashrc`.\\n\\n```\\nnano .bashrc\\n# Note Mac Users using zsh shell users should do this\\nnano .zshrc\\n```\\n\\nNow within Python you can access your API key by doing the following:\\n```\\nimport os\\n\\nyoutube_key = os.environ.get(\"YOUTUBE_DATA_API_KEY\")\\nopenai_key = os.environ.get(\"OPENAI_API_KEY\")\\n```\\n\\n### Connecting to a VPN\\n\\nTo ensure all things are constant, we connected to the UC San Diego VPN for all of our audits and downloads. We recommend you connect to a VPN in the same location as well. There are many different VPN providers.\\n\\n### Preserving User Agent\\n\\nWe recommend the audits be done on the same device, and if that is not possible, the same operating system. All of our audits were conducted on a Ubuntu device, however the platform you choose to audit with should not matter as long as they are consistent.\\n\\n## Running The Entire Project Pipeline (Not Recommmended)\\n\\nOur code uses a `run.py` file to help you run our code out of the box. You have the option to run the entire project pipeline using the folliwng command:\\n\\n`python run.py all`\\n\\nThis command will download all seed videos, create snippets and evaluate your seed videos using the classifier, conduct the audit and download all videos, and classify the videos from the audit.\\n\\nWe **DO NOT** recommend you run this. The entire pipeline may take multiple days to run depending on the size of your seed videos. (Note: With 140 seed videos, our entire pipeline takes three days to run.)\\n\\nIf you would like to validate that the pipeline and it\\'s sub-targets work, we instead recommended using:\\n\\n`python run.py test`\\n\\nWhich will run the pipeline end to end on a much smaller subset of data found in `test/youtube`. This takes around an hour.\\n\\nYou can continue reading below for more details, but here is an overview of all targets in the run.py file. They are listed in the order we recommend you run them. Running each target gives you finer control over what is downloaded, avoiding repeated efforts/wasted API calls.\\n```\\nseed - The following target downloads and processed seed data. Target will prompt user for what they want to download and if they want to process the seed data (create video snippets.) Video snippets for context is a concatenation of the video title + summarized transcript (via TextRank) + top ten video tags (via TF-IDF)\\n\\nclassify-seed - The following target classifies the seed videos, providing a baseline classification report of your prompt. The confusion matrix is saved in references/figures. Note that the classification report is printed in terminal and not saved.\\n\\naudit - The following target conducts the audit on a Firefox headless browser. It allows you to run a single or multiple audits. Config for each audit can be found in config/Audit.py\\n\\ndownload-audit - The following target allows you to download the results of your audit. You can choose to download homepage results, sidebar results, or both.\\n\\ncreate-audit-snippets - The following target creates snippets for each set of audit results. Note this may take a while as some videos have large transcripts. Video\\'s over an hour have no transcript removed, however by modifying the target code this can be removed if you are able to wait the additional time.\\n\\nclassify - Run classification on your audit snippets.\\n```\\n\\n## Downloading YouTube Video Data\\nUsing `python run.py seed` will download all seed videos and save it in `data/seed/youtube/videos_{}.csv`.\\n\\nUsing `python run.py download-audit` will download all videos from the audit and save the downloaded videos in `data/audit/youtube/videos_{}.csv`.\\n\\nCalling `python3 src/data/youTubeDownloader.py <video_ids seperated by spaces>` will download any videos you want and save it in `data/external/youtube/videos_{}.csv`. \\n\\n## GPT Prompting and Fine Tuning\\nUnlike traditional classifiers, GPT-3 is not trained on an existing dataset. Instead, predictions are generated through a set of \\'prompts\\', instructing the large language model what the task is.\\n\\nAt the start of this project, GPT-3 was used. GPT-3 can be fine-tuned using a labelled dataset and prompts. For those interested in using a fine-tuned GPT-3 model, we encourage you to use the following <a href=\"https://platform.openai.com/docs/guides/fine-tuning\">resource</a>.\\n\\nStarting March 1st, GPT-3.5 was released. This API powers ChatGPT. Due to significant reduction in cost and increases in accuracy, we migrated to <a href = \"https://platform.openai.com/docs/models/gpt-3\">GPT 3.5.</a>\\n\\nYou are open to modify the prompt for our model however you see fit. The prompt can be found in `gpt.py` in the `create_message()` function.\\n\\nThe following prompt was used for our classification:\\n```\\n{\"role\": \"system\", \"content\" : \\n\\n\"You are a classifier that determines if a YouTube video snippet falls under a label. A snippet is a concatenation of the video title, summarized transcript, and video tags. The labels and additional instructions will be included in the first user message.\"},\\n\\n{\"role\": \"user\", \"content\" : \\n\\n\"\"\"Labels:\\n\\nTraditional: Videos that recommend or educate about stocks, bonds, real estate, commodities, retirement accounts, or other traditional investments or keywords related to them.\\nBlockchain: Videos that recommend or educate about cryptocurrency (BTC, ETH, etc.), NFTs, or other Web3 investments or keywords related to them.\\nMixed: Videos that recommend or educate about both blockchain and traditional investments or keywords related to both.\\nUnrelated: Videos that do not recommend or educate about either blockchain or traditional investments or keywords related to them.\\n\\nInstructions:\\n- The classifier should consider the context and meaning of the keywords used to determine whether the snippet is related to traditional or blockchain investments.\\n- If talks about making money from jobs, side hustles, or other alternative assets (cars, watches, artificial intelligence, trading cards, art, etc), they are Unrelated.\\n- A video that is only downplaying an investment or discussing it negatively should be classified as Unrelated.\\n- Please return predictions in the format\" {Label} : {20 word or shorter rationale}\"\"\"},\\n\\n{\"role\": \"assistant\", \"content\": \\n\\n\"\"\"Understood. I will classify YouTube video snippets based on the provided labels and instructions. Here\\'s how I will format the predictions:\\n\\n    {Label} : {20-word or shorter rationale}\\n\\nPlease provide me with the YouTube video snippet you would like me to classify.\"\"\"}\\n```\\nNote that unlike GPT-3, the ChatCompletions API endpoint expects a message, not a prompt. The system message biases the model towards a specific task, while the user messages provide prompts and instructions. The assistant messages can be used to affirm what has already been stated in the user messages.\\n\\nYou can test out your prompt on your seed videos!\\n\\n`python run.py classify-seed`\\n\\nThis will run classification on the seed videos and return a classification report and confusion matrix. The confusion matrix can be found in `references/figures`.\\n\\nMore information on writing prompts can be found <a href = \"https://github.com/openai/openai-cookbook\">here</a>\\n\\nNote: Occasionally, the output of a prediction will be \\'Label\\'. This is due to GPT not adhering to the format we instruct it to. There is no known fix to this as of now.\\n\\nFor example:\\n```\\n### Expected Output\\nBlockchain: Rationale\\n\\n### Occasional Output\\nLabel: Blockchain: Rationale\\n```\\n\\nBy default, our seed classifier will mark videos with a prediction of \\'Label\\' as unrelated. This may result in accuracy being off by 1-2% when in reality it should be higher. Please inspect your output yourself to verify!\\n\\n## Conducting the Audit\\n\\nThe audit may take a very long time to run. Please make sure you\\'re connected to a VPN before you start. We recommend using some program to keep your computer awake (Caffeine, Amphetamine). \\n\\nIf you are using `python run.py audit`, the audit will prompt you if you want to conduct a single audit or all audits (in our case six audits.)\\n\\nIf you choose one audit, the one audit will be run based on these parameters in `config/Audit.py`\\n```\\n# AUDIT VARIABLES\\nUSER_AGE = \"old\"  #\\'young\\' or \\'old\\'\\nFINANCE_VIDEO_TYPE = \"blockchain\"  #\\'traditional\\', \\'blockchain\\', \\'mixed\\'\\n```\\n\\nIf you choose multiple audits, the code will iterate through the following dictionary in `config/Audit.py`, sleeping ten minutes between each audit.\\n```\\nAUDITS = [\\n    {\"type\": \"traditional\", \"age\": \"young\"},\\n    {\"type\": \"mixed\", \"age\": \"young\"},\\n    {\"type\": \"blockchain\", \"age\": \"young\"},\\n    {\"type\": \"traditional\", \"age\": \"old\"},\\n    {\"type\": \"mixed\", \"age\": \"old\"},\\n    {\"type\": \"blockchain\", \"age\": \"old\"},\\n]\\n```\\n\\nAfter running the audit, please run:\\n\\n```python\\npython run.py download-audit\\n\\npython run.py create-audit-snippets\\n```\\n\\nAudits are saved to `data/audit` with the following sub-folders:\\n```\\nraw - The raw video information. Please only keep the results of one run in this folder, as the pre-processing scripts read the entire directory to load in files. Having multiple audits in the folder will result in overriden and missing files in data cleaning steps.\\n\\nprocessed - The downloaded videos. The sub-folder `snippets` contains the downloaded videos with snippets appended.\\n\\nresults - The results of our predictions.\\n```\\n\\n## Running Inference\\n\\nRunning inference on the audit videos is easy!\\n\\n`python run.py classify`\\n\\nRunning the following command with run classification on every single audit file. Results are stored in `data/audit/results/`.\\n\\n## Analyzing Audit Results\\n\\nGPT models use a parameter called \\'temperature\\' to adjust how much risk their models take. 0 temperature means the model is deterministic, outputting the same response everytime. We\\'ve used a temeprate of 0.25 as we want the model to take a bit of risk. This, however, does resukt in cases where our predictions come back in the wrong format.\\n\\nFor example:\\n```\\n### Expected Output:\\n\\nBlockchain: This video is blockchain\\n\\n### Occassional Output:\\n\\nLabel: Blockchain: This video is blockchain\\n```\\n\\nBecause of this, we can not include a target for analyzing the results. It\\'s much easier to analyze the result on your own through a notebook. We have a reference notebook: `notebooks/insepectPredictions.ipynb` that demonstrate loading in the result data, cleaning it, and running a Chi-Squared test. Please use that for reference. You can also generate any plots you need using `matplotlib` and `seaborn`. \\n\\n## Acknowledgements\\n\\nWe\\'d like to make a special thanks to the Data Science Capstone faculty, HDSI, Professor Stuart Geiger, and all our friends and family who provied us the opportunity to work on this project.\\n',\n",
       "  \"This text is about a data science capstone project called CryptoWho. The project aims to investigate YouTube's recommendation algorithm and its role in promoting investment recommendations, particularly in the cryptocurrency and NFT (non-fungible token) space. The authors collected seed videos and created persona users to evaluate the recommendations across different age groups. They also used GPT-3, a language model, for video sentiment classification. The project includes code for downloading YouTube data, conducting the audit, running inference, and analyzing the results. The authors emphasize the importance of investigating platforms like YouTube to prevent scams and protect young investors.\"],\n",
       " 'https://github.com/gprasad125/dsc180b': ['# Analyzing U.S. Congressional Tweets with OpenAI GPT-3\\n\\n# Repository for the Spring 2023 Quarter (DSC180b)\\n\\nThis project covers Tweet sentiment analysis for Tweets originating from US Congresspeople as it relates to China.\\nThis is an extension to Quarter 1\\'s project found for each researcher below:\\n\\n[Annie\\'s Q1 codebase](https://github.com/AnnieeeeeF/DSC180A_Project1)\\n\\n[Gokul\\'s Q1 codebase](https://github.com/gprasad125/dsc180a_project)\\n\\nThis project continues the work by exploring the same topic through the lens of a Large Language Model (LLM). \\n\\n## Necessary Configurations:\\n\\nYou will *need* an API key from OpenAI to utilize the GPT-3 model.\\nSign up for an account and get a key [here](https://openai.com/api/)\\n\\nYou can then pass your API Key to our scripts in one of two ways:\\n\\n1. Export your key by running the following in your command line:\\n\\n`export OPENAI_API_KEY=...`\\n\\n2. Create a .env file in the root directory and paste in your key like so:\\n\\n`OPENAI_API_KEY=...`\\n\\n## Data Source:\\n\\nRaw data can be found [here](https://drive.google.com/drive/u/1/folders/1VSYdGh12UNVNhfxbSeHRdANvHr5xF8Ea). \\nDownload the file `SentimentLabeled_10112022.csv`, and place it inside the `data/raw` directory. \\n\\nYou can then run the `run.py` file with the following targets:\\n- `test`: runs the file on man-made test data\\n- `data` / `all`: runs the file on Twitter-API sourced data.\\n\\n## Explanation of File Structure:\\n\\n### 📁 Folders:\\n\\n#### config\\nContains JSON configuration for optimized & group-selected models. \\n\\n#### data\\nContains the data for and from the project, divided as such:\\n- raw: the base uncleaned data\\n- out: the output cleaned data used for visualizations and modeling\\n- test: test data used to debug the Python scripts\\n- results: visuals generated from the EDA and modeling, formatted as PNGs\\n\\n#### notebooks\\nContains initial Jupyter Notebooks for EDA / Modeling.\\nNot entirely cleaned up yet. Cleaned versions of this code will be found inside our `src` folder.\\n\\n#### src\\nContains the Python scripts needed to run the project, divided as such:\\n- data: \\n    - `make_dataset.py` cleans and processes the raw data\\n- models: \\n    - `classifier.py`: GPT-3 powered classifier to find \"relevant\" Tweets (i.e, Tweets about Chinese governmental impact on America.)\\n    - `sentiment.py`: GPT-3 powered sentiment scorer to find \"emotion\" of Tweet (i.e, is a Tweet favorable or negative towards China?)\\n- visuals: \\n    - `eda.py`: Generates summary visuals for the two cleaned dataframes going into modeling. Not the full EDA of the dataset. For that, check under `notebooks/EDA.ipynb`\\n- notebooks:\\n    - `nb_functions.py`: All necessary functions for notebook report + visuals. Uses the code from other folders with slight modifications to fit an ipynb environment. \\n\\n### 📜 Files:\\n\\n#### run.py\\nBaseline Python script to run via CLI with targets.\\nCurrent targets include `test` (`all`) and `data`. \\n\\n    - Creates cleaned data file\\n    - Generates exploratory visuals and saves them\\n    - Runs models on data\\n\\n### requirements.txt\\nNecessary Python packages to install via `pip install -r requirements.txt`\\n\\n',\n",
       "  'This project focuses on analyzing the sentiment of tweets from US Congresspeople regarding China. It builds upon the work done in Quarter 1 and utilizes a Large Language Model (LLM) called GPT-3. The necessary configurations include obtaining an API key from OpenAI and downloading the raw data file. The file structure consists of folders for configuration, data, notebooks, src (containing scripts for data processing, models, and visuals), and files such as run.py and requirements.txt.'],\n",
       " 'https://github.com/x6zeng/NLP-Active-Learning-Pipeline': [\"# NLP-Active-Learning-Pipeline\\nThis is the repository for our DSC180B section A12 Group B Quarter 2 Project, which consists of 2 machine learning models, with Active Learning approaches, that can be used to predict the relevance and sentiment toward China of the tweets posted by the members of the U.S. Congress, given the tweet's text content.\\n\\n## Main Content\\n- __data__: folder to store data, including test data and other data. It is also used to store the results data for the author\\n  - test: folder to store the test data\\n  - raw: empty, folder to store the raw data\\n  - result: folder to store the result data\\n- __notebook__: folder to store the pre-development notebooks\\n  - analyses: notebooks containing the active learning results analyses.\\n  - explorations: code explorations for active learning pipeline.\\n  - model_comparison: all the pre-development code for relevance and sentiment model, as well as the comparison between models using different hyperparameters.\\n- __src__: folder to store the files of obtaining the dataset, building the features, and the code for the 2 models\\n  - `utilities.py` - script to preprocess the raw data\\n  - `Relevance_AL_Committee.py` - script to train the relevance model\\n  - `Sentiment_AL_Committee.py` - script to train the sentiment model\\n\\n## Data Source\\nThe data used in this project was provided by the staffs from the China Data Lab at UC San Diego. Click [here](https://drive.google.com/drive/folders/1VSYdGh12UNVNhfxbSeHRdANvHr5xF8Ea?usp=sharing) for data. If running the models with the raw data, please place the `SentimentLabeled_10112022.csv` in the folder `data/raw`.\\n\\n## Important Files\\n- `Dockerfile`: contains the information for building the docker image\\n- `run.py`: the script to run the models. To run the models on test data, use the following command: \\n  - `python3 run.py test`\\n- `submission.json`: contains the submission information\",\n",
       "  'This repository contains the code for a project that focuses on predicting the relevance and sentiment toward China of tweets posted by members of the U.S. Congress. The project includes two machine learning models with active learning approaches. The repository has folders for data storage, pre-development notebooks, and source code files. The data used in the project was provided by the China Data Lab at UC San Diego. Important files include a Dockerfile for building a docker image, a run.py script to run the models, and a submission.json file with submission information.'],\n",
       " 'https://github.com/colts661/Incomplete-Text-Classification': ['# Incomplete Supervision: Text Classification based on a Subset of Labels\\n\\nIn this project, we explore the **I**n**c**omplete **T**ext **C**lassification (IC-TC) setting. We aim to design a text classification model that could suggest class names not belonging to the training corpus to unseen documents, and classify documents into a full set of class names.\\n\\n<div style=\"display:flex;\">\\n    <div style=\"width:100%;float:left\">\\n        Authors: Luning Yang, Yacun Wang<br>Mentor: Jingbo Shang\\n    </div>\\n</div>\\n\\n\\n### Model Pipeline\\n- Find seed words from the supervised set: TF-IDF\\n- Use full corpus to find word embeddings\\n  - Final Model: Pretrained `BERT` contextualized word embeddings using static representations guided by `XClass`, reduced dimensions using PCA\\n  - Baseline Model: Trained `Word2Vec` word embeddings\\n- Find document and class embeddings based on averaged word embeddings, from the documents or seed words\\n- Compute similarity as confidence score, predict argmax if confidence over threshold\\n- For other unconfident documents, run clustering and label generation (LI-TF-IDF or Prompted ChatGPT)\\n<p align=\"center\"><img width=\"60%\" src=\"others/model-pipeline.png\"/></p>\\n\\n\\n### Environment\\n\\n- [**DSMLP Users**]: Since the data for this project is large, please run DSMLP launch script using a larger RAM. The suggested command is `launch.sh -i yaw006/incomplete-tc:final -m 16 -g 1`. Please **DO NOT** use the default, otherwise Python processes will be killed halfway.\\n- Other options:\\n  - Option 1: Run the docker container: `docker run yaw006/incomplete-tc:final`;\\n  - Option 2: Install all required packages in `requirements.txt`.\\n\\n### Data\\n#### Data Information\\n- The datasets used in the experiments can be found on [Google Drive](https://drive.google.com/drive/folders/1kf3AXpKbwbZuQhcVSiaMzCiaSrWTdO7i?usp=sharing).\\n- The datasets used in the experiments are: `DBPedia`, `DBPedia-small`, `nyt-fine`, `Reddit`\\n- **Note**: `DBPedia-small` is the default experiment target dataset, as it contains a subset of documents for the full `DBPedia` dataset, and could be run in a few minutes.\\n\\n#### Get Data\\n- [**DSMLP Users**]: For the 3 datasets provided, convenient Linux commands to download and get the data are provided in the [documentation of raw data](data/raw/). Please run the commands in the **repository root directory**.\\n- Generally, under Linux command line, for any Google Drive zip file, \\n  - Follow the `wget` [tutorial](https://medium.com/@acpanjan/download-google-drive-files-using-wget-3c2c025a8b99)\\n    - Find the Large File section (highlighted code section towards the end)\\n    - Paste the `<FILEID>` from the `zip` file **sharing link** found on Google Drive\\n    - Change the `<FILENAME>` to your data title\\n  - Run `cd <dir>` to change directory into the data directory\\n  - Run `unzip -o <zip name>` to unzip the data\\n  - Run `rm <zip name>` to avoid storing too many objects in the container\\n  - Run `cd <root>` to change directory back to your working directory\\n  - Run `mkdir <data>` to create the processed data directory\\n- Under non-command line, go to the Google Drive link, download the zip directly, place the files according to the requirements in the **Data Format** section, and manually created the directory needed for processed files. See the **File Outline** section for example.\\n\\n#### Data Format\\n- Raw Data: Each dataset must contain a `df.pkl` placed in `data/raw/`. The file should be a compressed Pandas DataFrame using `pickle` containing two columns: `sentence` (for documents) and `label` (for the corresponding label).\\n- Processed Data: \\n  - The corpus will be processed after the first run, and processed files will be placed in `data/processed`.\\n  - The processed file will be directly loaded for subsequent runs.\\n\\n### Commands\\n[**DSMLP Users**]: \\n- The `test` target could be easily run as `python run.py test`.\\n- The `experiment` target could be run as `python run.py exp -d <dataset>`.\\n- When prompted from the prompt, insert values.\\n\\nThe main script is located in the root directory. It supports 3 targets:\\n- `test`: Run the test data. All other flags are ignored.\\n- `experiment` (or `exp`) [default]: Perform one vanilla run.\\n\\nThe full command is:\\n```\\npython run.py [-h] target [-d DATA] [-m MODEL]\\n\\nrequired: target {test,experiment,exp}\\n  run target. Default experiment; if test is selected, run baseline model on testdata.\\n\\noptional arguments:\\n  -h, --help                 show this help message and exit\\n  -d DATA, --data DATA       data path, required for non-testdata\\n  -m MODEL, --model MODEL    model pipeline, {\\'final\\', \\'baseline\\'}. Default \\'final\\'\\n```\\n**Notes**: \\n1. Due to data size constraints to run large BERT embeddings or train embeddings based on the corpus, the `test` target will run the baseline model with pre-trained `glove-twitter-25` word embedding to speed up computation time.\\n2. Due to time constraints and container constraints, the short experiments are chosen to run fast, which means performance is not guaranteed.\\n\\n\\n### Code File Outline\\n```\\nIncomplete-Text-Classification/\\n├── run.py                           <- main run script\\n├── data/                            <- all data files\\n│   ├── raw                          <- raw files (after download)\\n│   │   ├── nyt-fine\\n│   │   |   └── df.pkl               <- required DataFrame pickle file\\n│   |   └── ...\\n│   └── processed/                   <- processed files (after preprocessing)\\n├── src/                             <- source code library\\n│   ├── data.py                      <- data class definition\\n│   ├── word_embedding.py            <- word embedding modules\\n│   ├── similarity.py                <- computing similarities and cutoff\\n│   ├── unsupervised.py              <- dimensionality reduction and clustering\\n│   ├── generation.py                <- label or seed word generation\\n│   ├── evaluation.py                <- evaluation methods\\n│   ├── models.py                    <- model pipelines\\n│   └── util.py                      <- other utility functions\\n└── test/                            <- test target data\\n```\\n\\n---\\n### Citations\\n\\n#### Word2Vec\\n```\\n@article{word2vec,\\n    title={Efficient estimation of word representations in vector space},\\n    author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},\\n    journal={arXiv preprint arXiv:1301.3781},\\n    year={2013}\\n}\\n```\\n\\n#### XClass\\n```\\n@misc{wang2020xclass,\\n      title={X-Class: Text Classification with Extremely Weak Supervision}, \\n      author={Zihan Wang and Dheeraj Mekala and Jingbo Shang},\\n      year={2020},\\n      eprint={2010.12794},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n```\\n\\n#### BERT\\n```\\n@article{devlin2018bert,\\n  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},\\n  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\\n  journal={arXiv preprint arXiv:1810.04805},\\n  year={2018}\\n}\\n```\\n<div style=\"float:right\">\\n    <img width=\"25%\" src=\"others/HDSI.png\" alt=\"Logo\">\\n</div>',\n",
       "  'This project explores the Incomplete Text Classification (IC-TC) setting and aims to design a text classification model that can suggest class names not present in the training corpus to unseen documents. The model pipeline includes steps such as finding seed words, using word embeddings, computing similarity scores, and running clustering and label generation for unconfident documents. The code file outline includes various modules for data processing, word embeddings, similarity computation, unsupervised learning, label generation, evaluation, and model pipelines. The project provides options for running the code on DSMLP or using Docker containers. The datasets used in the experiments can be found on Google Drive.'],\n",
       " 'https://github.com/k6chan/reverse-dictionary-pokemon': ['# pokemon-reverse-dictionary\\n\\n## [Static Website](https://k6chan.github.io/reverse-dictionary-pokemon/)\\n\\nA reverse dictionary for Pokemon as a Python Flask web app.\\n\\n## Setup\\n\\n*With approval from my TA, a `run.py` script is not necessary for this website project.*\\n\\n### With Docker (automatically installs libraries and starts the dev server)\\n\\n**The Flask server hosting is not compatible with Docker on DSMLP. This Docker image can only be run locally, not on DSMLP.**\\n\\n```\\nDockerHub repository:\\n\\nhttps://hub.docker.com/repository/docker/k6chan/reverse-dictionary-pokemon\\n```\\n\\nRun the Docker image using `docker run -it --rm -p 5000:5000 k6chan/reverse-dictionary-pokemon:latest`.\\n\\nThe Flask server should automatically start up. Access the server in your browser, usually `http://127.0.0.1:5000/`.\\n\\n### Without Docker\\n\\nFirst, clone the repository to your device with `git clone`.\\n\\nInstall the Python libraries in `requirements.txt` with `pip install --no-cache-dir -r requirements.txt`.\\n\\nChange directory into `src/models` and use the following command to run the dev server:\\n\\n`python -m flask --app application run`\\n\\nAccess the server in your browser, usually `http://127.0.0.1:5000/`.\\n\\n## Contribution Statement\\n\\nSolo project by Kaitlyn Chan.\\n',\n",
       "  'This is a static website that serves as a reverse dictionary for Pokemon. It is built using Python Flask as a web app. The setup instructions are provided for running the website with or without Docker. The project was solely developed by Kaitlyn Chan.'],\n",
       " 'https://github.com/zaxiang/Spam_Filter': ['# Weakly Supervised Spam-Label Classification\\nDSC180 Quarter 2 Capstone Project \\n\\nUsing a list of categories and words that represent these categories, we classify harmful spam messages into categories such as insurance scams, medical sales, software sales, and more. Doing so, we hope to alleviate the burden on non technical people in todays world as spammers continue to get by detection systems - we want to find and highlight a pattern throughout them all. Leveraging models ranging from simple methods like TFIDF to complex large language models such as ConWea with BERT, we examine the differences between these models and if it is worth using such big, computation costly models.\\n\\nYou can see more details about our project on our [website](https://gbirch11.github.io/SpamLabelClassifier/).\\n\\n# Data\\nThe data is available on [Google Drive](https://drive.google.com/drive/folders/1uTRzRPkom6nUtRB2D4pOi8uOpSpqst7m?usp=share_link)\\\\\\nPlease unzip and place the files into the following locations; \\\\\\nAnnotated Spam Messages -> ```data/raw/spam/Annotated/``` \\\\\\nUnannotated Spam Messages -> ```data/raw/spam/Unannotated/``` \\\\\\nNon-spam (Ham) Messages -> ```data/raw/ham/``` \\n\\n\\nThe dataset should contain the following files:\\n1) Annotated Spam Messages \\\\\\n  ex) ```data/raw/spam/Annotated/medical-sales/xyz.txt```\\n    * Where xyz is any file name that was annotated to be medical sale spam\\n    * Other folders follow same pattern for each category\\n2) Non-spam (ham) Messages \\\\\\n  ex) ```data/raw/ham/xyz.txt```\\n3) Seedwords JSON file \\\\\\n  ex) ```data/out/seedwords.json```\\n\\n## Running the Project\\n**DSMLP Command**\\n``` \\nlaunch.sh -i gbirch11/dsc180b [-m d] [-g 1]\\n```\\nNote: -m is an optional argument to include more RAM on the machine; HIGLHLY RECOMMEND setting $d$ to 16 or 32 for faster processing \\\\\\nAlso highly recommended to run with -g 1, especially if running ConWea model.\\n``` \\nlaunch.sh -i gbirch11/dsc180b -m 32 -g 1\\n```\\n<br> <br>\\nTo run this project, execute the following command;\\n```\\npython run.py [test | data]\\n```\\nNote: If running ```python run.py test``` \\\\\\nVery simple set of test data will be used to produce results. \\\\\\nResult trend not consistent with running on full dataset.\\n\\nIf running ```python run.py data```: \\\\\\nWhole dataset will be used to produce results.\\n\\nExample commands include: \\\\\\n``` python run.py test ``` \\\\\\n``` python run.py data ```\\n\\nNote: The above commands only run on the TF-IDF, Word2Vec, and FastText models. To run our best model, ConWea, see the section below.\\n\\n## Running the ConWea Model\\nSince ConWea is a huge model using BERT, we have separated this model into the following separate commands;\\n1) Navigate to the ConWea model directory using \\\\\\n``` cd src/models/ConWea ``` <br> <br>\\n2) To contextualize the corpus and seed words run \\\\\\na) For testing: ``` python contextualize.py --dataset_path \"../../../test/testdata/\" --temp_dir \"temp/\" --gpu_id 0 ``` \\\\\\nb) For full data: ``` python contextualize.py --dataset_path \"../../../data/raw/spam/Annotated/\" --temp_dir \"temp/\" --gpu_id 0 ```  <br> <br>\\n3) To train model + observe results run \\\\\\na) For testing: ``` python train.py --dataset_path \"../../../test/testdata/\" --gpu_id 0 ``` \\\\\\nb) For full data: ``` python train.py --dataset_path \"../../../data/raw/spam/Annotated/\" --gpu_id 0 ```  <br> <br>\\n\\nNote: Be warned that running ConWea on the full dataset will ~ 3 hours to run. Running ConWea on test data runs in ~ 20 minutes. <br>\\nNote: ConWea trains using multiple layers and tons of epochs, since our test data is small it is safe to interrupt the terminal (CTRL+C) after first iteration has occured. The layers are kept for consistency for full datasets.\\n',\n",
       "  'The project aims to classify harmful spam messages into categories such as insurance scams, medical sales, and software sales. The team explores different models, from simple methods like TFIDF to complex large language models like ConWea with BERT, to determine their effectiveness and computational cost. The data for the project can be found on Google Drive, and the project can be run using the provided commands. Running the ConWea model requires separate commands for contextualizing the corpus and seed words, as well as training the model and observing results.'],\n",
       " 'https://github.com/ym820/foreground_window_forcast': ['# Intel Capstone: Improving App Launch Time with Deep Learning\\nAuthors: Yikai(Mike) Mao, Alan Zhang, Mandy Lee \\\\\\nWebsite: https://ym820.github.io/foreground_window_forcast/\\n\\n## Abstract\\nApplication launch time is a crucial element of the user experience. Long wait times can cause frustration and prompt users to upgrade to more powerful machines, resulting in increased electronic waste (e-waste) at landfills. Improving app launch time is vital in reducing e-waste by extending the average lifespan of computers. While software has become more resource efficient, it is still challenging to prevent large programs from being bloated and slow to run. In this paper, we propose utilizing neural networks to analyze system usage reports and pre-launch applications in the background before the user needs them. This approach can be universally applied to all computers, making it more economically viable than asking all developers to optimize their applications. We developed our data collection software to minimize resource usage and to identify applications that the system can pre-launch using Hidden Markov Model (HMM) and Long Short-Term Memory (LSTM) models.\\n\\n## Prerequisites\\n\\n> __Below we assume the working directory is the repository root.__\\n\\n### Install dependencies\\n- Using docker\\\\\\nYou may pull the docker image from `mikem820/intel_capstone:latest` and then clone this github repo\\n- Using pip3\\n\\n  ```sh\\n  # Install the dependencies\\n  pip3 install -r requirements.txt\\n  ```\\n\\n## Run \"test\" code\\nYou can use the below command to run the \"test\" code with a sample of our collected dataset. You **must** pass two arguments. The first is `test` or `all`, indicating whether to run test code or not. The second argument is to choose the model (must be either `hmm` or `lstm`) which corresond to the task we will explain in the later section. \\n```\\npython3 run.py test hmm/lstm\\n```\\nYou can also run the full pipeline with the entire dataset by the following command, with the default hyperparameters we implemented.\\n```\\npython3 run.py all hmm/lstm\\n```\\nIf you want to explore different sets of hyperparameters, we explained our tasks and specific instructions to run the scripts in the following section.\\n\\n## Task 1: Next-App Prediction with Hidden Markov Model (HMM)\\nIn this task, our goal is to predict the next application the user will use based on the previous usage data.\\n### Run\\n```\\ncd src/model/HMM\\npython3 run.py\\n```\\n### Arguments\\n\\n| Parameter                 | Default       | Description   |\\t\\n| :------------------------ |:-------------:| :-------------|\\n| -ts --test_size \\t       |\\t0.2\\t            |Test set size (percentage of entire dataset)\\n| -t --top  \\t\\t       | 1\\t           | Number of executables to predict for each data point\\n| -ex  --experiment \\t        | 1           | The experiment number\\n\\n### Notes\\nAfter running, there will be a folder created at `outputs` and named as \"HMM_expt_`experiment`\". Then, the parameters, transition matrix, model accuracy, and visualization will be stored in the folder.\\n\\n## Task 2: App Duration Prediction with Long Short-Term Memory (LSTM)\\nAs for the primary objective for our project, we aim to forecast the amount of time (in seconds) an individual will spend on a specific application within a specific hour. \\n### Run\\n```\\ncd src/model/LSTM\\npython3 run.py\\n```\\n### Arguments\\n\\n| Parameter                 | Default       | Description   |\\t\\n| :------------------------ |:-------------:| :-------------|\\n| -exe --exe_name\\t       |\\tfirefox.exe          |The executable name to predict\\n| -lb --lookback          | 5           |Lookback window (hyper-parameter) for dataset processing\\n| -ts --test_size \\t       |\\t0.2\\t            |Test set size (percentage of entire dataset)\\n| -e --epochs \\t       |\\t100\\t            |Number of epochs\\n| -lr --learning_rate  \\t\\t       | 0.001\\t           | Learning rate\\n| -l --loss \\t\\t           | mse             | Loss function\\n| -ex  --experiment \\t        | 1           | The experiment number\\n| -r  --random\\t        | False           | Whether to choose the start index for test set randomly\\n\\n### Notes\\nAfter running, there will be a folder created at `outputs` and named as \"LSTM_expt_`experiment`\". Then, the parameters, trained model, Keras training history, loss plot, and prediction plot will be stored in the folder.\\n\\n## Mentors\\nWe would like to express our gratitude to all the mentors at the Intel DCA & Telemetry team who provided invaluable guidance and support throughout this project. Special thanks to\\n- Bijan Arbab (Intel)\\n- Jamel Tayeb (Intel)\\n- Sruti Sahani (Intel)\\n- Oumaima Makhlouk (Intel)\\n- Teresa Rexin (UCSD)\\n- Praveen Polasam (Intel)\\n- Chansik Im (Intel)\\n',\n",
       "  'This paper discusses the use of deep learning to improve app launch time. The authors propose using neural networks to analyze system usage reports and pre-launch applications in the background before the user needs them. This approach can help reduce electronic waste by extending the lifespan of computers. The paper provides instructions for running test code for two tasks: next-app prediction with Hidden Markov Model (HMM) and app duration prediction with Long Short-Term Memory (LSTM). The authors express gratitude to the mentors who supported their project.'],\n",
       " 'https://github.com/miloncl/System-Usage-Analysis': ['# Intel & UCSD HDSI -- Data Science Capstone Project -- 2022-2023\\n\\n## Introduction\\n- Hello everyone, we are Thy Nguyen, Milon Chakkalakal, and Pranav Thaenraj from UC San Diego\\n- Our advisors are Jamel Tayeb, Bijan Arbab, Scruti Sahani, Oumaima Makhlouk, Praveen Polasam, and Chansik Im from Intel\\n- This is our Github repo including all the source codes and data files to do data collection and analysis for our capstone project _\"Discover User-App Interactions and Solutions to Reducing the Initial User-CPU Latency\"_\\n\\n## Overview\\n- We try to closely follow the template for Data Science projects by <a href=\"https://drivendata.github.io/cookiecutter-data-science/\">Cookie Cutter Data Sciece </a>\\n- Please check out the below template to understand how to navigate our repo\\n```\\nProject\\n├── .gitignore         <- Files to keep out of version control (e.g. data/binaries).\\n├── run.py             <- run.py with calls to functions in src.\\n├── README.md          <- The top-level README for developers using this project.\\n├── data\\n│   ├── temp           <- Intermediate data that has been transformed.\\n│   ├── out            <- The final, canonical data sets for modeling.\\n│   └── raw            <- The original, immutable data dump.\\n├── notebooks          <- Jupyter notebooks (presentation only).\\n|   ├── Process and EDA.ipynb\\n|   ├── HMM.ipynb\\n|   └── LSTM_RNN.ipynb\\n├── references         <- Data dictionaries, explanatory materials.\\n|   ├── data_dictionaries\\n|   |   ├── dataframe_description.txt\\n|   |   └── schema.txt\\n|   ├── weekly_presentation\\n|   |   ├── [DSC 180B] - Quarter 2 Week 2.pdf\\n|   |   ├── [DSC 180B] - Quarter 2 Week 3.pdf\\n|   |   ├── [DSC 180B] - Quarter 2 Week 4.pdf\\n|   |   ├── [DSC 180B] - Quarter 2 Week 5.pdf\\n|   |   └── [DSC 180B] - Quarter 2 Week 6.pdf\\n|   └── poster.pdf\\n├── requirements.txt   <- For reproducing the analysis environment, \\n├── src                <- Source code for use in this project.\\n│   ├── data           <- Scripts to download or generate data.\\n│   │   ├── make_dataset.py\\n│   │   └── foreground\\n|   |       ├── foreground.c\\n|   |       └── foreground.h \\n│   ├── features       <- Scripts to turn raw data into features for modeling.\\n│   │   └── build_features.py\\n│   ├── models         <- Scripts to train models and make predictions.\\n│   │   ├── hmm_model.py\\n│   │   └── lstm_model.py\\n│   └── visualization  <- Scripts to create exploratory and results-oriented viz.\\n│       └── visualize.py\\n├── outputs \\n|   └── HMM           <- HMM model results (LSTM/RNN model results are inside the notebook)\\n│       └── emission_mt_user1.txt\\n|       ├── emission_mt_user2.txt\\n|       ├── transition_mt_user1_top15apps.txt\\n|       ├── transition_mt_user1_top1app.txt\\n|       ├── transition_mt_user2_top15apps.txt\\n|       └── transition_mt_user2_top1app.txt\\n└── config\\n    ├── data-params.json <- Save the inputs for the function calls\\n    └── submission.json <- GitHub repo and Docker image links\\n\\n```\\n\\n## Instruction to Run the code\\n- For the Methodology Staff\\n    \\n    On DSMLP,\\n    1. Cloning our GitHub repository.\\n    2. Launching a container with your Docker image.\\n    3. Running ```python run.py test```.\\n\\n## Specific Links to Presentations and Source Code\\n\\n### Week 1: \\n- Introduction\\n### Week 2:\\n- Presentation: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20-%20Quarter%202%20Week%202.pdf\">Evaluate Data Quality and Conduct EDA</a>\\n- Source Code: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/Process%20and%20EDA.ipynb\">Process_and_EDA.ipynb</a>\\n\\n### Week 3:\\n- Presentation: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20-%20Quarter%202%20Week%203.pdf\">HMM: Transition Matrix, Model Accuracy, and Emission Matrix</a>\\n- Source Code: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/HMM.ipynb\">HMM.ipynb</a>,  <a href=https://github.com/miloncl/System-Usage-Analysis/blob/main/src/models/hmm_model.py>hmm_model.py</a>\\n- Outputs: <a href=\"https://github.com/miloncl/System-Usage-Analysis/tree/main/outputs/HMM\">HMM outfiles</a>\\n\\n### Week 4:\\n- Presentation: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20-%20Quarter%202%20Week%204.pdf\">Study LSTM: Data Prep, Data Viz, Research on RNN/LSTM</a>\\n- Source Code: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/Process%20and%20EDA.ipynb\"> Process_and_EDA.ipynb</a>\\n\\n### Week 5:\\n- Presentation: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20-%20Quarter%202%20Week%205.pdf\">RNN (Vanilla + LSTM)</a>\\n- Source Code: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/LSTM_RNN.ipynb\">LSTM_RNN.ipynb</a>,  <a href=https://github.com/miloncl/System-Usage-Analysis/blob/main/src/models/lstm_model.py>lstm_model.py</a>\\n\\n### Week 6:\\n- Presentation: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20%20-%20Quarter%202%20Week%206.pdf\">LSTM Experiments</a>\\n- Source Code: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/LSTM_RNN.ipynb\">LSTM_RNN.ipynb</a>, <a href=https://github.com/miloncl/System-Usage-Analysis/blob/main/src/models/lstm_model.py>lstm_model.py</a>\\n\\n### Week 7-9:\\n- Project Poster: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/references/poster.pdf\">Poster</a>\\n- Practice presentation and elevator pitch in class\\n',\n",
       "  \"This is a summary of the Intel & UCSD HDSI Data Science Capstone Project for 2022-2023. The project aims to discover user-app interactions and solutions to reducing the initial user-CPU latency. The project team consists of Thy Nguyen, Milon Chakkalakal, and Pranav Thaenraj from UC San Diego, with advisors from Intel. The project's GitHub repository includes all the source codes and data files for data collection and analysis. The repository follows the template for Data Science projects by Cookie Cutter Data Science. The code can be run by cloning the GitHub repository, launching a container with a Docker image, and running `python run.py test`. The summary also provides specific links to presentations and source code for each week of the project.\"],\n",
       " 'https://github.com/KeaganBenson/DSC180Flock': ['# DSC180Flock\\n\\nThis project\\'s model comprises of 3 sub-models (predicting average, amount, and standard deviation) that are used for an algorithm at the end\\nOn Command Prompt,for the first time, run the following to clone the repo for the first time:\\n```\\ngit clone https://github.com/KeaganBenson/DSC180Flock.git\\n```\\nThen open Anaconda Prompt, for the first time, enter the new folder, and run the following to create a new conda environment with the requirements.txt. \\n```\\ncd dsc180flock\\nconda create --name flock_env --file requirements.txt\\nconda activate flock_env\\npython run.py all\\n```\\nWhile python run.py all is being ran, plots and maps may open up on seperate windows during the execution, and the execution pauses until those windows are closed. After the execution is complete, observe the metrics printed in the console and close the anaconda\\nNow that the repo is cloned locally and the environment is created, anytime you want to run the model again, you open anaconda prompt and run:\\n```\\ncd dsc180flock\\nconda activate flock_env\\npython run.py all\\n```\\n\\n\\nTargets supported:\\n* **data** - performs the ETL that extracts data from online, and fills the empty data/raw folder with the raw data\\n* **features** - performs the data-cleaning and feature engineering for the intermediate data in data/temp folder and final data in data/out folder\\n* **model** - trains the data on the final data made by the features target, and outputs prediction accuracy metrics and confusion matrix from the validation data\\n* **clear** - empties the data folders raw, temp, and out\\n* **all** - complete cycle of ETL, data preparation (cleaning, feature engineering), training, and prediction. Equivalent of \"python run.py clear data features model\". \\n* **test-data**: all subsequent arguments will be using only the test-data folder, not the data folder.\\n* **test** - complete cycle but only on the \"test data\". Equivalent of \"python run.py test-data clear feature model\"\\n\\n\\n\\n',\n",
       "  'The project\\'s model consists of three sub-models used for an algorithm. To get started, clone the repository using the provided command. Then, create a new conda environment and activate it. Run \"python run.py all\" to execute the model. During execution, plots and maps may open in separate windows. After completion, observe the metrics printed in the console and close Anaconda. To run the model again, open Anaconda prompt and use the command \"python run.py all\". There are different targets supported such as data, features, model, clear, all, test-data, and test.'],\n",
       " 'https://github.com/taekunkim/flock-freight': ['\\n\\nData:\\nThis repo uses Orders and Offers data provided by FlockFreight.\\nThe threeDigitZipCode.json file comes from https://github.com/billfienberg/zip3.\\nCanadian postal codes are from postalcodes_ca, a Python library.',\n",
       "  'This summary mentions the data sources used in a repository, including Orders and Offers data from FlockFreight, a zip code file from GitHub, and Canadian postal codes from a Python library.'],\n",
       " 'https://github.com/ESR76/Capstone-Brick-Modeling': ['# Energy Cost and HVAC Optimization in Smart Buildings\\n\\nIn this project, we attempt to make predictions about future energy usage and cost in a building using energy data collected from UC San Diego\\'s EBU-3B (the Computer Science & Engineering) building\\'s HVAC system.\\n\\nWe also have a public-facing [poster](https://www.canva.com/design/DAFZKQlLOLo/2ALw0oHRO8qrPj--Q-8huw/view?utm_content=DAFZKQlLOLo&utm_campaign=designshare&utm_medium=link&utm_source=publishsharelink) from our poster session and [website](https://xenonition.github.io/) associated with this project.\\n\\n## Running the Code\\nTo test our code, please first clone the repository. \\n\\nIf you\\'re concerned that you won\\'t have the proper packages to run something in our code, use the [Docker image](https://hub.docker.com/repository/docker/esr76/capstone-brick-modeling/general) associated with this project - we recommend using the **\"final\"** tag (the \"latest\" tag should also work). It is designed to be run on UCSD\\'s JupyterHub service using the base UCSD notebook as a base, for which instructions to use it can be found [here](https://github.com/ucsd-ets/datahub-example-notebook).\\n\\nAnother option is running \"pip install -r requirements.txt\".\\n\\n- To run the modeling pipeline on the original data and compare to our paper/poster run the line:\\n    - \"python3 run.py all\"\\n    **OR** \\n    - \"python3 run.py data features model optimize visualize\"\\n\\nIf you try to run a later part of the pipeline (ie. model) before running the earlier parts (ie. data, features), this **will** raise an error.\\n\\n- To use a smaller set of test data and test the output for the pipeline run:\\n    - \"python3 run.py test\"\\n\\nResults from running this line will appear in the /test directory, with data in /test/testdata and visualizations in /test/testviz.\\n\\n- To remove any files created by running the script, please run:\\n    - \"python3 run.py clean\"\\n\\nAfter any call of run.py, the script will run through the steps called, creating files/file organization as necessary, and then will print which steps it took in the order it took them in.\\n\\n***NOTE:***\\nOnce you\\'ve run one of the stages in the main training pipeline or run whole the pipeline before, calling it again will SKIP regenerating the files and print what was skipped. Running the \"clean\" keyword is the only way to ensure that the files will regenerate. The test pipeline will rerun each time the \"test\" keyword is called.\\n\\n## Data Notes\\n\\n### Getting the Data\\nIdeally in our pipeline, data would be obtained by pairing sensor data with mappings from our building\\'s [Brick Schema](https://brickschema.org/) in order to query the locations and floors for relevant sensors to perform our calculation, then using UCSD\\'s Brick server.\\n\\nHowever, we were not able to obtain access to the Brick server in the time we had for the project, so we used data from [a data pull from a previous project](https://github.com/HYDesmondLiu/B2RL/tree/master/real_building_buffers). This represents 15 rooms worth of data on floors 2, 3, and 4 of UC San Diego\\'s EBU-3B (Computer Science) building with data from July 2017 to early January 2019. This data should download automatically when the data part of the pipeline runs - the code should also generate several directories for you, including: data and its subdirectories, test and its subdirectories, and visualizations.\\n\\nThe features part of the data pipeline is split into two parts that perform data cleaning steps that are detailed more on our website, but essentially:\\n1. We separate our data into training and testing sets based on dates in the original datasets, where approximately 70% of the data is before August 1, 2018 and the rest is August 1, 2018 and onwards.\\n2. We floor timestamps in the dataset to the nearest hour and use medians to aggregate into buckets of that time, since the energy values range because of the 15 unidentified rooms in the dataset.\\n3. We impute the training dataset with data based on the median for the value at that hour - this was the most stable trend that we found in the original data. We do not impute the testing dataset because we want to ensure that we are not evaluating the model on predictions of false values.\\n\\n### Other Data and Goals\\nAlong with predicting future energy usage using the energy values from the data pull above, we will be using data from UCSD\\'s pricing plan to scale this for energy. We will only be scaling our data by a constant (derived from UCSD\\'s cost of electricity in fiscal years 2017-2018/2018-2019), although we understand that with UCSD using both its own energy and energy from San Diego Gas and Electric, this likely leads to an underestimation.\\n\\nWhile we don\\'t use it in the final version of our model, we also have the EBU 3B Turtle file (ie. the building\\'s representation in Brick) in our raw data to understand relationships between components of the HVAC system in the building. If you\\'d like to take a look at this, here\\'s the [link](https://brickschema.org/ttl/ebu3b_brick.ttl). This will not auto-download for you.\\n\\nWe also initially pulled other temperature and climate information to use in this project from [NOAA (the National Oceanic and Atmospheric Administration)](https://www.noaa.gov/) and the [EIA (U.S. Energy Information Administration)](https://www.eia.gov/), but we did not end up having time to incorporate this data in our final model.\\n\\n## Credits\\n\\n### Authors\\nWe are four undergraduate Data Science students in our final year at UC San Diego.\\n\\nIf you\\'re interested in our work, here\\'s where you can find more about us:\\n| Name | GitHub | LinkedIn |\\n| ---- | ---- | ---- |\\n| **Jonah Bomwell** | [Link](https://github.com/Jbomwell) | [Link](https://www.linkedin.com/in/jonah-bomwell-0756191b7/) | \\n| **Alise Bruevich** | [Link](https://github.com/alisebruevich) | [Link](https://www.linkedin.com/in/alisebruevich/) |\\n| **William Nathan** | [Link](https://github.com/Xenonition) | [Link](https://www.linkedin.com/in/william-nathan-5019661b2/) |\\n| **Esperanza Rozas** | [Link](https://github.com/ESR76) | [Link](https://www.linkedin.com/in/esperanza-r/) |\\n\\n\\n### Acknowledgments\\nThis project was completed as a capstone project for the Data Science major at UC San Diego in Winter of 2023.\\nFor more information on the course, please read about the class [here](https://dsc-capstone.github.io/).\\n\\nWe\\'d also like to thank: \\n- Rajesh Gupta, our mentor and one of the creators of the Brick Schema.\\n- Xiaohan Fu and Hsin-Yu Liu, who provided us with the data we used and additional mentoring.\\n- Keaton Chia and the DERConnect team, who helped us generate ideas for this project, provided us with UCSD\\'s cost model, and discussed the possibilities of using Brick for future work in this area with us.\\n- Suraj Rampure, our instructor for the course.',\n",
       "  \"This project focuses on energy cost and HVAC optimization in smart buildings. The goal is to predict future energy usage and cost using data collected from UC San Diego's EBU-3B building's HVAC system. The project includes a poster session and a website. The code can be run by cloning the repository or using a Docker image. The data used in the project was obtained from a previous project due to limited access to the Brick server. The data pipeline involves cleaning and aggregating the data. The model also incorporates UCSD's pricing plan for scaling the energy data. The authors of the project are undergraduate Data Science students at UC San Diego, and the project was completed as a capstone project for the Data Science major. Acknowledgments are given to mentors, contributors, and instructors involved in the project.\"],\n",
       " 'https://github.com/SamuelBAguirre/DSC180A_Project': ['This is a repo for our DSC180A Q1 project. In this project we explore measuring the change in surface water over time for Lake Oroville.\\nPlease download image data from https://drive.google.com/drive/folders/1b5-gGvu5K4WNVqeRQ1BLvdQ5DwA2KkQg?usp=share_link, make sure to unzip file and place images directory inside of ./data/\\n\\nA quick breakdown of the structure of this repository:\\n\\nconfig: Contains config files for params used in our notebook  \\nnotebooks: Contains our data exploration notebooks  \\nsrc: Contains our source code  \\ntest: Contains some of our testing data  \\nout: Contains the processed images outputted from our source code\\n\\nWebsite: https://samuelbaguirre.github.io/\\n',\n",
       "  \"This is a repository for the DSC180A Q1 project, which focuses on measuring the change in surface water over time for Lake Oroville. The repository includes image data that can be downloaded from a Google Drive link provided. The repository is structured with folders for config files, notebooks, source code, testing data, and processed images. The project's website can be found at https://samuelbaguirre.github.io/.\"],\n",
       " 'https://github.com/alexmak001/SAR-satelite-image-ship-detection': ['# Maritime Ship Detection Using Synthetic Aperture Radar Satellite Imagery\\nSatellites are being launched into space at an exponential rate and are able to produce high quality images in relatively short intervals of time on any part of Earth. The amount of data and types of it are also increasing significantly and in this paper we specifically use Synthetic Aperture Radar (SAR) satellite imagery in order to detect ships traveling through bodies of water. We created a ship counting tool that intakes a start date, end date, and an area of interest and returns the number of ships for each day between the two dates. The images are first classified into offshore or inshore and a separate object detection algorithm counts the number of ships per image. The classifier and object detection networks are trained using the Large-Scale SAR Ship Detection Dataset-v1.0 (LS-SSDD-v1.0) and deployed on Google Earth Engine.\\n\\n## Testing:\\nWhen running on DSMLP, be sure to use use the launch script\\n`launch-scipy-ml.sh -g 1 -i snng/sar_ship_detection` to launch a pod with a GPU. Otherwise the script will fail to run. \\n\\nTo run the test, simply run python run.py test\\n\\n## run.py file\\nUsing the \"data\" target, it downaloads and formats the dataset locally. This also downloads the models as well. \\nThe \"train_ret\" target will start the training of the RetinaNet model, which requires the data to be loaded. Similarly, the \"train_faster\" will begin to train the Faster R-CNN model. The hyperparameters can be configured in the src/models/ folder for each of the models.\\nThe \"predict\" target uses the model to predict on all of the test data and returns the key metrics for both models. The \"viz\" target causes the models to predict the bounding boxes on the tif files saved in the src/visualization folder. It then saves the resulting image with bounding boxes in the same folder as a jpg.\\n\\n## HOW TO RUN THE SHIP COUNTING SCRIPT:\\n[//]: <> (Have to figure out what to do about json key)\\n1. Once you have activated your DSMLP environment using the launch script above, please ensure you have the private json key downloaded. This will be used for initializing and authenticating Google Earth Engine. [website](https://developers.google.com/earth-engine/guides/service_account)\\n2. We will have to get the coordinates for the desired area of interest from the Google Earth Engine [website](https://code.earthengine.google.com/).\\n    - Note: If you do not have a Google Earth Engine account you will have to make one.\\n    - To get the coordinates, navigate on the map to your desired place of interest\\n    - Then draw a bounding box over your area of interest using the shape tool. (The shape tool is button with the gray square underneath the scripts panel. \\n    ![tut1](https://user-images.githubusercontent.com/69220036/221438416-ca8513ea-412e-43c6-8a8e-5b87e30ac128.png)\\n    ![tut2](https://user-images.githubusercontent.com/69220036/221438475-eac5c729-4478-46bd-8691-88648845255a.png)\\n    - Once you get the desired vertices, the middle panel which is usually labeled new script will have a geometry variable and you expand that until you get the list of 5 vertices and those are the place coordinate values to pass into image downloader function.\\n  ![tut3](https://user-images.githubusercontent.com/69220036/221438515-9acf67df-450b-4f66-b4a7-deed39eb1013.png)\\n3. Once you have obtained your coordinates, run this command in terminal to start counting ships.\\n[//]: <> (Might have to change this command depending on how we implement shipcounter.py.)\\n\\n`python -c from shipcounter.py import shipcounter(place_coords, start_date, end_date, del_images)`\\n\\nwhere:\\n- `place_coords` are the coordinates from Google Earth Engine\\n- `start_date` is the desired start date in the format \\'MM/DD/YYYY\\'\\n- `end_date` is the desired end date in the format \\'MM/DD/YYYY\\'\\n- `del_images` set to `True` if you want to delete the images locally afterwards, `False` if not.\\n',\n",
       "  'This paper discusses the use of Synthetic Aperture Radar (SAR) satellite imagery to detect ships in bodies of water. The authors created a ship counting tool that takes a start date, end date, and area of interest as input and returns the number of ships for each day between the two dates. The tool uses a classifier to classify images into offshore or inshore, and an object detection algorithm to count the number of ships per image. The classifier and object detection networks are trained using the Large-Scale SAR Ship Detection Dataset-v1.0 (LS-SSDD-v1.0) and deployed on Google Earth Engine. The paper also provides instructions on how to run the ship counting script using the obtained coordinates from Google Earth Engine.'],\n",
       " 'https://github.com/rtvo20/dsc180_capstone_q2': ['# DSC 180 Capstone Project\\n\\n*Most of this description was from the README/documentation located on our previous repo, with some slight changes. The repo can be found here* [here](https://github.com/rtvo20/dsc180_quarter1_submission).\\n\\nThis project uses data from David Fenning\\'s Solar Energy Innovation Laboratory (SOLEIL) that creates solar cell samples. The data they collect are information from the manufacturing process of these solar cells, along with data they collect when testing the samples. Our project cleans and transforms the data, which are originally in JSON format, before saving them as CSVs that allow it to be imported and graphed in Neo4j, a Graph DBMS. The purpose of this task is to have a pipeline that can organize and transform the data so that it can be graphed in Neo4j and can be queried.\\n\\nInput data are JSON files containing information from the SOLEIL lab in the form of a worklist. Our functions extract data from these worklists, such as step names (e.g. \\'drop\\', \\'spin\\', \\'hotplate\\'), chemical names, and output data (measurements and tests done on the resulting sample). Running run.py on the input data gives us the output data file, which can be used to generate our output, a graph representation of the data.\\n\\nThe output data (CSVs) after they are cleaned and transformed are run through a function that generates a script file with queries in Neo4j\\'s query language, Cypher. The script (a .cypher file) automates the process to graph the data using Neo4j\\'s Cypher shell terminal. To run the script, we use a Docker container running Neo4j, which requires us to copy this \\'output.cypher\\' file into the docker container\\'s root directory before we can run it with a command. All of the instructions to do so are located in the section below.\\n\\nThis current iteration includes test data under test/testdata, which is one sample from some actual data to show how our code works on \"barebones\" test data.\\n\\nMore detailed instructions are below, however an overview of the process to reproduce our results:\\n\\nrun python run.py test to generate a script file with queries to generate a graph\\'s nodes and links.\\nUsing Docker, pull the latest Neo4j Docker image and start a container with this image.\\nCopy the script file from DSMLP to the local setting, then copy it to the docker container\\'s root directory\\nOpen a terminal in the docker container and run the script file and produce the results.\\n\\nMore detailed instructions are below, however an overview of the process to reproduce our results:\\n1. run ```python run.py test``` to generate a script file with queries to generate a graph\\'s nodes and links.\\n2. Using Docker, pull the latest Neo4j Docker image and start a container with this image.\\n   * Copy the script file from DSMLP to the local setting, then copy it to the docker container\\'s root directory\\n   * Open a terminal in the docker container and run the script file and produce the results.\\n\\n## To run the project use run.py and follow the instructions below.\\n\\n* The filepaths to the test data are already coded into ```run.py``` and are under the folder \"test/testdata\".\\n* The available targets for running ```python run.py <target>``` and the order of the targets are:\\n    * ```data```>```features```>```queries```\\n    * Alternatively, running the command ```python run.py test``` is equivalent to running each of the above targets sequentially.\\n* Running ```run.py``` cleans and transforms the data and creates queries in Neo4j\\'s query language (Cypher) that allows for nodes and links to be graphed. Each graph in our implementation currently requires 6 queries to create and link all the nodes, so to help automate the process, the output of ```run.py``` is a Neo4j script-type file (.cypher file) that performs all of these queries in less inputs than doing so manually.\\n  * Our output file is named \"output.cypher\" and will be located in the project\\'s root directory.\\n\\n## To run the script generated by the run.py script above, use Docker\\n\\n* The following docker run command sets up a docker container with all of the necessary flags and config settings. The command is all one line, it should be copied and pasted in its entirety in a local terminal.\\n    * ```docker run -p 7474:7474 -p 7687:7687 -v $PWD/data:/data -v $PWD/plugins:/plugins --name neo4j-apoc -e NEO4J_apoc_export_file_enabled=true -e NEO4J_apoc_import_file_enabled=true -e NEO4J_apoc_import_file_use__neo4j__config=true -e NEO4JLABS_PLUGINS=\\\\[\\\\\"apoc\\\\\"\\\\] neo4j:4.0```\\n    * In short, the flags set up permissions that allow for moving files between the Docker container\\'s storage volume and local storage. It also enables for usage of APOC, a Neo4j package used to help export queried data.\\n    * Wait for the Docker container to initialize and start up, and in an internet browser navigate to `localhost:7474`.\\n    * This opens up Neo4j\\'s browser UI and upon accessing it for the first time, should prompt the user to create a username/password; although it is possible to set it up with no authentication required under the \"Authentication type\" drop-down menu.\\n    * The default username/password is neo4j/neo4j. Once entered, it will then ask for a new password.\\n\\n* The next steps require copying the output of the `run.py` file, \"output.cypher\" and the output CSVs into the Docker container\\'s directory.\\n    * If the output.cypher file is located on DSMLP, it should be downloaded/copied to a local directory first.\\n    * In a local terminal, change directory to the location of the cypher file and CSVs, then run the following command\\n      * ```docker cp output.cypher neo4j-apoc:/var/lib/neo4j/import/output.cypher```\\n    * Then, perform the same process and copy the CSVs into the Docker container.\\n      * ```docker cp b19_sample0_chem.csv neo4j-apoc:/var/lib/neo4j/import/```\\n      * ```docker cp b19_sample0_action.csv neo4j-apoc:/var/lib/neo4j/import/```\\n      * ```docker cp b19_sample0_link.csv neo4j-apoc:/var/lib/neo4j/import/```\\n\\n* With the necessary files copied over to the docker container, go to the docker container and select \"Open in Terminal\", as seen in the image below. \\n\\n![image](https://user-images.githubusercontent.com/59627502/218381794-04ed9f95-5fc9-4102-aa9c-d5c87adcee41.png)\\n\\n  * In this docker terminal, `cd import`\\n  * and again, in the docker terminal, run the following command:\\n    * `cypher-shell -f output.cypher -u neo4j -p test` (replacing your username and password where `neo4j` and `test` are respectively.  \\n  * Back in the browser at `localhost:7474`, `MATCH(n) RETURN n` can be entered in the query field and run to return the nodes and relationships graphed by our output.\\n\\n![image](https://user-images.githubusercontent.com/59627502/218383326-7880d998-0aeb-4b63-8ce6-24aad0ae5f85.png)\\n* This is our result from a graph containing multiple samples, but the test data will contain just 1 sample.\\n\\n## Additional features\\nAt the final stage of our project, we moved from using Neo4j Docker to Neo4j Desktop, as this version has more practical use for the lab team. Our run.py and this README file contain instructions and functionality for Neo4j Docker in order to support reproducibility; however we have included a `preprocessing.ipynb` that contains several features used with Neo4j Desktop that is useful in practice for the lab team. \\nTo briefly describe those features:\\n  * Saving `action`, `chem`, and `link` CSV files along with cypher files to the appropriate folders within the local Neo4j Desktop installation\\n    * CSV files are saved to the `import` folder within the Neo4j Desktop database directory\\n    * Cypher files are saved to the `bin` folder within the Neo4j Desktop database directory \\n  * Version control to keep track of batches of data that have already been processed, to avoid unnecessary redundant processing of batchs.\\n  * Deleting CSV files after they are used to load data into Neo4j to reduce clutter in file storage.\\n',\n",
       "  \"This project uses data from David Fenning's Solar Energy Innovation Laboratory (SOLEIL) to create a pipeline that organizes and transforms the data so that it can be graphed in Neo4j and queried. The input data is in JSON format and contains information about the manufacturing process of solar cells, as well as test data. The project cleans and transforms the data, saves it as CSV files, and generates a script file with queries in Neo4j's query language (Cypher) to graph the data. The script file can be run using Docker to produce the results. The project also includes additional features for using Neo4j Desktop.\"],\n",
       " 'https://github.com/nahmann/DSC180-B16': ['# DSC 180 B16 - Decentralized Location Consensus\\n\\nGroup Members:  \\nNathan Ahmann  \\nMason Chan  \\nAlex Guan  \\nAlan Miyazaki  \\nMentor: Haojian Jin\\n\\n\\nOur Heroku site can be found [here](https://dsc180-decentralized-location.herokuapp.com/)  \\nA static verison of our website can be found [here](https://nahmann.github.io/DSC180-B16/)\\n\\nOur original work from Fall Quarter:  \\n[Video Demo](https://youtu.be/Ixj5MV3JIbA) that shows the server working with GET and POST requests.\\n  \\nOur new video showcasing the final project:  \\n[Video Demo](https://youtu.be/sOopTH0-ghM) that walks through an example scenario.\\n\\n\\n## Content\\n\\nThis repository contains all the files sent to our Heroku server which can be accessed [here](https://dsc180-decentralized-location.herokuapp.com/).  \\n\\nThis main server has the API for the backend and a landing page for the website. In conjunction with B16-2, the app team, an Android device will send get/post requests to this backend which will allow the app to verify the location of users. The verification is done via blacklisting malicious users by having Heroku run `trust_algorithm.py` on a set timer (currently on the hour). This file updates the database for the blacklist on the backend.\\n\\nIf you would like to run this website locally, you can install the necessary packages and run the following code in the terminal. Note that the local development version of the server utilizes sqlite3 as the database since it is easier to locally utilize than Postgres which we use on our Heroku server.\\n\\n`python manage.py makemigrations` - sets up the migrations for the database  \\n`python manage.py migrate` - makes the migration files  \\n`python manage.py runserver` - will run the server which can then be accessed at 127.0.0.1  \\n\\n## Contains parts of code modified from the following tutorials:\\nhttps://docs.djangoproject.com/en/4.1/intro/   \\nhttps://github.com/heroku/python-getting-started  \\nhttps://devcenter.heroku.com/articles/getting-started-with-python  \\nhttps://www.django-rest-framework.org/tutorial/quickstart/   \\nhttps://www.django-rest-framework.org/tutorial/1-serialization/ \\n\\nand our static webpages contain CSS from Bootstrap\\n',\n",
       "  'This is a summary of the project \"Decentralized Location Consensus\" by group members Nathan Ahmann, Mason Chan, Alex Guan, and Alan Miyazaki. The project involves a Heroku site that serves as the backend API and landing page for the website. The site allows an Android app to verify the location of users by sending get/post requests to the backend. The verification process involves blacklisting malicious users using a trust algorithm that updates the database on the backend. The project also includes tutorials and code modifications from various sources.'],\n",
       " 'https://github.com/acanonig/DSC180B-Proxensus-': [\"# DSC 180B Project Code - Proxensus\\nGroup Members:\\nFrans Timothy Juacalla,\\nAndrew Canonigo,\\nMartin Thai,\\nAryaman Sinha,\\n\\nThis code was adapted and modified from the Pre-standard Codebase of the DP3T SDK for Android.\\nhttps://github.com/DP-3T/dp3t-sdk-android/releases/tag/prestandard\\n\\nDP3T SDK\\nhttps://github.com/DP-3T/dp3t-sdk-android\\n\\n### How to run project\\nTo Run the project in Android Studio, Please open the 'calibration-app' instead of the entire project. The project needs at least 3 Android smartphones with working Bluetooth.\\n\\nDemonstration Video: https://youtu.be/stuOTvUJUmk\\n\",\n",
       "  'The Proxensus project code is adapted from the Pre-standard Codebase of the DP3T SDK for Android. It can be found at https://github.com/DP-3T/dp3t-sdk-android/releases/tag/prestandard. The project requires at least 3 Android smartphones with working Bluetooth to run. A demonstration video can be found at https://youtu.be/stuOTvUJUmk.'],\n",
       " 'https://github.com/pnagasam/dsc180a_capstone_project': ['# dsc180a_capstone_project\\n\\nThis program takes care of all data loading and preprocessing, model building and training, and visualizations regarding our DSC 180 Capstone project.\\n\\n## Usage\\nAll functionality is used by running `python3 run.py` followed by some arguments in the root directory. The first argument is `exec_type` which indicates what action you would like to take. Possible values are:\\n```\\ndata     # for data loading\\n\\ntrain    # for model training\\n\\nOT       # for optimal transport\\n\\neval     # for gathering results\\n\\nviz      # for creating visualizations\\n\\nall      # for doing all of the above\\n\\ntest     # for testing all of the above on dummy data\\n         # (except downloading the data)\\n```\\n\\nThe second and final argument is `-c` or `--clean`, which, when included cleans the save directories used by the specified `exec_type`.\\n\\n### All\\nThe `all` function runs the other functions sequentially, so you don\\'t have to run each individual function. It is up to the user to set values in the `config/` files correctly. To run:\\n```bash\\npython3 run.py all\\n```\\n\\nTo remove all models, OT, results, and visualizations generated by the program, run:\\n```bash\\npython3 run.py all -c    # or --clean\\n```\\nNote: running this will NOT remove the 13.1 GB of data downloaded by running the `data` function. This is to prevent headache on behalf of the user. You\\'re welcome.\\n\\n### Data Loading\\nThe dataset is downloaded from the WILDS project using their python package, which is a prerequisite. To install run:\\n```bash\\npip install wilds\\n```\\n\\nRunning the following command in the root project directory will download the dataset to the proper location:\\n```bash\\npython3 run.py data\\n```\\n\\n### Model Training\\nThe model we used for this project was a custom CNN built in pytorch and trained entirely on either urban or rural data from one country. To train a model with settings specified in the `config/train.json`, run:\\n```bash\\npython3 run.py train\\n```\\nTo change which country the model should be trained on, whether an urban/rural model should be trained, the percentile cutoffs the model uses for classification, or other training parameters, please consult the config section below.\\n\\nTo remove all trained models, run:\\n```bash\\npython3 run.py train -c    # or --clean\\n```\\n\\n### Optimal Transport\\nOptimal transport was achieved using the python optimal transport package, which is a prerequisite. To install run:\\n```bash\\npip install ot\\n```\\n\\nIn our implementation, optimal transport is used for domain adaptation. The goal is to adapt the color profiles of one country to another, in the hope that the CNN trained on only one country can more accurately classify images from another country.\\nIn order to fit the optimal transport to transport images from a source country to a target country, first edit the source and target country fields in `config/OT.json`, then run:\\n```bash\\npython3 run.py OT\\n```\\n\\nTo remove all saved OT models, run:\\n```bash\\npython3 run.py OT -c    # or --clean\\n```\\n\\n### Results\\nThe results are gathered and using data, models and OT objects saved from above. This function will error if run without running `data`, `train`, and `OT` first with aligning configuration to generate objects. To run:\\n```bash\\npython3 run.py eval\\n```\\n\\nTo remove all saved results, run:\\n```bash\\npython3 run.py eval -c    # or --clean\\n```\\n\\n### Visulizations\\nSimilarly to results, the `viz` function requires previous data to be present in the directories specified in each `config/` file. Visualizations are generated using matplotlib and pandas, both prerequisites. To run:\\n```bash\\npython3 run viz\\n```\\n\\nTo remove all saved visualizations, run:\\n```bash\\npython3 run.py viz -c    # or --clean\\n```\\n\\n### Test\\nThe `test` function is similar to the `all` function except it uses randomly generated data. A valid `config/` is still required for the program to run entirely. And onjects will still be saved. To run:\\n```bash\\npython3 run.py test -c    # or --clean\\n```\\n\\nRunning `python3 run.py test -c    # or --clean` won\\'t remove anything. To remove all saved objects, run:\\n```bash\\npython3 run.py all -c    # or --clean\\n```\\n\\n## Config\\nBelow is the default `config/` files along with descriptions of each option.\\n\\n### Train\\n`config/train.json`\\n\\n```json\\n{\\n    \"country\": \"nigeria\",  // the country to train the model on (usually same as \"target_country\")\\n    \"train_proportion\": 0.7,  // proportion of data to use to train the model\\n    \"valid_proportion\": 0.2,   // proportion of data to use to validate the model\\n    \"urban\": true,  // whether to train a model on urban data\\n    \"rural\": true,  // whether to train a rural on urban data\\n    \"low_quantile\": 0.3333333,  // the lower percentile used for classification cutoff\\n    \"high_quantile\": 0.6666666,  // the lower percentile used for classification cutoff\\n    \"n_epochs\": 300,   // number of epochs to train the model on (best one will be used for evaluation)\\n    \"batch_size\": 48,  // batch size to use\\n    \"save_path\": \"models/\",  // path to save models at\\n    \"random_seed\": 10  // random seed to use for training\\n}\\n```\\n\\n### OT\\n`config/OT.json`\\n\\n```json\\n{\\n    \"target_country\": \"nigeria\",  // country to transport to\\n    \"source_country\": \"mali\",  // country to transport from\\n    \"n_samples\": 500,  // number of pixel samples to take from each country\\n    \"reg\": 0.1,  // sinkhorn\\'s regularization parameter\\n    \"batch_size\": 10,  // batch size to use for OT\\n    \"save_path\": \"OT/\",  // path to save OT objects to\\n    \"random_seed\": 10  // random seed to use for OT\\n}\\n```\\n\\n### Results\\n`config/eval.json`\\n\\n```json\\n{\\n    \"target_country\": \"nigeria\", // OT \"target_country\" (usually the same as \"country\" in config/train.json)\\n    \"source_country\": \"mali\",  // OT \"source_country\"\\n    \"urban\": true,  // whether to evaluate results on urban model\\n    \"rural\": true,  // whether to evaluate results on rural model\\n    \"batch_size\": 10,  // batch size to use when evaluating\\n    \"save_path\": \"results/\",  // path to save results at\\n    \"random_seed\": 76  // random seed to use for evaluation\\n}\\n```\\n\\n### Visualizations\\n`config/viz.json`\\n\\n```json\\n{\\n    \"asset_index_dist\": true,  // whether to show asset index distribution visualization\\n    \"clf_cutoffs\": true,  // whether to show classification cutoff visualization\\n    \"target_country\": \"nigeria\",  // OT \"target_country\" and train \"country\"\\n    \"source_country\": \"mali\",  // OT \"source_country\"\\n    \"low_quantile\": 0.33333,  // the lower percentile used for classification cutoff\\n    \"high_quantile\": 0.66666,  // the lower percentile used for classification cutoff\\n    \"training_info\": {\\n        \"urban\": true,  // whether to show training info for urban model\\n        \"rural\": true  // whether to show training info for rural model\\n    },\\n    \"source_confusion_matrix\": {\\n        \"urban\": {\\n            \"without_OT\": true,  // whether to show confusion matrix for urban model without OT\\n            \"with_OT\": true  // whether to show confusion matrix for urban model with OT\\n            \\n        },\\n        \"rural\": {\\n            \"without_OT\": true,  // whether to show confusion matrix for rural model without OT\\n            \"with_OT\": true  // whether to show confusion matrix for rural model with OT\\n        }\\n    },\\n    \"show_changed\": true,  // whether to an example of where optimal transport changed the models prediction\\n    \"save_path\": \"viz/\", // path to save visualizations to\\n    \"random_seed\": 53 // random seed to use for visualizations\\n}\\n```\\n',\n",
       "  'This program is designed for the DSC 180 Capstone project. It handles data loading, preprocessing, model building and training, and visualizations. The program can be run with different arguments to perform specific actions such as data loading, model training, optimal transport, result gathering, and visualization creation. The program also provides the option to clean the save directories used by each action. The configuration files specify various options for each action, such as the country to train the model on, the proportion of data to use for training and validation, whether to train on urban or rural data, classification cutoffs, number of epochs, batch size, save paths for models and results, and random seeds.'],\n",
       " 'https://github.com/BillChen24/DSC180B-Project-B319-2': ['# DSC180B-Project-B319-2\\n# Domain Adaptation of CNN in Animal Classification Task\\n\\n### For Test Trails:\\nAfter downloading the githubt repo (including the sample data in data folder), run the following code in terminal\\n```\\npython run.py test\\n```\\nThis code will train a custom CNN model on the sample data and generate a loss curve over epochs.\\n\\nThe loss plot will be saved in the path printed at the end of the execution.\\n\\nThe trained model will be saved in the path printed at the end of the execution.\\n\\nCode will create a \"result/\" folder if such folder doesn\\'t exist in the local repository and store the loss plot and model in this folder\\n\\n#### After Runing\\nTo clear all output files, run the following command in terminal:\\n```\\nrm -r result/\\n```\\n\\n\\n### To Get Whole dataset\\nCreate environment for loading iwildcam dataset \\n\\n0: Open a terminal \\n\\n1: ssh in tothe dsmlp environment:\\nssh <user_name>@dsmlp-login.ucsd.edu\\n\\nExample: ssh zhc023@dsmlp-login.ucsd.edu\\n\\n2: launch-scipy-ml.sh -c 8 -m 50 -i billchen24/dsc180b-project -P Always // request 8 cpu and 50 GB, and create specific environment with my docker image \\n\\n3: Create new terminal \\n\\n4: ssh -N -L 8889:127.0.0.1:16585 zhc023@dsmlp-login.ucsd.edu\\n\\n5: Go to http://localhost:8889/user/zhc023/tree/ \\n\\n### To Run the Full Experiment\\nRun the following command\\n```\\npython run.py main 10\\n```\\n\"10\" specified the maximum number of epoch the model will train, Feel free to update it to any value.\\nSome other hyperparameters can be view and change in \"run.py\"\\n\\nSimilar to the test trail, all output from can be removed by \\n```\\nrm -r result/\\n```\\n',\n",
       "  'This document provides instructions for running test trials and obtaining the whole dataset for the domain adaptation of CNN in an animal classification task. To run a test trial, download the GitHub repository and run the command \"python run.py test\" in the terminal. The code will train a custom CNN model on the sample data and generate a loss curve over epochs. The loss plot and trained model will be saved in a \"result/\" folder. To clear all output files, use the command \"rm -r result/\". To obtain the whole dataset, follow the provided steps to create an environment for loading iwildcam dataset and access it through a web browser. To run the full experiment, use the command \"python run.py main 10\" where \"10\" specifies the maximum number of epochs for training. Output files can be removed using \"rm -r result/\".'],\n",
       " 'https://github.com/TallMessiWu/dota2-drafting-backend': ['Project DOTA 2 drafting.\\n\\n[API Documentation](https://docs.opendota.com/)\\n',\n",
       "  'The given link leads to the API documentation for Project DOTA 2 drafting.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_67': ['# DSC180B_Capstone_Project\\nRyan Cummings,\\nGregory Thein,\\nJustin Kang,\\nProf. Shannon Ellis,\\nCode Artifact Checkpoint\\n\\n#### In this you will find our Checkpoint Code, We are in the B04 Genetics domain and this is our Capstone Project. For our Capstone Project we are looking at Alzheimer\\'s Diseased Patient\\'s Blood miRNA Data. Our Pipeline functions are seen in the all_pipeline.py file. Running the full pipeline takes multiple hours to run and implements the tools in our Genetics Pipeline (FastQC, CutAdapt, Kallisto, DESeq2). Our project implements both python and R to perform successful analysis on our dataset of blood based miRNA in which we find miRNAs with significantly changed expression level.\\n\\n#### Our repo consists of 4 folders, and 3 files (a .gitignore, the README, and the run.py). The 4 folders consist of: config, notebooks, references, src. Inside config is our data-params.json file, eda-params.json file, report-params.json, analyze-params.json, viz-params.json, and test-params.json. These files specifies the data-input and output locations/file paths that is necessary for this Checkpoint\\'s data retrieval. The eda-params file specifies the input and output of the report generated by the `eda` call, while the test-params has the names of the samples that we run the `test` keyword argument on, report-params has the input and output locations of the full report that is generated at the end of the `all` call, analyze-params has the the filepaths for the input/output of the analyze notebook that is ran when `analyze` is passed as a target, viz-params has the locations for the notebook that is generated when the `viz` param is called. Notebooks folder consists of all of our .ipynb files that we used for testing, and as a dev tool (to see what we did along the way). It also contains the notebooks that are converted for each of the targets that can be passed into our program. References has our SRARunTable from the patients we used in our project, and also contains static images that are loaded for some of the notebooks when converting to output report. The data folder is where we created the symlink between our folder and the dataset on DSMLP. The data folder (and test/testdata) will also consist of the data/out information once the `test` keyword is ran, specifically the output from Kallisto is stored here. The contents of our src folder contains our etl.py file, eda.py file, utils.py file, test_pipeline.py, and all_pipeline.py. Our etl.py file is where our file is extracting the dataset from the DSMLP\\'s /teams dataset. Utils.py is where we created a function that turns a notebook into an HTML format, which then outputs that HTML file as a report. test_pipeline and all_pipeline contain the pipeline that is created for our project, varying slightly since test is only ran on a portion while all is ran on the entire dataset!\\n\\n### Project Decisions\\n\\n- Our project focus shifted from looking at gene expression data for Alzheimer\\'s Disease patients, to observing blood sample data of patients diagnosed with Alzheimer\\'s Disease and a control group. This was done in large part because of the lack of access to the databases we initially wanted to retrieve data from\\n- After spending time searching for a viable replacement dataset on Recount2, we set on data from SRA Study SRP022043 and downloaded the data onto DSMLP from the SRA Run Selector Tool \\n- We initially implemented the dockerfile for this project based on the dockerfile used in last quarters replication and had hoped to implement TrimGalore as a new tool into our pipeline. Incompatibility issues, however, led us to drop TrimGalore as tool and stick with running Cutadapt and FastQC separately.\\n- The Kallisto reference file was originally stored in our data file in our Github but the `.gitignore` was hiding that file when we would pull the repo. We need it in order to run Kallisto so we moved it to our teams directory on DSMLP.\\n\\n\\n### Project Targets:\\n#### all\\nRuns entire pipeline on all of the data. Running `all` will run the full pipeline from scratch, this does take hours and sometimes even days to run, it can be ran from scratch but is not needed to be ran from scratch to see our results!\\n```\\n{\\n    \"outdir\": \"data/report\",\\n    \"report_in_path\": \"notebooks/Alzheimers-Biomarker-Analysis.ipynb\",\\n    \"report_out_path\": \"report/Alzheimers-Biomarker-Analysis.html\"\\n}\\n```\\n#### test\\nRuns part of pipeline on a couple fastq files. Implements fastqc and kallisto. Then generates this report!\\n```\\n{\\n  \"test_1\": \"SRR837440.fastq.gz\",\\n  \"test_2\": \"SRR837444.fastq.gz\"\\n}\\n```\\n#### data\\nIn Progress! Gets and outputs the data and generates the report as well!\\n```\\n{\\n  \"file_path\": \"/teams/DSC180A_FA20_A00/b04genetics/group_1/raw_data\"\\n}\\n```\\n#### eda\\nRuns EDA process. Makes report with data and plots figures.\\n```\\n{\\n    \"outdir\": \"data/report\",\\n    \"report_in_path\": \"notebooks/EDA.ipynb\",\\n    \"report_out_path\": \"notebooks/EDA.html\"\\n}\\n```\\n#### viz\\nRuns Visualization process. Simply outputs all the charts and graphs used in the project.\\n```\\n{\\n    \"outdir\": \"data/report\",\\n    \"report_in_path\": \"notebooks/Viz.ipynb\",\\n    \"report_out_path\": \"notebooks/Viz.html\"\\n}\\n```\\n\\n#### analyze\\nRuns the Notebook used for our Analysis portion of the project. Generating the plots that are used to explain our results.\\n```\\n{\\n    \"outdir\": \"data/report\",\\n    \"report_in_path\": \"notebooks/analyze.ipynb\",\\n    \"report_out_path\": \"notebooks/analyze.html\"\\n}\\n```\\n\\n\\n#### Running `python run.py all` will run the full pipeline from scrath, this does take hours and sometimes even days to run, it can be ran from scratch but is not needed to be ran from scratch to see our results! Other keywords that can be passed into the funciton are `test eda data viz analyze`. Running `python run.py test` is actually the most recommended one, this gives you the full pipeline experience on a fraction of the data, running in just a few minutes. Portions of the code can also be ran with `python run.py data` or `python run.py eda` or a combination of these: `python run.py data eda` etc. We also printed steps along the way to notify the user what is currently running in the pipeline. Our code assumes it is ran on the DSMLP Servers! Without running on the DSMLP Servers we would not be able to access the data, which is why it is important to be connected to the server.\\n\\n\\n\\n### Responsibilities\\n\\nRyan: \\nRyan created the Pipeline that we are using for our project so far: FastQC, Cutadapt, FastQC (2), and Kallisto. Along with formatting the Github repo to the Cookiecutter Data Science standard. \\n\\nJustin: \\nJustin worked mainly on getting the report side of the project complete. He, alongside Gregory, spent time researching what MicroRNA and biomarkers are to include as part of our background. Researching additional information about Alzheimer’s Disease was also completed. He also worked on getting the initial structure of the report completed prior to the checkpoint. \\n\\nGregory: \\nGregory, alongside Justin worked on the researching miRNA and biomarkers, and their relation to AD. Furthermore, he helped research various parameters and settings for parts of the pipeline. \\n\\nAll assisted in the implementation of the pipeline alongside editing/reviewing each other’s work. \\n',\n",
       "  \"This is a summary of the DSC180B Capstone Project. The project focuses on analyzing Alzheimer's Diseased Patient's Blood miRNA Data. The pipeline functions include FastQC, CutAdapt, Kallisto, and DESeq2. The project uses both Python and R for analysis. The repository consists of four folders: config, notebooks, references, and src. The project targets include all, test, data, eda, viz, and analyze. Running the full pipeline takes multiple hours to run. The responsibilities of the team members are also mentioned.\"],\n",
       " 'https://github.com/nickthegroot/recipe-recommendation': ['<h1 align=\"center\">\\n   <img src=\"reports/badges/ucsdseal.png\" width=20% />\\n   <img src=\"reports/badges/tigergraph.png\" width=20% />\\n\\nPersonalized Recipe Recommendation Using Heterogeneous Graphs\\n\\n</h1>\\n\\n**Authors**:\\n\\n- Nicholas DeGroot (Halıcıoğlu Data Science Institute, UC San Diego)\\n\\n## Description\\n\\nThis project was created for UCSD\\'s DSC 180: Data Science Capstone. According to the university, the course:\\n\\n> Span(s) the entire lifecycle, including assessing the problem, learning domain knowledge, collecting/cleaning data, creating a model, addressing ethical issues, designing the system, analyzing the output, and presenting the results.\\n>\\n> https://catalog.ucsd.edu/courses/DSC.html#dsc180b\\n\\n## Getting Started\\n\\nThis project is configured with `devcontainer` support. This automatically creates a fully isolated environment with all required dependencies installed.\\n\\nThe easiest way to get started with `devcontainers` is through [GitHub Codespaces](https://github.com/features/codespaces).\\n\\n1. Click [here](https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=571806935) to create a new codespace on this repository.\\n   - Alternatively, this can be done through the `gh` CLI.\\n2. Configure the codespace to your liking. We recommend the 8-core machine.\\n3. Start the codespace and connect. It might take a minute to install all the dependencies. Grab a :coffee:!\\n4. Connect to the codespace through your preferred method (browser / VS Code).\\n\\n## Testing\\n\\nThis project is setup with an array of tests using `pytest` to ensure things are working. With a working environment, run the following command.\\n\\n```\\nmake test\\n```\\n\\n### Testing on DSLMP\\n\\nFor UCSD students & staff, we\\'ve ensured that everything works on the Data Science Machine Learning Platform servers.\\n\\nThe (auto!) published Docker image contains everything you need to test the project. Under the hood, it\\'s running the same container that any `devcontainer` is.\\n\\nIn DSMLP: log in with your credentials, then run the following:\\n\\n```\\nlaunch.sh -s -i ghcr.io/nickthegroot/recipe-recommendation:main\\ncd /app\\nmake test\\n```\\n\\nThis will begin a full run of every test in the project. Currently, this includes a full pipeline test and a smaller data processing test.\\n\\n## Downloading/Preparing the Data\\n\\n1. Download the data by creating an Kaggle account and downloading the [`shuyangli94/food-com-recipes-and-user-interactions`](https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions) dataset.\\n2. Unzip the data into `data/raw`.\\n   - You should see a number of files, including `data/raw/RAW_interactions.csv` and `data/raw/RAW_recipes.csv`\\n3. Run `make data` to clean the data into its cleaned form.\\n\\n## Running\\n\\nAll models can be trained using `python src/cli/train.py`.\\n\\n- Run `python src/cli/train.py --help` for all configuration options\\n- In general, all models can be trained via `python src/cli/train.py --model {model}`\\n  - For example, `LightGCN` is trained with `python src/cli/train.py --model LightGCN`\\n',\n",
       "  \"This project is about personalized recipe recommendation using heterogeneous graphs. It was created for UCSD's DSC 180: Data Science Capstone course. The project provides instructions on how to get started, run tests, download and prepare the data, and train the models.\"],\n",
       " 'https://github.com/mjw49/DSC180B-Quarter-2-Project': [\"# Running the Project\\nFor building, please run the commands below in this order \\n\\n- `launch-scipy-ml.sh -i mjw49/q1project`\\n- `git clone https://github.com/mjw49/DSC180B-Quarter-2-Project.git`\\n- `python run.py test`\\n- `python run.py sampling_city_single (takes around 10 minutes to run)`\\n\\n# Obtaining the Data Locally\\n\\nThe raw data for this project is obtainable from the website for Stanford's Network Analysis Project (SNAP): http://snap.stanford.edu/higher-order/data.html\\n\\nOnce downloaded, extract the compressed zip file and drop the file into the `data/raw` folder.\\n\\n\",\n",
       "  \"To run the project, follow these steps in order:\\n1. Run `launch-scipy-ml.sh -i mjw49/q1project`\\n2. Clone the repository using `git clone https://github.com/mjw49/DSC180B-Quarter-2-Project.git`\\n3. Execute `python run.py test`\\n4. Run `python run.py sampling_city_single` (this step takes approximately 10 minutes)\\n\\nTo obtain the data locally, download it from Stanford's Network Analysis Project (SNAP) website: http://snap.stanford.edu/higher-order/data.html. Extract the compressed zip file and place it in the `data/raw` folder.\"],\n",
       " 'https://github.com/camille-004/Graph-HSCN': ['<h1 align=\"center\">\\nGraphHSCN: Heterogenized Spectral Cluster Network for Long Range Representation Learning</h1>\\n<div align=\"center\">\\n\\n  <a href=\"https://camille-004.github.io/\">Camille Dunning</a>, <a href=\"https://www.linkedin.com/in/zhishang-luo-a51a8120b/\">Zhishang Luo</a>, <a href=\"https://dylantao.github.io/\">Sirui Tao</a>\\n  <p><a href=\"https://datascience.ucsd.edu/\">Halıcıoğlu Data Science Institute</a>, UC San Diego, La Jolla, CA</p>\\n</div>\\n\\n<p align=\"center\">\\n  <a href=\"https://drive.google.com/file/d/1kODg7Qw4hAj1e2Ct91R_tvom8MHdeGln/view\" alt=\"Paper\">\\n        <img src=\"https://img.shields.io/badge/Project-Paper-%238affca?style=plastic\" /></a>\\n        \\n  <a href=\"https://graphhscn.github.io//\" alt=\"Website\">\\n        <img src=\"https://img.shields.io/badge/Project-Website-%238affca?style=plastic\" /></a>\\n        \\n  <a href=\"https://github.com/camille-004/Graph-HSCN/actions/workflows/build-and-push.yml\" alt=\"Build\">\\n        <img src=\"https://github.com/camille-004/Graph-HSCN/actions/workflows/build-and-push.yml/badge.svg\" /></a>\\n\\n</p>\\n<hr/>\\n\\n\\n<!-- [![Paper (First Draft)](https://img.shields.io/badge/Project-Paper-9cf)](https://drive.google.com/file/d/1kODg7Qw4hAj1e2Ct91R_tvom8MHdeGln/view) -->\\n\\n## :rocket: Highlights and Contributions\\n\\nTODO: Flowchart figure\\n\\n>**<p align=\"justify\"> Abstract:** *Graph Neural Networks (GNNs) have gained tremendous popularity for their potential to effectively learn from graph-structured data, commonly encountered in real-world applications. However, most of these models, based on the message-passing paradigm (interactions within a neighborhood of a few nodes), can only handle local interactions within a graph. When we enforce the models to use information from far away nodes, we will encounter two major issues: oversmoothing & oversquashing. Architectures such as the transformer and diffusion models are introduced to solve this; although transformers are powerful, they require significant computational resources for both training and inference, thereby limiting their scalability, particularly for graphs with long-term dependencies. Hence, this paper proposes GraphHSCN—a Heterogenized Spectral Cluster Network, a message-passing-based approach specifically designed for capturing long-range interaction. On our first iteration of ablation studies, we observe reduced time complexities compared to SAN, the most popular graph transformer model, yet comparable performance in graph-level prediction tasks.*\\n\\n### Main Contributions\\n1. **Graph coarsening via spectral clustering**: We propose a scheme to coarsen graph representation via spectral clustering with the relaxed formulation of the MinCUT problem, as presented in the [paper](https://arxiv.org/abs/1907.00481) from Bianchi et. al. We observe the structural patterns uncovered by SC reveal which long-range virtual connections should be made.\\n2. **New connections learned by a heterogeneous network**: We create an intra-cluster connection with a virtual node, and learn the new relationship as a graph indepdenent of the original graph. A heterogeneous convolutional network is trained on these separate relations, further coarsening the representations. On our set of ablation studies, and after hyperparameter tuning, Graph-HSCN out-performs the traditional message-passing architectures by up to 10 percent, achieving metrics similar to those of SAN while reducing the time complexity.\\n\\n## Getting Started\\n\\n### Prerequisites\\nTo set up the environment and install all dependencies, run `make env`. The `logs` and `datasets` directories will be created automatically at the project level.\\n  \\n### `.devcontainers` Support\\nTODO\\n### Running with CLI\\nTODO\\n### Running in Prefect UI\\nTODO\\n\\n<hr/>\\n\\n## Hyperparameter Tuning & Results\\nTODO\\n\\n<hr/>\\n\\n## Contact\\nFeel free to open an issue on this repository or e-mail adunning@ucsd.edu.\\n  \\n## Acknowledgements\\nThe code in this project is heavily adapted and modified from the following repositories:\\n1. [Long Range Graph Benchmark](https://github.com/vijaydwivedi75/lrgb)\\n2. [torch_geometric GraphGym](https://github.com/pyg-team/pytorch_geometric/tree/master/graphgym)\\n3. [Hierarchical Graph Net](https://github.com/rampasek/HGNet)\\n',\n",
       "  'The paper titled \"GraphHSCN: Heterogenized Spectral Cluster Network for Long Range Representation Learning\" proposes a message-passing-based approach called GraphHSCN for capturing long-range interactions in graph-structured data. The authors introduce a scheme to coarsen graph representation using spectral clustering and create new connections using a heterogeneous network. GraphHSCN outperforms traditional message-passing architectures in graph-level prediction tasks while reducing time complexity. The paper provides details on getting started with the code, hyperparameter tuning, and contact information.'],\n",
       " 'https://github.com/bliu8923/dsc180b-project': ['## GNN Performance on Long Range Node Classification and Graph Classification\\n\\nThis repository holds the code to test 4 different neural network architectures\\non 2 different long range datasets. \\n\\nNetwork architectures can be found under src/models, and test data (Cora) can\\nbe found under the test directory.\\n\\nTo test the model\\'s performance on a small dataset, use the docker repo b6liu/dsc180b (cpu or gpu for tag) and run:\\n```azure\\npython run.py --test True --bz (number)\\n```\\nIF ON DSMLP: Run a smaller bz if GAN errors, defaults to 32 for test. Also, we recommend using\\n16+ GB of ram as the networks tend to have large numbers of parameters (especially for GAN/SAN).\\n\\nWe would also recommend a GPU for running these tests/benchmarks, in this case, you should pull\\nthe GPU docker image that has Cuda 11.7 support. (b6liu/dsc180b:gpu)\\n\\nDifferent parameters can be run on the file as well.\\n\\n```--datatype```: Where to extract dataset (LRGB for pascal and peptides, 3D for PSB)\\n\\n```--dataset```: Dataset to run, currently only support all LRGB datasets. Defaults to PascalVOC-SP\\n\\n```--model```: Model to run, currently GNN, GatedGCN, GIN, GAT, SAN\\n\\n```--bz```: Batch size, defaults to 32\\n\\n```--epoch```: Number of epochs to run the model\\n\\n```--criterion```: Loss function, defaults to cross entropy \\n\\n```--optimizer```: Optimizer to use, defaults to adam\\n\\n```--lr```: Learning rate, defaults to 0.0005\\n\\n```--momentum```: Momentum term, defaults to 0.9\\n\\n```--weight-decay```: Weight decay term, defaults to 5e-6\\n\\n```--task```: task for network, defaults to node level\\n\\n```--metric```: Accuracy metric to perform, defaults to macro f1, support for AP\\n\\n```--gamma```: (SAN only) sparcity of attention, 0 indicates sparse attention while 1 indicates no bias\\n\\n```--hidden```: Hidden parameters, made after linearly encoding data\\n\\n```--scheduler```: Enable or disable scheduling on plateau\\n\\nShortcut methods have been added:\\n\\n```--add_edges```: ratio of edges to be created (fake, random connections between nodes)\\n\\n```--encode```: Positional encoding, we support \"lap\" for laplacian encoding or \"walk\" for RWSE\\n\\n```--encode_k```: number of features to be added by encoding\\n\\nAnd for partial (GAT supported, distance weighting):\\n\\n```--partial```: Number of distance weighted layers, should be 1\\n\\n```--space```: Spacial representation of data (2 for 2d, 3 for 3d, etc)\\n\\n```--k```: for KNN in distance weighting\\n\\nThese are the recommended commands to run all datasets on the best models:\\n\\n```python run.py --model san --dataset PascalVOC-SP --metric macrof1  --add_edges 1 --encode lap --encode_k 10```\\n\\n```python run.py --model san --dataset peptides-func --task graph --metric ap  --encode lap --encode_k 10```\\n\\n```python run.py --model san --datatype 3d --dataset psb --metric ap --encode walk --encode_k 10 --add_edges 1```\\n\\nYou can find the results in the results folder, under the model, timestamped.\\n\\n### Citations\\nThank you to Long Range Graph Benchmarks for the SAN implementation and datasets.\\n```\\n@article{dwivedi2022LRGB,\\n  title={Long Range Graph Benchmark}, \\n  author={Dwivedi, Vijay Prakash and Rampášek, Ladislav and Galkin, Mikhail and Parviz, Ali and Wolf, Guy and Luu, Anh Tuan and Beaini, Dominique},\\n  journal={arXiv:2206.08164},\\n  year={2022}\\n}\\n```\\nAnd to GraphGPS, for many loss functions, SAN, and encoders:\\n```\\n@article{rampasek2022GPS,\\n  title={{Recipe for a General, Powerful, Scalable Graph Transformer}}, \\n  author={Ladislav Ramp\\\\\\'{a}\\\\v{s}ek and Mikhail Galkin and Vijay Prakash Dwivedi and Anh Tuan Luu and Guy Wolf and Dominique Beaini},\\n  journal={Advances in Neural Information Processing Systems},\\n  volume={35},\\n  year={2022}\\n}\\n```',\n",
       "  'This repository contains code for testing four different neural network architectures on two long-range datasets. The network architectures can be found in the src/models directory, and the test data (Cora) is located in the test directory. To test the model\\'s performance on a small dataset, you can use the provided docker repository and run the command \"python run.py --test True --bz (number)\". It is recommended to have at least 16GB of RAM and a GPU for running these tests. Different parameters can be specified when running the file, such as dataset type, model, batch size, number of epochs, loss function, optimizer, learning rate, momentum term, weight decay term, task for network, accuracy metric, sparsity of attention (for SAN), hidden parameters, scheduler enable/disable option, and shortcut methods. The recommended commands to run all datasets on the best models are also provided. The results can be found in the results folder under the corresponding model and timestamped. Citations are given to Long Range Graph Benchmarks for the SAN implementation and datasets and to GraphGPS for various loss functions, SAN, and encoders.']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_readme_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(report_pdf_content_dict.items())[2][1]\n",
    "report_readme_dict\n",
    "\n",
    "# open a file, where you ant to store the data\n",
    "file = open(\"../data/report_readme_dict.pkl\", 'wb')\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump(report_readme_dict, file)\n",
    "\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_01/': ['# wiki-capstone\\nPublic repository for DSC 180B senior capstone project exploring racial bias in Oscars and Golden Globes Award Shows.\\n\\nHow to Run:\\n  This project consists of two parts: data collection and creating visuals. Both \"test-project\" and \"full-project\" targets\\n  will run both parts. \\n\\n  Targets:\\n  1. \"clean\" : Removes any test data created and removes any visualizations created. Does not remove full data because that is needed for the visualizations.\\n  2. \"test-project\": Runs the data collection process for only years 1934-1935. It also creates visuals using data from years 1934-2008.\\n  3. \"full-project\": Runs the data collection process for all years 1934- 2008. It also creates visuals using this data. (Delete \"data/\" folder before running this target)\\n  \\n  \\n',\n",
       "  'This is a public repository for a senior capstone project called \"wiki-capstone\" that focuses on exploring racial bias in Oscars and Golden Globes Award Shows. The project consists of two parts: data collection and creating visuals. There are three targets: \"clean\" removes test data and visualizations, \"test-project\" runs the data collection process for years 1934-1935 and creates visuals from years 1934-2008, and \"full-project\" runs the data collection process for all years 1934-2008 and creates visuals (requires deleting the \"data/\" folder before running).'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_02/': ['# DSC180B Wikipedia Engagement\\n\\nThis project is focused on helping people understand engagement on a wikipedia page through analysis of supply and demand on the page.\\n\\n## Usage\\n```\\nlaunch-scipy-ml.sh\\ngit clone https://github.com/Jwlin17/DSC180B.git\\ncd DSC180B\\npython run.py test-project\\n```\\n\\n```\\nlaunch-scipy-ml.sh -i jwlin/wiki-engagement\\n```\\n\\n## Files\\n\\n**./config/test-params.json** - holds input values to run test-project command\\n\\n**./config/env.json** - docker config data\\n\\n**./config/data-params.json** - directory where data should be output to\\n\\n**./notebooks/WikiEngagementEDA.ipynb** - EDA analysis for our data set\\n\\n**./src/engagement_score.py** - contains relevant functions for creating the engagement score\\n\\n**./src/wikiparser.py** - contains relevant functions for parsing wikipedia dump files\\n\\n**./website_data/article_titles.txt** - contains article titles to populate search bar\\n\\n**./website_data/content_score.png** - content score formula\\n\\n**./website_data/editor_score.png** - editor score formula\\n\\n**./website_data/engagement_scores.json** - engagement scores to graph on website\\n\\n**./index.html** - website html\\n\\n**./requirements.txt** - required packages\\n\\n**./run.py** - call run.py to run data analysis\\n\\n## Output Files\\n\\n**./data/raw/zips** - holds all the zipped files from wikidump / lightdump\\n\\n**./data/raw/extracted** - holds all the extracted zip files\\n\\n**./data/temp** - holds the created lightdump data parsed from en-wiki dump\\n\\n**./data/out** - contains the .png charts of mscore over time and csv of article title / mscore\\n\\n## Sources\\n\\nLink to my writeup report: https://docs.google.com/document/d/17lQ96NeAWvI133FgFkBsB234vdQLecg9g2wfLc_Y79o/edit?usp=sharing\\n\\nWiki-Media Dumps I Used: https://dumps.wikimedia.org/enwiki/20200101/\\n\\nWiki-Monitor Lightdump information: http://wwm.phy.bme.hu/\\n\\nPaper that I replicated: https://arxiv.org/pdf/1107.3689.pdf\\n\\nAlternative methods of studying controversy: https://www.opensym.org/ws2012/p18wikisym2012.pdf\\n',\n",
       "  'This project focuses on analyzing supply and demand on a Wikipedia page to understand engagement. It provides instructions for usage and includes various files such as code, data, and website resources. The output files include zipped and extracted data, as well as charts and CSV files. The project sources include a writeup report, Wikipedia dumps, information on Wiki-Monitor Lightdump, and a research paper on studying controversy.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_03/': ['# DSC180B Final Project: Investigating the Biaseness of Wikipedia and the Media in the Scope of COVID-19\\nWhich is the most unbiased source of COVID-19 information: A analysis of 20 News agency and Wikipedia\\n\\n## Overview\\n\\nCOVID-19 has gained increasing attention since its breakout in China in the early 2020. At this stage where full understanding of the virus is still developing, information related to COVID-19 in the media coverage may sometimes be misleading. There is diverging, even self-conflicting, news coverage from newspapers. It is becoming increasingly difficult to find out to figure out which statements being thrown at us are facts that we should be listening to or simple just rumors we should ignore. Wikipedia viewing and editing data is a record of people’s information-seeking and engagement behavior that could be used as an unbiased indicator of public opinion. It might reveal what people actually learn from the media coverage and how people react to the COVID-19. On the other hand, it could be seen as a platform where “random people” get to contribute and therefore making it a biased and untrustworthy source. We defined the trustworthiness of a source through a topic distribution matrix. Since we can divide the broad topic of COVID-19 into sub topics such as origins of the virus, symptoms, economic implications, how it spreads etc, we will measure how trustworthy a source is by how many of these subtopics it covers. Specifically, we wanted to answer the question, “How biased are certain popular news outlets in how they cover the effects and the spread of COVID-19?\"\\n\\n## Usage\\n\\nOur project was implemented with a combination of python and R. We use python for data ingesting, cleaning, and transformation. We use R for running the STM model since the package is R-exclusive. And finally, Python for final measurements and visualization.\\n\\nThe project pipeline could be run by:\\n```\\nlaunch-scipy-ml.sh\\ngit clone git@github.com:SchootHuang/DSC180B-Coronavirus-Wikipedia.git\\ncd DSC180B\\npython run.py #USER-SPECIFIED-TARGET#\\n```\\n\\nFor demoing purpuse:\\n1. Please refer to EDA_news.ipynb for the EDA process of news dataset\\n\\n2. please refer to 2020-COVID.R for topic modeling in R.\\n\\n3. Finally, refer to the visualization.ipynb notebook for visualization, and Biaseness_measurement.ipynb for the biaseness evaluation with Wasserstain distance and Forbenius Norm distance.\\n\\n\\n## Resources\\n\\nLink to our Project Website: https://schoothuang.github.io/DSC180B-Coronavirus-Wikipedia/ \\n\\nWiki-Media Dumps dataset Used: https://dumps.wikimedia.org/enwiki/20200101/\\n\\nWiki-Monitor Lightdump information: http://wwm.phy.bme.hu/\\n\\nAll the News 2.0 dataset: https://components.one/datasets/all-the-news-2-news-articles-dataset/ \\n',\n",
       "  'This project investigates the bias in COVID-19 information from news agencies and Wikipedia. The goal is to determine which source is the most unbiased. The project uses Python and R for data analysis and visualization. The pipeline for running the project is provided, along with resources such as datasets and a project website.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_04/': ['# Name That Raga: Classification and Analysis of Indian Classical Music\\n\\n## Background\\nIndian Classical music contains two primary divisions - North Indian Classical (_Hindustani_), and South Indian Classical (_Carnatic_). While both styles have their fundamental differences, the underlying structure of styles can be captured in a _raga/raag/ragam_. A raga is defined as “a pattern of notes having characteristic intervals, rhythms, and embellishments, used as a basis for improvisation.” A raga can be compared to a type of scale in Western classical music. Though Western classical music does not have a direct equivalent to this concept, a raga is somewhat comparable to certain scales, such as a natural harmonic minor or a major scale. Every song has a raga that it is set to. For example, the raga _Kirvani_ is equivalent to the natural harmonic scale in Western music.\\n\\nOur goal with this project is to quantify ragas in a way that we can then build a raga identification tool. This tool would be able to “listen” to an audio clip and be able to identify the raga that the song is set to. It would mimic what seasoned listeners of Indian classical music do already: try to identify a raga while listening to music. By quantifying the features of a ragam, we will attempt to build a raga identification classifier. \\n\\nTo manage the scope of this project with the time we have, we will build this tool to be functional for 10 prominent Hindustani/Carnatic ragas. These 10 prominent ragas are known as _thaats_. In Hindustani music, these are known as: _Asavari, Bilawal, Bhairav, Bhairavi, Kafi, Kalyan, Khamaj, Marva, Poorvi,_ and _Todi_. The corresponding ragas in Carnatic are known as _Natabhairavi, Dheerashankarabharanam, Mayamalavagowlai, Hanumatodi, Karaharapriya, Kalyani, Harikhamboji, Gamanashama, Kamarvardhani_, and _Shubhapantuvarali_. \\n\\n\\n## How To Run: Example\\n\\nOur repository includes a small amount of test data that you can run this pipeline on and obtain results. \\n\\n1. Clone this repository to have a local copy of these files.\\n2. On the command line, navigate to this repository locally \\n3. The command *python run.py test-project* runs the pipeline with the test-project target. This will load, clean, extract features, and build a model with the small amount of data included in the *testdata_raw* folder. \\n4. You should now have a csv for loaded data, cleaned data, and the model that was built. \\n\\n## How To Run: Classification of Your Own Audio Files\\n\\n1. Clone this repository. \\n2. On the command line, navigate to this repository locally. \\n3. Add your own data to *data/raw*\\n4. On the command line, the command *python run.py full-project* runs the pipeline with the full-project target. This will load, clean, extract features, and build a model with the data that you have included in the *data/raw* folder.\\n5. You should now have a csv for loaded data, cleaned data, and the model that was built.\\n',\n",
       "  'This article discusses the classification and analysis of Indian classical music, specifically focusing on the concept of ragas. Ragas are patterns of notes used as a basis for improvisation in Indian classical music. The goal of the project is to develop a tool that can identify the raga of a song by analyzing its audio clip. The project focuses on 10 prominent ragas from both Hindustani and Carnatic music traditions. The article also provides instructions on how to run the tool using test data or your own audio files.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_05/': ['<p align=\"center\">\\n    <!-- <b style=\"font-size: 45px;\">Red Means Go</b><br> -->\\n    <img width=\"614\" height=\"345\" src=\"https://raw.githubusercontent.com/codencoding/Red-Means-Go/gh-pages/images/site-logo.png\">\\n</p>\\n\\n## Abstract\\nYouTube has become a significant source of income for many content creators, and they are always looking for the best way to grow their channel. When a YouTube video gets more views, the Content Creator makes more money. The purpose of our project is to analyze the significance of the various features of a YouTube preview thumbnail that can contribute to a video’s success. We believe that there are features in YouTube thumbnails that can be extracted and used to identify what makes a video more appealing to potential viewers. \\n\\n## Introduction\\nOur research question is what YouTube thumbnail features, if any, have an effect on the amount of views that the video gets. Our hypothesis is that the more popular videos will likely have more provocative thumbnails that grab that attention of viewers. We believe that popular thumbnails will have more contrast, brightness, engaging subject matter, or just general attractiveness to potential viewers. \\nWe used YouTube’s Data API (v3) to create a data set that’s sourced from a YouTube search for “Fortnite”. We scrape the first 200 query results, and then scrape the first 100 videos from each unique channel, resulting in around 10,000 videos. Then we create a gamut of statistics from the metadata to best assess how well the video is doing according to YouTube. Because our data is from YouTube, which is an ever changing ecosystem, our results are only indicative of the data we scraped on April 16th, 2020. The thumbnails were created by YouTube content creators and/ or their thumbnail designers, while video metadata was provided by YouTube.\\nWe think our analysis is an interesting investigation because YouTube is a very prominent cultural influence, and so being able to better attract a larger audience would be of general interest to anyone pursuing a YouTube-based career. Because of the recent monetization of YouTube on a large scale, more and more people are trying to make a living off of YouTube. We hope that our results will help give any potential YouTuber insight into what features are most relevant in a thumbnail. In addition, as avid YouTube viewers, we have an intuition that thumbnails play a factor in YouTube’s recommendation algorithm. We hoped to learn more about this rather vague recommendation system and figure out what factors help in a video’s success, if any.\\n\\n## Methods\\nThe features we will use to address our question are a combination of metadata and image features. For the metadata features, we use the view count of the YouTube video as well as a z-scored view count, calculated by taking fortnite videos from the same channel within the same month, that way we can compare channel to channel through the z-scores. For our image features, we are using image brightness, saturation, hue, unique_rgb_ratio (number of unique rgb values divided by the number of pixels), and the number of faces present (using DeepFace). For our metadata features, we used the YouTube Data API (v3) to get our metadata such as views. For the image features, we used the skimage library to extract image brightness, saturation, hue, and rgb values. For face recognition, we used DeepFace from DLib. We  computed our own extracted features such as unique_rgb_ratio and z-score views. \\nThe analytical techniques we are using are as follows. For initial eda, we computed the correlation between each image feature column and the z_views column. This way we can see if there is any correlation between a specific image feature and the amount of views the video got (relative to similar videos from the same channel). For a deeper analysis, we wanted to combine images features to see if a certain combination of image features would attract more viewers. To do this, we first tried feeding all image feature columns into a random forest regressor and gradient boosted regressor. Then we plotted the predicted values vs. the real values to see any patterns in the predictions. We also multiplied sets of two/three/four features together to make higher level features, then trained a linear regression model with these features. We also looked at the predictions for this model to see any patterns in the predictions. We did not use a neural net to generate features for regression as the generated high level features are not clear enough to make a distinction between which image features have impact on the video views.\\n\\n## Results\\nThe results of our deep dive into how image features of thumbnails relate to video views are that there is no strong correlation between our thumbnail image features and video views. To show this, we’ve selected a couple visualizations to help see these results.\\n\\n<p align=\"center\">\\n    <img width=\"614\" height=\"345\" src=\"https://raw.githubusercontent.com/codencoding/Red-Means-Go/gh-pages/images/fig1.png\">\\n</p>\\n\\n<p align=\"center\">\\n    <small> Figure (1): Random Forest Regression on the z-score for video views </small>\\n</p>\\n\\n<p align=\"center\">\\n    <img width=\"614\" height=\"345\" src=\"https://raw.githubusercontent.com/codencoding/Red-Means-Go/gh-pages/images/fig2.png\">\\n</p>\\n\\n<p align=\"center\">\\n    <small> Figure (2): Gradient Boosted Regression on the z-score for video views </small>\\n</p>\\n\\nThe first two charts we chose to include are scatterplots of the predicted z_views vs the actual z_views. These predictions were obtained by training a random forest regressor and a gradient boosted regressor on the numerical image features gathered from our image processing (\\'unique_rgb_ratio\\',\\'mean_hue\\', \\'mean_saturation\\',\\'mean_brightness\\', \\'contrast\\', \\'edge_score\\', numFaces’). The purpose of this graph is to show the lack of correlation between the predictions and the real values, which is shown by the score (coefficient of determination R<sup>2</sup>) for each being close to 0. A score of 0 is achieved by always predicting the mean value of z_views, making these regressors worse than the most naive approach. This shows the lack of relationship between the image features we used and the value of z_views that the video gets.\\n\\n<p align=\"center\">\\n    <img width=\"640\" height=\"360\" src=\"https://raw.githubusercontent.com/codencoding/Red-Means-Go/gh-pages/images/fig3.png\">\\n</p>\\n\\n<p align=\"center\">\\n    <small> Figure (3): Thumbnail image statistics </small>\\n</p>\\n\\nThe third visualization we chose to include is a combination of  bar charts comparing the good performance videos (z_views > 1) and the poor performance videos (z_views < -1). We plot the values of the standard descriptive statistics to give a summary of values for the selected numerical image features. The adjacent bars allow for easy comparison for the different subsets of videos. This represents a conditional subset approach we used. By splitting up the data and looking at videos that did “well” and videos that did “poorly” , we are able to see some promising differences in thumbnail image features. Looking at the image features “unique_rgb_ratio” and “mean_hue”, we see consistent higher values for good videos than bad videos. This sheds light on the theory that more colorful thumbnails see greater success. We also see consistent differences in “contrast” and “edge_score”, this time the poorly performing videos having higher values. This alludes to “busier” thumbnails seeing less success. However, these differences are slight and not statistically significant, but we have hope that these features will direct some future analysis. \\n\\n## Discussion\\nFor all models, the predictions scored worse than predicting the mean for all values, indicating no such patterns exists, alluding to a lack of correlation between our image features and video views. The score for each model is either negative or very close to 0. According to SKLearn’s documentation, a score of 0 would be achieved by predicting the mean of the target column for all values. Because our models scored around 0 while trained on thumbnail features, we cannot say that there is any significant correlation between our thumbnail features and video views. We think this result is likely due to the relative importance of the thumbnail to the content of the video. We additionally looked at the correlation of the individual image features and video views, and found no significant correlation.\\n\\nSince there are so many aspects that go into whether someone watches the video such as title, thumbnail, video duration, and most importantly, the contents of the video, it makes sense that there would not be a strong correlation between thumbnail image features and video views. It is also worth noting that these results are specifically for Fortnite Gaming videos uploaded in March/April 2020, and our image features were relatively basic. If the scope of this project was larger, we could look at more genres and more videos per genre, along with more advanced image features. We still think that video thumbnails affect video views, but we lack the quantifiable results to say so. \\n\\nWith the growth of social media platforms such as YouTube, the job title of ‘content creator’ has become a more common and financially viable occupation. As such, our work on YouTube thumbnails will help creators put numbers to the trends they inherently sense in the ever changing YouTube thumbnail meta, allowing them to make changes to their thumbnails with less guesswork and backed with more relevant data. Besides the impact on those creating thumbnails, our work also explores how audiences of specific genres of videos react to different types of thumbnails because thumbnails are the front cover of a video and holds the potential to attract millions of users who aren’t already followers.\\n\\nOur approach takes advantage of the digital format as we were able to scrape and process mass amounts of data which wouldn’t have been easy to do manually which enabled us to acquire and work with a substantially larger dataset. Since we parameterized our work, it is very flexible and can be configured to analyze different genres of Youtube videos which could be useful for a Youtuber as it would provide them with a snapshot of the current YouTube thumbnail meta and act as a soft guide when they are making their own.\\nWe could expand our project scope in the future by looking at other video games in the YouTube Gaming category or even other YouTube categories (such as make-up videos or VLOGS). Another direction is creating a live thumbnail meta analysis. We suspect that certain features in thumbnails rise and fall in popularity similar to fashion, so having a live trend analysis of thumbnails could prove useful. For instance, if faces in the thumbnail start becoming less popular, YouTubers might want to stay away from putting their face on a thumbnail. However, they might use the thumbnail meta analysis as a way to go against the meta which would make their thumbnails stick out. \\n\\n## Getting Started / Extending this Project\\n\\n### Running the Pipeline\\n\\nTo run the main pipeline, run the command “python run.py” in the repository’s root directory. To run this command you also need a youtube API key. The key can be obtained by following this guide: (https://developers.google.com/youtube/v3/getting-started). Once obtained, you need to create a file named “api_key.json” in the root directory of the repo. It needs to have one key called “api_keys”, which is a list of API keys as strings. You can include as many keys as you’d like to gather more datasets, as the youtube API has a daily limit. Additionally, the command “python run.py test-project” can be run, which runs the analysis on a curated test dataset. This is good for a quick option that does not require fiddling with configuration parameters or creating an API key. \\n\\n### Output Files\\n\\nWhen running this pipeline locally, 5 output files will be generated in the “data/local/*selected-game*/video_data/” directory. The first file is the “scrape_MM_DD_YY.json” file, which represents a dataset of video_ids returned by the search result, and for each video_id, includes the specified number of video_ids from that channel that also match the *selected-game*. The second file is “*selected-game*_full_features_MM_DD_YY.csv”, which represents the dataset of metadata features, basic image features, and advanced image features for all videos in the dataset created on the date in the filename. The third file is “*selected-game*_full_metadata_MM_DD_YY.csv”, which contains only the metadata features for the dataset gathered on the specified date. The fourth file is “*selected-game*_requests.json”, which contains the massive amount of data returned by the youtube api for each video_id. This is constantly updated for any new videos that are scraped, which is why it does not have a date in the filename. The last file is “*selected-game*_summary_metadata_MM_DD_YY.csv”, this contains just the metadata for only the videos that appear in the search results, and not the videos within the channels that are contained in “*selected-game*_full_features_MM_DD_YY.csv”.\\n\\n\\n### Further Analysis\\n\\nOnce the pipeline has been run, further analysis of the data gathered can be found in the notebook “notebooks/eda/cwynne_combined_analysis.ipynb”. In the third cell of the notebook, specify which dataset that the analysis should be done on, usually selecting one of the generated output files mentioned above. The variable “data_path” should be set to the desired dataset. The target columns can also be changed in this cell, although it is recommended that the defaults are used. \\n\\n\\n### Configuration Parameters\\nThe run.py script will access the configuration file stored in “config/config-scraping.json”. This json file contains different parameters that allow you to change what data is scraped / analyzed. Below are the parameters in further detail.\\n\\n\\n- selected-game: this  key represents what keyword will be used to search for videos. This does not have to be a game, but can be any keyword of your choosing. \\n- thumbnail-qual: This key holds a dictionary which can be thought of as a switch, put a 1 in any quality that you want a dataset of thumbnails for. \\n- test-videos-dir: This is the filepath of the test video folder, which contains files such as the video ids and metadata of the test subset.\\n- test-thumbs-dir: This is the filepath of the directory of thumbnails for the test dataset.\\n- api-service-name: This generally should not change, if the youtube api changes this argument, then this should be changed. \\n- api-version: This is the version of the API, currently the most updated version is “v3”, but this could be changed to use later or earlier versions, however the code has not been tested on different api versions.\\n- videos-dir: This is the location of the currently scraped video data directory. Files in this folder are scraped based on the keyword and include the csv of video ids in the dataset and metadata information. This is only a local directory and is used for anything that is not the test dataset\\n- full-metadata-csv-write-path: This is the filepath to write the metadata for all videos that were scraped. This includes the search result videos and the specified number of videos per channel by the key “videos-per-channel”).\\n- summary-metadata-csv-write-path: This is the filepath to write only the initial search result videos and not the extra channel videos. \\n- requests-dic-read-path: This is the file path for a local json of request data that could exist from a previous run of this pipeline. If nothing is provided it will create a new file using “requests-dic-write-path” that can be used for reruns of the pipeline. This is to prevent re-requesting the same info for videos and making unnecessary API calls.\\n- requests-dic-write-path: This is the path where the requests data json file is written for quicker reruns or saving of new requests data. \\n- num-recent-videos: This is the number of search results that will be gathered based on the “selected-game” keyword being searched with the youtube api. \\n- videos-per-channel: This is the videos per channel that are scraped, based on the different channels that uploaded videos in the search result for our keyword.\\n- scrape-write-dir: This is the directory that all of our scraped video data gets written to. This includes the dataset of video ids, and the metadata dataset for these videos. This changes based on the “selected-game” keyword. \\n- full-features-write-name: This is the name of the file that contains metadata, basic image features, and advanced image features. It is stored in the directory specified by “videos-dir”\\n- overwrite: if set to true, it will overwrite any previously scraped data that was scraped on the same calendar date. If set to false, it will simply add a number to the end of the new requests data file. \\n\\n\\n## References\\nLouise Myers 2019, accessed April 6, 2020, \\n\\n[https://louisem.com/198803/how-to-youtube-thumbnails](https://louisem.com/198803/how-to-youtube-thumbnails)\\n\\nEmpLemon 2020, accessed May 4, 2020,\\n\\n[https://www.youtube.com/watch?v=-6-i75wDIBE](https://www.youtube.com/watch?v=-6-i75wDIBE)\\n',\n",
       "  'The purpose of this project was to analyze the significance of various features in YouTube thumbnails that contribute to a video\\'s success. The researchers used YouTube\\'s Data API to create a dataset of around 10,000 videos sourced from a search for \"Fortnite\". They analyzed both metadata and image features of the thumbnails to determine if there was a correlation with the number of views. However, their analysis found no strong correlation between thumbnail image features and video views. They also explored different visualizations and statistical analyses but did not find any significant results. The researchers suggest that there are many factors that contribute to video views, such as title, video duration, and content, making it difficult to establish a strong correlation with thumbnail image features alone. They also mention that their results are specific to Fortnite gaming videos uploaded in March/April 2020 and that more advanced image features and a larger dataset could yield different results. The researchers believe that their work can still provide insights for content creators in improving their thumbnails and understanding audience reactions. They also suggest future directions for expanding the project, such as analyzing other genres of YouTube videos or creating a live trend analysis of thumbnails.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_06/': [\"# DSC180B Project - Quantifying Style\\n# RestoreNet: Quantifying the Restoration of WWII Documents\\n\\nWelcome to the my current version of the project.\\n\\n### Prerequisites\\n\\nWhat things you need to use the program (TBD)\\n\\n```\\nBeautifulSoup - pip install beautifulsoup4\\nREGEX - pip install regex\\nRequests - pip3 install requests\\n\\nTensorFlow - pip3 install tensorflow\\nPyTorch - pip3 install torch torchvision\\n```\\n\\n### Getting Started\\n\\nFirst, go ahead and clone this repository to your local directory\\n```\\ngit clone https://github.com/Emmanuel-Diaz/DSC180B-Project.git\\n```\\n\\nthen install all necessary packages\\n\\n```\\npip install -r requirements.txt\\n```\\n\\n\\n### Usage\\n\\n\\n**Collecting World War II Images**\\n\\nEnter the following command to collect from [ww2db](http://ww2db.com/photo.php)\\n\\n```\\npython run.py scrape [NUM_IMAGES] [TIME_PERIOD]\\n```\\n\\n```NUM_IMAGES``` Number of World War II images to scrape<br>\\n```TIME_PERIOD``` Time period of images [Pre-War, Mid-War, Late-War]<br>\\n\\n\\n**Computing features**\\nEnter the following command to compute features on the data collected\\n\\n```\\npython run.py features\\n```\\n\\n**Restoring a test example**\\n\\nEnter the following command to run the network on the 'Spoils of War' image, just like in the report output.\\n```\\npython run.py test-project\\n```\\n\\n### Training\\n\\n**Performing a trained restoration**\\nIn order to do a trained restoration, you will either need to\\n1. Download Pre-trained weights found [here]() and place them into your ```data/out``` directory.\\n2. Train your own weights using your collected data. (TBD)\\n\\n**Running with custom image and mask**\\n\\nTo use your own custom image and mask for restoration, please perform the following steps.\\n1. Place your image to be restored ```img``` in the ```data/input``` directory. (```img``` can be ```.png```,```.jpeg```,```.jpg```)\\n2. Place your custom mask ```mask``` in the ```data/input``` directory. (```mask``` must be ```.png```)\\n3. Run the following command with your file names.\\n\\n```\\npython run.py -t [True/False] -i <img> -m <mask.png>\\n```\\n\\n```-t``` Use a trained model (e.g weights are located in ```data/out```)<br>\\n```-i``` Degraded image file name<br>\\n```-m``` Mask image file name<br>\\n\\n\\n## Built With\\n\\n* [Python](https://www.python.org/) - Language used\\n* [PyTorch](https://www.pytorch.org) - Net Framework\\n\\n\\n## Version\\n\\n1.2 Added selective mask inpainting\\n\\n## Author\\n\\n* [Emmanuel Diaz](https://github.com/Emmanuel-Diaz)\\n\\n* *Deep Generative Prior* by Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, Ping Luo \\n\\t* [Report](https://arxiv.org/abs/2003.13659)\\n\\t* [GitHub](https://github.com/XingangPan/deep-generative-prior)\\n\",\n",
       "  'This project, called \"RestoreNet: Quantifying the Restoration of WWII Documents,\" focuses on quantifying the restoration of World War II documents. The project provides instructions on how to collect World War II images, compute features on the collected data, and restore test examples using a network. It also includes information on training the network and running it with custom images and masks. The project is built with Python and PyTorch. The current version is 1.2 and it was authored by Emmanuel Diaz.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_08/': ['# SuperBowlCapstone\\n\\nCollects superbowl and non-superbowl commercials from ispot and adforum respectively.\\nGenerates features from their audio and video content\\nBuilds multiple models based off of the generated features.\\nCreates multiple visualizations.\\n\\n## Usage Instructions\\n\\nPotential run.py arguments:\\n* data: does the webscraping\\n* fxt: feature extraction, requires data to be run\\n* analyze: model and visual generation, requires fxt to be run\\n* test-project: using given test commercials, tests project.\\n\\n## Project Contents\\n\\n```\\nROOT FOLDER\\n├── .gitignore\\n├── README.md\\n├── config\\n│\\xa0\\xa0 ├── data-params.json\\n│\\xa0\\xa0 ├── test-params.json\\n│\\xa0\\xa0 └── env.json\\n├── chosen data folder\\n│\\xa0\\xa0 ├── chosen superbowl commercial folder\\n│\\xa0\\xa0 │\\xa0\\xa0 └── chosen audio folder\\n│\\xa0\\xa0 └── chosen non-superbowl commercial folder\\n│\\xa0\\xa0     └── chosen audio folder\\n├── notebooks\\n│\\xa0\\xa0 └── .gitkeep\\n├── run.py\\n└── src\\n    └── etl.py\\n```\\n\\n### `src`\\n\\n* `ad_scraper.py`: Library code that collects non-superbowl commercials from adforum based off of collected superbowl commercials.\\n* `ispot_scrape.py`: Library code that collects superbowl commercials from ispot.\\n* `video_processing.py`: Library code that generates visual features.\\n* `extract_audio_features.py`: Library code that generates audio features.\\n* `vis_video.py`: Library code that generates visualizations for generated features.\\n* `predictor.py`: Library code that generates predictions from Logistic Regression and Random Forest models.\\n\\n### `config`\\n\\n* `data-params.json`: Common parameters for getting data, serving as\\n  inputs to library code.\\n  \\n* `test-params.json`: parameters for running small process on small\\n  test data.\\n\\n### `notebooks`\\n\\n* Jupyter notebooks for analysis, mainly audio-orientated.\\n',\n",
       "  'This project is focused on collecting Super Bowl and non-Super Bowl commercials from iSpot and Adforum, respectively. It then generates features from the audio and video content of these commercials and builds multiple models based on these features. The project also includes visualizations for the generated features. The `src` folder contains various library code files for different tasks such as scraping commercials, processing video and audio, generating visualizations, and making predictions using logistic regression and random forest models. The `config` folder contains parameter files for data collection and testing, while the `notebooks` folder contains Jupyter notebooks for analysis, mainly focusing on audio-related tasks.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_09/': ['# IlliterateBot\\n\\n\\n### To test our code use the command:\\n\\npython run.py test\\n\\n\\n## Limitations of Code \\n\\n### CNN Feature Vectors\\nParts of the code that we have used we are unable to include into our pipeline. \\nThis includes creating the VGG16 CNN feature vectors, when transforming our book cover images\\nto feature vectors. When running our code that transforms our book cover images to feature vectors on our code, \\nour code would constantly time out, and thus we were only able to run it on our jupyter notebooks where we\\nwould need to split the data accordingly, and save the feature vectors immediately before the kernel crashes, \\nand we would have to run the code again, and save the feature vectors separately before it crashes again. Because of\\nthis, it is difficult to include it into our pipeline without changing the code each time we run. \\n\\n\\n\\n### Tesseract OCR\\nIn our report we discussed our experiences using Tesseract OCR. The code used to generate the visualizations found in our report is in our Tesseract Jupyter Notebook. \\nWe were not able to add this code to our main pipeline since PyTesseract, a Python wrapper for Tesseract, requires a local installation of the Tesseract engine and requires placement directly\\nin the local AppData folder.\\n\\n\\n',\n",
       "  'The code has limitations with regards to creating CNN feature vectors and using Tesseract OCR. The code for creating VGG16 CNN feature vectors constantly times out and can only be run on Jupyter notebooks, requiring data splitting and saving before crashes. The code for generating visualizations using Tesseract OCR is not included in the main pipeline as it requires a local installation of the Tesseract engine in the AppData folder.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_10/': ['# Fair-Policing-Capstone',\n",
       "  'The topic is Fair Policing Capstone.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_11/': ['# DSC180B Capstone Project\\n\\n## Description of Contents\\n\\nThe project consists of these portions:\\n```\\nPROJECT\\n├── .gitignore\\n├── README.md\\n├── config\\n│   ├── data-params.json\\n│   ├── env.json\\n│   ├── model.json\\n│   ├── process-params.json\\n│   ├── test-data-params.json\\n│   ├── test-model.json\\n│   └── test-process-params.json\\n├── imgs\\n├── notebooks\\n│   ├── prop_score.ipynb\\n│   └── sandbox.ipynb\\n├── references\\n│   └── .gitkeep\\n├── requirements.txt\\n├── run.py\\n├── index.html\\n├── index.md\\n└── src\\n│   ├── etl.py\\n│   ├── model_vod.py\\n│   ├── model.py\\n│   └── viz.py\\n```\\n\\n### `src`\\n\\n* `etl.py`: Script to perform Extract Transform Load.\\n* `model_vod.py`: Script to perform Veil of Darkness analysis.\\n* `model.py`: Script to perform propensity score analysis.\\n* `viz.py`: Script to visualize findings from both sets of analysis.\\n\\n### `config`\\n\\n* `data-params.json`: Common parameters for getting data, serving as inputs to library code.\\n* `env.json`: Environment paramters for GitHub repository and Docker image.\\n* `model.json`: Model parameters for for performing propensity score analysis.\\n* `process-params.json`: Common parameters for cleaning and processing data.\\n* `test-data-params.json`: Common parameters for getting test data, serving as inputs to library code.\\n* `test-model.json`: Model parameters for for performing propensity score analysis on test data.\\n* `test-process-params.json`: Common parameters for cleaning and processing test data.\\n\\n### `notebooks`\\n\\n* `prop_score.ipynb`: Imports code from `src` for the purpose of running the propensity score analysis. \\n* `sandbox.ipynb`: Imports code from `src` for the purpose of analysis. \\n\\n### Description of Targets\\n\\n* `!python run.py data`: Collects traffic stops data from Stanford Open Policing Portal and cleans it.\\n* `!python run.py model`: Performs propensity score and veil of darkness analysis on traffic stops.\\n* `!python run.py test-project`: Ingests, cleans, and runs model on a subset of the traffic stops data for the purpose of testing.',\n",
       "  'The DSC180B Capstone Project consists of several components including source code, configuration files, notebooks, and targets. The `src` directory contains scripts for ETL, Veil of Darkness analysis, propensity score analysis, and visualization. The `config` directory contains JSON files with parameters for data processing and model analysis. The `notebooks` directory includes Jupyter notebooks for running the propensity score analysis and performing general analysis. The project targets include collecting and cleaning traffic stops data, performing propensity score and veil of darkness analysis on the data, and testing the model on a subset of the data.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_12/': ['# DSC180B\\nExploring Predictive Policing in San Diego for DSC180B Capstone Project\\n\\n[Here](https://chuanyuanyeh.github.io/predpol_study/) is the link to the website.\\n\\nLink to the GIS map can be found at https://arcg.is/1CmX0r\\n\\n## Usage Instructions\\n\\nTo replicate the entire (or subsets of the) project, copy and paste `python run.py` in the command line while in the root directory followed by the arguments below:\\n* `data`: Ingests raw data from online sources.\\n* `process`: Runs the pipeline for cleaning and formatting raw datasets.\\n* `eda`: Performs exploratory data analysis and outputs visualizations.\\n* `analyze`: Performs statistical tests on differences in observed proportions between PredPol and non-PredPol instances.\\n* `test-project`: Runs the entire pipeline from start to end on a smaller, versioned test data.\\n\\nFor example, running the code below would reproduce the entire project:\\n\\n`python run.py data process eda analyze`\\n\\n## Description of Contents\\n\\nThe project consists of these portions:\\n```\\nPROJECT\\n├── config\\n    ├── data-params.json\\n    ├── process-params.json\\n    ├── eda-params.json\\n    ├── analyze-params.json\\n    ├── test-data-params.json\\n    ├── test-process-params.json\\n    ├── test-eda-params.json\\n    ├── test-analyze-params.json\\n    └── env.json\\n├── data\\n    ├── raw\\n    └── cleaned\\n├── notebooks\\n    └── .gitkeep\\n├── references\\n    ├── arrest_charges.json\\n    ├── arrest_types.json\\n    ├── crime_charges.json\\n    ├── crime_types.json\\n    ├── divisions_mapper.json\\n    ├── nhgis0005_ds172_2010_block_codebook.txt\\n    └── races.json\\n└── src\\n    ├── etl.py\\n    ├── eda.py\\n    ├── analyze.py\\n    └── geospatial.py\\n├── test_data\\n    ├── raw\\n    └── cleaned\\n├── viz\\n    ├── EDA\\n        ├── Arrests\\n        ├── Crime\\n        └── Stops\\n    └── Analysis\\n        ├── Arrests\\n        ├── Crime\\n        └── Stops\\n├── .gitignore\\n├── Dockerfile\\n├── README.md\\n├── requirements.txt\\n├── run.py\\n```\\n\\n### `config/`\\n\\n* `data-params.json`: Common parameters for getting data, serving as\\n  inputs to library code.\\n* `process-params.json`: Parameters for processing data.\\n* `eda-params.json`: Parameters for exploratory analysis on each dataset.\\n* `analyze-params.json`: Parameters for statistical testings and analyses.\\n* `env.json`: Parameters for loading virtual environment.\\n* Also contains similar configurations for test data.\\n  \\n### `data/`\\n\\n* `raw/`: Raw datasets from original source.\\n* `cleaned/`: Cleaned datasets.\\n\\n### `notebooks/`\\n\\n* Jupyter notebooks for *analyses* and *code development*\\n  - notebooks will be removed after migration to library code.\\n\\n### `references/`\\n\\n* Data Dictionaries, references to external sources.\\n\\n### `src/`\\n\\n* `etl.py`: Library code that executes tasks useful for getting data.\\n\\n### `test_data/`\\n\\n* Versioned test data.\\n\\n### `viz/`\\n\\n* Visual outputs from EDA and analyses pipelines.\\n\\n### `Dockerfile`\\n\\n* Docker image to replicate the environment the project was developed in. \\n\\n### `requirements.txt`\\n\\n* Python libraries/modules used as well as their corresponding versions.\\n\\n### `run.py`\\n\\n* Main driver for project replication\\n',\n",
       "  'This is a summary of the DSC180B Capstone Project on Exploring Predictive Policing in San Diego. The project includes various components such as data ingestion, data processing, exploratory data analysis, statistical tests, and visualization. The project also provides instructions on how to replicate the project using the command line. The project structure includes directories for configuration files, raw and cleaned data, notebooks, references, source code, test data, visualizations, Dockerfile for replication environment, requirements.txt for required libraries/modules, and run.py as the main driver for project replication.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_13/': ['# RML vs Traffic Collisions\\nHow recreational marijuana legalization(RML) affects traffic-related scenes in California using Difference in Differences model.\\n',\n",
       "  'This study examines the impact of recreational marijuana legalization (RML) on traffic-related incidents in California using a Difference in Differences model.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_14/': ['# DSC180B Visualization\\n\\nhttps://siqihuang47.github.io/dsc180b_visualization/\\n',\n",
       "  'The link provided leads to a webpage for DSC180B Visualization.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_15/': ['# DSC180BPipeline\\nOur pipleine allows you to download data from the GWAS Catalog, Clean the data, Merge the data, and then create plots such as Q-Q plots, P-value Histograms, and Manhattan Plots. These plots allow you to analyze the GWAS data and to determine which SNPs are significant. Our pipeline runs on a small data set but the images it produces is not very significant because the data is to small. When we run the data on the actual data we are using the images are much better and meaningful to analyze. \\nTo run the pipepline on the sample data in the direction run this command in the terminal:\\n  python run.py test-project\\nThis command should output a histogram plot, a q-q plot, and a Manhattan plot.\\n\\nTo run the data downloading portion run this command:\\n  python run.py test\\nThis command should output a directory of the data you are trying to download.\\n\\nTo run the cleaning and plot making portion run this command:\\n  python run.py clean\\n This command should clean the data and output the charts.\\n \\nYou can modify the code to work for you specific trait you are analyzing from the GWAS Catalog in the data collection portion and the whole pipeline should work\\n',\n",
       "  'The DSC180BPipeline allows you to download, clean, merge, and analyze GWAS data. It generates plots such as Q-Q plots, P-value histograms, and Manhattan plots to identify significant SNPs. The pipeline works better with larger datasets. To run the pipeline on sample data, use the command \"python run.py test-project\" in the terminal. To download data, use \"python run.py test\", and to clean and generate plots, use \"python run.py clean\". The code can be modified for specific traits in the GWAS Catalog.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_16/': [\"# Predicting Disease Risk Through Machine Learning\\n\\nTraditional epidemiology techniques, most notably polygenic risk scoring, have been used by researchers and well-known companies, such as Takeda, MiCom Labs, and 23andMe, to calculate the disease risk of patients and consumers. However, recent research has shown limitations in polygenic risk scoring due to its inability to model high dimensional data with complex interactions (Wai, 2019). As humans, millions of potentially disease-contributing genetic variants exist in the genome, so the inability to leverage such information limits the power of polygenic risk scoring to accurately determine the disease risk of individuals. In this project, the viability of machine learning in disease risk prediction for Coronary Artery Disease, Alzheimer’s, and Diabetes Mellitus is explored. It is shown how machine learning models, including Support Vector Machines (SVMs), Logistic Regression, K Nearest Neighbors, Decision Trees, Random Forest, and Gaussian Naive Bayes, compare in their ability to effectively predict disease risk and how they may offer alternate and possibly better methods over traditional techniques. \\n\\n## Usage Instructions\\n\\nIn order to use the different components of this project, please run `python run.py` along with a target of your choice:\\n\\n* `clean`: Cleans the data directory\\n* `data`: Downloads the data from GWAS Catalog according to data-params.json\\n* `simulate-one`: Simulates a SNP population for the training GWAS\\n* `simulate-both`: Simulates a SNP population for both the training GWAS and the test GWAS\\n* `model`: \\n   * If there is no simulated data for the test GWAS: \\n          Splits the training GWAS simulated data into a train and test subset. Model is trained on the training subset, filtered to only contain SNPs also present in the test GWAS, in order to simulate sampling. The model is then tested on the test subset and results are reported (and saved).\\n   * If there is simulated data for the test GWAS (run via `simulate-both` target):\\n          Model is trained on simulated data (filtered to contain SNPs present in both GWAS's) from the training GWAS. Model is tested on simulated data from the test GWAS and results are reported (and saved).\\n* `test-project`: Tests project using test data\\n* `run-project`: Runs entire project according to config files\\n\\n## Description of Contents\\n\\nThe project consists of these portions:\\n```\\nPROJECT\\n├── config\\n│\\xa0\\xa0 ├── data-params.json\\n│\\xa0\\xa0 ├── env.json\\n│\\xa0\\xa0 ├── model-params.json\\n│\\xa0\\xa0 └── test-params.json\\n├── notebooks\\n│\\xa0\\xa0 ├── Build_Model.ipynb\\n│\\xa0\\xa0 └── Simulate_Data.ipynb\\n├── src\\n│\\xa0\\xa0 ├── etl.py\\n│\\xa0\\xa0 ├── model.py\\n│   └── visualize_data.py\\n├── testdata\\n│\\xa0\\xa0 ├── alzheimer's\\n│\\xa0\\xa0 ├── coronary_artery\\n│\\xa0\\xa0 └── diabetes_type1_melittus\\n├── .gitignore\\n├── README.md\\n├── requirements.txt\\n└── run.py\\n```\\n\\n### `root`\\n\\n* `run.py`: Python script to run main command.\\n\\n### `src`\\n\\n* `etl.py`: Library code that executes tasks useful for getting data and transforming it into a machine-learning-ready format.\\n\\n* `model.py`: Library code that builds and tests multiple models, and reports the results.\\n\\n* `visualize_data.py`: Library code that generates a variety of visualization that are useful for analysis.\\n\\n### `config`\\n\\n* `data-params.json`: Parameters for downloading data from the GWAS Catalog and preparing for model building\\n\\n* `env.json`: Environment information\\n\\n* `model-params.json`: Contains the (sklearn) models that are tested, and the parameters to use for each model.\\n\\n* `test-params.json`: Parameters for preparing test data for model building\\n\\n### `testdata`\\n\\nThis directory contains two summary data files from the GWAS catalog for different diseases, one is used for building a training set and the other is use for a test set. Which is which can be found in the `data-params.json` configuration file (and changed).\\n\\n* `alzheimer's`: Contains two summary statistics CSV's from GWAS studies on Alzheimer's Disease.\\n\\n* `coronary_artery`: Contains two summary statistics CSV's from GWAS studies on Coronary Artery Disease.\\n\\n* `diabetes_type1_melittus`: Contains two summary statistics CSV's from GWAS studies on Diabetes Type I.\\n\\n### `notebooks`\\n\\n* `Build_Model.ipynb`: Notebook walking through the model building/validation process\\n\\n* `Simulate_Data.ipynb`: Notebook walking through the population simulation process\\n\",\n",
       "  \"This project explores the use of machine learning in predicting disease risk for Coronary Artery Disease, Alzheimer's, and Diabetes Mellitus. Traditional techniques like polygenic risk scoring have limitations in modeling complex data interactions. The project compares different machine learning models like Support Vector Machines, Logistic Regression, K Nearest Neighbors, Decision Trees, Random Forest, and Gaussian Naive Bayes to determine their effectiveness in predicting disease risk. The project also provides usage instructions and a description of its contents, including code files and data directories.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_17/': ['# Clustering-Germ-Layers\\n\\n## __General__\\n\\nData scraping from https://portal.gdc.cancer.gov/ through selenium for analysis across the different germ layers and cancers. \\n\\n<br>\\nThis is done through the use of Google\\'s chromedriver. The chromedriver.exe included is for Windows with Chrome 83. If this does not work for your computer, go to this site https://chromedriver.chromium.org/ and download the matching driver and change chrome_driver_location to that of the correct one. MacOS may need to rename the driver to include the .exe extension.\\n\\n<br><br>\\n\\n## __File Usage__\\n\\n### [Param_config.json](config/param_config.json)\\n\\n- createDict \\n    - Required keys: \\n        - chrome_driver_location\\n        - data_dict (if not specified in command line)\\n            - if done through command line, remember to change it in param_config to file you want to use\\n    - Optional:\\n        - headless\\n        - time_wait, implicit_wait, after_sort_wait\\n- queryData\\n    - Required:\\n        - chrome_driver_location\\n        - data_dict (made from createDict)\\n        - samples\\n    - Optional:\\n        - headless\\n        - time_wait, implicit_wait, after_sort_wait\\n        - sort_using, sort_direction\\n        - file_names\\n- downloadData\\n    - Required:\\n        - chrome_driver_location\\n        - keep_tar_files, tar_dir, maf_dir\\n        - manual_csv_files (if not specified by pattern on the command line)\\n        - download_inds\\n    - Optional:\\n        - headless\\n        - time_wait, implicit_wait, after_sort_wait, download_wait\\n\\n\\nExample:\\n```\\n{\\n    \"chrome_driver_location\" : \"chromedriver.exe\",\\n    \"headless\" : true,\\n    \"data_dict\": \"references/data_dictionary.csv\",\\n    \"file_names\" : [\"Q1.csv\", \"Q2.csv\"],\\n    \"sort_using\" : [\"Size\", \"Project\"],\\n    \"sort_direction\": [\"up\", \"up\"],\\n    \"samples\": [133, 272],\\n    \"time_wait\" : 2,\\n    \"implicit_wait\" : 3,\\n    \"after_sort_wait\" : 5,\\n    \"download_wait\" : 60,\\n    \"manual_csv_files\" : [\"Q1.csv\"],\\n    \"download_inds\" : [\"1-2,7-12\"],\\n    \"keep_tar\" : false,\\n    \"tar_dir\" : \"testdata/tars\",\\n    \"maf_dir\": \"testdata/mafs\"\\n}\\n```\\n<br>\\n\\n### [Query_config.json](config/query_config.json)\\n\\n- queryData\\n    - All categories are up to user to pick and choose. A list of categories, types, and descriptions are in data dictionary file made using createDict.\\n        - Number after is the number of queries to be made using it so make sure that the numbers used match. The queries are made in order.\\n            - Must match with files_names and samples from param_config.json\\n        - Ex.  \\n        > { \"files.data_format\" : [\"maf\", 2] } -> [\"files.data_format in [\"maf\"]\", \"files.data_format in [\"maf\"]\"] \\n\\n        > { \\\\\\n            \"data_format\" : [\"maf\", 1, \"vcf\", 1], \\\\\\n            \"access : [\"open\", 2] \\\\\\n          } -------> \\\\\\n          [\"files.data_format in [\"maf\"] and files.access in [\"open\"], \"files.data_format in [\"vcf\"] and files.access in [\"open\"]]\\n\\n    - Incomplete category names may be autofilled to include the starting class name by the dictionary file, if they match to existing entries. \\n        - Ex.\\n        > { \"data_format\" : [\"maf\", 2] } -> [\"files.data_format in [\"maf\"]\", \"files.data_format in [\"maf\"]\"]\\n\\n        > { \"data_for\" : [\"open\", 2] } -> Error\\n\\n\\nExample:\\n```\\n{\\n    \"data_category\" : [\"Simple Nucleotide Variation\", 2],\\n    \"data_format\" : [\"maf\", 1, \"vcf\", 1],\\n    \"cases.primary_site\" : [\"bronchus and lung\", 2],\\n    \"file_size_min\": [\"> 1000\", 2],\\n    \"file_size_max\": [\"< 10MB\", 2],\\n    \"access\" : [\"open\", 2]\\n}\\n```\\n<br><br>\\n\\n## __Command Line used by [run.py](run.py)__\\n\\n<> means optional argument\\n```\\n(base) py run.py createDict config/param_config.json <Data_dict.csv>\\n```\\n- createDict creates a dictionary file in location specified in param_config.file. Overidden by CSV path on command line. \\n    - if want to use command line CSV in queryData, then change the data_dict path in the param_config.file\\n\\n<br>\\n\\n```\\n(base) py run.py queryData config/param_config.json config/query_config.json\\n```\\n- queryData creates CSV files that match parameters in both the parameter and query config files with the URLs that link to the matching files.\\n\\n<br>\\n\\n```\\n(base) py run.py downloadData config/param_config.json <CSV matching pattern> <pattern 2> <...>\\n```\\n- downloadData uses indicies from param_config file and either the entered CSV files in param_config or the ones specified in the command line to find the files to download. The created annotations file is the same name as the created maf.gz file for ease of matching together.\\n\\n<br><br>\\n\\n## Appendix\\n\\nParameter file keys are the only ones included here as all information on the query keys can be found from the data file created by createDict. Grouped if highly related.\\n\\nUsed by more than one command\\n\\n- chrome_driver_location \\\\\\n    Path location of the chromedriver.exe file \\n\\n- data_dict \\\\\\n    Data dictionary file made from createDict and can be used in queryData. In createDict, this value can be overridden by the CSV named in the commandline and only the latter CSV file is created.\\n\\n    - It is scraped from TCGA using a vague query that holds all the different combinations for the keys. However, the possible values for those keys must be found from https://portal.gdc.cancer.gov/query and searching those terms in the query bar. This gives a list for all the possible values for that key. \\n    - This file is only to locally see all the keys, types, and descriptions, as well as to help fix some user input errors in queryData.\\n\\n- headless \\\\\\n    Set to False if want to see movement across the site. \\\\\\n    **WARNING**: Webpages load faster when True so if code breaks, then either change back or increase wait times.\\n    \\n- time_wait, implicit_wait, after_sort_wait, download_wait \\\\\\n    Timing variables to change. There are default values if they are not specified; however, slower connections must be fixed by increasing these times so page is loaded.\\n\\n    - Time_wait is the time the code waits after each step.\\n    - Implicit_wait is the time the code waits after loading a new page.\\n    - After_sort_wait is the time waiting for the data to be sorted which usually is kept higher than the others as it takes much longer to do.\\n        - However, if you are not sorting the data then setting the time much longer works.\\n    - Download_wait is the time that the program waits to download each file so larger files can be given more time to download.\\n\\nqueryData\\n\\n- file_names \\\\\\n    The list of CSV names that the queryData results will be saved to. If not specified, uses default names for queries.\\n\\n- samples \\\\\\n    The number of top results from a query search that will be saved to that query\\'s CSV file. The max samples per CSV file is 1000 results.\\n\\n- sort_using, sort_direction \\\\\\n    Before the data is scraped, it is sorted by user specifications and then the top results are saved to the CSV file in queryData. If not specified, then uses the default sorting that TCGA stores files as. \\n    - Sorts are done in order given so sorting by [\"Size\",\"Project\"] is different than [\"Project\", \"Size\"]\\n    - Sort_using values are limited to:\\n        - \"Access\", \"Data Category\", \"Data Format\", \"File Name\", \"Project\", \"Size\"\\n    - Sort_direction gives direction of sorting and is limited to:\\n        - \"Up\", \"Down\"\\n\\n\\ndownloadData\\n\\n- download_inds \\\\\\n    Indexes of the files in manual_csv_files that want to be downloaded to make downloading specific samples easier.\\n    - Ex. \\n    > array_conv([\"1-3,5\", \"1,3-4\"]) => [[1,2,3,5], [1,3,4]]\\n    \\n    This downloads the files at these positions from the specified CSV files. \\\\\\n    **WARNING** : As the first row of the CSV file is the column names, row 2 is the first data value and python starts at a 0 index so [1] points to the 3rd row in the CSV file. \\n\\n- keep_tar_files, tar_dir, maf_dir    \\n    - keep_tar_files is a boolean, that when True, says to keep the downloaded tar files along with the extracted maf.gz files. Otherwise, only the maf.gz files.\\n    - tar_dir is the directory where the chromedriver will save the downloaded tar files into.\\n    - maf_dir is the directory that the extracted maf.gz and corresponding annotation files are placed into.\\n\\n    **WARNING**:  If there is an outer directory used by tar_dir and maf_dir and it is not created before running downloadData, the code will break.\\n    - In the param_config.json example, the testdata directory must be made in advance. However, the individual directories of tar_dir and maf_dir will be made through the code if they do not already exist.\\n\\n- manual_csv_files \\\\\\n    CSV file names that hold the location of the desired files to be downloaded. If new pattern is given in command line, this is overridden by CSVs found using that pattern. However, these CSVs found using the pattern are still subject to matching the download_inds.\\n',\n",
       "  \"This document provides an overview of the Clustering-Germ-Layers project, including information on file usage and command line instructions. The project involves data scraping from a cancer database using Selenium and Google's chromedriver. The document includes details on the parameter and query configuration files used in the project, as well as examples of their contents. It also provides a summary of the command line commands used in the project, including createDict, queryData, and downloadData.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_18/': ['\\n# Understanding miRNA in pre-Type 1 Diabetes\\n\\nBy Pete Sheurpukdi & Derrick Liu\\n\\nThis repository contains the code used our the paper [Understanding miRNA in pre-Type 1 Diabetes](https://github.com/Derrick56007/miRNA_preT1Diabetes/raw/master/report.pdf)\\n\\nRequirements\\n------------\\n\\n- Docker: https://docs.docker.com/get-docker/\\n\\nInstall\\n--------------\\n\\n```\\ndocker pull derrick56007/mirna_pre_t1d:latest\\n```\\n\\nUsage\\n------------\\n\\n```\\ndocker run -ti derrick56007/mirna_pre_t1d:latest python run.py test-project\\n```\\n',\n",
       "  'This repository contains the code used in the paper \"Understanding miRNA in pre-Type 1 Diabetes\". The code can be installed using Docker and run using the provided command.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_19/': [\"# DSC180B_Genome_Project -- GWAS on Alzheimer’s Disease\\n\\n### Tony Zhang, Zhuoyuan Ren, Haoshu Qin\\n\\nVisualization Website: https://tonyzhanghm.github.io/adgwas_website/\\n\\n## Introduction\\n\\nIn this study, we conducted a Genome-Wise Analysis Study on Late-Onset Alzheimer's Disease (LOAD). For experiment details, please refer to the [paper](https://github.com/TonyZhanghm/DSC180B_Genome_01/blob/master/GWAS_on_Alzheimer_s_Disease_report.pdf). \\n\\n## Usage\\n\\n### Environment (Docker)\\nDocker images: https://hub.docker.com/repository/docker/tonyzhanghm/genetics\\n\\n### Commands\\nClone the repo: `git clone https://github.com/TonyZhanghm/DSC180B_Genome_01.git`\\n\\nTo run the whole experiment: `python run.py test-project`\\n\\nTo run the project step by step: `python run.py` with following flags:  \\n`get_data`: download the raw data  and the tools needed.   \\n`filter`: filter the dataset with [PLINK 1.9](https://www.cog-genomics.org/plink/). The specific parameter choices could be found in the paper.    \\n`pca`: run principal component analysis with [PLINK 1.9](https://www.cog-genomics.org/plink/).  \\n`plot_pca`: plot pariplots for the first 5 principal components with [seaborn](https://seaborn.pydata.org/).  \\n`plot_eigenval`: plot the scree plot.   \\n`logistic`: run the association test with logistic regression.   \\n`manhattan`: plot the manhattan plot with [bioinfokit](https://reneshbedre.github.io//blog/howtoinstall.html).  \\n`regional`: plot regional plots for the nine genes of interests.   \\n`qqplot`: plot a qqplot on the test results.  \\n`meta`: run metal analysis with [METAL](https://genome.sph.umich.edu/wiki/METAL_Documentation).  \\n\\nThe data will be stored in `data/` and the experiment results will be store in `data/output/`. \\n\\n## Development Updates\\n\\n### Checkpoint-1 (04/12/2020)\\n- Request data from source: UK Biobank and NIAGADS\\n- Understand the analysis methods: meta analysis, Manhattan plot, regional association plot. \\n- Write a survey of the data you are using, the relationship and appropriateness of the data to the problem under examination, and the context in which the data was created.\\n- Summarize relevant details of the data generating process, describing the population that the data represents, whether that population is relevant to the question at hand, while addressing possible questions of data reliability.\\n- Understand how to use population stratification on our data so that it can apply to other races besides European descent.\\n- no new code added\\n\\n### Checkpoint-2 (04/26/2020)\\n- Describe the source of the backup dataset, the population that the data represents, whether that population is relevant to the question at hand, while addressing possible questions of data reliability. (Scott)\\n- Perform preprocessing quality controls using Plink commands (Jared, Tony)\\n- Statistically assess the quality of the data (Tony)\\n- EDA (Barplot, PCA, Scatter matrix plot, Scree Plot) (All)\\n- Perform multi-covariate association analysis with logistic regression (Tony)\\n\\n\",\n",
       "  \"This study conducted a Genome-Wise Analysis Study on Late-Onset Alzheimer's Disease (LOAD). The researchers provide a visualization website and instructions on how to run the experiment. The study includes steps such as data filtering, principal component analysis, association testing, and plot generation. Development updates mention data collection, analysis methods, data reliability, and quality control.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_20/': ['# HinDroid-with-Embeddings\\n\\n![Docker Cloud Build Status](https://img.shields.io/docker/cloud/build/davidzz/hindroid-xl)\\n\\n## Overview\\n\\nMalware detection for android applications is an expanding field with the introduction of the [Hindroid](https://www.cse.ust.hk/~yqsong/papers/2017-KDD-HINDROID.pdf) model (DOI:[10.1145/3097983.3098026](https://doi.org/10.1145/3097983.3098026)). It proposes a method that transforms the semantic relationships between Android applications and their decompiled source code to a Heterogeneous Information Network (HIN) and uses similarities from various meta-paths between apps to construct a model for malware classification. To further explore the field, we aim to extend the HinDroid model to improve the accuracy in specific subsets of the AMD dataset. Our effort will be focused on finding better representations for both apps as well as APIs and discovering methods to incorporate them as additional features in a new model. In the meantime, we plan to evaluate how the proposed model captures the features that are relevant to the classification task and compare to that of the HinDroid baseline. Our contributions can be utilized in systems where the analysis of malware and interpretable features are more important than mere detection.\\n\\n## Usage\\nDocker image on Docker Hub:\\n[davidzyx/hindroid-xl](https://hub.docker.com/repository/docker/davidzz/hindroid)\\n\\nOn Datahub, create a pod using a custom image with 4 CPU and 32 GB of memory. If you use another configuration, use at least 6GB memory per CPU.\\n```bash\\nlaunch.sh -i davidzz/hindroid-xl -c 4 -m 32\\n```\\n\\nModify the config file located in `config/data-params.json`. If you want to run a test drive, use `config/test-params.json`. Put either `data` or `test` as the first argument.\\n\\nThe HinDroid baseline uses the driver file `run.py` with 3 targets: `ingest`, `process`, and `model`. Put each target space-separated as arguments in the call. To run the whole pipeline, use\\n```bash\\npython run.py data ingest process model\\n```\\n\\n`process` target will save `.npz` files in `data/processed/` for generating various embeddings.\\n\\n## System Prerequisites and Definitions\\n\\n- APK - Executable file for Android\\n- Smali code - Human readable code decompiled from Dalvik bytecode contained the APK\\n- Heterogeneous Information Network (HIN) - A graph where its nodes may not be of the same type\\n- API Extraction\\n  - Use regex to match specific patterns\\n  - API calls and method blocks\\n![api](https://i.imgur.com/nx3rKKv.png)\\n\\n## HinDroid Efficiency Improvements\\n\\nIn HinDroid, the amount of APIs that are used in the final gram matrix calculations can have dimension of several millions. Even in sparse format, these matrices take up huge computational resources for calculation. We are able to reduce the number of API used in HinDroid to 1000 APIs while retaining the same level of accuracy. Both the time and space complexity for training and inference can be improved by a few orders of magnitude. We achieve this by selecting the top important (larger absolute coefficients) APIs (words) from fitting a logistic regression on each app (document) after applying BM25 extraction on the counts of each API for each app.\\n\\n| Method            | # of Apps | # of APIs used | RAM used | train+test time |\\n|-------------------|-----------|----------------|----------|-----------------|\\n| HinDroid-original | 1670      | 2024313        | 68GB     | 3h29m41s        |\\n| HinDroid-reduced  | 1670      | 1000           | 0.3GB    | 20s             |\\n\\n## Embedding Techniques Explored\\n\\nAs we are using NLP approaches such as word2vec which cannot be directly applied to application source code and matrices from HinDroid, our data ingestion pipeline generates text corpus by traversing the graphs following an user defined metapaths and the length of a random walk. Using a metapath `ABPBA` with random walk length 5000, the text corpus may look like \\n\\n`app_3 -> api_500 -> api_321 -> api_234 -> api_578 -> app_321 -> api_123…`\\n\\nwhere each token represents an application or api node and the neighbouring tokens are\\nconnected by edges in the graph.\\n\\n### Word2Vec\\n\\nIn a graph where there are two types of nodes: application and api, Word2Vec is the first approach that we attempt to capture the relationship beyond application and apis that have a direct connection in the graph. This traditional and powerful NLP embeddings techniques helps us to learn the similarity between applications not just limited to the shared api, and also the ability to identify the clusters connection between application and api that do not always have a direct connection. Using gensim’s word2vec model, we are able to generate vector embeddings for each application and api found in the text corpus. We successfully converted decompiled Android source code into a vector of numbers for each application and this information can be easily used in a machine learning model.\\n\\nTo evaluate the effectiveness of the generated embeddings, we visualize the embedding clusters by applying dimensionality reduction into two dimensional vectors. The embedding visualization for metapath APA is shown below:\\n\\n![APA](https://i.imgur.com/TnPyamV.png)\\n\\nAs word2vec does not generate embeddings for unseen words, test applications in our case, we trained a decision tree regressor using the true embeddings for training application as the labels, and the average of all the embeddings of each application’s associated api as the training data. Using this regressor we are able to generate embeddings for test applications using its associated api appear in the training corpus.\\n\\n### Node2Vec\\n\\nIn node2vec, the entire Heterogeneous Information Network is regarded as an large homogeneous graph and the only theoretical difference to the word2vec approach is the random walk procedure. The graph traversal method is based on a graph where all different types of edges are merged together to be one. This change is adapting to the inability of node2vec to traverse according to a metapath but instead a truly random walk with no specific rules restricting where the next node would be. We choose a return parameter of 2 and a in-out parameter of 1 empirically to perform walks beginning on each app node for 100 times. This results in a corpus similar to the word2vec approach, so we could use the same methodology to match and predict different distributions of app and API embeddings.\\n\\nAs node2vec also does not generate embeddings for test appication, we use the similar approach to generate the embeddings for test application using associated API embeddings.\\n\\n![node2vec](https://i.imgur.com/auK5rqj.png)\\n\\n### Metapath2Vec\\n\\n![metapath2vec equation](https://i.imgur.com/AINz4lr.png) (1)\\n\\nMetapath2Vec is used as a technique of sampling our next node. We sample our next node using equation (1). Let\\'s use an example to illustrate the process.\\n\\nImagine that we have these matrices set up, and our defined metapath is **ABA**. Our metapath-chosen sentence will look like \"app_A API_Y API_Z app_B\". An sample matrix looks something like the following:  \\n![Matrices](https://i.imgur.com/XIYFrc3.png)\\n\\nSimplified steps:\\n\\n1. Pick an app. This will replace app_A.\\n2. Go to the matrix corresponding to the metapath. For example, the first path in **ABA** is A, so we will look at the A matrix.\\n3. Go to the row corresponding to the app or API that was chosen.\\n4. Pick an API. Within a row, the APIs that have a value of 1 is picked using a uniform probability.\\n5. Repeat 2, 3, and 4 until you are ready to pick an app (app_B). With the API that was chosen (API_Z), look at the column and pick an app that has value 1 with uniform probaility.\\n\\n![ABA](https://i.imgur.com/8N8IeYi.png)\\n![ABPBPBBPA](https://i.imgur.com/Wi5C3KW.png)\\n![ABABBABBBABBBBABBBBBA](https://i.imgur.com/etgIVjM.png)\\n\\n\\n## Results\\n\\nLet\\'s take a look at the different accuracies for the original HinDroid approach and the HinDroid approach with additional embedding techniques.\\n\\n**HinDroid:**\\n\\n| Metapath | train_acc | test_acc | F1     | TP    | FP   | TN    | FN   |\\n|----------|-----------|----------|--------|-------|------|-------|------|\\n| AA       | 1.0000    | 0.9561   | 0.9562 | 158   | 10   | 147   | 4    |\\n| APA      | 1.0000    | 0.9373   | 0.9412 | 155   | 14   | 145   | 6    |\\n| ABA      | 0.9149    | 0.8558   | 0.8671 | 147   | 27   | 130   | 19   |\\n| APBPA    | 0.9040    | 0.8339   | 0.8408 | 140   | 32   | 126   | 22   |\\n\\n**Reduced:**\\n\\n| Metapath | train_acc | test_acc | F1     | TP    | FP   | TN    | FN   |\\n|----------|-----------|----------|--------|-------|------|-------|------|\\n| AA       | 1.0000    | 0.9561   | 0.9562 | 158   | 10   | 147   | 4    |\\n| APA      | 1.0000    | 0.9373   | 0.9412 | 155   | 14   | 145   | 6    |\\n| ABA      | 0.9149    | 0.8558   | 0.8671 | 147   | 27   | 130   | 19   |\\n| APBPA    | 0.9040    | 0.8339   | 0.8408 | 140   | 32   | 126   | 22   |\\n\\n**Word2Vec**\\n\\n| Metapath  | Accuracy | F1     | TN  | FP | FN | TP  |\\n|-----------|----------|--------|-----|----|----|-----|\\n| ABA       | 95.06%   | 95.02% | 639 | 32 | 34 | 630 |\\n| ABPBA     | 94.61%   | 94.59% | 634 | 37 | 35 | 629 |\\n| APA       | 95.73%   | 95.68% | 647 | 24 | 33 | 631 |\\n| APBPA     | 94.61%   | 94.55% | 638 | 33 | 39 | 625 |\\n\\n**Node2Vec**\\n\\n| Metapath  | Accuracy | F1     | TN  | FP | FN | TP  |\\n|-----------|----------|--------|-----|----|----|-----|\\n| N/A       | 97.75%   | 97.77% | 648 | 18 | 12 | 657 |\\n\\n**Metapath2Vec:**\\n\\n| Metapath              | train_acc | test_acc | F1     | TP    | FP   | TN    | FN   |\\n|-----------------------|-----------|----------|--------|-------|------|-------|------|\\n| AA                    | 0.9736    | 0.9476   | 0.9466 | 621   | 27   | 644   | 43   |\\n| APA                   | 0.9955    | 0.9296   | 0.9277 | 603   | 33   | 638   | 61   |\\n| ABA                   | 0.9864    | 0.9633   | 0.9626 | 630   | 15   | 656   | 34   |\\n| APBPA                 | 0.9900    | 0.9438   | 0.9419 | 608   | 19   | 652   | 56   |\\n| ABPBA                 | 0.9982    | 0.9524   | 0.9524 | 614   | 10   | 661   | 50   |\\n| ABPBPBBPA             | 0.9973    | 0.9476   | 0.9455 | 607   | 13   | 658   | 57   |\\n| ABABBABBBABBBBABBBBBA | 0.9545    | 0.9026   | 0.9027 | 603   | 69   | 602   | 61   |\\n\\n## Conclusion\\n\\nFrom our initial testing, all of our proposed graph embedding techniques are able to achieve similar accuracy and metrics scores. Although it does not seem that the different graph embeddings obtained a higher accuracy score, we believe that using these other graph embedding techniques not only matches with the results of HinDroid, but are also more robust in the sense that hackers are not able to easily rearrange APIs to avoid detection.  \\n\\nThere is definitely further research to do. One being where dummy nodes are added. This will provide us with more evidence of the robustness of the different graph techniques used. The clusters that are present in the visualizations are suggestive of further analysis. Also, we should test out several more different meta-paths. We can test which meta-paths work the best and investigate why it works. We could also do concatenation of embeddings from different meta-paths to form a longer representation and test its effectiveness. \\n\\n## References\\n\\n[1] Passi, Harpreet. Introduction to Malware: Definition, Attacks, Types and Analysis. GreyCampus  \\n[2] Hou, Shifu and Ye, Yanfang and Song, Yangqiu and Abdulhayoglu, Melih. 2017. HinDroid: An Intelligent Android Malware Detection System Based on Structured Heterogeneous Information Network.  \\n[3] Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey. 2013. Efficient Estimation of Word Representations in Vector Space.  \\n[4] Grover, Aditya and Leskovec, Jure. 2016. node2vec: Scalable Feature Learning for Networks.  \\n[5] Dong, Yuxiao and Chawla, Nitesh and Swami, Ananthram. 2017. metapath2vec: Scalable Representation Learning for Heterogeneous Networks  \\n',\n",
       "  'The HinDroid model is a method for malware detection in Android applications. It uses a Heterogeneous Information Network (HIN) to represent the relationships between apps and their decompiled source code. The model constructs a graph using various meta-paths between apps and uses similarities to classify malware. The goal of this project is to extend the HinDroid model to improve accuracy in specific subsets of the AMD dataset by finding better representations for apps and APIs. The project also explores different embedding techniques such as Word2Vec, Node2Vec, and Metapath2Vec. The results show that these techniques achieve similar accuracy scores compared to the original HinDroid approach. Further research is needed to investigate different meta-paths and evaluate the effectiveness of concatenating embeddings from multiple meta-paths.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_21/': ['# Hinreddit\\n\\n[project website](https://syeehyn.github.io/hinreddit/)\\n\\n![Docker Cloud Build Status](https://img.shields.io/docker/cloud/build/syeehyn/hinreddit)\\n\\nAs social platforms become accessible nowadays, more and more people get used to posting opinions on various topics online. The existence of nagetive online behaviors such as hateful comments is also unavoidable. These platforms thus become prolific sources for hate detection, which motivates many people to apply various techniques in order to detect hateful users or hateful speeches.\\n\\nThis project investigates contents from Reddit. its goal is to classify hateful posts from the normal ones. This not only enables platforms to improve user experiences, but also helps to maintain a positive online environment.\\n\\n- [Hinreddit](#hinreddit)\\n  - [Getting Started](#getting-started)\\n    - [Prerequisite](#prerequisite)\\n      - [Use Dockerfile](#use-dockerfile)\\n    - [Usage](#usage)\\n      - [etl](#etl)\\n      - [graph](#graph)\\n      - [embedding](#embedding)\\n      - [pipeline](#pipeline)\\n  - [For Developers](#for-developers)\\n  - [Contribution](#contribution)\\n    - [Authors](#authors)\\n    - [Advisors](#advisors)\\n  - [References](#references)\\n  - [License](#license)\\n\\n----\\n\\n## Getting Started\\n\\n### Prerequisite\\n\\nThe project is mainly built upon following packages:\\n\\n- Data Preprocessing & Feature Extraction\\n\\n  - [Pandas](https://pandas.pydata.org/)\\n  \\n- Labeling & ML Deployment\\n\\n  - [Pytorch = 1.5](https://pytorch.org/)\\n  \\n  - [PyTorch Geometric = 1.5](https://github.com/rusty1s/pytorch_geometric)\\n\\n#### Use Dockerfile\\n\\n  You can build a docker image out of the provided [DockerFile](Dockerfile)\\n\\n  ```bash\\n  docker build . # This will build using the same env as in a)\\n  ```\\n\\n  Run a container, replacing the ID with the output of the previous command\\n\\n  ```bash\\n  docker run -it -p 8888:8888 -p 8787:8787 <container_id_or_tag>\\n  ```\\n\\n  The above command will give an URL (Like http://(container_id or 127.0.0.1):8888/?token=<sometoken>) which can be used to access the notebook from browser. You may need to replace the given hostname with \"localhost\" or \"127.0.0.1\".\\n\\n### Usage\\n\\n#### etl\\n\\n - Modify the config file located in `config/data-params.json`. For testing, use `config/test-params.json`, you may define an output root `[data-path]` under `config/data-params.json`.\\n\\n - **The HinReddit\\'s etl process uses the python script file `run.py` with target `data[-test]`.**\\n\\n - You may change the `nlp_model.zip` file with custom nlp labeling rules.\\n\\n - The etl process result will be under \"\\\\<data-path>/raw\" and \"\\\\<data-path>/interim/label\" directories.\\n\\n#### graph\\n\\n- **The HinReddit\\'s graph process uses the python script file `run.py` with target `graph[-test]`.**\\n\\n- The graph process result will be under \"\\\\<data-path>/interim/graph/*.mat\"\\n\\n#### embedding\\n\\n- Modify the config files located in `config/embedding/graph_<1/2>/[test-]<informax/metapath2vec/node2vec>.json` for corresponding parameters of the embedding models.\\n\\n- **The HinReddit\\'s embedding process uses the python script file `run.py` with following targets:**\\n\\n  - `node2vec[-test]`: for node2vec embedding.\\n  - `metapath2vec[-test]`: for metapath2vec embedding.\\n  - `infomax[-test]`: for deep graph infomax (DGI) embedding.\\n\\n#### pipeline\\n\\n- run `$ python run.py data[-test] graph[-test] node2vec[-test] metapath2vec[-test] infomax[-test]`\\n\\n- You can find a detailed explaination of configuration arguments [here](./writeups/PARAMS.md)\\n\\n----\\n\\n## For Developers\\n\\n[Development Guide](./writeups/DEVGUIDE.md) is provided and under `./writeups/DEVGUIDE.md`\\n\\n----\\n\\n## Contribution\\n\\n### Authors\\n\\n- [Chengyu Chen](https://github.com/anniechen0127)\\n- [Yu-chun Chen](https://github.com/yuc330)\\n- [Yanyu Tao](https://github.com/lilytaoyy)\\n- [Shuibenyang Yuan](https://github.com/shy166)\\n\\n### Advisors\\n\\n- [Aaron Fraenkel](https://afraenkel.github.io/)\\n- [Shivam Lakhotia](https://github.com/shivamlakhotia)\\n\\n<sup>Authors contributed equally to this project</sup>\\n\\n----\\n\\n## References\\n\\n``` \\n@paper{Hou/Ye/2017,\\n  title={HinDroid: {An Intelligent Android Malware Detection System Based on Structured Heterogeneous Information Network}},\\n  author={Hou, Ye, Song, Abdulhayoglu}\\n  year={2017}\\n}\\n@inproceedings{Fey/Lenssen/2019,\\n  title={Fast Graph Representation Learning with {PyTorch Geometric}},\\n  author={Fey, Matthias and Lenssen, Jan E.},\\n  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},\\n  year={2019},\\n}\\n@article{turc2019,\\n  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},\\n  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\\n  journal={arXiv preprint arXiv:1908.08962v2 },\\n  year={2019}\\n}\\n@course{Koutra/2018,\\n  title={Mining Large-scale Graph Data},\\n  author={Danai Koutra},\\n  link={http://web.eecs.umich.edu/~dkoutra/courses/W18_598/},\\n  year={2018}\\n}\\n@collection{src-d/2019,\\n  title={Awesome Machine Learning On Source Code},\\n  author={src-d},\\n  link={https://github.com/src-d/awesome-machine-learning-on-source-code},\\n  year={2019}\\n}\\n```\\n\\n----\\n\\n## License\\n\\n[Apache License 2.0](LICENSE)\\n',\n",
       "  'The Hinreddit project aims to classify hateful posts from normal ones on the social platform Reddit. By detecting hateful users or speeches, this project helps improve user experiences and maintain a positive online environment. The project uses various packages such as Pandas, PyTorch, and PyTorch Geometric for data preprocessing, feature extraction, labeling, and machine learning deployment. Developers can refer to the provided development guide for more information. The project is contributed by Chengyu Chen, Yu-chun Chen, Yanyu Tao, and Shuibenyang Yuan, with advisors Aaron Fraenkel and Shivam Lakhotia. The project is licensed under Apache License 2.0.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_22/': ['# recordLinkage\\nDSC 180B Project: Probabilistic Record Linkage\\n\\n---\\n![Machine Learning Pipeline](./reports/images/pipeline.png)\\n\\nThe machine learning pipeline can be broken down in 3 steps:\\n1. graph construction\\n2. node2vec embedding\\n3. classifier.\\n\\n---\\nTo run the program, refer to the config folder for the parameters that can be changed for this program, and use the *python run.py* command to train, test, and evualate the model.Refer to the run.py file for the appropriate command line inputs.\\n\\nFor visualizations and testing can be found in the notebooks directory.\\n\\nThe paper associated with this paper can be found [here](./reports/final_report.pdf)\\n\\n---\\n## Example Code\\n\\nIn order to generate the artificial dataset, you can use the command below:\\n```\\npython3 gen-data\\n```\\n\\nIn order to perform the graph construction, use the command:\\n\\n```\\npython3 create-small-graphs\\n```\\n\\nLastly, in order to test any changes to the pipeline on example test, use the command:\\n```\\npython3 test-project\\n```\\n\\n---\\n## Configuration\\n\\n1. refer to ./config/datasetGenConfig.json to change the hyperparameters associated with the artificial dataset generation.\\n2. refer to ./config/test_config.json and ./config/train_config.json to change the hyperparamters associated with generating the graphs and training the classifier.\\n\\n(add information about the configurations)',\n",
       "  'This document provides an overview of the DSC 180B project on probabilistic record linkage. It describes the machine learning pipeline, which consists of three steps: graph construction, node2vec embedding, and classification. The program can be run by changing the parameters in the config folder and using the \"python run.py\" command. The paper associated with this project is also provided. Additionally, example code is given for generating an artificial dataset, performing graph construction, and testing changes to the pipeline. The configuration files are located in the ./config directory.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_23/': [\"# Malware-detection\\n\\n## Abstract\\n\\nRecent work introduced a model using a Heterogeneous Information Network (HIN) representation of Android applications utilizing a meta-path approach to link applications through the API calls contained within them. It was found with multi-kernel learning, the model was able to identify malicious applications with high accuracy. This recent work was the first approach of this kind to be published; therefore, a replication process would allow for deeper understanding of this approach. In this paper, we introduce a framework for improving upon the model through scalability and testable measures with the purpose of maintaining or increasing accuracy while creating an easily executable pipeline. In particular, we employ dimensionality reduction and stochastic techniques to achieve reasonably replicable results. Additionally, we attempt to understand, through model explainability practices, the inner mechanisms of the complex model to better understand possible inaccuracies which may arise in creating a scaled version of a HIN approach.\\n\\n\\n## Usage Instructions\\n\\nIn a terminal or command window, navigate to the top-level project directory `malware-detection/` and run\\n\\n`python3 run.py test`\\n\\nor\\n\\n`python3 run.py`\\n\\n\\n* Instructions\\n\\n`python3 run.py test` - Runs code on test set of 3 benign and 3 malware apps\\n\\n\\n`python3 run.py` - Runs project on 1000 benign and 1000 malware apps including data collection pipeline\\n\\n## Output \\n\\n\\n`test-data/processes/app_to_api.json`: structure to create A matrix\\n\\n`test-data/processes/code_block.json`: structure to create B matrix\\n\\n`test-data/processes/library_dic.json`: structure to create P matrix\\n\\n`test-data/processes/test_app_api.json`: structure to create A matrix for test set\\n\\n`test-data/processes/unique_api.text`: txt file with all unique API's\\n\\n`test-data/matrix/a_matrix.npz`: Sparse format of A matrix \\n\\n`test-data/matrix/b_matrix.npz`: Sparse format of B matrix \\n\\n`test-data/matrix/p_matrix.npz`: Sparse format of P matrix \\n\\n`results/scores.csv`: Performace metrics of model on diffrent kernels\\n\\n`charts`: Conatins all ouput charts \\n\\n\\n## Description of Contents\\n\\nThe project consists of these portions:\\n```\\nPROJECT\\n\\n├── README.md\\n├── config\\n│\\xa0\\xa0 ├── data-params.json\\n|\\xa0\\xa0 └── test-params.json\\n|   └── env.json\\n├── test-data\\n│\\xa0\\xa0 ├── smalli\\n│\\xa0\\xa0 └── processes\\n│\\xa0\\xa0     ├── app_to_api.json\\n|       ├── code_block.json\\n|       └── libray_dic.json \\n├── notebooks\\n│\\xa0\\xa0 ├── eda.ipynb\\n|   ├── coefficient_explaining.ipynb\\n|   ├── feature_extraction.ipynb\\n|   ├── model.ipynb\\n|   └── malware_type.ipynb\\n├── reports\\n|   └── malware detection.pdf    \\n├── src\\n|    ├── __init__.py\\n|    ├── build_features.py\\n|    ├── make_dataset.py\\n|    ├── elt.py\\n|    ├─ multi-kernel.py\\n|    ├─ model.py\\n|    └──coefficient_analysis.py\\n├── requirements.txt\\n├── run.py\\n\\n``` \\n\\n### `src`\\n\\n* `etl.py`: Library code that executes tasks useful for getting data.\\n* `make_dataset.PY`: Library code that excutes task useful to cleaning and building dataset\\n* `build_features.py`: Library code that excutes task to extract features from dataset\\n* `model.py`: Library code  that excutes task to create and test model.\\n* `Multi-kernel.py`: Library code pertaining the creation of multi kernel\\n* `coefficient_analysis.py` : Library code that conatins analysis of model outputs \\n\\n### `config`\\n* `env.json`: conatains dokcer conatiner id and outpaths of all files created after running run.py\\n\\n* `params.json`: Common parameters for getting data, serving as\\n  inputs to library code.\\n  \\n* `test-set.json`: parameters for running small process on small\\n  test data.\\n\\n\\n### `notebooks`\\n\\n* Jupyter notebooks for *analyses*\\n  - Contains data cleaning, eda, building features and building the model.\\n\\n\\n\\n\\n\",\n",
       "  'This paper introduces a framework for improving the model used in malware detection. The model utilizes a Heterogeneous Information Network (HIN) representation of Android applications and a meta-path approach to link applications through API calls. The framework aims to increase accuracy while maintaining scalability and testability. It employs dimensionality reduction and stochastic techniques to achieve replicable results. The paper also explores model explainability practices to better understand possible inaccuracies in a scaled version of the HIN approach.\\n\\nThe project directory structure includes various files and folders for data processing, feature extraction, model building, and analysis. The `src` folder contains library code for tasks such as data extraction, dataset cleaning, feature extraction, model creation, multi-kernel creation, and coefficient analysis. The `config` folder includes JSON files with parameters for data processing and testing. The `notebooks` folder contains Jupyter notebooks for data cleaning, exploratory data analysis (EDA), feature extraction, and model building.\\n\\nTo run the project, navigate to the top-level project directory in a terminal or command window and use the command `python3 run.py test` to run the code on a test set of benign and malware apps. Alternatively, use `python3 run.py` to run the project on a larger dataset including data collection pipeline.\\n\\nThe output of the project includes various files such as JSON structures for creating matrices, a text file with unique APIs, sparse format matrices, performance metrics of the model on different kernels stored in a CSV file, and output charts stored in the \"charts\" folder.\\n\\nOverall, this project aims to enhance malware detection using a HIN approach by improving scalability, replicability, and accuracy while providing insights into possible inaccuracies through model explainability practices.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_24/': ['# The Food Chain - A Personalized Restaurant Recommender System\\n\\nLeveraging Yelp data to develop a user-customizable, similarity-based restaurant recommendation system.\\n\\n- About this project: http://team05.pythonanywhere.com/about  \\n\\n- Try it for yourself:  http://team05.pythonanywhere.com/  \\n\\n## Getting Started \\n\\n1.  `pip install -r requirements.txt`\\n2.  `python -m nltk.downloader all`\\n3.  `python -m textblob.download_corpora lite`\\n4.  `python main.py`\\n\\n\\n## Test Project Locally\\n\\n1.  Install dependencies (see **Getting Started** through step 3).\\n2.  `python run.py test-project`\\n\\n---\\nQuestions? Please reach out to lpdoyle@ucsd.edu or dmarcusthierry@gmail.com :) \\n',\n",
       "  'This project is about developing a personalized restaurant recommendation system using Yelp data. You can learn more about the project and try it out for yourself by visiting the provided links. To get started, you need to install the required dependencies and run the main.py file. There is also an option to test the project locally. If you have any questions, you can reach out to lpdoyle@ucsd.edu or dmarcusthierry@gmail.com.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_25/': [\"# CodeHonestly\\r\\n### Utilize AST graphs to detect code plagiarism\\r\\n\\r\\n![](logo.png)\\r\\n\\r\\n`python run.py test-project` to run the project\\r\\n\\r\\nNote 1: Please make sure there are at least two Python files in data folder, not any subfolder. It is functional out of box.\\r\\n\\r\\nNote 2: We didn't wrap all the codes into library codes as possible because we might use the code in actual product that we don't want to release a version that 's easy for future development by others.\\r\\n\\r\\nWebsite: https://www.codehonestly.com/\\r\\n\\r\\nThis project was founded in DSC 180B, a capstone project for data science undergraduate students at UCSD.\\r\\n\",\n",
       "  'CodeHonestly is a project that uses AST graphs to detect code plagiarism. It can be run by executing the command `python run.py test-project`. The project requires at least two Python files in the data folder. It was founded in DSC 180B, a capstone project for data science undergraduate students at UCSD.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_26/': [\"Presentation: https://drive.google.com/open?id=1ick7HAF_w0bjRSVxWURykakYpwKsDwDY\\n\\n\\nWebsite: https://nancyvuong.github.io/dsc180b_website/\\n\\nTo run pipeline on test data run python3 run.py test-project.\\n \\n \\nThe project consists of these portions:\\n```\\nPROJECT\\n├── .gitignore\\n├── README.md\\n├── config\\n│   ├── data-params.json\\n│   ├── test-params.json\\n│   └── env.json\\n├── Notebooks\\n│   ├── API_Relationships (Model Explainability).ipynb\\n│   ├── Co_Occurence_EDA.ipynb\\n│   ├── Create_matrix.ipynb\\n│   ├── EDA.ipynb\\n│   ├── Lime.ipynb\\n│   ├── SVM_Explained.ipynb\\n│   ├── Scrape-APK.ipynb\\n│   └── Statistical test.ipynb\\n├── references\\n│   └── Hindroid.pdf\\n├── run.py\\n├── test-data\\n│   ├── APK/dating\\n│   ├── apk_data\\n│   └── xml_files/dating\\n└── src\\n    ├── Correlation_Coef.py\\n    ├── Filter_Coef.py\\n    ├── Malware_Types.py\\n    ├── Percent_API.py\\n    ├── baseline.py\\n    ├── create_matricies.py\\n    ├── model.py\\n    └── scrape_apk.py\\n    \\n```\\n\\n### `src`\\n\\n* `Correlation_Coef.py`: Calculated correlation coef for each API in each kernel\\n\\n* `Filter_Coef.py`: Filters API by corelation coef values\\n\\n* `Malware_Types.py`: Obtains type and category of malware from amd.arguslab.org \\n\\n* `Percent_API.py`: Ranking Algorithim for APIs\\n\\n* `baseline.py`: Baseline Hindorid model\\n\\n* `create_matricies.py`: Create A,B and P matricies/kernels for the Hindroid Model\\n\\n* `model.py`: Final model used for multi-class malware detection\\n\\n* `scrape_apk.py`: Obtains the Benign apps from https://apkpure.com/sitemap.xml\\n\\n\\n\\n### 'Reference'\\n\\n  * http://yes-lab.org/files/HinDroid_KDD2017_Slides_Ye.pdf\\n  \\n\",\n",
       "  'The project consists of a presentation and a website. It includes various notebooks, source code files, and references. The source code files perform tasks such as calculating correlation coefficients, filtering APIs, obtaining malware types, ranking algorithms for APIs, creating matrices for the Hindroid model, implementing the final model for multi-class malware detection, and scraping APKs. The reference provides additional information on the project.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2019-2020/tree/master/project_27/': ['# Graph-Based-Malware-Prediction\\nThis project analyzes multiple graph embeddings for malware predictions based on their smali codes. Project results and interactive demonstrations can be found on our [project website](https://maishuliang.github.io/malware-detection-viz/)\\n\\n## Overview\\n\\nNowadays, Android is dominating the smartphone market as an open source and customizable operating system. Many hackers targeted Android applications by disseminating malwares, posing serious threats to users. Historically, mobile security products such as Norton and Lookout, are heavily relied upon as major defense against such threats. Recently, many machine learning based methods have been invented for malware detection. A successful one of them is creating features from a Heterogeneous Information Network ([HinDroid](https://www.cse.ust.hk/~yqsong/papers/2017-KDD-HINDROID.pdf)). However, it is confined in such a way that it ignores more comprehensive information which can be extracted from graph representation. In this project, we will explore different meta-paths and incorporate various graph embedding methods in the task of malware prediction. We propose to build upon our previous work in HinDroid replication, more specifically we will attempt to use deep learning graph embedding techniques including Node2vec and Metapath2vec.\\n\\n## Usage\\nDocker image on Docker Hub:\\n[b4zhang/malware_detection_with_graph_embedding](https://hub.docker.com/r/b4zhang/malware_detection_with_graph_embedding)\\n\\nOn Datahub, create a pod using the custom image.\\n\\nTo test-run the project, use or modify the default `config/test-params.json` and run\\n```bash\\npython run.py test\\n```\\n\\nTo run the project, modify the config file `config/data-params.json` and run\\n```bash\\npython run.py data process\\n```\\nor run `data` and `process` separately.\\n\\n`data` target will save decompiled apk files under the folder as specified by the argument `apk_out_path` in `config/data-params.json`.\\n\\n`process` target will save the Word2Vec model and Neural Network Model under the folder as specified by the argument `model_out_path` in `config/data-params.json`.\\n',\n",
       "  'This project focuses on analyzing multiple graph embeddings for malware predictions based on their smali codes. The goal is to explore different meta-paths and incorporate various graph embedding methods in the task of malware prediction. The project builds upon previous work in HinDroid replication and utilizes deep learning graph embedding techniques such as Node2vec and Metapath2vec. The project provides a Docker image for easy usage and includes instructions on how to test-run and run the project.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_53': ['# FaceMaskDetection\\nIn an attempt to bring more transparency in artificial intelligence in a high stakes situation such as the Coronavirus pandemic, our aim was to create a model that would be able to determine if an individual was wearing a mask correctly, incorrectly, or not at all. Utilizing a subsection of the dataset\\xa0MaskedFace-Net, we were able to train a model with the Inception Resnet V1 model. Moreover, as this dataset further breaks down incorrect mask usage into why, such as uncovered chin, mouth, or nose area, we aimed to apply GradCAM in order to build transparency and trust, and ultimately ensure that our model was coming to the conclusion for the right reasons.\\n\\n\\n## Project Stucture\\n\\n#### Config files\\n\\nThese files contain important links and file paths to images and our dataset that are used by our run files \\n\\n#### Presentation\\n\\nThese contain brief snippets of our notebook to give you an idea of how our model was built and the underlying code and output for different features of our project. Our notebook GradCam EDA looks at implementing an algorithm that can identify what our neural network would look at to identify whether a mask would be work correctly and this is displayed in the gradcam presentation\\n\\n#### run.py\\n\\nThis is our main file run file that calls in methods given in gradcam.py and etl.py. To run on a given image type the command  ```python run.py test``` and in order to edit the file_path for another image, run the command  ```python run.py run_grad``` and give the file path in the data_input.py file to test it out. \\n\\n### src\\nThe src folder contains information on the functions used to train the model and our config folder contains parameter information that simplifies the working of run.py. \\n\\n## Usage of GradCam\\n\\nIn order to run out code on a given input path, type the command ```python run.py test``` from the main directory. \\n\\nThis calls the etl.py function which presents a list of stats for our images in our dataset as well as invokes the gradcam class defined in **gradcam.py** \\n\\nBased on a predefinied path, Gradcam will be applied to the image rendering a heatmap of what the netowrk looked at to make our prediction. As seen below here are examples of what GradCam looked at to make a prediction regarding the correct wearing of FaceMaks. This increases one trust in the Neural Netowrk as it bceomes more Explainable to the Human Eye. \\n\\n![image](https://drive.google.com/uc?export=view&id=10EIantVsmZLYXwfyJI6VtpXCQ1fwNJhS)\\n![image](https://drive.google.com/uc?export=view&id=1kqw8QJYPR7vOBCco7p4XcVZ7xQKexdIR)\\n\\nLooking at these images, neural networks make a lot more sense intuitively as we know why our network made that prediction.\\n\\n\\n# Results and Discussion\\n\\nThe result of our model was an accuracy of 96% in being able to classify between the three classes: improper face mask usage, no mask, and proper face mask usage. In terms of Grad-CAM, the implementation was successful in building trust and transparency within our model: the model was looking at the correct areas to determine the face mask usage.\\n\\n# Website and Frontend links\\nCheckout our website and demo: https://elizabethmkim.github.io/FaceMaskDetection/\\n\\nCheckout our frontend repo:  https://github.com/elizabethmkim/FaceMaskDetection \\n',\n",
       "  \"The aim of the project was to create a model that can determine if an individual is wearing a mask correctly, incorrectly, or not at all. The model was trained using the MaskedFace-Net dataset and the Inception Resnet V1 model. GradCAM was applied to provide transparency and trust in the model's predictions. The project structure includes config files, presentation snippets, and a main run file. The usage of GradCam involves running the code on a given input path. The results showed an accuracy of 96% in classifying mask usage, and Grad-CAM successfully identified the correct areas for determining mask usage. The website and frontend repository links are provided for further exploration.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_52': [\"# DSC180B Face Mask Detection\\n\\nThis repository focuses on the creation of a Face Mask Detection report.\\nLink to website: https://athena112233.github.io/DSC180B_Project_Webpage/\\n\\n-----------------------------------------------------------------------------------------------------------------\\n\\n### Introduction\\n* This repo is about training an Convolutional Neunral Network(CNN) image classification model on MaskedFace-Net. MaskedFace-Net is a dataset that contains more than 60,000 images of person either wearing a mask not. For images that contain a person wearing a mask, the dataset is further splited into either a person is wearing a mask properly or not. In this repo, we've trained a model on this dataset and also implemented a Grad-CAM algorithm on the model.\\n\\n##### config \\n* This folder contains the parameters for running each target. (Make sure the paths to the model and image are correct!)\\n\\n##### model\\n* This folder contains a trained model parameters(model.pt)\\n\\n##### my_image\\n* This folder contains all the custom images that you want the model to test on. This folder will only be created when a custom image path is provided in /config\\n\\n##### notebook\\n* This folder contains the exploratory data analysis(EDA) of the MaskedFace-Net.\\n\\n##### result\\n* This folder contains the images that display the result of model prediction, Grad-CAM algorithm, and Integrated Gradient.\\n\\n##### src\\n* This folder contains the .py files for model architecture, training procedure, testing procedure, Integrated Gradient, and Grad-CAM algorithm.\\n\\n##### run.py\\n* This `run.py` file will the specified target.\\n\\n##### submission.json\\n* `submission.json` contains the general structure of this repo.\\n\\n### How to run this repo with explanation:\\n*  Please visit the `EDA.ipynb` inside the `notebook folder` to understand the MaskedFace-Net. Once you understand the daatset, then you can process to run the repo\\n\\nTo run this repo on GPU (highly recommended), run the following lines in a terminal\\n\\n```\\nlaunch-scipy-ml-gpu.sh -i j0e2r1r0/face-mask-detection -c 4 -m 8\\ngit clone https://github.com/gatran/DSC180B-Face-Mask-Detection\\ncd DSC180B-Face-Mask-Detection\\n```\\n\\nOR\\n\\nTo run this repo on CPU, run the following lines in a terminal\\n\\n```\\nlaunch.sh -i j0e2r1r0/face-mask-detection -c 4 -m 8\\ngit clone https://github.com/gatran/DSC180B-Face-Mask-Detection\\ncd DSC180B-Face-Mask-Detection\\n```\\n\\nThen you can start to run the various targets we've provided in ```src/``` folder\\n\\nTo train a model on your own, run the following line in a terminal\\n\\n```\\npython run.py training\\n```\\n\\nTo test the model, run the following line in a terminal\\n\\n```\\npython run.py testing\\n```\\n\\nTo implement the Grad-Cam algorithm on the model, run the following in a terminal\\n\\n```\\npython run.py gradcam\\n```\\n\\nTo implement the Integrated Gradient algorithm on the model, run the following in a terminal\\n\\n```\\npython run.py ig\\n```\\n\\n##### Contributions\\n* Gavin Tran: train the model and generate output\\n* Che-Wei Lin: implement gradcam and generate output\\n* Athena Liu: create a report based on those outputs\\n\",\n",
       "  'This repository focuses on Face Mask Detection using a Convolutional Neural Network (CNN) image classification model trained on the MaskedFace-Net dataset. The repository includes various folders such as config, model, my_image, notebook, result, and src. It also provides a run.py file for running different targets such as training the model, testing the model, implementing the Grad-CAM algorithm, and implementing the Integrated Gradient algorithm. The contributions to this project were made by Gavin Tran, Che-Wei Lin, and Athena Liu.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_51': [\"The purpose of this code is generating captions from an image and creating attention maps to help explain the model’s reasoning for the captions generated.\\nIn order to test the robustness of the model we also use counterfactual images to see how the model's prediction changes when certain object are removed. \\nUsing this infomation we can also generate an object importance map to show which object in an image are most important to the caption generation process.\\n\\nThis project has six targets: data, train, evaluate_model, generate_viz, counterfactual_production, and explain_model. \\n  - **data**: This target loads in the COCO dataset and prepares it for our image captioning model. \\n  - **train**: This target builds the encoder and decoder in our image captioning model and trains it with the COCO dataset. \\n  - **evaluate_model**: This target evaluates the trained model using beam search caption generation and BLEU score. \\n  - **generate_viz**: This target generates a visualization of the attention maps at each stage of the caption generation process.\\n  - **counterfactual_production**: This target creates all of the files necessary to generate the counterfactuals (such as masks) and \\n                                   then produces the counterfactual images.\\n  - **explain_model**: This target takes all of the counterfactual images and generates caption based on the new counterfactuals. \\n                       Then compares the caption change from the original caption to generate a visualization to explain object \\n                       importance using BERT similarity score.\\n\\nTo run the four targets, clone our repo to the dsmlp server and execute the command ‘python run.py all’ to run all the targets in sequence or \\n'python run.py <target>' to run a single target. To run on a small set of test data execute: ‘python run.py test’. The output images will be saved \\n to the same directory as run.py.\\n\\n\\nDocker Repo: https://hub.docker.com/layers/140345085/afosado/capstone_project/final_docker/images/sha256-198c698d15e7a67d1bba8180a30c21dbf00dfd5e839189a94c06e6ffe96f9fac?context=explore\\n\\nDemo Website: https://afosado.github.io/180b_capstone_xai/index.html\\n\",\n",
       "  \"This code generates captions from an image and creates attention maps to explain the model's reasoning. It also uses counterfactual images to test the model's robustness. The project has six targets: data, train, evaluate_model, generate_viz, counterfactual_production, and explain_model. To run the targets, clone the repo and execute the command 'python run.py all' or 'python run.py <target>'. The output images will be saved in the same directory as run.py. A Docker repo and a demo website are also provided.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_50': ['# StockMarket_explainableAI\\nContributing Members: \\n- Sohyun Lee\\n- Shin Ehara\\n- Jou-Ying Lee\\n\\n## Abstract\\nDeep learning architectures are now publicly recognized and repeatedly proven to be powerful in a wide range of high-level prediction tasks. While these algorithms’ modeling generally have beyond satisfactory performances with apposite tuning, the long-troubling issue of this specific learning lies in the un-explainability of model learning and predicting. This interpretability of “how” machines learn is often times even more important than ensuring machines outputting “correct” predictions. Especially in the field of finance, users’ ability to dissect how and why an algorithm reached a conclusion from a business standpoint is integral for later applications of i.e., to be incorporated for business decision making, etc. This project studies similar prior work done on image recognition in the financial market and takes a step further on explaining predictions outputted by the Convolutional Neural Network by applying the Grad-CAM algorithm. \\n\\nProject Website at: https://connielee99.github.io/Explainable-AI-in-Finance/\\n\\n## Instructions on Runing Project\\n* This project aims to apply the Grad-CAM technique to a CNN model trained on images that represent closing prices during the first hour of market exchange. \\n* **To engineer data and create a CNN model**, you would need to run each notebook in `notebooks` folder in the following order:\\n\\t* **1. Run every cell in `Data Processing.ipynb`**\\n\\t\\t* This notebooke is preprocessing the raw data by extracting closing prices during first hour after market open and labeling depends on prices increasing or decreasing\\n\\t\\t* **Input:** `raw_NIFTY100.csv`\\n\\t\\t* **output:** `first_combined.csv` contains closing prices during the first hour of market exchange\\n\\t* **2. Run every cell in `Image Conversion.ipynb`**\\n\\t\\t* This notebook is for an image conversion with `first_combined.csv` data. We will converse data into image with Gramian Angular Algorithm.\\n\\t\\t*  **Input:**`first_combined.csv`\\n\\t\\t*  **output** `.png` images in `imgs` folder\\n\\t* **3. Run every cell in `CNN.ipynb`**\\n\\t\\t* This notebook uses FastAI, a PyTorch-based deep learning library, to build the neural network, which is able to figure out the relationship between input features and find hidden relationship with them. The input data is an image dataset with labels, which is converted from time series with Gramian Angular Field algorithm as described in the previous sections.\\n\\n* **To run Grad-CAM**: \\n\\t- Clone the Grad-CAM submodule we have included in repo homepage.\\n\\t- Navigate to <i>StockMarket_explainableAI/test</i> and put <i>test_imgs</i> folder inside this cloned submodule folder.\\n\\t- Set your directory to be in this submodule, and run the following command (feel free to modify the last part in the code for specific images):\\n\\t\\t* python3 main.py demo1 -a resnet34 -t layer4 -i test_imgs/2017-01-03.png -k 1\\n\\n## Directory Structure\\n* **config**</br>\\n\\tThis folder contains json files for main and testing parameters\\n\\t* `data_params.json`</br>contains parameters for running main on all data\\n\\t* `test_params.json`</br>contains parameters for running main on test data\\n* **data**</br>\\n\\tThis folder contains all stock data from time series to image representation</br>\\n\\t**imgs**</br>\\n\\t* This folder contains all images converted from time series. ex) 2017-01-02.png\\n\\t\\n\\t**raw data**</br>\\n\\t* `raw_NIFTY100.csv`</br>contains raw stoack market data; time series data\\n\\n\\t**processed data**</br>\\n\\t* `first_combined.csv`</br>contains closing prices during the first hour of market exchange\\n\\t* `gramian_df.csv`</br>contains data after implementing gramian angular algorithm\\n\\t* `label_dir_2.csv`</br>contains data with label Whether the price goes up or down that day\\n* **gradcam_submodule @ fd10ff7**</br>\\n\\tThis folder is the submodule for gradcam\\n\\t\\n* **notebooks**</br>\\n\\tThis folder is the notebook directory\\n\\t\\n\\t* `CNN + Grad-CAM.ipynb`</br>is the development notebook for CNN and GradCam implementation\\n\\t* `Data Processing.ipynb`</br>is the notebook that wraps together data cleaning to feature engineering\\n\\t* `EDA.ipynb`</br>is the notebook with eda work demonstration\\n\\t* `Image Conversion.ipynb`</br>is the notebook with image conversion work done\\n* **references**</br>\\n\\tThis folder contains additional information/references in regards to our project\\n\\t\\n\\t**report_img**</br>\\n\\t* This folder contains images extracted from coded notebooks and included in the written report\\n\\n* **src**</br>\\n\\tThis folder contains library codes extracted from notebooks\\n\\t\\n\\t**features**</br>\\n\\t* `build_features.py`</br>scripts to build features from merged data\\n\\t* `build_labels.py`</br>scripts to create labels for image classification\\n\\t* `build_images.py`</br>scripts to convert and save time series data to images\\n\\t\\n\\t**model**</br>\\n\\t* `gradcam.py`</br>scripts to implement gradcam\\n\\n* **test**</br>\\n      This folder contains test results and test images\\n      \\t\\t\\n* **`Dockerfile`**</br>\\n\\tThis is the dockerfile necessary to build the environment for this project development\\n* **`run.py`**</br>\\n\\tThis is the main python file to execute our program\\n',\n",
       "  'This project focuses on applying the Grad-CAM algorithm to explain predictions made by a Convolutional Neural Network (CNN) trained on stock market data. The project aims to provide interpretability in the field of finance by allowing users to understand how and why the algorithm reached its conclusions. The project website provides more information and instructions on running the project. The directory structure includes folders for data, notebooks, references, and source code, among others.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_49': ['# Racial_Classification_XAI_Model\\n\\nkeyword: Deep Learning, Convolutional Neural Network, Integrated-Gradient, Grad-CAM, Web Application\\n\\nWebsite: https://michael4706.github.io/XAI_Website/\\n\\nStatic Web app: this [demo](https://nicole9925.github.io/facial-analysis-frontend/) (once you clicked the demo, just press submit to run it) is our web application that runs sample image. Make sure you visit the [Web Application](https://michael4706.github.io/XAI_Website/webapp/) (or just click this link) to play with it. If you want to run the web application with your own image, please visit the Web Application section below and follow the intrusctions.\\n\\n\\n![sample result](sample_result.png)\\n\\n### Introduction\\nThis project is about visualizing Convolutional Neural Network (CNN) with XAI techniques: Grad-cam and Integrated-Gradient. We used the FairFace dataset to train our models. This dataset contains about 80000+ training images and 10000+ validation images. The dataset contains three different categories(labels): age range(9 classes), race(7 classes), and gender(2 classes). We implemented a model that combined the first 14 layers from resnet50 as pre-trained layers with our self-defined layers. We trained three models on each of the different categories using the same model structure except changing the number of outputs from the final layer to match each category\\'s number of classes. Then, we applied XAI to visualize models\\' decision-making with heatmaps. We want to examine what features or regions the models focus on given an image. Also, we are interested in comparing the heatmaps generated by the biased and unbiased models. The FairFace Dataset has an equal distribution of race. Therefore, we created a dataset with an unequal distribution of race and trained a biased model with this dataset.\\n\\n##### config\\n* The parameters to run the scripts. Make sure to visit this file before running the code.\\n\\n##### run.py\\n* script to train the model, run integrated-gradients, and calculate the statistics for the model.\\n\\n##### src\\n* folder that contains the source code.\\n\\n##### models\\n* Contained dlib_mod that helps to preprocess the images. You also recommend you to save your trained model here.\\n\\n##### test_data\\n* Contains sample data from FairFace Dataset.\\n\\n### How to run the code\\n1. please the my docker image: `michael459165/capstone2:new8` and run the code inside this container.\\n2. please go to the config file to change the parameters. This file has 5 sections, each corresponds to a set of parameters to execute a particular task.\\n3. Type `python run.py train_model` to train your model.\\n4. Type `python run.py generate_stats` to generate statistics and plots.\\n5. Type `python run.py run_test` to generate just ONE heatmap for both Integrated Gradient and Grad-CAM on the test sample. This will generate the heatmaps of the class with the HIGHEST predictive probability.\\n6. Type `python run.py run_custom_img` to generate just ONE heatmap for both Integrated Gradient and Grad-CAM on YOUR own image. This will generate the heatmaps of the class with the HIGHEST predictive probability.\\n\\nNote: All the functions in util.py are well documented. Please feel free to explore and modify the code!\\n\\n### Web Application\\nWe also made a Web App to showcase our work. Please clone [this repository](https://github.com/nicole9925/facial-analysis-webapp) and follow the instruction to run it locally. If you want to deploy the Web App online, please visit [frontend](https://github.com/nicole9925/facial-analysis-frontend) and [backend](https://github.com/nicole9925/facial-analysis-backend) repositories for further instruction. \\n\\n### More stuff you can do\\n* If you don\\'t want to train the model (because it takes a long time), then please visit the \"temp\" branch from this repository. There are trained models under the models folder and sample integrated-gradient results and statistics under the visualization folder. Please just download the files you need and run the code from this main branch. \\n\\n### Reference\\n[1]Selvaraju, Ramprasaath R., et al. \"Grad-cam: Visual explanations from deep networks via gradient-based localization.\" Proceedings of the IEEE international conference on computer vision. 2017.\\n\\n[2]Grad-CAM implementation in Keras[Source code]. https://github.com/jacobgil/keras-grad-cam.\\n\\n[3]Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. \"Axiomatic attribution for deep networks.\" International Conference on Machine Learning. PMLR, 2017.\\n\\n[4]Integrated Gradients[Source code]. https://github.com/hiranumn/IntegratedGradients.\\n\\n[5]@inproceedings{karkkainenfairface,\\n      title={FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation},\\n      author={Karkkainen, Kimmo and Joo, Jungseock},\\n      booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},\\n      year={2021},\\n      pages={1548--1558}\\n    }\\n\\n[6] FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age[Source code].https://github.com/dchen236/FairFace.\\n\\n[7] Draelos, Rachel. “Grad-CAM: Visual Explanations from Deep Networks.” Glass Box, 29 May 2020. https://glassboxmedicine.com/2020/05/29/grad-cam-visual-explanations-from-deep-networks/#:~:text=Grad%2DCAM%20can%20be%20used%20for%20understanding%20a%20model\\'s%20predictions,choice%20than%20Guided%20Grad%2DCAM.\\n',\n",
       "  \"This project focuses on visualizing a Convolutional Neural Network (CNN) using XAI techniques such as Grad-CAM and Integrated-Gradient. The models were trained on the FairFace dataset, which contains images categorized by age range, race, and gender. The project aims to examine the features and regions that the models focus on when making decisions. A biased model was also trained using a dataset with an unequal distribution of race. The code can be run using a Docker image and various commands are available for training the model, generating statistics, and generating heatmaps. A web application is also provided to showcase the project's work. Additional information, references, and instructions are available in the original source.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_54': ['# Snake NeuralBackedDecisionTrees\\nDSC180B Group 6 Snake Classification using Neural Backed Decision Trees\\n\\nTeam Members:\\n\\nNikolas Racelis-Russell - A15193225\\n\\nWeihua (Cedric) Zhao - A14684029 \\n\\nRui Zheng - A15046475\\n\\n## Abstract\\n\\nOur project focuses on building explanable image classification models on snake images from https://www.aicrowd.com/challenges/snakeclef2021-snake-species-identification-challenge/dataset_files. Our plan is to apply gradcam to images of different snake species, construct a Densenet, and transform them into decision trees to visualize the classification process. \\n\\n### Demo\\n\\nLink to our website:  https://nikolettuce.github.io/DSC180B_06_NeuralBackedDecisionTrees/\\n\\n#### GradCAM\\n\\nOur approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses the class-specific gradient information flowing into the final convolutional layer of a CNN to produce a coarse localization map of the important regions in the image. [1]\\n\\n#### Neural Backed Decision Tree\\n\\nNowadays, machine learning has been applied in multifaceted areas of our life. While its prominence grows, its interpretabilty leaves people insecure because of the fact that people hardly see through the classfication decision process. Many attempts to solve this problem either ends up with the cost of interpretability or the cost of accuracy. Intended to avoid this dilemma in our snake classification process, we applied Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network\\'s final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model\\'s accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging.[2] \\n\\n##### Induced Hierarchy\\n\\nWhen creating the hierarchy tree, one thing to note is that \"[it] requires pre-trained model weights\". We took row vectors wk : k ∈ [1, K], each representing a class, from the fully- connected layer weights W; then, we ran hierarchical agglomerative clustering on the normalized class representatives wk/kwkk2. Last but not least, we built the leaf nodes based on the weights. [2]\\n\\n##### Loss Conversion\\n\\n### Practical Use\\n\\nOur project has valid applications in real life. Wild snakes are prevalent on mountains, and hikers have high possibilities to encounter them. A genuine snake classifier would provide useful information to hikers about whether the snake is venomous or not; thus, they can avoid the snake if a certain dangerous species emerge.\\n\\n### Methods\\nOur classiciation starts with a baseline Densenet model with 5 epochs. In terms of performance, it reached a F-score of 0.495 on validation data and accuracy of 0.66 on validation data. We then managed to improve the model later with higher accuracy.\\n\\nThen we applied Grad-CAM to five different snake pictures with different features. The first category includes pictures where snakes blend in with the background. The second category includes pictures where snakes differ from the background. The third category includes pictures where snakes appear with other objects, like hand. Grad-CAM performs well on localizing the target object. [3]\\n\\nLast, but not least, we are currently working on transforming the CNN models to decision trees. For now, we have a general hierarchy tree where you can see each decision. Though the decision is not clear for now, we will manage to elucidate them in the upcoming weeks.\\n\\nWhen creating the hierarchy tree, one thing to note is that \"[it] requires pre-trained model weights\". We took row vectors wk : k ∈ [1, K], each representing a class, from the fully- connected layer weights W; then, we ran hierarchical agglomerative clustering on the normalized class representatives wk/kwkk2. Last but not least, we built the leaf nodes based on the weights. [4]\\n### Results\\n\\n#### Heatmaps\\n\\n<img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/0a00cdd2b8.jpg\" width=\"200\"/> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam%201.jpg\" width=\"200\"/> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam_gb%201.jpg\" width=\"200\"> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/gb%201.jpg\" width=\"200\"> \\n\\n\\n<img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/0a7eded849.jpg\" width=\"200\"/> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam%204.jpg\" width=\"200\"/> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam_gb%204.jpg\" width=\"200\"> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/gb%204.jpg\" width=\"200\"> \\n\\n<img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/0a54501d6d.jpg\" width=\"200\"/> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam%207.jpg\" width=\"200\"/> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/cam_gb%207.jpg\" width=\"200\"> <img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/gb%207.jpg\" width=\"200\"> \\n\\n#### Hierarchy Trees\\n\\n<img src=\"https://github.com/nikolettuce/DSC180B_06_NeuralBackedDecisionTrees/blob/reputation/Screen%20Shot%202021-02-07%20at%205.32.34%20PM.png\">\\n\\n### Conclusions\\n\\n## Installation\\n\\nTo use this project, please run build.sh and allocate at least 30 GB of hard drive space to install the data.\\n\\nThen run python run.py data to first process the data before using the test target\\n\\nTo run the CNN and test on the snake dataset, run python run.py test\\n\\n## Resources\\n\\n1. Grad-CAM Paper https://arxiv.org/pdf/1610.02391v1.pdf\\n2. @misc{wan2021nbdt, title={NBDT: Neural-Backed Decision Trees}, author={Alvin Wan and Lisa Dunlap and Daniel Ho and Jihan Yin and Scott Lee and Henry Jin and Suzanne Petryk and Sarah Adel Bargal and Joseph E. Gonzalez}, year={2021}, eprint={2004.00221}, archivePrefix= {arXiv}, primaryClass={cs.CV} }\\n\\n3. Grad-CAM code  https://github.com/jacobgil/pytorch-grad-cam\\n\\n4. NBDT: https://github.com/alvinwan/neural-backed-decision-trees\\n\\n\\n',\n",
       "  'This project focuses on building explainable image classification models for snake images. The team plans to use Grad-CAM to visualize important regions in the images and transform a Densenet model into decision trees. The project has practical applications in identifying venomous snakes for hikers. The team has also applied Grad-CAM to different snake pictures and is currently working on transforming CNN models into decision trees. Results include heatmaps and hierarchy trees. The installation process is provided, along with resources used for the project.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_16': ['# DSC-180B-Team6\\n\\nWe were able to replicate the ThunderHill race track using the Unity 3D game engine and integrated Unity with the track and robot into the LGSVL simulator. Once the integration was complete we were able to see our robot with the Thunderhill Track as our map in the simulator. We were then able to virtualize the functions of the IMU, odometry and lidar sensors and RGB-D cameras to better visualize what our robot perceives in the simulation. Finally we were able to fully visualize what our robot sees with the virtual sensors using Autoware Rviz which displays the location and point cloud map of the vehicle and its surroundings.\\n',\n",
       "  \"The team successfully replicated the ThunderHill race track using Unity 3D game engine and integrated it with the LGSVL simulator. They virtualized the IMU, odometry, lidar sensors, and RGB-D cameras to visualize the robot's perception in the simulation. Autoware Rviz was used to display the vehicle's location and point cloud map.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_14': ['DSC 180B Autonomous Vehicle Team 1 [Organization\\nRepository](https://github.com/UCSDAutonomousVehicles2021Team1) to\\nserve as a collection of all the repositories built for this project.\\n\\n* [autonomous_navigation_image_segmentation](autonomous_navigation_image_segmentation)\\n* [autonomous_navigation_light_sensitivity](autonomous_navigation_light_sensitivity)\\n* [autonomous_nav_mapping_docker](autonomous_nav_mapping_docker)\\n* [camera_mapping_navigation_website](camera_mapping_navigation_website)\\n* [f1tenth_racecar_dl_custom](f1tenth_racecar_dl_custom)\\n* [rtabmap_mapping_tuning](rtabmap_mapping_tuning)\\n\\n\\n',\n",
       "  'The DSC 180B Autonomous Vehicle Team 1 has created an organization repository on GitHub to house all the repositories built for their project. The repositories included are: autonomous_navigation_image_segmentation, autonomous_navigation_light_sensitivity, autonomous_nav_mapping_docker, camera_mapping_navigation_website, f1tenth_racecar_dl_custom, and rtabmap_mapping_tuning.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_13': ['# Data Visualizations and Interface For Autonomous Robots\\n\\nThis project aims to create data visualizations and an interactive interface for autonomous robots. The intent and design of visualizations created for this project were catered towards optimizing racing performance on the [Thunderhill track](https://www.thunderhill.com/). Visualizations include birdseye view of optimal path on mapped track, live camera feed, lidar readings, IMU data (position and orientation) visualized, battery status display, and various other visualizations to show the health and status of the vehicle. The interface that displays all of these various tools and visualizations are meant to be interactive and communicate with the Autonomous Robot via Rosbridge, which not only allow users to control the Gazebo Robot action by simple interface interaction like clicking button or inputing text, but also monitor the realtime situation of the robot navigating in map. The visualizations will be primarily illustrated through Python, ROS, Gazebo, RViz, and other robotics software. After analyzing the performance of A* algorithm versus the performance of RRT* algorithm, it is determined that RRT* performs better. Ultimately, RRT* algorithms is the primary navigation algorithm used and illustrated in this project.\\n\\n## Running the project\\nFirst clone the repository:\\n```\\n$ git clone https://github.com/dannyluo12/Autonomous_robot_data_visualization_and_interface.git\\n```\\nLaunch docker container using image:\\n```\\n$ launch.sh -i dannyluo12/visualization_and_interface:latest -c 4 -m 8 -P Always\\n```\\n* This command launches a [dockerhub](https://hub.docker.com/repository/docker/dannyluo12/visualization_and_interface) container with the necessary OS libraries, tools, and dependencies to successfully run the project. Certain dependencies will be vital for creating the visualizations and genearting the interface.\\n\\n## Building the project using `run.py`\\n* Use the command `python run.py data` to create data folder. Will contain directories to properly store image and sensor data that is outputted.\\n* Use the command `python run.py clean` to ensure that data is scaled properly to optimize runtime. Includes imaging data for running navigation algorithms as well as executing interface.\\n* Use the command `python run.py analyze` to compare the performance of A* algorithm to RRT* for navigation on the same map.\\n* Use the command `python run.py test` to run the visualization of RRT algorithm in test data, output images can be found in the testdata/step_out and testdata/test_out directories.\\n* Use the command `python run.py all` to run the visualization of RRT algorithm on cleaned data/map, output images can be found in the data/step_out and data/test_out directories.\\n\\n### Contributions:\\n<b>Yuxi Luo</b> <br />\\nContributed to developing visualizations for RRT* and A* algorithms. Tested performance of each navigation algorithm to benchmark each and determine better performer. Collected and cleaned data from alternative groups to enable visualization and interface development. Tested different ROSBAGS for data type compatibility. Investigated various forms of visualization from different ROS topics (diff sensors, camera, lidar, etc.). Helped in managing and updating Github repo, report, and project website.\\n\\n<b>Seokmin Hong</b> <br />\\nContributed by implementing the UCSD simulated track inside the Gazebo simulator, as well as implementing the RRT* and A* algorithms that can be used for G-Mapping SLAM. Also wrote Rviz scripts and interactive interface scripts to allow autonomous navigation with a simple pressing of a button. Helped teammates by creating and writing the report, as well as creating the demonstration videos of the interactive interface and Gazebo simulations.\\n\\n<b>Jia Shi</b> <br />\\nContributed to the research of visualization and interface. Developed an interactive interface with roslibjs and webridge to connect ROS with web page. Worked with teammate to integrate interface with Gazebo robot to allow controlling. Also created visualization demo with ROS bag data from other teams. Helped teammates with the coding and helped with the setup and completion of github repo.\\n',\n",
       "  \"This project focuses on creating data visualizations and an interactive interface for autonomous robots, specifically optimizing racing performance on the Thunderhill track. The visualizations include a birdseye view of the optimal path, live camera feed, lidar readings, IMU data visualization, battery status display, and other visualizations to show the vehicle's health and status. The interface allows users to control the robot's actions and monitor its real-time situation using Rosbridge. The primary navigation algorithm used in this project is RRT*. \\n\\nTo run the project, clone the repository and launch a docker container with the necessary dependencies. The `run.py` script can be used to create data folders, clean data for optimization, analyze algorithm performance, and run visualizations.\\n\\nThe contributions of team members include developing visualizations for RRT* and A* algorithms, implementing the UCSD simulated track in Gazebo simulator, implementing navigation algorithms for G-Mapping SLAM, creating an interactive interface with ROS integration, and assisting with coding and project management tasks.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_12': ['# DSC 180 Autonomous Systems\\n\\nNeghena Faizyar, Garrett Gibo, Shiyin Liang\\n\\n## Data\\nTo get sample data: \\n[Link to Data](https://drive.google.com/drive/folders/1wh7EtgtrS8Wi8xBIe1VwzFDBnp751XHv?usp=sharing)\\n\\nDownload this data to put into the data/raw folder.\\n\\n## Usage \\n\\n### ROS\\n\\nA large portion of this project is a series of ROS packages that can be launched\\ndirectly or integrated in other ROS packages. This repo has been made in such a\\nway that it serves as a full ROS workspace, thus to run the packages that are\\ncontained here, simply run:\\n\\n``` sh\\n# build project\\ncatkin_make\\n\\n# source ROS packages\\nsource devel/setup.bash\\n\\n# launch main node\\nroslaunch simulation main.launch\\n```\\n\\nThis will launch a gazebo simulation containing a test vehicle with sensors that\\nwere used for this project. Because the simulation requires both ROS and gazebo\\nwhich have large graphical portions, these packages must be run a system that\\nhas ROS setup already and also has some type of grapical interface, for example\\nX on linux based systems.\\n\\n### Analysis\\n\\nThe second portion of this project is analysis that is done on the data that\\nis gathered from both simulation and real sensors. \\n\\nTo run any of the following targets, the command is:\\n\\n```sh\\npython run.py <target>\\n```\\n\\nInformation on the targets is found below.\\n\\n#### Targets\\n\\n* `cep`: Calculates the Circular Error Probable (CEP), and \\n2D Root Mean Square (2DRMS), and then plots and creates a graph of the CEP \\nand 2DRMS circles with the datapoints. \\n\\n* `clean_data`: Extract, transform, and clean the raw GPS data so\\nthat it can be used for anaylsis.\\n\\n* `get_path`: Takes in CSV of GPS coordinates and cleans/filters points to create\\na usable path.\\n\\n* `ground_truth`: Plot ground truth coordinates against estimated coordinates \\nfor reported GPS values.\\n\\n* `robot`: Creates an instance of the\\n[dronekit-sitl](https://dronekit-python.readthedocs.io/en/latest/develop/sitl_setup.html),\\nwhich can be used to generate realistic sensor data that can be used\\nas a template for the following targets.\\n\\n* `robot_client` Provides the interface to connect to a specified robot.\\nThe client connects over tcp or udp and uses the\\n[MAVLink](https://mavlink.io/en/messages/common.html), standard for\\nthe messages.\\n\\n* `test`: Runs our projects test code by extracting, transforming, and then \\ncleaning the raw GPS test data such that it could be used. \\n\\n* `visualize`: Create visualizations for all of our data using bokeh. It will \\nplot the line the GPS reports it has traveled and uploads it into the vis \\nfolder of our repository. \\n',\n",
       "  'This document provides information about the DSC 180 Autonomous Systems project. It includes details about the data used, usage instructions for running ROS packages, and analysis targets. The analysis targets include calculating error probabilities, cleaning and transforming GPS data, creating usable paths, plotting ground truth coordinates, generating realistic sensor data, connecting to a robot, running test code, and creating visualizations using bokeh.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_11': ['# Autonomous Vehicles Capstone: Odometry and IMU \\n\\n\\nWe are Team 4 in the Autonomous Vehicles Data Science Capstone Project. Our project revolves around the IMU, Odometry efforts while we collectively work to build a 1/5 scale racing autonomous vehicle. \\n\\nFor a vehicle to successfully navigate istelf and even race autonomously, it is essential for the vehicle to be able localize itself within its environment. This is where Odometry and IMU data can greatly support the robot’s navigational ability. Wheel Odometry provides useful measurements to estimate the position of the car through the use of wheel’s circumference and rotations per second. IMU, which stands for Interial Measurement Unit, is 9 axis sensor that can sense linear acceleration, angular velocity, and magnetic fields. Together, these data sources can provide us crucial information in deriving a Position Estimate (how far our robot has traveled) and a Compass Heading (orientation of the robot/where it’s headed).\\n\\nOur aim is to calibrate, tune, and analyze Odometry and IMU data to provide most accurate Position Estimate, Heading, and data readings to achieve high performant autonomous navigation and racing ability.\\n\\n*Developed by: Pranav Deshmane and Sally Poon*\\n\\n### Usage\\n\\n```\\npython run.py <target>\\n```\\nThe Targets are: \\n \\n* `conversion` - This will extract the data from the raw ROS bags, clean them, and convert them to csv files to be analyzed\\n \\n* `viz_analysis` - This will run the visualizations used in our analysis for IMU and Odometry calibration, tuning, and testing\\n\\n* `test` - This will test the conversion and visualization process with sample data chosen from our raw data\\n\\n### Resources\\nIn the resources folder:\\n\\n* `Openlog_Artemis_IMU_Guide` - Guide we developed for SparkFun Openlog Artemis IMU to improve future experience and aid in the installation, setup, and integration with Jetson NX and ROS.\\n\\n* `Calibration_OLA_Artemis` - Calibration guide we developed for SparkFun Openlog Artemis IMU to aid in calibration process, analysis for future students/users\\n\\n* `Setup for Odometry_IMU` - Guide we developed for the Odometry to aid in tuning process, analysis, and setup of Odometry and VESC interaction to improve experience for future students/users.\\n\\n* `Apollo3`, `ICM-20948`, `Artemis_Hardware` - Sparkfun Openlog Artemis IMU Hardware Specifications, used to cross reference PIN headers that were needed to be configured correctly during integration process. \\n\\n\\n\\n\\n### Additional ROS package \\n* `ros_imu_yaw_pkg` \\nROS package we developed to aid in the integration of the OLA Artemis IMU to ROS. It allows the orientation quaternion readings derived from the IMU to be easily converted into Euler angles and Yaw heading. This is to improve the debugging process within ROS and helps to easily visualize the Yaw heading. This package can be run in parallel as a complement to the main ROS package used to interface with the OLA Artemis IMU and can easily integrate with the rest of your current ROS system in place as a separate node. Overall, this is to aid in the development process within ROS when deriving Yaw Heading from the OLA Artemis IMU. \\n\\n',\n",
       "  \"Team 4 is working on building a 1/5 scale racing autonomous vehicle for the Autonomous Vehicles Data Science Capstone Project. They are focusing on the IMU and Odometry efforts to help the vehicle navigate and race autonomously. Wheel Odometry uses measurements from the wheels to estimate the car's position, while the IMU is a 9-axis sensor that senses linear acceleration, angular velocity, and magnetic fields. By calibrating and analyzing the Odometry and IMU data, they aim to achieve accurate position estimates, heading, and data readings for high-performance autonomous navigation and racing ability. They have also developed resources such as guides for the IMU calibration and setup, as well as a ROS package for integrating the IMU with ROS.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_55': ['# Webpage Link\\n[https://yikaihao.github.io/DSC180_Webpage/](https://yikaihao.github.io/DSC180_Webpage/)\\n',\n",
       "  'The provided link leads to a webpage.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_58': [\"# DSC180B-Project\\nThe data we have are grabbed from the /teams directory: malware and popular-apps.\\n\\nThe purpose is to perform search on each software folder to find its smali files\\nand perform method-call analysis to build markov chain and get holistic\\ninformation of specific software and finally build a improved MAMADroid to\\nclassify specific ware to be benign or malware.\\n\\nIt consists process_smali() to parse smali file and generate call-analysis.\\n\\nTo run it, execute python run.py <targets>.\\nTargets including 'feature', 'model', 'analysis', 'test'\\n\\n### Responsibilities\\n\\n* Jian Jiao developed code which parses content, generates features,\\n  builds model, perform analysis, improve model, generate results.\\n* Zihan Qin developed report and help partner to test code and debug.\\n\\n### Project Webpage\\nhttps://kamui-jiao.github.io/DSC180B-Page/\\n\",\n",
       "  'The project involves analyzing software files to classify them as benign or malware. The data is obtained from the /teams directory, specifically the malware and popular-apps folders. The process includes searching for smali files in each software folder, performing method-call analysis to build a markov chain, and using an improved MAMADroid to classify the software. The responsibilities of the team members are also mentioned, with Jian Jiao developing the code and Zihan Qin assisting with testing and debugging. More information can be found on the project webpage: https://kamui-jiao.github.io/DSC180B-Page/'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_57': [\"# hindroid_replication\\n## HinDroid: An Intelligent Android Malware Detection System. Based on Structured Heterogeneous Information Network.\\n# Malware Detection Using API Relationships + Hindroid\\n# HOW TO RUN \\n`python run.py test`\\n+ `requirements.txt`\\n\\nBy July 2020, Android OS is still a leading mobile operating system that holds 74.6% of market share worldwide, attracting numerous crazy cyber-criminals who are targeting at the largest crowd.¹ Also, due to its open-resource feature and flexible system updating policy, it is 50 times more likely to get infected compared to ios systems.² Thus, developing a strong malware detection system becomes the number one priority.\\n\\nThe current state of malware(malicious software) detection for the growing android OS application market involves looking solely at the API(Application Programming Interface) calls. API is a set of programming instructions that allow outside parties or individuals to access the platform or the application.³ A good and daily example will be the login options displaying on the app interface like “Login with Twitter”.4 Malwares can collect personal information easily from APIs, so analyzing APIs is a critical part of identifying malwares.\\n\\nHindroid formulates a meta-path based approach to highlight relationships across API calls to aggregate similarities and better detect malware.Individual APIs appearing in the ransomware could be harmless, but the combination of them could indicate “this ransomware intends to write malicious code into system kernel.”5 You wouldn’t want to see a group of “write”, “printStackTrace”, and “load” APIs appearing in your app’s smali file.5\\n\\n\\n# Data Generation Process\\nThe data generation process and its relationship to the problem (i.e. for domain problems)\\nThe data for identifying malware is primarily the android play store, although in order to obtain the respective APK’s for these apps the data is directly downloaded from `https://apkpure.com/`.\\n\\nThis data is then unpackaged using the apktools library that allows us to view the subsequent smali code and app binaries.\\n\\nThe smali code and app binaries contain a lot of the information derived from the Java source code that allows us to map the number of API calls and the relationships between them. \\n\\n# Observed Data \\nOverall, from each android app, what’s most relevant to classifying Malware vs Benign - are the API calls, code blocks(methods) and packages these API calls occur in, classified as matrices \\nA, P, B, I. This form of organizing data explains the relationship between these API calls. It provides a story, more in depth, than just the sheer number of API calls per app. \\n\\nThe Hindroid model observes the same relationship of data to better classify malware or not, through the relationship of the above defined matrices. Reducing the ability of apps to just add a larger number of API calls to get classified as benign. \\n\\n# Conclusion\\nThe process of identifying the relationship of API calls, is taking the idea of the subsequent network it creates - thus to not just look at the information queried by the call but also the way it interacts with other API’s in different levels of the codebase. The applicability of this lies beyond that of malware detection in android apps, but probably in the roots of graph theory and how relationships with API calls can be better identified and mapped out to provide more insight\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n## Responsibilities:\\n## Report:\\nNeel Shah: Malware Detection Using API Relationships + Hindroid, Data Generation Process, Observed Data, Conclusion\\nMandy Ma: Malware Detection Using API Relationships + Hindroid, Citation\\n## Code:\\n\\tNeel Shah: Refining codes, create repository, transfer code format to be able to run from terminal\\n\\tMandy Ma: Code algorithms,Debug code, Refining code\\n\\n\\n## Citation\\nO'Dea, Published by S., and Aug 17. “Mobile OS Market Share 2019.” Statista, 17 Aug. 2020, www.statista.com/statistics/272698/global-market-share-held-by-mobile-operating-systems-since-2009/. \\nPanda Security Panda Security specializes in the development of endpoint security products and is part of the WatchGuard portfolio of IT security solutions. Initially focused on the development of antivirus software. “Android Devices 50 Times More Infected Compared to IOS - Panda Security.” Panda Security Mediacenter, 14 Jan. 2019, www.pandasecurity.com/en/mediacenter/mobile-security/android-more-infected-than-ios/\\nApp-press.com. 2020. What Is An API And SDK? - App Press. [online] Available at: <https://www.app-press.com/blog/what-is-an-api-and-sdk#:~:text=API%20%3D%20Application%20Programming%20Interface,usually%20packaged%20in%20an%20SDK.> [Accessed 31 October 2020].\\n“5 Examples of APIs We Use in Our Everyday Lives: Nordic APIs |.” Nordic APIs, 10 Dec. 2019, nordicapis.com/5-examples-of-apis-we-use-in-our-everyday-lives/. \\nShifu Hou, Yanfang Ye ∗ , Yangqiu Song, and Melih Abdulhayoglu. 2017. HinDroid: An Intelligent Android Malware Detection System Based on Structured Heterogeneous Information Network. In Proceedings of KDD’17, August 13-17, 2017, Halifax, NS, Canada, , 9 pages. DOI: 10.1145/3097983.3098026\\n\",\n",
       "  'The paper titled \"HinDroid: An Intelligent Android Malware Detection System\" discusses the importance of developing a strong malware detection system for the Android operating system. The paper highlights the use of API relationships in detecting malware and introduces the Hindroid model, which utilizes meta-path based approaches to identify similarities and detect malware. The data generation process involves obtaining APKs from the Android Play Store and analyzing the smali code and app binaries. The observed data focuses on API calls, code blocks, and packages to classify malware versus benign apps. The conclusion emphasizes the significance of understanding the relationships between API calls and suggests that this approach can be applied beyond malware detection.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_56': ['# Malware Detecting using Control Flow Graphs\\nBy [Edwin Huang](https://www.linkedin.com/in/edwin-huang-671a77100/), [Sabrina Ho](https://www.linkedin.com/in/sabrinaho7/)\\n\\n[Link to Webpage](https://iamsabhoho.github.io/dsc180b-malware/)\\n\\n## Introduction\\nThere are many malware detection tools available in the market, including pattern-based, behavior-based methods, etc, with the prompt development of artificial intelligence, many modern data analysis methods are applied to detecting malware in recent years. We are interested in investigating the effectiveness of different data analysis methods for detecting certain types of malware.\\n\\nAs the number of malicious software (malware) increases throughout the past few decades, malware detection has become a challenge for app developers, companies hosting the apps, and people using the apps. There are many pieces of research conducted on malware detection since it first appeared in the early 1970s. Just like the paper we studied in our first quarter, it uses the HIN (Heterogeneous Information Network) structure to classify the Android applications. It also compared its own method against other popular methods such as Naive Bayes and Decision Tree, and other known commercial mobile security products, to test its performance. The result showed that their method performs better than the other methods with an accuracy of 98% while other others only achieve an average of 90\\\\%. After studying the paper, we are more curious about the detecting effectiveness of an analysis method when applied to a certain type of malware.\\n\\n![Project Pipeline](/img/flow.png)\\n\\nNot everyone has access to tools that can detect whether or not the app they just downloaded is malicious or not. Our motivation to conduct this research is to hope to produce a recommending tool that can be easily accessed by the general public for detecting malware. Optimistically, we want to reduce the chance of people downloading malicious apps and potentially prevent their devices from being hacked. To achieve that, we will be classifying applications using Control Flow Graphs and different similarity-based methods including k-nearest neighbors (kNN) as well as Random Forest classifier to see if different methods can detect certain types of malware or any specific features.\\n\\nWe are interested in analyzing whether one classifier has better performance in detecting certain types of malware or specific features, and designing a framework for recommending a method with a specific set of parameters for a certain type of malware and provide users a more friendly interface. With the similarity-based approach, we believe that it will detect malware with much higher accuracy and will be more flexible for applications that evolved over time as they become more complicated.\\n\\n\\n## Related Work\\n\\n### Mamadroid\\nMamaDroid is a system that detects Android malware by the apps\\' behaviors. This method extract call graphs from APKs, which are represented using nodes and edges in a graph object. From each graph, sequences of probabilities are extracted, representing one feature vector per APK. These probabilistic feature vectors are used for malware classification. MamaDroid also abstracts each API call to the family and package level, which inspired us to abstract to the class level. This is discussed further later.  \\n\\n### Hindroid\\nHindroid is a system that parses SMALI code extracted from APKs and uses them to create four different graphs, which are represented by large matrices. Within these matrices, each value in a matrix corresponds to an edge. A combination of these matrices are used to classify malicious software and benign apps.  \\n\\n### Metapath2Vec\\nMetapath2Vec is a node representation learning model that uses predefined paths based on the node types. These paths define where the the program can traverse the graph. Following is an example of a metapath. In this case below, the metapath is Type 1 → Type 2 → Type 1 → Type 3 → Type 1.\\n\\n![Metapath2vec Example](/img/m2v.png) \\n\\nWith predefined meta-paths, we can traverse a graph according to these node types to generate a large corpus, which is then fed into Node2Vec to obtain representations of words. This method will obtain one vector for one node within the graph.\\n\\n### Word2Vec\\nWord2vec is a model that turns text into numerical representations. It is trained on a large corpus, and outputs a representation for each word in the corpus. Below is a famous example of Word2Vec: King and Queen and Men and Women.\\n\\n![Word2vec Example](/img/w2v.png) \\n\\nSince Word2Vec measures the similarity between words using Cosine similarity, we can see from the above vector space that the word King is similar to Queen, and Men is similar to Women.\\n\\n### Doc2Vec\\nSimilar to Word2Vec, Doc2Vec turns a whole document/paragraph into numerical representations instead of word representations. If we can obtain one corpus from each of the apps by applying metapath2vec, then we can treat each corpus as its own document, and then feed it into the Doc2Vec model to learn representations for each of the documents. These vector representations can then be used in the classification process.\\n\\n\\n## Data\\nThe data we will be using is randomly downloaded from APK Pure and AMD malware dataset. It consists of labeled malware and other popular and unpopular (random) applications. Among our random apps downloaded from APK Pure, there might be one app out of five that might be a malware since they are apps that have little or no reviews. Rather than using .SMALI files, we will be working with APK files directly. From the APK files, we will be extracting a new form of representation called Control Flow Graphs. With APK files, we can easily generate control flow graphs through Androguard, which is a powerful tool to disassemble and decompile Android applications.\\n\\n### Control Flow Graphs\\nA Control Flow Graph (CFG) is a representation using graph notation of all paths that might be traversed through a program during execution. Firstly, a CFG consists of nodes and edges. Control Flow is the order in which individual statements, instructions, or function calls of an imperative program are executed or evaluated. Imperative meaning statements that change a program’s state. Each node in the CFG represents a basic block, or a straight-line piece of code without any jumps or jump targets. In our case, a node in the CFG is an API or method call. A jump statement is a statement that changes the program’s flow into another place of the source code. For example, from line 4 to line 60, or from file 1 to file 6. The following figures are two simple control flow graphs.\\n\\n![A Simple Control Flow Graph](/img/cfg.png)\\n\\nA node in our CFG can call another API (node). A node can be visualized as one of the circles in Figure 4, and the \"call\" action can be visualized by the arrow(edge). Each node has attributes. There are 7 Boolean attributes for each node, and 3 different edge types.\\n\\n| Node Attributes | |\\n| --- | --- |\\n| External | If a node is an external method |\\n| Entrypoint | If a node is not called by anything |\\n| Native | If a node is native |\\n| Public | If the node is a public method |\\n| Static | If the node is a static method |\\n| Node | If none of the above are True |\\n| APK Node | If the node is an APK |\\n\\nWe can pick from the 6 Boolean attributes and create node types based, such as: “external, public Node”, “external, static Node” and “entrypoint, native Node”. There can be more than 20 different node types.  \\n\\n| Edge Types | |\\n| --- | --- |\\n| Calls | API to API |\\n| Contains | APK to API |\\n| In | API to APK |\\n\\nTogether, nodes and edges can build paths like: “external, public Node - calls -> external, static Node” or “APK - contains -> external, public Node”. The following is a control flow graph example with code block to explain how nodes are called:\\n\\n```\\nClass: Lclass0/package0/exampleclass; ## let’s call this A\\n# direct methods\\n.method public constructor <init>()V\\n    if ....:\\n    \\t# api, call this B:\\n    \\tLclass1/package1/example;->doSomething(Ljava/lang/String;)V\\n\\telse:\\n    \\t# api, call this C:\\n    \\tLclass2/package2/example;->perform(L1/2/3)F\\n```\\n\\n![Control Flow Graph With Code Block Example](/img/cfg1.png)\\n\\nThe method **constructor** calls API A, which calls API B: **doSomething** and calls API C: **perform**. Since API B and API C will jump to other places within the source code, the flow of the program is broken, and this jump is recorded in the control flow graph. \\n\\nThe Control Flow Graph from an app records tens of thousands of these calls, and represents them as edges, where each edge contains two nodes.\\n\\n\\n### Common Graph\\nSince we obtained a large number of CFGs for a large number of APKs, we need to figure out a way to connect all of these graphs so the representations for each API will be the same. We want the API representations to be the same so when we are classifying we know that all feature vectors are built the same way. This is to avoid us creating random feature vectors, and will result in the model classifying randomly. To make sure we are building features correctly, we must create a common graph that links every CFG together. Also, during the testing phase, we can use these node embeddings to build a feature vector for an unseen app.\\n\\nThe common graph we built contains a total of 1,950,729 nodes, and 215,604,110 edges. Our common graph is simply a union of all the control flow graphs that we obtained from separate apps. This is not only to make sure that each distinct API node are consistent throughout our training and testing process, but to make sure that all our CFGs are on the same space. First, all the edges of each separate graphs are extracted, along with their weights and node types of each edge. Then, these information are loaded altogether to become a common graph. \\n\\n![Common Graph Example](/img/common_graph2.png)\\n\\nThe figure above demonstrates what a common graph looks like by combining two control flow graphs from two different apps. On the left we have red and blue applications, which both have five nodes consisting of A, B, and C nodes connected together and other nodes of its own. When combining them, we generate the graph on the right, which merge the shared nodes with each other. Duplicate nodes are joined to be one, while the edges are still preserved. As you can see, the similar A, B, C sequence is preserved as well. This is important when we are building feature vectors for an app. The common graph ensures the same node representations for two graphs. This means that when we are building feature vectors for apps, the same representations are used for two similar apps. Conversely, if two apps are not similar and do not share the same sequences, then their representations will be very different. Like in the common graph in Figure 6, this will make sure that the similarity and differences between the apps are preserved.\\n\\n### Data Generating Process/ETL\\nThe raw data we are investigating is code written by app developers. In order to turn something into a malware, you have to alter the source code, which will allow hackers to plant certain types of malicious code. If a developer were to hijack a device, then the app would need special Root permissions. Often targeting API calls that represent System Calls is one of the ways to alter the source code. For that reason, source code is an essential part in determining whether an application is malicious or not. \\n\\nAs mentioned above, we will be using control flow graphs converted directly from the APK files. We are looking at the sequence of which these system calls are made and define them as meta paths. We were able to obtain one CFG for each APK. We extracted this by using Androguard’s AnalysisAPK method in its misc module which returns an analysis object. Afterwards, we called .get\\\\_call\\\\_graph() on the analysis object to obtain the CFG. At this stage, we also perform some feature extraction specifically on the nodes of the graph. We extract the string representation of these nodes as well as node type. The string representations of nodes is used to build the corpus, and the node type is used to build meta paths. We then exported this graph as a compressed gml file to save on disk. We hypothesize that our method will perform better than our baseline model, since metapath2vec can capture the relations and context within the graphs, giving the feature vector much more information. Also, our metapaths are traversed in the beginning of our process to learn all possible metapaths. Using this method, we ensure that the model learns the different sequences that a malware could have, and use this information in future classification. \\n\\n### EDA\\nWe have a total of 8435 malicious software and a total of 802 benign applications, which is a combination of popular apks and random apps. While generating control flow graph objects from the APK files, there was an error of “*Missing AndroidManifest.xml*,” so we were not able to generate those graphs and will be working with fewer benign apps \\\\footnote{We looked into why there might be missing Android Manifest files error, interestingly, we found that some of the apps having this issue contain the manifest while some do not. However, the apps that have this issue do not decompile correctly, and do not create a graph correctly as well.}. To counteract the imbalance between malware and benign apps, we calculated class weights and used it in the classification process to ensure we are penalizing the model in a balanced way.\\n\\n![Benign vs. Malicious](/img/bm_counts.png)\\n\\nTo further understand our benign and malicious data, we perform analysis on these graph objects by comparing the node types, as well as the counts of both nodes and edges.\\n\\n![Benign vs. Malicious: Node Type Counts](/img/bv_node_types.png)\\n\\nThe figure above shows the comparisons of node types between benign applications and malicious code. From the two distributions, we can see that malware contains a lot more types of nodes compared to benign apps. Specifically, most of the malware contains five types of node. If we limit the range of that bar, we can see the figure below for a more clear distribution of benign apps. The left distribution also indicates that majority of the benign apps have five types of nodes.\\n\\n![Benign vs. Malicious: Node Type Counts](/img/bv_node_types1.png)\\n\\nBut because of how imbalanced our data is, we plot the number of node types based on the percentage, as shown in the below figure. From this figure we can conclude that over half of both benign and malicious apps have more than 5 types of nodes. \\n\\n![Benign vs. Malicious: Node Type Counts (%)](/img/bv_node_types_p.png)\\n\\nTo further look into what are these node types, we analyze the top node types from both benign and malware separately. The following figures are node types distributions. On the left shows the benign node types spread, which over 50\\\\% of the nodes are Public Node, followed by Node, External Node, Public Static Node, Static Node, and the other types. Similarly, for malware, Public Node is the top most node type found in the graphs. Followed by External Node, Node, Public Static Node, Static Node, and the others. Both benign and malware have similar top nodes.\\n\\n![Benign: Top Node Types and The Counts](/img/counts_pie_b.png)\\n![Malicious: Top Node Types and The Counts](/img/counts_pie_m.png)\\n\\nThe figure below is a scatter plot of number of edges and number of nodes for both benign (red) and malware (blue). Although it seems that benign has a lot more apps in this plot, malware are just all packed together. We can also see that benign apps have larger number of edges and nodes compared to malware, this is because benign apps are larger in terms of APK sizes and that they might be more complicated.\\n\\n![Benign vs. Malicious: Number of Edges and Number of Nodes](/img/bm_edges_nodes.png)\\n\\nSince the figure above has outliers for benign apps and we want to focus on the malware, we limit the range so that it looks like the figure below. From this figure we see that malware is indeed packed together and that they have a lot less edge and nodes compared to the benign apps.\\n\\n![Benign vs. Malicious: Number of Edges and Number of Nodes (Malware Focused)](/img/bm_edges_nodes1.png)\\n\\n\\n## Methods\\n\\n### Feature Extraction\\nFor our baseline, we extract probabilistic sequences from all possible edges of the APK, which serves as the feature vector for classification. For the Metapath2Vec model, we first create a common graph, then traverse it using Metapath2Vec to learn representations of nodes, which is used to build feature vectors. For our Doc2Vec method, we treat each APK as one document, and Doc2Vec produces one feature vector for one document.\\n\\n### Baseline: Mamadroid\\nWe build Mamadroid as our baseline model. As introduced earlier, it extract call graphs that are represented using nodes and edges. With the graphs, it extracts all the possible edges based on the family or package level. It then extract sequences of probabilities of the edges occurring. This probabilistic feature vector is used for classification. We abstract API calls to both Family and Package level.\\n\\nFor example, \\n```\\nExample API call = \"LFamily/Package\"\\nFamily Level = \"LFamily\"\\nPackage Level = \"LFamily/Package\"\\n```\\n\\nIn Family level, there are seven possible families and 100 total possible edges. However, in Package level, there are 226 possible packages and a total of 51,239 possible edges. The number of possible families and packages are found on Android\\'s Developers page. Those families and packages that are not found on that webpage is abstracted to \"self-defined\". Specifically, in family level, we will obtain a feature vector of 100 elements for one app. In package level, we obtain a feature vector with 51,239 elements for one app. These feature vectors are then used for classification. After obtaining the vector embeddings, we classify using Random Forest model, 1-Nearest Neighbors, and 3-Nearest Neighbors.\\n\\n### Metapath2Vec Model Using Common Graph\\nAs mentioned earlier, we also abstracted our API calls to the class level. For example, an API call looks like this: \"Lfamily/package/class; → doSomething()V\" at the class level, it is: \"Lfamily/package/class;\". The reason for this is there could be user-defined classes, which is not picked up in MamaDroid. We hope that we can obtain more information by abstracting to the class level, but not get too much information at the API level which might result in performance issues. We do not abstract anything to be \"self-defined\" as MamaDroid has.\\n\\n1. Run a Depth First Search to explore all the node types that could be in an APK, and create metapaths.\\n2. Build a common graph by combining all the separate control flow graphs representing different apps.\\n3. Perform an uniform metapath walk on the common graph to obtain a huge corpus.\\n4. Perform node2vec to learn node representations of the huge corpus.\\n5. Build feature vectors for each app, using the node embeddings learned from step 3, by combining embeddings of unique nodes of each app.\\n6. Classification using built feature vectors.\\n\\nExplanations: \\nWe run a depth first search to explore all node types and metapaths since we do not know how convoluted an app\\'s CFG may be and we need flexible metapaths for each app. Also, there is the possibility of the malware being intentionally obfuscated. Therefore, we need flexible metapaths for each app, which we will later use as the predefined metapaths in our metapath2vec step. The reason our feature vector is a component wise combination of node embeddings is because when two vectors are added together, a new vector is obtained. As visualized below:  \\n\\n![Vector Addition And Subtraction](/img/vec.png)\\n\\nThis will provide more information about the APKs that we will classify. The model can more easily learn the distinction between similar and different vectors, by the direction and magnitude to where they point. Of course, the component wise combination can also be other aggregations, such as taking an average, percentiles, and dot products. When encountering an unseen app, unique nodes of that app is extracted. Representations of each of the unique nodes are then found from the trained word2vec model, and some component wise combination is performed to obtain a feature vector for classification. \\n\\n### Doc2Vec Model\\n1. For each app, extract all possible metapaths using Depth First Search, as well as perform metapath2vec on that app to obtain a corpus.\\n2. Take each corpus from each app, append them, and turn them into a list of Tagged Documents.\\n3. Run the Tagged Documents into Doc2Vec to obtain a vector representation for each app.\\n4. Take the vector representations for each tagged document, and use them as feature vectors for classification.\\n\\nThe Doc2Vec model is very straight forward, taking in documents and returning representations for those documents. When there is an unseen app, a corpus is extracted from that app using metapath and is treated as a document. This document is then fed into the Doc2Vec model, and a vector representation is \"inferred\" using the .infer_vector() method.  \\n\\n\\n## Results and Analysis\\n### Baseline Results\\nThe following tables are results from our baseline model, MamaDroid, corresponding to Family and Package mode. Surprisingly, our MamaDroid using control flow graphs performs better than its original model. Let\\'s first take a look at the Family mode. The table below is the confusion matrix, we calculated precision and recall scores based on it.\\n\\n| | Random Forest | 1-NN | 3-NN |\\n| --- | --- | --- | --- |\\n| True Negative | 68 | 61 | 59 |\\n| False Negative | 4 | 9 | 8 |\\n| False Positive  | 11 | 18 | 20 |\\n| True Positive | 1269 | 1264 | 1265 |\\n\\nWe compare the results to the original MamaDroid model. In the table below, we list the original MamaDroid results on it as well to better compare it. We see that our version of MamaDroid has better performance in all F1-Score, precision and recall scores, where we obtain an F measure of 0.994 and the original model only has 0.880. Similarly to precision and recall scores, we obtain 0.991 and 0.997, where the original model has 0.840 and 0.920 as their results. In addition to Random Forest, our 1-NN and 3-NN models also outperform the original MamaDroid model. But all our three models have similar results.\\n\\n<table>\\n    <tbody>\\n        <tr>\\n            <td rowspan=2>PCA = 10 Component</td>\\n            <td colspan=2>Random Forest</td>\\n            <td rowspan=2>1-NN</td>\\n            <td rowspan=2>3-NN</td>\\n        </tr>\\n        <tr>\\n            <td>Original</td>\\n            <td>Ours</td>\\n        </tr>\\n        <tr>\\n            <td>F1-Score</td>\\n            <td>0.880</td>\\n            <td>0.994</td>\\n            <td>0.980</td>\\n            <td>0.994</td>\\n        </tr>\\n        <tr>\\n            <td>Precision</td>\\n            <td>0.840</td>\\n            <td>0.991</td>\\n            <td>0.985</td>\\n            <td>0.994</td>\\n        </tr>\\n        <tr>\\n            <td>Recall</td>\\n            <td>0.920</td>\\n            <td>0.997</td>\\n            <td>0.994</td>\\n            <td>0.994</td>\\n        </tr>\\n    </tbody>\\n</table>\\n\\nNext, we have results for our MamaDroid Package mode. The following table is the confusion matrix. The numbers are close to what we obtain for Family level. However, the true negatives for all three similarity-based models are slightly larger.\\n\\n| | Random Forest | 1-NN | 3-NN |\\n| --- | --- | --- | --- |\\n| True Negative | 70 | 70 | 70 |\\n| False Negative | 1 | 6 | 1 |\\n| False Positive  | 15 | 15 | 15 |\\n| True Positive | 1266 | 1261 | 1266 |\\n\\nWe also compared the results of Package mode to the original MamaDroid results. In the table shown below, we also list out the results of original MamaDroid on the left to compare it with the ones we obtain. As a result, our model was able to achieve an F measure of 0.993, precision score of 0.988, and recall score of 0.999, whereas the original model only has performance of 0.940, 0.940, and 0.950. Both 1-NN and 3-NN models also have very similar numbers as Random Forest model.\\n\\n<table>\\n    <tbody>\\n        <tr>\\n            <td rowspan=2>PCA = 10 Component</td>\\n            <td colspan=2>Random Forest</td>\\n            <td rowspan=2>1-NN</td>\\n            <td rowspan=2>3-NN</td>\\n        </tr>\\n        <tr>\\n            <td>Original</td>\\n            <td>Ours</td>\\n        </tr>\\n        <tr>\\n            <td>F1-Score</td>\\n            <td>0.940</td>\\n            <td>0.993</td>\\n            <td>0.992</td>\\n            <td>0.994</td>\\n        </tr>\\n        <tr>\\n            <td>Precision</td>\\n            <td>0.940</td>\\n            <td>0.988</td>\\n            <td>0.988</td>\\n            <td>0.988</td>\\n        </tr>\\n        <tr>\\n            <td>Recall</td>\\n            <td>0.950</td>\\n            <td>0.999</td>\\n            <td>0.995</td>\\n            <td>0.999</td>\\n        </tr>\\n    </tbody>\\n</table>\\n  \\n### Metapath2Vec/Common Graph (Partial)\\nFor our Metapath2Vec model, we unfortunately do not have the complete results due to large computational time it takes to build the common graph with all the data we have. The complete common graph consists of 1,950,729 nodes, and 215,604,110 edges. However, we did obtain results working with a smaller subset of the Common Graph, consisting of: 87,539 nodes and 15,617,223 edges.\\n\\n| | Random Forest | 1-NN | 3-NN |\\n| --- | --- | --- | --- |\\n| True Negative | 94 | 89 | 73 |\\n| False Negative | 16 | 31 | 36 |\\n| False Positive  | 20 | 25 | 41 |\\n| True Positive | 1679 | 1664 | 1659 |\\n\\nWe tested on the entire test set, and surprisingly the performance was okay. Initially, we thought that there might be an error, however, upon inspecting our code, we were traversing the smaller common graph correctly. We believe we can obtain this result because of the large amount of edges in the common graph as well as our walk length of 500. We set the walk length to 500 to compensate for the smaller subset of graphs that we are using, and therefore can capture more information per walk. Because of this, the Word2Vec model can learn more about those nodes and provide a better representation. \\n\\n| | Random Forest | 1-NN | 3-NN |\\n| --- | --- | --- | --- |\\n| TF1-Score | **0.989** | 0.983 | 0.977 |\\n| Precision | 0.992 | 0.966 | 0.982 |\\n| Recall | 0.642 | 0.959 | 0.959 |\\n\\nEven though the F1-Scores were high, our True Negative and False Positives are higher than our baseline and Doc2vec model. This is again due to the smaller subset that we are using for this experiment. Because of the smaller subset, we do not have a lot of representations for nodes. There could have been some nodes that the word2vec model has never seen before, and therefore cannot infer a good representation for it. \\n\\n### Doc2Vec\\nThe following tables are the results for Doc2Vec with our similarity-based models: Random Forest, 1-NN, and 3-NN. Table below is the confusion matrix, which is used to compute the precision and recall scores. The Doc2Vec performed worse than the baseline model on the Random Forest model, and it seems like it was struggling with classifying benign apps.\\n\\n| | Random Forest | 1-NN | 3-NN |\\n| --- | --- | --- | --- |\\n| True Negative | 109 | 56 | 43 |\\n| False Negative | 606 | 68 | 71 |\\n| False Positive  | 5 | 58 | 31 |\\n| True Positive | 1089 | 1627 | 1664 |\\n\\nIn the table below, we have our F measure, precision, and recall scores. We notice that our Random Forest model only has an F measure of 0.781, which is a lot lower than our baseline. On the other hand, both our k Nearest Neighbors perform much better than Random Forest. The Random Forest classifier has an emphasis on certain features when training, and focuses on some feature more than others. However, the 1-NN and 3-NN models both look at an unseen vector\\'s closest neighbors, therefore utilizing all the features in the vector. We believe this is why the 1-NN and 3-NN models performed better in this experiment.\\n\\n| | Random Forest | 1-NN | 3-NN |\\n| --- | --- | --- | --- |\\n| TF1-Score | 0.781 | 0.963 | **0.970** |\\n| Precision | 0.992 | 0.966 | 0.982 |\\n| Recall | 0.642 | 0.959 | 0.959 |\\n\\n\\n## Conclusion\\nIn conclusion, our baseline model is able to achieve a better performance than the original work that we have studied. Although our Doc2Vec did not perform better than the baseline Random Forest model, our k Nearest Neighbors models performed almost as good as our baseline. From this, we can see that Control Flow Graphs might be a good choice when it comes to choosing representations for source code. Again, control flow graphs show the jumps in code. From our EDA: Figure 12, even though malware has a small number of nodes, they have a large amount of edges. This means that there could be lots of instances where the program is jumping around in the source code. All this is recorded in the CFG representation and could provide much more information about an APK.\\n\\n Although we successfully created a complete common graph, we were unable to obtain all the node embeddings from it due to time and memory constraints. Therefore we built a smaller common graph to see how it performs. If time and resources allowed, we hope to finish the metapath traversal of the complete common graph. Judging from the results using the smaller common graph, if we were to scale up the model might out-perform our baseline.\\n\\nFor our future work, we plan on investigating other vector embeddings technique and perhaps instead of using only similarity-based models, we could also implement graph neural networks (GNN). In addition to neural networks, we are also interested in graph classification specifically. Since our data format is already in the form of multiple apps, it can be easy to normalize and transform data for a GNN model. Of so many researches we have seen on malware detection, not a lot of them uses control flow graphs as their input data. Since our experiments confirmed that using control flow graphs is not any worse than using other forms of data, we are curious to know if control flow graphs can outperform in other models.\\n\\n## Acknowledgement\\nWe would like to express our gratitude to our mentors for our Capstone project: Professor Aaron Fraenkel, who provided us with lots of resources and ideas throughout the entire process of our research, and Shivam Lakhotia, who guided us through the project and assisted us every week. \\n\\n## References\\n[1] MamaDroid, https://arxiv.org/pdf/1612.04433.pdf\\n\\n[2] Hindroid, https://www.cse.ust.hk/~yqsong/papers/2017-KDD-HINDROID.pdf\\n\\n[3] Metapath2Vec, https://ericdongyx.github.io/papers/KDD17-dong-chawla-swami-metapath2vec.pdf\\n\\n[4] Word2Vec, https://radimrehurek.com/gensim/models/word2vec.html\\n\\n[5] Doc2Vec, https://radimrehurek.com/gensim/models/doc2vec.html\\n\\n[6] Androguard, https://androguard.blogspot.com/2011/02/android-apps-visualization.html\\n\\n[7] StellarGraph, https://github.com/stellargraph/stellargraph\\n\\n[8] SDK, https://developer.android.com/studio/releases/platforms\\n\\n[9] x2vec, https://iopscience.iop.org/article/10.1088/2632-072X/aba83d/pdf\\n\\n[10] Learning Embeddings of Directed Networks with Text-Associated Nodes—with Application in Software Package Dependency Networks, https://arxiv.org/pdf/1809.02270.pdf\\n\\n',\n",
       "  'The authors of this research paper investigate the effectiveness of different data analysis methods for detecting certain types of malware. They focus on analyzing control flow graphs and using similarity-based methods such as k-nearest neighbors and random forest classifiers to detect malware. They also compare their results to a baseline model called MamaDroid, which extracts call graphs from APKs and uses probabilistic feature vectors for classification. The authors also explore the use of metapath2vec and doc2vec models for feature extraction. They analyze the data, evaluate the performance of their models, and discuss future work and potential improvements.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_59': ['![Docker Cloud Build Status](https://img.shields.io/docker/cloud/build/rcgonzal/m2v-adversarial-hindroid)\\n\\n# m2vDroid: Perturbation-resilient metapath-based Android Malware Detection\\nAn extension of the [HinDroid malware detection system](https://www.cse.ust.hk/~yqsong/papers/2017-KDD-HINDROID.pdf), but using [metapath2vec](https://ericdongyx.github.io/metapath2vec/m2v.html) to encode apps in the Heterogeneous Information Network. We then hope to make the model resilient to adversarial ML like Android HIV. See our [blog post](https://rcgonzalez9061.github.io/m2v-adversarial-hindroid/) for more details on our methods.\\n\\n## Setup and Usage\\nTo recreate our results, please use our Docker image: `rcgonzal/m2v-adversarial-hindroid` and have access to a directory of Android apps decompiled into smali. (This can be done with the [APKtool](https://ibotpeaches.github.io/Apktool/) and [Smali](https://github.com/JesusFreke/smali) -- included with our Docker image)\\n\\nOur project can be run using `python run.py [data] [analysis] [model]` with each tag corresponding to different workflows and being executed in the order shown.\\n\\nThe `data` flag will trigger our ETL workflow. It parses apps, constructs the HIN, and then generates `features.csv` using metapath2vec. It will read additional parameters from `config/etl-params/etl-params.json`. Note that including `data_source` will search for apps in that directory and subdirectories, creating `app_list.csv`. Otherwise, an `app_list.csv` can be specified by simply placing it within `outfolder`:\\n\\n```json\\n{\\n    \"outfolder\": \"Path where graph data will be saved\",\\n    \"parse_params\": {\\n        \"data_source\": \"Path to folder of decompliled apps, optional.\",\\n        \"nprocs\": \"Number of threads to use when parsing\",\\n        \"recompute\": \"Boolean, whether or not to reparse apps. Default, skips apps that exist in app data heap\"   \\n    },\\n    \"feature_params\": {\\n        \"walk_args\": \"Arguments for stellargraph.data.UniformRandomMetaPathWalk\",\\n        \"w2v_args\": \"Arguments for gensim.models.Word2Vec, excl. walks\"\\n    }\\n}\\n```\\n\\nThe `analysis` flag will generate analysis on our data and any necessary plots, reading in additional parameters from `config/analysis-params/analysis-params.json`. `jobs` is a dictionary of jobs to be performed along with their parmeters. Currently the only available job is `plots`:\\n\\n```json\\n{\\n    \"data_path\": \"path to folder of data to load (akin to the outfolder in etl-params)\",\\n    \"jobs\": {\\n        \"plots\": {\\n            \"update_figs\": \"Boolean, whether or not to update figures in report and blog post\",\\n            \"no_labels\": \"Boolean, whether or not to include class labels on plots\"\\n        }\\n    }\\n}\\n```\\n\\n',\n",
       "  'The m2vDroid project is an extension of the HinDroid malware detection system that uses metapath2vec to encode apps in the Heterogeneous Information Network. The goal is to make the model resilient to adversarial machine learning attacks like Android HIV. To recreate the results, use the Docker image rcgonzal/m2v-adversarial-hindroid and have access to a directory of Android apps decompiled into smali. The project can be run using python run.py [data] [analysis] [model], with each tag corresponding to different workflows. The data flag triggers the ETL workflow, while the analysis flag generates analysis on the data and necessary plots.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_38': ['### Background\\n\\nThe purpose of phrase mining is to extract high-quality phrases from a large amount of text corpus. It identifies the phrases instead of an unigram word, which provides a much more understanding of the text.  In this study, we apply AutoPhrase method into two different datasets and compare the decreasing quality ranked list of phrase ranked list in multi-words and single word. Our datasets are from the abstract of Scientific papers in English with the English knowledge base from Wikipedia. Through this project, we will be able to understand the advantages of the AutoPhrase method and how to implement Autophrase in two datasets by identifying different outcomes it produces. \\n\\n\\n### Requirements\\n##### If you run in the local:\\nLinux or MacOS with g++, Java and gensim installed.\\n\\n\\n##### You can also use our docker images to run the code. No need any install. It stores in submission.json file.\\n\\n\\n### Purpose of the Code\\n\\nFor Final Replication, our code would do the data ingestion proportion first, to pull data as the input corpus for future use from the cloud. Then to perform some basic EDA on it. We would run the autophrase algorithm along with phrasal segmentation, analyzing the results. At the end, we manually label the high-quality phrases and select 3 phrases and put those into the phrase embedding model to return five most similar phrases as the result.\\n\\n### Code Content\\nSome Python Scripts, involved in etl.py, eda.py, auto.py,visual.py, and example.py to download, process data, analyze, visualize data and find the most similar phrase by building the model.\\n\\n\\t\\n### How to Run the Code\\n\\n##### To get the data:     -      run python run.py etl\\n\\n\\nThis downloads the data from Illinois University in the directory specified in config/etl-params.json and do data cleaning process. You can find the result in the data/raw folder.\\n\\n\\n\\n\\n##### To do the EDA for the data     -       run python run.py eda\\n\\n\\nThis performs exploratory data analysis and saves the figures in the location specified in config/eda-params.json. You can find the graphs in the data/eda folder.\\n\\n\\n\\n##### To run autophrase algorithm and get the segementation result       -        run python run.py auto\\n\\n\\nThis performs autophrase algorithm and phrasal segmentation saves the results in the location specified in config/auto-params.json. You can find the result in the data/output folder.\\n\\n\\n\\n##### To analyze the output of autophrase           -         run python run.py visual\\n\\n\\nThis performs analysis on the results and saves the figures in the location specified in config/visual-params.json. You can find the two distributions in the data/output folder.\\n\\n\\n##### To find the most 5 similar phrases           -         run python run.py example\\n\\n\\nThis ask the users to manually label the high-quality phrase. It builds the word2vec model on the phrasal segmentation results to obtain phrase embedding based on random sampleing. It also report the top-5 similar phrases based on the 3 high-quality phrases from your previous annotations.However, if the users want to try their own sampling, they can manually label the high-quality phrases in sample.txt, which stored the output file by changing the configuration. You can find the result in the data/output/example folder.\\n\\n\\n##### To run whole project       -          run python run.py all\\n\\nIt will complete the whole process with results. The defualt is the dataset DBLP.txt, if you want to try other dataset, edit the configuation file to your own dataset. All of the input and result can find the in the data folder.\\n\\n\\n##### To make a test run.          -        run python run.py test\\n\\nIt will implement dataset DBLP.5k.txt, which is a test data to check the whole process is working. DBLP.5k.txt is sampled from the original dataset DBLP.txt. This compares the result between tf-idf scores, autophrase quality scores, and their multiplication.\\n\\n\\n### Notebook Contents\\nIn the notebook, it will help the users visulized all the results from the run.py with some brief explanations.\\n\\n\\n\\n### Work Cited\\n\\nProfessor Jingbo Shang’s Github: https://github.com/shangjingbo1226/AutoPhrase\\n\\n\\nJingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, Jiawei Han, \"Automated Phrase Mining from Massive Text Corpora\", accepted by IEEE Transactions on Knowledge and Data Engineering, Feb. 2018.\\n\\n\\n\\n\\n### Responsibilities\\nWe discussed the general idea of the replication project and outlined the steps of the process together.\\n\\n\\nTiange Wan: some of code portion and report portion, and revised the report portion.\\n\\n\\nYicen Ma: some of code portion and report portion, and revised the code portion.\\n\\n\\nAnant Gandhi: participated in another repo\\n\\n\\n\\n\\n\\n',\n",
       "  'The purpose of this study is to extract high-quality phrases from a large text corpus using the AutoPhrase method. The study compares the quality of phrases ranked in multi-words and single words using two different datasets. The code provided performs data ingestion, exploratory data analysis, autophrase algorithm, phrasal segmentation, and analysis of the results. It also allows users to manually label high-quality phrases and find the five most similar phrases using a phrase embedding model. The code can be run using different commands specified in the instructions. The notebook provides visualizations of the results obtained from running the code. The work cited includes a GitHub repository and a research paper on automated phrase mining. The responsibilities for this project were divided among the team members.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_42': ['# Analyzing Movies Using Phrase Mining\\n\\nhttps://a04-capstone-group-02.github.io/movie-analysis-webpage/\\n\\n## Setup\\n\\n### Clone the repository\\n\\n```\\ngit clone --recursive https://github.com/A04-Capstone-Group-02/movie-analysis.git\\n```\\n\\n### Download dataset\\n\\nDownload the [CMU Movie Summary Corpus dataset](http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz) and move its files to `data/raw/`, or run the `download` target.\\n\\nNote that to run this repository on the UCSD DSMLP server, the dataset must be manually uploaded, since the DSMLP server cannot connect to the data source link.\\n\\n### Docker\\n\\nBuild a docker container with the `Dockerfile` or the remote image `991231/movie-analysis` in the docker hub.\\n\\n### Note\\n\\nTo run the `clustering` target, we highly recommend enabling GPU to ensure reasonable running time, since this target heavily interacts with a transformer model. Running other targets without GPU will not be an issue.\\n\\n## Run\\n\\nExecute the running script with the following command:\\n\\n```\\npython run.py [all] [test] [download] [data] [eda] [classification] [clustering]\\n```\\n\\n### `all` target\\n\\nRun `data`, `eda`, `classification` and `clustering` targets in this exact order.\\n\\n### `test` target\\n\\nRuns the same 4 targets in the same order as the `all` target, but using the test data in `test/data/raw` and the test configurations.\\n\\n### `download` target\\n\\nDownload the CMU Movie Summary Corpus dataset and set up the `data` directory.\\n\\n### `data` target\\n\\nRun the ETL pipeline to process the raw data. This target will run AutoPhrase to extract quality phrases, clean categories, combine the processed data into a dataframe, and generate a profile report of the dataset.\\n\\nThe configuration file for this target is `etl.json` (or `etl_test.json` for `test` target), which contains the following items:\\n\\n- `data_in`: the path to the input data (relative to the root)\\n- `false_positive_phrases`: phrases to remove from the quality phrase list\\n- `false_positive_substrings`: substrings to remove from the quality phrase list\\n\\nThe configuration file for the AutoPhrase submodule is `autophrase.json`, which contains the following items:\\n\\n- `MIN_SUP`: the minimum count of a phrase to include in the training process\\n- `MODEL`: the path to the output model (relative to the root)\\n- `RAW_TRAIN`: the path to the raw corpus for training (relative to the root)\\n- `TEXT_TO_SEG`: the path to the raw corpus for segmentation (relative to the root)\\n- `THREAD`: the number of threads to use\\n\\n### `eda` target\\n\\nRun the EDA pipeline. This target will find the temporal change of quality phrase distributions and generate visualizations to show the findings.\\n\\nThe configuration file for this target is `eda.json` (or `eda_test.json` for `test` target), which contains the following items:\\n\\n- `data_in`: the path to the input data (relative to the root)\\n- `data_out`: the path to the output directory (relative to the root)\\n- `example_movie`: example movie to profile\\n- `year_start`: the earliest year to analyze\\n- `year_end`: the latest year to analyze\\n- `decade_start`: the earliest decade to analyze\\n- `decade_end`: the latest decade to analyze\\n- `phrase_count_threshold`: the minimum count of a quality phrase to be included in the analysis\\n- `stop_words`: the stop words to ignore in the analysis\\n- `compact`: whether to output a full or compact visualization\\n- `n_bars`: number of bars to display in the bar plots\\n- `movie_name_overflow`: number of characters in visualization until ellipses\\n- `dpi`: subplot dpi (dot per inches)\\n- `fps`: fps (frame per second) of the bar chart race animation\\n- `seconds_per_period`: the time each subplot will take in the bar chart race animation\\n\\n### `classification` target\\n\\nRun the classification pipeline. This target will transform the data into a TF-IDF matrix, fit a one-vs-rest logistic regression as the classifier and tune the parameters if specified.\\n\\nThe configuration file for this target is `classification.json`, which contains the following items:\\n\\n- `data`: the path to the input data (relative to the root)\\n- `baseline`: a boolean indicator to specify running baseline (true-like) or parameter tuning (false-like)\\n- `top_genre`: a number to specify the number of genres in the final output plot, default is 10 \\n- `top_phrase`: a number of specify the number of words/phrases in the final output plot, default is 10\\n\\n### `clustering` target\\n\\nRun the clustering pipeline. This target will pick representative sentences based on average sublinear TF-IDF score on the quality phrases, calculate document embeddings by average the Sentence-BERT embeddings of the representative sentences, and visualize the clusters.\\n\\nThe configuration file for this target is `clustering.json`, which contains the following items:\\n\\n- `clu_num_workers`: the number of workers to use\\n- `clu_rep_sentences_path`: the path to the checkpoint representative sentences file (relative to the root), or an empty string `\"\"` to disable the checkpoint\\n- `clu_doc_embeddings_path`: the path to the checkpoint document embeddings file (relative to the root), or an empty string `\"\"` to disable the checkpoint\\n- `clu_dim_reduction`: the dimensionality reduction method to apply on the document embeddings for visualization, choose one from `{\"PCA\", \"TSNE\"}`\\n- `clu_sbert_base`: the sentence transformer model to use, can be either a pretrained model or a path to the saved model\\n- `clu_sbert_finetune`: enable finetuning or not\\n- `clu_sbert_finetune_config`: configurations for finetuning, will only be used if finetuning is enabled\\n  - `train_size`: total number of training pairs to sample\\n  - `sample_per_pair`: number of training pairs to sample per sampled document pair\\n  - `train_batch_size`: batch size for training\\n  - `epochs`: number of epochs to train\\n- `clu_num_clusters`: number of clusters to generate\\n- `clu_num_rep_features`: number of top representative features to store\\n- `clu_rep_features_min_support`: the minimum support of a feature to be analyzed with summarizing the clusters\\n\\n## Contributors\\n\\n- Daniel Lee\\n- Huilai Miao\\n- Yuxuan Fan\\n',\n",
       "  'The given text provides instructions and information about analyzing movies using phrase mining. It includes details on how to set up the project, run different targets, and the configuration files required for each target. The text also mentions the contributors of the project.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_41': [\"# AutoPhrase for Financial Documents Interpretation \\n\\nOur main targets are data preparation, feature encoding, eda (optional), train, report (optional), and test. Users can configure parameters for these targets in the ./config files.\\n\\n\\n## Data Prep\\n\\nThe data preparation target scrapes, cleans, and consolidates companies' 8-K documents. Furthermore, it curates features such as EPS as well as price movements for the given companies.\\n<br />\\n* `data_dir` is the file path to download files: 8-K's, EPS, etc.\\n* `raw_dir` is the directory to the raw data\\n* `raw_8k_fp` is the file path with newly downloaded 8-K's (should be the same as to_dir)\\n* `raw_eps_fp` is the file path with newly downloaded EPS information (should be the same as to_dir)\\n* `processed_dir` is the directory to the processed data\\n* `testing` is the status of whether we are doing testing (by default is false)\\n\\n\\n## Feature Encoding\\n\\nThe feature encoding target creates encoded text vectors for each 8-K: both unigrams and quality phrases outputed by the AutoPhrase method.\\n<br />\\n* `data_file` is the file path with all data files from data prep target: processed, raw, models, etc.\\n* `phrase_file` is the file path to the quality phrases outputted by AutoPhrase\\n* `n_unigrams` sets the top n unigrams to be encoded based on PMI (may not be exacly `n_unigrams` total due to overlap of top unigrams within each class)\\n* `threshhold` takes quality phrases with a quality score above it to be encoded\\n\\n\\n## Train\\n\\nTrains Random Forest models using 3 feature sets on encoded data: baseline, baseline + unigrams, and baseline + phrases. The selected classifier and set model parameters were decided through comparing validation accuracy.  \\n\\n* `data_dir` is the file path with all data files from data prep targed: processed, raw, models, etc.\\n* `input_file` is the file path (from `data_dir`) to outputed files by the feature encoding target\\n* `output_file` is the desired file path to download trained, outputed models\\n* `testing` is the status of whether we are doing testing (by default is false)\\n\\n\\n## EDA and Report (optional)\\n\\nExports Jupyter notebooks to HTML with EDA and result analysis from the models.\\n<br />\\n* `report_name` is the desired name of report\\n* `data_dir` is the file path with all data files from data prep target: processed, raw, models, etc.\\n* `notebook_dir` is the file path containing the repo's notebooks\\n* `notebook_file` is the desired file path (from `notebook_dir`) of outputed notebook\\n* `report_dir` is the desired directory our outputed HTML report\\n* `report_file` is the desired file name of outputed report in `report_dir`\\n\\n\\n## Test\\n\\nTest target will run the whole project with only test data\\n\\n\\n## Correct order of excution\\n\\n* data_prep: `python run.py data_prep`\\n* feature_encoding: `python run.py feature_encoding`\\n* (optional) eda: `python run.py eda`\\n* train: `python run.py train`\\n* (optional) report: `python run.py report`\\n\\n\\n## Project Links\\n\\n* [Project Website](https://shy218.github.io/dsc180-project/)\\n* [AutoPhrase](https://github.com/shangjingbo1226/AutoPhrase)\\n\",\n",
       "  \"The AutoPhrase for Financial Documents Interpretation project has several targets: data preparation, feature encoding, exploratory data analysis (optional), training, reporting (optional), and testing. Users can configure parameters for each target in the config files. \\n\\nThe data preparation target involves scraping, cleaning, and consolidating companies' 8-K documents. It also curates features such as EPS and price movements for the companies. \\n\\nThe feature encoding target creates encoded text vectors for each 8-K using unigrams and quality phrases generated by the AutoPhrase method. \\n\\nThe training target trains Random Forest models using three feature sets: baseline, baseline + unigrams, and baseline + phrases. The selected classifier and model parameters are determined by comparing validation accuracy.\\n\\nThere is an optional EDA and report target that exports Jupyter notebooks to HTML with exploratory data analysis and result analysis from the models.\\n\\nThe test target runs the entire project with only test data.\\n\\nThe correct order of execution is: data_prep, feature_encoding, (optional) eda, train, (optional) report.\\n\\nProject links include a project website and the AutoPhrase repository on GitHub.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_40': [\"# DSC180B-NER-Project\\nThis project focuses on the task of document classification using a BBC News Dataset and a 20 News Group Dataset. We implemented various feature based classification models and compared the results. We have analysed the advantages and shortcomings of each method.\\n\\n## Webpage\\n* https://dsc180b-a04-capstone-group-06.github.io/News-Classification-Webpage/\\n\\n## Datasets Used\\n* BBC news: https://www.kaggle.com/pariza/bbc-news-summary </br>\\n  * Download this dataset\\n* 20 news group: http://qwone.com/~jason/20Newsgroups/ \\n  * This dataset is fetched by using the sklearn package\\n## Environment Required\\n* Please use the docker image: ``` littlestone111/dsc180b-ner-project  ```\\n\\n## Run\\n```\\n$ launch-180.sh -i littlestone111/dsc180b-ner-project -G [group]\\n$ python run.py [all] [preprocessing] [autophrase] [model] [test]\\n```\\n\\n```test``` :        target will build the Tf-Idf models on the small subset of 20 new groups dataset and save the models to the model folder.</br>\\n```all```:          target will run everthing inlcuded in project, and return the final prediction on the test dataset for document classification.</br>\\n```preprocessing```: target will preprocess 20 news group data for AutoPhrase, so that they can be used for training the model.</br>\\n```autophrase```:   target will run Professor Shang's Autophrase model to extract quality phrases from the dataset.</br>\\n```model```:        target will build the SVM+ALL+TF-IDF combined vocab list model for 20 news group dataset. </br>\\n\\n</br>\\nOutput: <br>\\n\\n* ```model.pkl```: the parameter of the final model.\\n\\n## Group Members\\n* Rachel Ung\\n* Siyu Dong\\n* Yang Li\\n\\n## Our Findings\\n\\nThe BERT classification on the five-class BBC News dataset does not outperform any of our implemented models. From our results table, we observed that our models have F1-Score and Accuracy performances at around 0.95, indicating they are high-performing classifiers. The best of them is the SVM+ALL(TF-IDF) classifier, or the Support Vector Machine with the All Vector Vocabulary List and Tf-Idf Representations, which uses the vocabulary from both NER results and AutoPhrase results. Because the quality phrases between different domains are likely to differ, we expect these results to be optimal features for our predictors. \\n\\nFor the 20 News Group dataset, the SVM+ALL(TF-IDF) classifer also outperformed the other models, with the F1-Score and Accuracy being 0.84. Considering the classes are huge (i.e. 20 classes), these results verify our model is high-performing. Applying our best model on the five-class BBC News dataset, we attained a F1-Score at 0.9525, and Accuracy at 0.9528; while for the 20 News Group, we yielded a F1-Score at 0.8463 and Accuracy at 0.8478. \\n\\n\\n\\n\\n\",\n",
       "  'This project focuses on document classification using the BBC News Dataset and the 20 News Group Dataset. Various feature-based classification models were implemented and compared. The advantages and shortcomings of each method were analyzed. The best performing model was the SVM+ALL(TF-IDF) classifier, which used a combination of NER results and AutoPhrase results as features. For both datasets, this model achieved high F1-Score and Accuracy values.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_39': ['# AutoLibrary - A Personal Digital Library to Find Related Works via Text Analyzer\\r\\n- Website: https://yichunren.pythonanywhere.com/autolibrary/\\r\\n- DSC180B - Capstone Project (Winter 2021)\\r\\n- Section A04 Group03: Yichun Ren, Jiayi Fan, Bingqi Zhou\\r\\n- Note: This is an application of [AutoPhrase](https://github.com/shangjingbo1226/AutoPhrase) by Jingbo Shang.\\r\\n\\r\\n## Docker\\r\\n- The docker repository is `jfan1998/dsc180a-docker`.\\r\\n- Note: The docker uses dsmlp base container. Please login to a dsmlp jumpbox before entering the command below.\\r\\n```\\r\\nlaunch-scipy-ml.sh -i jfan1998/dsc180a-docker:latest\\r\\n```\\r\\nUse port-forwarding on dsmlp to open the website:\\r\\n  - Instruction: https://docs.google.com/document/d/15ehCaVIKSXwgh2jvH3034l5uSPNLrZRgkczwl-xWNEU/edit?usp=sharing\\r\\n\\r\\n## Website\\r\\n- Our published website enables users to upload papers via URLs/Local Machine\\r\\n- If you are curious about our code for published website, you can switch the branch to website.\\r\\n\\r\\n## Local Run\\r\\n- Please refer to `requirements.txt` to check if all the packages and libraries needed are installed.\\r\\n### Default Run: open AutoLibrary website\\r\\n```\\r\\npython run.py\\r\\n```\\r\\n- The home page of website: `http://127.0.0.1:8000/autolibrary/`\\r\\n### Target 1: Convert the input .pdf file into .txt\\r\\n```\\r\\npython run.py data\\r\\n```\\r\\n### Target 2: Run AutoPhrase on the input file\\r\\n```\\r\\npython run.py autophrase\\r\\n```\\r\\n### Target 3: Apply weight to the quality scores of phrases according the corresponding quality score in its domain\\r\\n```\\r\\npython run.py weight\\r\\n```\\r\\n### Target 4: Webscrape the search results on Semantic Scholar with keywords and domains\\r\\n```\\r\\npython run.py webscrape\\r\\n```\\r\\n### Target 5: Convert jupyter notebooks to html\\r\\nNote: Files with human annotations are in the references directory.\\r\\n```\\r\\npython run.py report\\r\\n```\\r\\n### Target 6: Test all previous targets on test data\\r\\nNote: For the test run, raw test data and domain for search is in test/testdata directory.\\r\\n```\\r\\npython run.py test\\r\\n```\\r\\n### Target 7: Activating the website\\r\\n```\\r\\npython run.py website\\r\\n```\\r\\n\\r\\n#### Note for local run:\\r\\nSince AutoLibary does not have access right to your local documents, if you would like to try other papers, please put the papers in ```~/AutoLibrary/website/autolibrary/documents``` and refresh the local website.\\r\\n\\r\\n### Responsbilities: \\r\\n- Yichun Ren: Dataset, Weight, Website Development\\r\\n- Jiayi Fan: Data, Result Analysis, Website Development\\r\\n- Bingqi Zhou: Dataset, Webscrap, Experiments\\r\\n',\n",
       "  'AutoLibrary is a personal digital library that helps users find related works through text analysis. It is a capstone project developed by Yichun Ren, Jiayi Fan, and Bingqi Zhou. The project utilizes AutoPhrase by Jingbo Shang.\\n\\nTo run AutoLibrary locally, make sure all the required packages and libraries are installed as specified in the `requirements.txt` file. The default run opens the AutoLibrary website at `http://127.0.0.1:8000/autolibrary/`. Other targets include converting a PDF file to TXT, running AutoPhrase on the input file, applying weights to quality scores of phrases, webscraping search results on Semantic Scholar, converting Jupyter notebooks to HTML, testing previous targets on test data, and activating the website.\\n\\nThe responsibilities of the team members are as follows:\\n- Yichun Ren: Dataset, Weight, Website Development\\n- Jiayi Fan: Data, Result Analysis, Website Development\\n- Bingqi Zhou: Dataset, Webscraping, Experiments'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_37': ['# Restaurant Recommender System\\nThere are multiple factors that go into a rating: wait time, service, quality of food, cleanliness, or even atmosphere - for example, a restaurant could have positive sentiment towards the food but negative sentiment towards the service. In order to solve this problem, our aim is to include such sentiments that can be found in the review text and turn that into data which can be used to further improve business recommendations to users.\\n\\nThis repository is a recommender system with a primary focus on the text reviews analysis through TF-IDF (term frequency-inverse document frequency) and targeted sentiment analysis with AutoPhrase to attach sentiments to aspects of a restaurant. In building the recommender system, we learned that review texts can hold the same importance as the numerical statistics because they contain key phrases that characterize how they felt about the review. The ultimate goal is designing a website for deploying our recommender system and showing its functionality.\\n\\nVisit our `website` branch to try some queries on a preprocessed Las Vegas / Phoenix dataset!\\n\\n## Important Things:\\n* This repository is contains two branches. The `main` branch contains the source code for our methods. The `website` branch contains the code to run our recommender sebsite on Flask.\\n* In our implementation and analysis, we use the Autophrase as our core NLP analysis method by submoduling into our repository.\\n* The Docker Image and Tag is `launch.sh -i catherinehou99/yelp-recommender-system:latest -c 8 -m 20 -P Always`\\n- If you would like to learn more details about the AutoPhrase method, please refer to the original github repository: https://github.com/shangjingbo1226/AutoPhrase. Namely, you will find the system requirements, all the tools used and detailed explanation of the output.\\n- Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, Jiawei Han, \"**[Automated Phrase Mining from Massive Text Corpora](https://arxiv.org/abs/1702.04457)**\", accepted by IEEE Transactions on Knowledge and Data Engineering, Feb. 2018.\\n\\n## Before You Run:\\n* In order to use Yelp\\'s academic dataset, you will need to go to their [Website](https://www.yelp.com/dataset) and agree to the Terms of Use Agreement before you download the dataset. Save the dataset to the directory `data/raw`\\n* This repo uses AutoPhrase as the git submodule, run the command `git submodule update --init` after cloning this repo.\\n\\n## How to Use this Repository:\\n1. Run the test target in the `main` branch if you would like to test the targets.\\n2. In `config/data-params` you can choose which city you would like to subset. For test target, the city can be Las Vegas or Phoenix.\\n3. Once you successfully run the targets, the generated files will be saved to `data/tmp`. These files need to be used in the `data` folder of the`website` branch.\\n4. If you would like to run the website on Flask, head over to the website branch!\\n\\n## Default Run\\n\\n```\\n$ python3 run.py -- all\\n```\\nThe default running file is run.py and can be simply run through the command line: python3 run.py -- all\\nThis will run all the targets below (data, sentiment, eda, tfidf)\\n\\nFor each of the target:\\n* data: prepares necessary folders, reads in Yelp json files, and filters the dataset to contain rows relevant to the specified city.\\n* sentiment: performs sentiment analysis on the reviews. It will take in the reviews dataframe and output the positive/negative sentences counts.\\n* eda: performs the eda analysis of the dataset and autophrase result.\\n* test: runs the above targets on a test dataset which runs around 3mins.\\n* clean: removes all the files generated with keeping the html report in the `data/eda` folder.\\n\\n```\\n$ python3 run.py -- data\\n```\\nThe default running file is run.py and can be simply run through the command line: python3 run.py -- data\\n* Check if the reference folder exists in the user local drive. If not, create all the necessary folder for projects\\n* Read the dataframes for further analysis.\\n\\n```\\n$ python3 run.py -- sentiment\\n```\\nThe default running file is run.py and can be simply run through the command line: python3 run.py -- sentiment\\n* Perform sentiment analysis on the reviews text\\n* Outputs a city_name.csv in the `data/tmp` folder contains, for each restaurant, the positive phrases and the number of times they were mentioned in a review.\\n\\n```\\n$ python3 run.py -- eda\\n```\\nThe default running file is run.py and can be simply run through the command line: python3 run.py -- eda\\n* Perform the EDA analysis on the AutoPhrase result of individual user\\n* Perform the EDA analysis on individual city review dataset such as sentiment analysis, feature exploration\\n* Convert all the EDA analysis into an HTML report stored under data/eda\\n* After running this tag go to the data/eda directory to see the report.html\\n\\n```\\n$ python3 run.py -- tfidf\\n```\\nThe default running file is run.py and can be simply run through the command line: python3 run.py -- tfidf\\n* Run restaurant-restaurant based recommendation methods using TF-IDF and cosine similarity score\\n* Generate the TF-IDF results CSV file of two cities, Las Vegas and Phoenix and store in the reference folder\\n* Two TF-IDF results CSV are using in the website building as backend database for generating recommendation\\n\\n\\n```\\n$ python3 run.py -- clean\\n```\\nThe default running file is run.py and can be simply run through the command line: python3 run.py -- clean\\n* Remove all the generated files, plots, dataframes under the reference folder\\n* Keep the HTML file in the eda/data folder for report visualization\\n* Be careful: this will clear all the outputs running so far and can not be reversed!!\\n\\n```\\n$ python3 run.py -- test\\n```\\nThe default running file is run.py and can be simply run through the command line: python3 run.py -- test\\n* Run all the targets on the test data set we generated \\n\\n### Responsibilities\\n* Catherine Hou developed the sentiment/eda/data tag and the food query on the website.\\n* Vincent Le created the dockerfile and developed the website (not in main branch).\\n* Shenghan Liu developed TF-IDF/clean/data tag, user AutoPhrase EDA, and the restaurant query on the website.\\n',\n",
       "  \"This repository is a restaurant recommender system that focuses on analyzing text reviews using TF-IDF and sentiment analysis. The goal is to improve business recommendations by incorporating sentiments found in the review text. The repository contains two branches: the main branch with the source code and the website branch for running the recommender system on Flask. The repository also uses AutoPhrase as a core NLP analysis method. To use Yelp's academic dataset, you need to agree to their terms of use agreement and save the dataset to the specified directory. The repository provides several targets for running different tasks such as data preparation, sentiment analysis, exploratory data analysis (EDA), and TF-IDF-based recommendation methods. Each target can be run using the command line. The responsibilities for developing different parts of the repository are also mentioned.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_48': [\"# ForumRec\\n\\n## Introduction\\nThis repository is for the ForumRec project, a recommendation system, that recommends users questions they are adept to answer on the [Super User](superuser.com) forum of [StackExchange](https://stackexchange.com). The website can be reached at [jackzlin.com](jackzlin.com) and the repository for the website can be reached at [ForumRecWeb](https://github.com/okminz/ForumRecWeb).\\n\\n##  Files\\n\\nFor this project, we have files for retriving the data, running the models, and processing it into the desired output. The files are described below and explain the purpose of each part of the repository.\\n\\n> etl.py: Passes in the configs file related to it. The process for taking the data from the data files, extracting the necessary information, and splitting them into questions and answers after a certain date. This is to maintain a higher predictive function and while utilizing as much of the information as e can after extracting.\\n\\n> api_etl.py: Passes in the configs file path related to it. It uses the [Stack Exchange API](https://api.stackexchange.com/) on the Super User forum to gather recent questions and answers from the Super User forum, concatenates the questions and answers and saves them into a continuously updating file so it can be used for new recommendations.\\n\\n> run.py: Runs etl on the data. Runs api_etl on the api parameters. Runs the hybrid model (collaborative and content-based filterling) creation and recommendation files creation along with evaluating the model. Runs the baseline file to get the baseline comparison values. Can also run all of these files on test data.\\n\\n> create_model.py: Passes in the configs file related to it. This file will run create_inputs.py and gather inputs from that file. It will then use those inputs to a generate a model. \\n\\n> create_inputs.py: Passes in the configs file related to it. This file will use processed questions and answers to generate the necessary matrices and files and save them so that they may be used in create_model.py to create a model.\\n\\n> model.py: Passes in the configs file related to it a boolean parameter to determine if running for baselines (default is False). This file the generated model and gather its inputs in order to make recommendations for based on the interactions between users, questions, answers, text, and tags. These recommendations will be returned in a file. It will also produce the evaluations of the model using precision, recall, auc score, and recriprical rank.\\n\\n> new_user.py: Passes in the configs file related to it. This file will take in user response data from the website in order to gather new and fresh recommendations for the user by fitting partially to the already generated model and replacing it. This script is used for the website's cold start function mostly. \\n\\n> create_baseline.py: Passes in the configs file related to it. Baseline file that return the recommendations given to a user using a simple collaborative filtering model so that it can be compared against the model's recommendations and evaluation metrics.\\n\\n> requirements.txt: Contains the amount of processing resources recommended to run the files and the packages needed and the versions that were used to run all the processes.\\n\\n> LICENSE: A file that contains the reuse and use licenses for this repository.\\n\\n> SuperUser EDA.ipynb: Inside the notebook directory. Notebook containing the exploratory data analyses that was taken on the data to further understand and gain insight on the data we were using, and how we can use the data to build the recommendation system we wanted.\\n> \\n> SuperUser API.ipynb: Inside the notebook directory. Notebook containing the explorationa and trials of using the Stack Exchange API to further understand and gain insight on how to get the desire questions and answers from it. It was used to build the api-etl process.\\n\\n> ETL.ipynb: Inside the notebook directory. Notebook containing the etl processes and the results to look at the etl.\\n\\n> NLP.ipynb: Inside the notebook directory. Notebook containing the natural language processing code that looks at the text data and creates a model through that code.\\n\\n##  Directories\\n\\nThe following directories were created by us in order to be able to store and retain the necessary information needed for different purposes.\\n\\n> config: Contains a list of all the config files that determines the parameters of each file. Use these files according to their use to change the parameters and change which subset of data you are running the processes on. Make sure you are changing the file paths correctly and throughout the entire config file.\\n\\n> data: Location to put the original raw data in. It also would contain a final directory which would contain the data retrieved after running the repository.\\n\\n> test: Contains inside the data directory and inside that the data used to run the repository on a small subset of the data to ensure the models and the scripts are running correctly.\\n\\n> src: Contains inside the src, models, and baselines folders which contain the etl, model creation, input creation, model, new user, and baselines python files that are described above.\\n\\n> notebooks: Contains multiple notebooks that explore the data. The notebooks are described above.\\n\\n## Running the Code\\nPrior to running the code, make sure that you install all the packages listed in *requirements.txt* \\n\\nIn order to obtain the data, one can follow the processes below:\\n\\n### Creating the Data\\n\\nTo create the processed data, run this following command on the command line terminal:\\n```\\npython run.py data\\n```\\nWhere the data will be replaced processed and be returned into new files usable by our models in this project and placed in the data directory.\\n\\n### Gathering data from the API\\n\\nTo get new questions and answers from Super User since the last pull of API data, run this following command on the command line terminal:\\n```\\npython run.py api\\n```\\nWhere the API data will be processed and be returned into a usable continuous file placed in the data directory.\\n\\n### Running the cosine models\\n\\nTo run the hybrid filtering model on the data, run this following command on the command line terminal:\\n```\\npython run.py models\\n```\\nWhere hybrid filtering model and its inputs will be created and used to generate new recommendations and then evaluated using specific metrics.\\n\\n### Comparing the model to baselines\\n\\nTo determine how our model compares to the baselines model, run this following command on the command line terminal:\\n```\\npython run.py baselines\\n```\\nWhere the baseline model will be evaluated and recommendations will be returned using the same data as the hybrid filtering model.\\n\\n### Running all the model targets\\n\\nIf you want to run all of these together, run this following command on the command line terminal:\\n```\\npython run.py all\\n```\\nWhere the all 4 targets (excluding *test*) will run one after another in the order presented above.\\n\\n### Testing all the model targets\\n\\nTo test how if the repository and all the models and scripts are working, run this following command on the command line terminal\\n```\\npython run.py test\\n```\\nWhere all of the targets ('data', 'api', 'models' and 'baselines') above will all be run one after another in the order presented above, but on small test data so that we can observe how the models and scripts are working.\\n\\n## Responsibilities\\nYo Jerimijenko-Conley: \\n\\nJasraj Johl: Created the ETL process, worked with the Super User API, created the code repository and made sure it was clean and runnable, and created the website's design through HTML and CSS integrating it partially with Flask.\\n\\nJack Lin: \\n\",\n",
       "  'This repository is for the ForumRec project, a recommendation system that recommends users questions they are adept to answer on the Super User forum of StackExchange. The repository contains various files and directories for data retrieval, model running, and processing. The code can be run to create processed data, gather data from the API, run the hybrid filtering model, compare the model to baselines, and test all the targets. The responsibilities of each team member are also mentioned.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_47': ['\\n\\n# OnSight: Outdoor Rock Climbing Recommendations\\n\\nRecommendations for outdoor rock climbing has historically been limited to word of mouth, guide books, and most popular climbs. With our project OnSight, we believe we can offer personalized recommendations for outdoor rock climbers.\\n\\nDisclaimer: With rock climbing, especially outdoors, there is an inherent risk that is taken when you decide to climb. Although our recommender tries to offer routes similar to the ones users have done, there is still a risk that the route may be too hard and therefore dangerous. This is not a problem that is solely put on the recommender, but a problem with rock climbing as a whole. There is no standard in climbing grades, but rather it is an agreement among the climbers that have climbed that route. Therefore climbing grades are subjective, and climbs may be harder and more dangerous than a user expects. We realize this, and we encourage everyone to look at the safety information of each climb on its corresponding climbing page on Mountain Project.\\n\\n## How To Run\\n\\nWe suggest for casual users to simply use our website to run the project. The website URL is https://dsc180b-rc-rec.herokuapp.com/. Note that this project runs on a free dyno, so if you are the first user to open the website in about half an hour, then the website may take a minute to load. Be patient!\\n\\nHowever, if you are interested in making changes or diving deep into the code, you can run the project and customize it by either creating your own Heroku project or running the project on the command line.\\n\\n### Creating your own Heroku project\\nTo have your own version of OnSight running on Heroku, do the following steps:\\n 1. Fork the OnSight GitHub repository to your own GitHub account\\n 2. Create a new project on Heroku\\n 3. In the Heroku app dashboard, go to the \"Deploy\" tab\\n 4. Under the \"Deployment method\" section, select GitHub and connect the Heroku app to your forked repository\\n 5. In the Heroku app dashboard, go to the \"Settings\" tab\\n 6. Under the \"Config Vars\" section, click on \"Reveal Config Vars\" and fill in the config variables as shown in the table below. \\n\\n|Config Vars|Value|Description|\\n|-|-|-|\\n|PROJECT_PATH|mysite/|Since the django webserver is stored in the \"mysite/\" folder, but the Procfile (tells Heroku how to start the web server) is in the project root, we need to tell Heroku to look in the \"mysite/\" folder for the webserver code|\\n|GOOGLE_MAPS_API_KEY|Your API key|Your Google Maps API key needs to have the following APIs enabled on the key: \"Maps JavaScript API\", \"Places API\", and \"Maps Embed API\". This key is not strictly necessary, but without the key the map will not work and location can only be entered by manually typing in a latitude and longitude, which is not very UX friendly.|\\n\\n7. Make sure you deploy the Heroku app again from the \"Deploy\" tab on the Heroku dashboard and you should be all set!\\n\\n### Running the Project on the Command Line\\n\\nTo run the project, every command must start with \"python run.py\" from the root directory of the project. By default, \"python run.py\" will do absolutely nothing. You must use at least one of the following flags to actually get some response:\\n\\n|Flag|Type|Default Value|Description|\\n|-|-|-|-|\\n|-d, --data|bool|False|Use this flag to run all data scraping code. This will take a very long time, upwards of one week total to scrape all the data. It is recommended *not* to run this. Be aware that this will only store data locally. |\\n|-c, --clean|bool|False|Use this flag to run all data cleaning code. It is expected that all files defined in the \"state\" key of data_params.json are present in the raw data folder.|\\n|-\\\\-data-config|str|\"config/data_params.json\"|The location at which data parameters can be found|\\n|-\\\\-web-config|str|\"config/web_params.json\"|The location at which web parameters can be found. These parameters simulate a user using the website.|\\n|-\\\\-top-pop|bool|False|Use this flag to print the top N most popular climbs. This does not use locally saved data, but rather uses saved data in MongoDB. Additionally, the exact climbs and number of climbs are determined by the web_params.json file.|\\n|-\\\\-cosine|bool|False|Use this flag to print the top N most similar climbs to the users favorite. This does not use locally saved data, but rather uses saved data in MongoDB. Additionally, the exact climbs and number of climbs are determined by the web_params.json file.|\\n|-\\\\-test|bool|False|Use this flag to run the two implemented models based on default config files. Using the --test flag will override all other present flags and is equivalent to running \"python run.py --top-pop --cosine --debug\".|\\n|-\\\\-delete|bool|False|Use this flag to wipe out all data from MongoDB. This will not do anything since the MongoDB login is set to read only.|\\n|-\\\\-upload|bool|False|Use this flag to upload cleaned data to MongoDB. This will not do anything since the MongoDB login is set to read only.|\\n|-\\\\-debug|bool|False|Use this flag activate various print statements throughout the project.|\\n\\n### Description of Parameters\\n\\n#### Data Parameters\\n\\n|Parameter Name|Type|Default Value|Description|\\n|-|-|-|-|\\n|raw_data_folder|str|\"data/raw/\"|The location at which raw data will be saved. Note that this path is relative to the project root.|\\n|clean_data_folder|str|\"data/cleaned/\"|The location at which clean data will be saved. Note that this path is relative to the project root.|\\n|states|dict|Too long to copy here...|Although the parameter is called states, this is really just the areas to scrape/clean and the urls at which they can be found. The file will be named based on the key string, and the area url to scrape is the value string. By default this contains all 50 states, with the state name as key and state area url as value.|\\n\\n#### Web Parameters\\nBe aware that all these parameters do is simulate a user using the website. Each of the parameters here refer to a form element on the website.\\n\\n|Parameter Name|Type|Default Value|Description|\\n|-|-|-|-|\\n|user_url|str|Too long to copy here...|The \"Mountain Project URL\" form element on the website. This value is only used if the user requests personalized recommendations. The default value is a user with a lot of climbs rated, about 600 in March 2021. You can find the actual default value in the config file.|\\n|location|[float, float]|[43.444918, -71.707888]|The \"Latitude\" and \"Longitude\" form elements on the website. This location is the center of the circle where climbs will be looked for. The default value is some random location in New Hampshire.|\\n|max_distance|int|50|The \"Max Distance (mi)\" form element on the website. This value is the radius of the circle where climbs will be looked for. The default value is 50 miles, which should be sufficient to encompass any climbing area.|\\n|recommender|str|\"top_pop\"|The \"Recommenders\" form element on the website. This is the recommender to use and will be any of \"top_pop\" or \"cosine_rec\". There is an additional hidden debug recommender that uses the string of \"debug\". The debug recommender is not accessible without modifying the \"mysite/bootstrap4/forms.py\" file|\\n|num_recs|int|10|The \"Number of Recommendations\" form element on the website. This is the maximum number of recommendations that will be displayed once the user hits submit.|\\n|difficulty_range|{\"boulder\": [int, int], \"route: [int, int]}|{\"boulder\": [0, 3], \"route\": [11, 16]}|The \"Boulder\", \"V_-V_\", \"Route\", and \"5.\\\\_-5.\\\\_\" form elements on the website. Due to the way data is cleaned, bouldering V grades and route 5. grades are converted to integers starting at 0 on different scales. You can find the scales defined in code. The two default difficulty ranges correspond to V0-V3 and 5.8-5.10d. Note that if the user does not want boulders or routes, the corresponding difficulty range will be [-1, -1]|\\n\\n',\n",
       "  'The project OnSight offers personalized recommendations for outdoor rock climbers. It provides recommendations based on climbs that users have done, but there is still a risk that the recommended route may be too difficult and dangerous. The project can be run on a website or customized by creating your own Heroku project or running it on the command line. The command line options include data scraping, data cleaning, printing popular climbs, printing similar climbs, running test models, deleting data from MongoDB, uploading cleaned data to MongoDB, and activating debug mode. The project also has data parameters for saving raw and clean data, and web parameters for simulating user inputs on the website.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_46': [\"# DSC180b-Capstone\\n\\nProject repository for Recommender Systems group 3\\n\\nThis project is focused on creating music recommendations for users and their parents.\\n\\n# HOW TO RUN\\n\\nOur project's current targets are: load-data, task0, task1, task2, all, test\\n\\nOur project's current config files are: test.json and run.json\\n\\n### Targets\\n\\nload-data: Pulls our training data from a S3 bucket where we store it and creates a new 'data'\\nrepository to store it.\\n\\ntask0: Generates a list of sample parent recommendations and saves them to data/recommendations as a csv file.\\n\\ntask1: Generates a list of parent-user recommendations and saves them to data/recommendations as a csv file.\\n\\ntask2: Generates a list of user-parent recommendations base on a user's listening history. Saves\\nthese recommendations to data/recommendations as a csv file.\\n\\nall: runs load-data, task0, task1, and task2 in succession\\n\\ntest: Runs through the same load-data, task0, task1, task2 pipeline but uses a pre-stored user access code.\\nThis allows us to 'test' our recommendation models without having to authenticate ourselves every\\nsingle time. The test data in this case is a user account that we have permission to read from.\\n\\n### Configs\\n\\nOur configuration files are relatively simple given our project's reliance on listening histories and\\notherwise limited user information. The values in each file are the same, but we have created two files\\nso that logic can be quickly tested without having to constantly change the configuration parameters during\\ndevelopment.\\n\\nusername: The username of the Spotify account that we are creating recommendations for\\nparent_age: The age of the user's parent\\ngenre: The parent's preferred genre of music\\nartist: The parent's preferred artist\\n\\nThis information would normally be provided by users through a form on our website, but for this situation\\nwe just reference configuration files.\\n\\n\\n### IN THE FUTURE\\n\\nWe plan on adding a clean-data target that isolates some of the small preprocessing that our code does: dropping na values\\nsmall transformations, etc.\\n\\nInstead of caching an auth_token for a specific user, we want to just directly load listening history. This would be a more\\nstraightforward procedure, the problem is that we inevitably have to authenticate regardless of if we have our user data predownloaded or not. Spotify has another authentication flow that would better suit itself to this situation, and we will be looking into that in the future.\\n\\n\",\n",
       "  \"This project focuses on creating music recommendations for users and their parents. The project's current targets include loading data, generating sample parent recommendations, generating parent-user recommendations, and generating user-parent recommendations based on a user's listening history. There are also configuration files for specifying the username, parent age, preferred genre of music, and preferred artist. In the future, the project plans to add a target for cleaning data and explore a more straightforward authentication procedure with Spotify.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_45': [\"## Asnapp\\n\\nAsnapp is a workout video recommender web application. \\n\\nAuthors: Amanda Shu, Peter Peng, Najeem Kanishka\\n\\n### Website URL\\nThe website is now live on: https://workout-recommender.herokuapp.com/\\n\\n### Video Demonstration\\nFor a demonstration of the project, visit: https://www.youtube.com/watch?v=QJFg0HguGuI\\n\\n### Data\\nThe data is scraped from https://www.fitnessblender.com/. We are using the data for academic purposes only.\\n\\n### Code Organization\\n\\n- `run.py`: Run to get data and model results.\\n- `app.py`: Runs flask web application.\\n- `workout_db.sql`: Contains sql statements for creation of tables in database.\\n- `requirements.txt`: Python packages required to run project\\n- `wsgi.py & Procfile & runtime.txt`: Entrypoint for Heroku, used in website deployment\\n\\n**Source**\\n- The `src/data` folder contains `scrape.py`, the web-scraping script that writes three data files into `data/raw` folder. `fbpreprocessing.py` takes these raw data files and outputs cleaned/transformed data files into `data/preprocessed` folder. `youtube.py` grabs youtube related data from the Youtube API. `model_preprocessing` reads in preprocessed data and transforms the data into what is needed for model inputs.\\n- `src/models` contains `run_models.py` which trains and evaluates the models. Models are implemented in `lightm_fm.py` and `top_popular.py`\\n- The `src/utils` folder has `clean.py` which implements the standard target `clean`.\\n- The `src/app` folder holds files for the web application. `forms.py` contains wtforms classes for registration/login pages. `recommendations.py` holds code for filtering user preferences and building recommendation lists. `register.py` contains helper functions to create the sql insertion/update statements for registering users and updating their workout preferences.\\n\\n**Static**:\\n- The`images` folder holds a gif ([source](https://www.pinterest.at/pin/512495632597411529/)) used for the loading page. No copyright intended.\\n- The `js` folder contains several javascript files. `overlay.js` is for the display of the popup videos on the recommendation page. `workout_info.js` is for registration and update preferences pages. `rec.js` is for the loading page and recommendation engine logic on the recommendations page.\\n- `libraries/slick` has several files for the carousel, `styles` has a css file, and `favicon.io` is the dumbbell icon\\n- `vendor` holds several javascript files (Bootstrap, JQuery) for styling/theming of the website\\n\\n**Config**: `data-params.json` has file paths outputs for data collection/preprocessing and `test-params.json` has the data paths for the test target. To webscrape, this folder should also include `chromedriver.json`. To gather Youtube data, `api_key.json` specifies the api key. To run the app, `db_config.json` has the database configurations.\\n\\n**Notebook**: `eda.ipynb` is a notebook with exploratory data analysis on scraped data. `param_comparision.ipynb` is a notebook reporting the recommendation models' performance across a couple parameters. `top_popular_extension.ipynb` is a notebook looking into adding Youtube API data into the top popular recommender. `KNN_collab.ipynb` contains results of a KNN collaborative filtering model from surprise package and a pure collaborative filtering from LightFM.\\n\\n**Templates**: Holds html files for the various endpoints.\\n\\n**Testdata/raw**: These are fake datasets meant to be used with the test target.\\n\\n**Docker**: Docker related files. See [here](https://github.com/amandashu/Workout_Recommender/blob/main/docker/README_DOCKER.md)\\n\\n**Materials**: Contains pdfs for presentation slides and a report detailing our methods/implementations.\\n\\n### Set Up Project Environment\\nThere are two ways to run this project: a) Docker (preferred) or b) Locally <br>\\na) To Run in Docker:<br>\\n  1) Pull the container with `docker pull nkanishka/workout-recommender`\\n  2) Run the container using:\\n  * General Use: `docker run -it -p 5000:5000 workout-recommender`\\n  * DSMLP Only: `launch.sh -i nkanishka/workout_recommender_dsmlp -c 4 -m 8` <br><br>`kubectl port-forward <Kubernates Cluster Name> 5000`<br><br> `ssh -N -L 5000:127.0.0.1:5000 <AD Name>@dsmlp-login.ucsd.edu`\\n  3) Inside container/cluster, type `cd Workout_Recommender`. Note that in the DSMLP environment, you will need to manually clone this repo.\\n  4) If using website, go to [localhost:5000](localhost:5000)\\n    <br>\\n\\nb) To run locally, install requiremnents.txt into a virtualenv. Make sure you have Python 3.8+ and Pip installed.\\n\\n### Run the Project Stages\\n- To get the data, run `python run.py data`. This scrapes the data and cleans the data and saves these files into `/data/raw` and `data/preprocessed` respectively.\\n  - Note: for scraping, this assumes that there is a file `config/chromedriver.json` that specifies where the path to the downloaded chromedriver.exe file for your Chrome version lies in the attribute `chromedriver_path`.\\n  - Note: for making requests to Youtube API, this assumes that there is a file `config/api_key.json` that specifies the api key in the `api_key` attribute.\\n- To run model results, run `python run.py model`. This takes in the preprocessed data, trains the models, and prints out the NDCG scores for each model.\\n- Standard target `clean` is implemented, and it will delete the `data` folder.\\n- Standard target `all` is implemented, and it equivalent to running `python run.py data model`.\\n- Standard target `test` is implemented, and runs the data preprocessing and modeling results on the test data. The purpose of this target is purely to check the implementation of the code.\\n- Use `python app.py` to run the app locally.\\n  - Note: this assumes that there is a file `config/db_config.json`, which has database host, user, password, and name information.\\n  - And a file `config/flask_keys.json` which has a Flask secret key\\n\",\n",
       "  'Asnapp is a workout video recommender web application created by Amanda Shu, Peter Peng, and Najeem Kanishka. The website is live at https://workout-recommender.herokuapp.com/ and there is a video demonstration available at https://www.youtube.com/watch?v=QJFg0HguGuI. The data for the application is scraped from https://www.fitnessblender.com/ for academic purposes only. The project code is organized into different files and folders, including source code for data scraping, model training, and web application functionality. The project environment can be set up using Docker or by running it locally. There are also instructions provided for running different stages of the project, such as getting the data and running model results.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_43': ['# Recipe-Recommendation\\n\\n## Introduction\\nThis repository contains the code and models needed to create a recommender system for recipe recommendations. Included in the repository are a few simple baselines as well as the final model utilized by our recommender system for recipe recommendation. The data for this project was pulled from Kaggle (https://www.kaggle.com/shuyangli94/food-com-recipes-and-user-interactions) and (https://www.kaggle.com/kaggle/recipe-ingredients-dataset/home).\\n\\n##  Files\\n\\nFor this project, we have files for running the code, retriving the data, and processing it into the desired output.\\n\\nThere are several files that will be obtained from Kaggle, but are not part of this repository. This is because they are data files and are too large to be version controlled.\\n\\nThe following files were created by us in order to create and run our baselines and final model.\\n\\n> run.py: Passes in the location of the data folder. Runs etl on the Kaggle data and stores the data in the data output folder. Runs baselines and model on the dataset. Evaluates models to see how well the baselines did in comparison to the final model.\\n\\n> mostPop.py: Baseline file that uses a top popular model to determine what users would rate certain recipes.\\n\\n> randomFor.py: Baseline file that uses a random forest model to determine what cuisine type each recipe would fall under.\\n\\n> conBased.py: Baseline file that uses a content based model with cosine similarity to determine what recipe to recommend based on what ingredients are listed by the user.\\n\\n> requirements.txt: Contains the amount of processing resources recommended to run the files within a few hours each, and the packages needed and the versions that were used to run all the processes.\\n\\n> Preliminary EDA.ipynb: Inside the notebook directory. Notebook containing the exploratory data analyses that was taken on the data to further understand and gain insight on the data we were using.\\n\\n##  Directories\\n\\nThe following directories were created by us in order to be able to store and retain the necessary information needed for different purposes.\\n\\n> config: Contains a list of all the config files that determines the parameters of each file. Use these files according to their use to change the parameters and change which subset of data you are running the processes on. Make sure you are changing the file paths correctly and throughout the entire config file.\\n\\n> data: Contains the data retrieved after running the repository, and the cleaned and processed data by us.\\n\\n> testData: Contains the randomly generated testData that we would run our models against to see how well they performed. Much smaller than full dataset, and allows for easy tracking to gain the most insight from how the model works.\\n\\n## Running the Code\\nPrior to running the code, make sure that you install all the packages listed in *requirements.txt* \\n\\nIn order to obtain the data, one can simply run the run.py python file with the command data or all. This will prompt the script to download the data and store it in the proper place for you.\\n\\n### Creating the Data\\n\\nTo create the processed data, run this following command on the command line terminal:\\n```\\npython run.py data\\n```\\nWhere the data will be returned into new files usable by our models in this project and placed in the data directory.\\n\\n### Running all the model targets\\n\\nIf you want to run all of these together, run this following command on the command line terminal:\\n```\\npython run.py all\\n```\\nWhere the all targets (excluding *test*) will run one after another.\\n\\n### Testing all the model targets\\n\\nTo test how if the repository and all the models and scripts are working, run this following command on the command line terminal\\n```\\npython run.py test\\n```\\nWhere the targets above will all be run one after another, but on small test data so that we can observe how the models and scripts are working.\\n\\n## Repository Organization\\n\\nTo ensure the code runs properly, it must have the same folders and files locations. etl.py must be inside src and data folders, and data must be in the data folder. mostPop.py, must be in the src and baselines folders.\\n',\n",
       "  'This repository contains code and models for a recipe recommender system. The data was obtained from Kaggle and there are several files for running the code, retrieving the data, and processing it. The repository includes baselines and a final model for recipe recommendation. There are also directories for config files, data, and test data. To run the code, install the required packages listed in requirements.txt. The data can be obtained by running run.py with the command \"data\" or \"all\". The processed data will be stored in the data directory. To test the models, run run.py with the command \"test\".'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_44': ['# Makeup Recommender\\n\\nThis project is a recommender that will provide a one-stop shop experience where a user will get recommended an array of products to create an entire makeup look based on similar products that the user enjoys, products that similar users have purchased, as well as products that are personalized to the user including skin type, skin tone, allergies, and budget. Our project aims to utilize collaborative filtering recommendations along with content-based filtering to ensure user satisfaction and success when creating their desired look.\\n\\n\\n## How To Run\\n\\nTo install all dependencies, run the following command from the root directory of the project:\\n> ```pip install -r requirements.txt```\\n\\n### Running the Project\\n\\nTo run the project, each command must start from the root directory of the project with:\\n> ```python run.py```\\n\\nThis base command can be modified with various different flags:\\n| Flag                | Type | Default Value             | Description                                                       |\\n|---------------------|------|---------------------------|-------------------------------------------------------------------|\\n| --item_data              |      |                      | Scrape item dataset from www.sephora.com.                              |\\n| --review_data              |      |                      | Scrape review dataset from www.sephora.com.                              |\\n| --features          |      |                           | Clean dataset and create features.                                |\\n| --model             |      |                           | Run model to create recommendations.                              |\\n| --accuracy          |      |                           | Evaluate accuracy of model.                                       |\\n| --test              |      |                           | Train model on a test dataset.                                    |\\n| --all               |      |                           | Train and evaluate accuracy of baseline model and our recommender model.      |\\n\\n\\n## Website\\n\\nTo visit the webpage copy and paste this URL into your browser: https://makeup-recommender.herokuapp.com/\\n\\n\\n## Document History\\n\\n**Project Proposal**: https://docs.google.com/document/d/1bAXSUrQHcss8uU_eeqIJ4ewX3N-I8RLAjGJi77Zqw6c/edit\\n\\n**Check-in**: https://docs.google.com/document/d/18rve8FbhRN8VXoO99ixB6nh96uWYtR-DNQMrjIwCr8Y/edit\\n\\n**Report Checkpoint**: https://docs.google.com/document/d/1Xh3Ddskyy4niA7ZbzBUc9hoaR0bsrLKWgni9YzXTtZY/edit#\\n\\n**Final Report**: https://docs.google.com/document/d/1WRj9ukUa-ozK3xtao-khCdradUQsR1k23tn4ELvQE3U/edit#\\n\\n**Presentation Slides**: https://docs.google.com/presentation/d/1WYmy2IKTuVGE193Pq5t9v2BRTVCKxFHXy5zfLZN1K_I/edit?usp=sharing\\n\\n\\n## Credits\\n\\n### For Usage of Scraping Script\\n\\nhttps://github.com/jjone36/Cosmetic\\n\\n\\n\\n### Responsibilities\\n\\n**Alex Kim**:\\n* Scraped eye products and reviews\\n* Designed website\\n* Cleaned dataset\\n* Encode ingredient data\\n* Write rough draft of report (Abstract, Description of Data, Method, Metrics, Results)\\n* Deploy website on Heroku\\n* Use Flask to connect model to website (not implemented in final product)\\n* Fix UI with Streamlit\\n* Filter recommendations\\n* Edit report\\n\\n**Justin Lee**:\\n* Fixed script to scrape data from Sephora\\n* Scraped lip products and reviews\\n* Coded data ingestion pipeline\\n* Incorporate run.py\\n* Code collaborative filtering model\\n* Create docker container\\n* Deploy website on Heroku\\n* Use Flask to connect model to website (not implemented in final product)\\n* Organize code and documentation\\n\\n**Shayal Singh**:\\n* Fixed script to scrape data from Sephora\\n* Scraped face and cheek products and reviews\\n* Coded website layout\\n* Top popular baseline\\n* Write rough draft of report (Website, Conclusion)\\n* Deploy website on Heroku\\n* Use Flask to connect model to website (not implemented in final product)\\n* Display recommendations\\n* Visual presentation slides\\n* Fix UI with Streamlit\\n* Filter recommendations\\n* Finalize website\\n',\n",
       "  \"The Makeup Recommender is a project that aims to provide users with personalized recommendations for creating a makeup look. The recommender takes into account the user's preferences, such as products they enjoy and similar users' purchases, as well as their skin type, skin tone, allergies, and budget. It utilizes collaborative filtering and content-based filtering techniques to ensure user satisfaction. The project includes instructions on how to run it and a website where users can access the recommender. The document also provides links to the project proposal, check-in, report checkpoint, final report, and presentation slides. The credits section acknowledges the usage of a scraping script and lists the responsibilities of each team member.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_71': ['# Opioid-Use Prevalance Analysis Project \\n```\\n### \\n* Author: Flory Huang\\n* Date: 03.19.2021\\n```\\n\\nThis repository contains code for extraction mentioned drug terms and emotions of drug use discussion in Reddit data. \\nThe code takes in reddit post that are discussing drugs and ontology list(RxNorm and Mesh),used to extract drug names. \\nThe emotion in reddit post will be analyzed.\\n\\nThe data_reddit.py takes care of loading data and formating data\\nThe analysis.py conduct the similar matching to extract drug terms.\\nThe emotion.py will analyze emotion in the reddit post\\nThe model.py procude a report of matching result\\n\\nThe code can be excuted by \\n    - python run.py test \\n    - python run.py data \\n    - python run.py analysis\\n\\nTarget explaination \\n    0: test: test the whole process of code\\n    1: data： read in, text, and ontology data. And process them into format (e.g.parse nouns from text and put into a dictionary with ontology terms) that can be used for following analysis steps.\\n    2: analysis: use similar matching to extract drug terms and perform emotion analysis and store result into output csv files. \\n\\n',\n",
       "  'This repository contains code for extracting drug terms and analyzing emotions in drug use discussions on Reddit. The code takes in Reddit posts discussing drugs and uses an ontology list to extract drug names. The emotion in the Reddit posts is also analyzed. The code can be executed for testing, loading data, or performing analysis.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_63': [\"# DSC180B-Capstone-Project\\n- dataset available for download at https://www.kaggle.com/crawford/gene-expression\\n  - #Make sure to unzip files into 'data/raw' folder#\\n- get data: in the command line enter `Rscript run-data.R data`\\n- analysis: in the command line enter `Rscript run-data.R analysis`\\n  - the resulting graphs will be in the data/out folder \\n\\n- data: contains the raw and cleaned versions of the datasets we're working with. Also will hold the graphs from analysis\\n- src: contains the analysis, cleaning, and data etl scripts.\\n  - analysis: golubAnalysis.R contains the script we used to do tests and generate plots\\n  - cleaning: golubCleaning.R contains the script we used to clean the raw datasets found in data/raw\\n  - data: etl.R contains the scirpt to extract the data for run-data.R\\n  \\nAcknowledgements\\n- Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression\\n\\n  - Science 286:531-537. (1999). Published: 1999.10.14\\n\\n  - T.R. Golub, D.K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J.P. Mesirov, H. Coller, M. Loh, J.R. Downing, M.A. Caligiuri, C.D. Bloomfield, and E.S. Lander\\n\",\n",
       "  \"This is a summary of the DSC180B-Capstone-Project. The dataset can be downloaded from the provided link and should be unzipped into the 'data/raw' folder. To get the data, enter `Rscript run-data.R data` in the command line. To perform analysis, enter `Rscript run-data.R analysis` in the command line. The resulting graphs will be saved in the 'data/out' folder. The project includes raw and cleaned datasets, as well as scripts for analysis, cleaning, and data extraction. The project acknowledges the Molecular Classification of Cancer study by Golub et al., published in Science in 1999.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_77': [\"# DSC180B_A07\\n\\nThis repository contains the completed codes for this project that focuses on multiple testing. The src folder contains the main python scripts, with all the functions inside, and the data folder, where all the datasets are held. \\n\\n\\nThe function will return the eda statistic, various plots that show the differences between the healthy people and cardial patients\\n\\nThe congfig folder contains one json files for the use of functions' parameters. \\n\\nURL to our webpage: https://larryzly.github.io/CardiovascularClassifier/\\n```\\n### Responsibilities\\n\\nAll the work and code are produced after our group discussion.\\n* Wentaon Chen developed the run.py and found out how to set the paths for the scripts\\n* Leyang Zhang developed some functions of model.py and upload the reports and datasets\\n* Zimin Dai developed functions in model.py and built the structure for the repository, docker image\\n```\\n\\n\",\n",
       "  'This repository contains completed codes for a project on multiple testing. It includes main python scripts in the src folder, with functions for analyzing data and generating plots. The data folder holds the datasets. The config folder contains a json file for function parameters. The webpage for this project can be found at https://larryzly.github.io/CardiovascularClassifier/. The responsibilities of the group members are also listed.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_29': ['\\n\\n# Using Epidemiology Model To Predict Case Numbers for COVID-19\\n\\n## Table of contents\\n* [General info](#general-info)\\n* [Technologies](#technologies)\\n* [Setup](#setup)\\n* [Directions](#directions)\\n* [Processing](#in_processing)\\n## General info\\n- Use covid-19 datasets provided by JHU to fit epidemiology model to U.S.. After figuring out the infection parameter, we can then predict \\n\\n## Introduction\\nFitting Epidemiology Model with Covid-19 JHU U.S. Data\\n## Technology\\nProject is created with:\\n* Image : https://hub.docker.com/repository/docker/caw062/test\\n## Setup\\n- Before running, use `pip install -r requirements.txt` to install all the required packages\\n- on terminal, run `python run.py data` to retrieve the most current data from JHU & Apple Data\\n\\n## Directions\\n`python run.py test` to first download test data, and then build epidemiology model on the test data. \\nIt will return the beta (infection rate), and D (infection duration) for the entire United States.\\nIt will also return a prediction for counties in Southern California on 1/22/2021 based on case counts on 1/21/2021 (previous day)\\n',\n",
       "  'This document provides information on using an epidemiology model to predict case numbers for COVID-19. It includes general information, technologies used, setup instructions, and directions for running the model. The model is fitted with COVID-19 datasets provided by JHU and can be used to predict case numbers for the United States and specific counties in Southern California.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_22': ['# Political Analysis on Senatorial Twitter Account Using Machine Learning\\n \\n### Project Description\\nThe modern American political landscape often seems void of bipartisanship. Nowhere is this stark divide between red and blue more evident than in the halls of the US Capitol, where the Senate and House of Representatives convene to carry out the duties of the legislative branch. While us average Americans rarely watch the daily proceedings of the Senate or House, Twitter has given us a unique window into the debates and discourse that shape our democracy. In fact, the 116th Congress, which served from January 3, 2019 to January 3, 2021 broke records by tweeting a total of 2.3 million tweets! As such, it is clear that Twitter is quickly becoming a digital public forum for American politicians. This surplus of tweets from the 116th Congress enables us to analyze the Twitter (following-follower) relationships between politicians on and across the two sides of the aisle. This project’s main inquiry is into whether there is a tangible difference in the way that Democrat members of Congress speak and interact on social media in comparison to Republican members of Congress. If there are such differences, this project will leverage them to train a suitable ML model on this data for node classification. That is to say, this project aims to determine a Senator’s political affiliation based off of a) their Twitter relationships to other Senators b) their speech patterns, and c) other mine-able features on Twitter. In order to truly utilize the complex implicit relationships hidden in the Twitter graph, we can use models such as Graph Convolutional Networks, which apply the concept of “convolutions” from CNNs to a graph network-oriented framework, and GraphSage model.\\n\\n### run.py\\nWe implement the GCN and GraphSage models as our main models for training and comparison.\\n\\n- parameters:\\n  - model: The choice of models. We only implement the GCN and GraphSage. \\n  - dataset: The choice of datasets. There are multiple datasets, including data_voting, data_voting_senti. The differences between these datasets are features. The adjacency matrix of each dataset stays the same.\\n  - output_path: The output of project will be stored in json file.\\n  - agg_func: The choice of aggregated function in the graphSage model. We only support MEAN aggregator. The default is MEAN.\\n  - num_neigh: The number of neighbors in the graphSage model. The default is 10.\\n  - n: The number of hidden layers in the GCN model. This can be tuned to reach higher accuracy.\\n  - self_weight: The weight of self-loop in the GCN model.\\n  - hidden_neurons: The number of hidden neurons in the GCN model. The default is 200 and it can be tuned to reach higher accuracy.\\n  - device: The device for training the model. We only support cuda.\\n  - epochs: The number of epochs for both models. The default is 200 epochs.\\n  - lr: The learning rate for both models. The default is 1e-4. This can be tuned for higher accuracy.\\n  - val_size: The size of testing data. The default is 0.3.\\n  - test: The parameter for running test data on models.\\n\\n- some examples for using the project:\\n  - python run.py\\n  - python run.py --test\\n  - python run.py --n_GCN\\n  - python run.py --dataset data_voting\\n  - python run.py --model n_GCN --n 2 --self_weight 20\\n\\n  \\n\\n# Our Project Website\\n\\nPlease view our website [here](https://anuragpamuru.github.io/dsc-180b-capstone-b03/)\\n\\n\\n### Contributers: \\nYimei Zhao, Anurag Pamuru, Yueting Wu\\n',\n",
       "  'This project aims to analyze the Twitter accounts of senators using machine learning techniques. The goal is to determine if there are differences in the way Democrat and Republican members of Congress speak and interact on social media. The project will use the Twitter relationships between politicians, their speech patterns, and other features to train a machine learning model for node classification. The models used in this project are Graph Convolutional Networks (GCN) and GraphSage. The \"run.py\" file provides information on how to run the models with different parameters. The project website can be accessed [here](https://anuragpamuru.github.io/dsc-180b-capstone-b03/). The contributors to this project are Yimei Zhao, Anurag Pamuru, and Yueting Wu.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_20': [\"# GNN-on-3d-points\\n\\n### Abstract:\\n   This research focuses on 3D shape classification. Our goal is to predict the category of shapes consisting of 3D data points. We aim to implement Graph Neural Network models and compare the performances with PointNet, a popular architecture for 3d points cloud classification tasks. Not only will we compare standard metrics such as accuracy and confusion matrix, we will also explore the model's resilience of data transformation. Besides, we also tried combining PointNet with graph pooling layers.\\n   \\n   \\nSee project website here: https://ctwayen.github.io/Graph-Neural-Network-on-3D-Points/\\n\\nDocker name: ctwayen/project_docker\\n\\nDocker web path: https://hub.docker.com/repository/docker/ctwayen/project_docker\\n\\n### Instruction:\\n\\n   If it is the first time you running this project, please download the data through python run.py all --mode download; You could also use the parameter --method to choose knn or fix-radius to construct the graph you like. Their corresponding parameters are --k and --r.The raw dataset would automatically download into the path 'data/modelnet/ModelNet40'. The points data is stored in 'data/modelnet/modelnet_points'. The consturcted graph trainning data is in 'data/modelnet/modelnet_(knn/fix_radius)(k/r){your param}'. Do not move those files. It may cause problems\\n   \\n   We support training Pointnet and GCN two models.Using --model to choose which one you want to train: GCN or pointNet.\\n   \\n   Shared paramaters are (default values are the best combination we found):\\n   \\n   --lr：Learning rate\\n   \\n   --bs: batch size\\n   \\n   --base: dataset path. If you are using default data, you do not need to specify this.Otherwise, write the graph dataset you just constructed\\n   \\n   --data: 10 or 40. Choose 10 to run 10 categories classfication and 40 for 40 categories\\n   \\n   --epoch: epoch\\n   \\n   --val_size: validation size. For example, 0.2 will have 20 % data as validation data\\n   \\n   --model_path: The path to store trained_model. Default is 'trained_models'\\n   \\n   --ouput_path: The path to store the ouput csv file\\n   \\n   Parameters that only used in GCN:\\n   \\n   --pool: Which pooling to use. SAG or ASA\\n   \\n   --ratio: The pooling ratio. For example, 0.4 will pool out 60% of data each time\\n   \\n#### Important! Training GCN will take about 7-10 minutes for one epoch. Training PointNet will take about 50s for one epoch. Please manage your time for training process.\\n\\n   \\n### Notebooks and results:\\n\\n   Besides training your own models, we also offered few trained models. You could check notebooks/Analyzing results to see our training results for different hyperparamters setting. You could also check how to use a trained_model there.\\n   \\n   You could also check the data augmentation's effects on models in the notebook/analyzing resistence file\\n   \\n   You could check how pooling layer affect result in the notebook/analyzing pooling file\\n\\nAuthor: @Xinrui Zhan.If you find any bug, contact me through the email: ctwayen@outlook.com \\n   \\n   \\n\",\n",
       "  'This research focuses on 3D shape classification using Graph Neural Network models. The goal is to predict the category of shapes consisting of 3D data points. The performance of these models will be compared with PointNet, a popular architecture for 3D point cloud classification tasks. The research also explores the resilience of the models to data transformation and combines PointNet with graph pooling layers. The project website and Docker information are provided for further reference. Instructions are given for downloading the data, choosing graph construction methods, and training the models. Notebooks and results are available for analyzing training results, data augmentation effects, and the impact of pooling layers on the results. Contact information is provided for any bug reports or inquiries.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_19': ['# Graph-based Product Recommendation\\nDSC180B Capstone Project on Graph Data Analysis\\n\\nProject Website: https://nhtsai.github.io/graph-rec/\\n\\n## Project\\nAmazon Product Recommendation using a graph neural network approach.\\n\\n### Requirements\\n- dask\\n- pandas\\n- torch\\n- torchtext\\n- dgl\\n\\n## Data\\n### Datasets\\nAmazon Product Dataset from Professor Julian McAuley ([link](http://jmcauley.ucsd.edu/data/amazon/links.html))\\n* Product Reviews (5-core)\\n* Product Metadata\\n* Product Image Features\\n\\n## GraphSAGE Model\\n\\n## PinSAGE\\n\\n### Graph & Features\\nThe graph is a heterogeneous, bipartite user-product graph, connected by reviews.\\n * Product Nodes (`ASIN`)\\n   * Features: `title`, `price`, image representation\\n * User Nodes (`reviewerID`)\\n * Edges (`user`, `reviewed`, `product`) and (`product`, `reviewed-by`, `user`)\\n   * Features: `helpful`, `overall`\\n\\n### Data Configuration (`config/data-params.json`)\\n\\n### Model\\nWe use an unsupervised PinSage model (adapted from [DGL](https://github.com/dmlc/dgl/tree/master/examples/pytorch/pinsage)).\\n\\n### Model Configuration (`config/pinsage-model-params.json`)\\n- `name`: model configuration name\\n- `random-walk-length`: maximum number traversals for a single random walk, `default: 2`\\n- `random-walk-restart-prob`: termination probability after each random walk traversal, `default: 0.5`\\n- `num-random-walks`:  number of random walks to try for each given node, `default: 10`\\n- `num-neighbors`: number of neighbors to select for each given node, `default: 3`\\n- `num-layers`: number of sampling layers, `default: 2`\\n- `hidden-dims`: dimension of product embedding, `default: 64 or 128`\\n- `batch-size`: batch size, `default: 64`\\n- `num-epochs`: number of training epochs, `default: 500`\\n- `batches-per-epoch`: number of batches per training epoch, `default: 512`\\n- `num-workers`: number of workers, `default: 3 or (#cores - 1)\\n- `lr`: learning rate, `default: 3e-4`\\n- `k`: number of recommendations, `default: 500`\\n- `model-dir`: directory of existing model to continue training\\n- `existing-model`: filename of existing model to continue training, `default: null`\\n- `id-as-features`: use id as features, makes model transductive\\n- `eval-freq`: evaluates model on validation set when `epoch % eval-freq == 0`, also evaluates model after last training epoch\\n- `save-freq`: saves model when `epoch % save-freq == 0`, also saves model after last training epoch\\n\\n## References\\n* [GraphSAGE Homepage](http://snap.stanford.edu/graphsage/)\\n* [GraphSAGE Research Paper](https://arxiv.org/abs/1706.02216)\\n* [PinSage Article](https://medium.com/pinterest-engineering/pinsage-a-new-graph-convolutional-neural-network-for-web-scale-recommender-systems-88795a107f48)\\n* [PinSage Research Paper](https://arxiv.org/abs/1806.01973)\\n',\n",
       "  'This project focuses on Amazon product recommendation using a graph neural network approach. The data used includes the Amazon Product Dataset, which consists of product reviews, product metadata, and product image features. The graph used in the model is a heterogeneous, bipartite user-product graph connected by reviews. The model used is an unsupervised PinSage model, and the configuration parameters for the model are provided. References to GraphSAGE and PinSage are also included for further reading.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_18': [\"# NBA-Game-Prediction\\nProject Group: MengYuan Shi, Austin Le\\n\\nThis repository contains a data science project that discover the NBA Game Prediction. We investigate the social network for individual NBA players and the relationship between each team. We will use team's statistics and players' statistics and analysis for predicting who wins the games by leveraging the team's statistics and players' statistics from 2015 season to 2019 season. We will use GraphSAGE which is a generalizable embedding framework to create a graph classification.\\n\\n\\n### Warning\\nOur group has altered and used the graphsage implementation that can be received from https://github.com/williamleif/GraphSAGE . We made minor changes within the model and inputs to align with the goals of our project, but we would like to cite them as a source for the main graphsage implementation.\\n\\n\\n### Running the project\\n- `python run.py` can be run from the command line to ingest data, train a model, and present relevant statistics for model performance to the shell\\n  - Reads in CSV file from eightthirtyfour for play by play data\\n  - Runs algorithm to create network between each player based on their playing time\\n  - Appends all player edges from all season onto a single graph \\n  - Embedd player categorical statistics onto each node \\n  - runs graphSage model to learn over features\\n\\n### Outputs\\n  - The outputs printed will be the corresponding accuracies obtained after the training\\n  - ~5min runtime\\n\\n### Responsibility \\n- Austin Le: Responsible for the data cleaning and data scraping and the coding part as well as the report.\\n- MengYuan Shi: Responsible for the paper researching and writing the report part as well as the visualization.\\n\",\n",
       "  'This repository contains a data science project focused on predicting NBA game outcomes. The project investigates the social network of individual NBA players and the relationship between teams. It uses team and player statistics from the 2015 to 2019 seasons to predict game winners. The project utilizes GraphSAGE, a graph classification framework, to create embeddings for analysis. The code for this project is based on an altered version of the GraphSAGE implementation from https://github.com/williamleif/GraphSAGE. Running the project involves ingesting data, training a model, and presenting performance statistics. The outputs include accuracy metrics obtained after training, with a runtime of approximately 5 minutes. Austin Le is responsible for data cleaning, scraping, coding, and report writing, while MengYuan Shi focuses on research, report writing, and visualization.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_21': [\"# DSC180B_Project\\n\\nURL: https://sdevinl.github.io/DSC180B_Project/\\n\\n**Disclaimer:**   \\n  We want to make it clear that the graphsage implentation found in this repo is not our own. We have made minor alterations to the code in order to better serve our overall project in regards to NBA team rankings. The original graphsage implementation can be found here https://github.com/williamleif/GraphSAGE . We would also like to cite their paper:\\n  \\n     @inproceedings{hamilton2017inductive,\\n\\t     author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},\\n\\t     title = {Inductive Representation Learning on Large Graphs},\\n\\t     booktitle = {NIPS},\\n\\t     year = {2017}\\n\\t   }\\n  For more information on how to run graphsage as well as the requirements for grapshage be sure to checkout the original graphsage's implementation.\\n\\n**About:**  \\n  This repository contains an implementation of a GraphSAGE for node classification on an NBA dataset. The goal being able to classify the ranks of NBA teams using player stats and a graph representation of matchups between teams in a season. \\n  \\n**Setting Up Docker Image**  \\n  The docker image that was created in order to have an environment able to run this project is found on the repo at aubarrio/graphsage . \\n    \\n**Model**  \\n  data: The data we use in this project is a compound of multiple webscraped data found on https://www.basketball-reference.com we used stats such as player and team stats, along with team schedules for the season. The seasons for which we collected data range from 2011 to the 2019 season.  \\n\\n**Basic Parameters**  \\n  Since we are in early stages of developing we only have one parameter of choice and that is [train, test]. This is to distinguish the data being input into the model.  \\n    train: Parameter train will train the model on all available features (181 different features)  \\n    test: Parameter test will train the model on 2 features (Rank and Id) which is meant to use as an evaluation of how our data is performing  \\n    \\n**Examples run.py**  \\n  python run.py test  \\n  python run.py  \\n  \\n  The run.py file will run a graphsage model with the mean aggregator on our NBA data. For more information on how to change the overall model or change the parameters of a model refer to the original graphsage implementation.\\n  \\n**Output**  \\n  Direct terminal output outlining the training, validation and test accuracies of our model.  \\n  In the sage_outputs folder you will find files that contain data on the model:\\n  \\tThe rawOutputs folder contains a json file that includes the raw output probabilites of our NBA test set.\\n\\tThe stats folder will contain all accuracies and losses captured whilst training your model.\\n\\t\\n**Acknowledgements**   \\n  As mentioned in the disclaimer the original version of this code can be found here https://github.com/williamleif/GraphSAGE. An appreciation to the original creators of this code and a thank you for allowing this code to be available and ready to use on projects such as ours. \\n\",\n",
       "  'This repository contains an implementation of GraphSAGE for node classification on an NBA dataset. The goal is to classify the ranks of NBA teams using player stats and a graph representation of matchups between teams in a season. The data used in this project is a combination of webscraped data from basketball-reference.com, including player and team stats, as well as team schedules for the season. The model can be trained on all available features or on a subset of features for evaluation purposes. The output includes terminal output with training, validation, and test accuracies, as well as files in the sage_outputs folder containing raw output probabilities and training statistics. The original code for GraphSAGE can be found at https://github.com/williamleif/GraphSAGE.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_76': ['Name: Jason Chau, Sung-Lin Chang, Dylan Loe\\n\\nWelcome to our Stock Predictor. \\n\\nIMPORTANT!\\nFor running actual models:\\n\\nIn order to run our stock predictor, just make sure that you are in the current directory and run the command \\n\\npython run.py all - this will let you scrape the data, as well as running our model\\n\\n\\npython run.py test - this will let you run our model on the data pulled from the webscraper, as well as predicting which stocks will\\nbe bullish or bearish in Dow Jones 30.\\n\\npython run.py fcn - this will let you run our Fully connected network model on the data pulled from the webscraper\\n\\npython run.py build - This command builds the test portion of the code. Please keep in mind this will pull in data from the yahoo finance api,\\nso it will require an internet connection to pull in the data and calculate it.\\n\\n\\n------ CONFIG FILE--------------\\n\\n\"NUM_EPOCHS\" : 100, ( this is the number of trials you want to use)\\n\"LEARNING_RATE\" : 0.001, ( this is the learning rate)\\n\"NUM_HIDDEN\" : 32, ( number of hidden features)\\n\"num_days\": 5, (number of lag days you will have, we chose 5 because there are 5 trading days)\\n\"nfeat\" : 20, ( this is 4 * num_days)\\n\"nclass\" : 1, \\n\"dataset\" : \"./data/dowJonescorrelation0.4graph.csv\", (this is the correlation graph we use for our model)\\n\"thresh\" : 0.4, ( the threshold for the node adjacency)\\n\"filepath\" : \"./data/12modowJonesData/\", (the dataset to use)\\n\"timeframe\" : \"12mo\" ( which time period, there is 12mo and 6 mo\\n\\nRequired packages\\n\\nyfinance == 0.1.55\\npandas-datareader = 0.9.0\\nbeautifulsoup4 4.9.3\\ntensorflow == 1.12.0\\nnetworkx == 2.1\\nnumpy == 1.14.3\\nscipy == 1.1.0\\nsklearn == 0.19.1\\nmatplotlib == 2.2.2\\n',\n",
       "  'This is a summary of the instructions and configuration details for running the Stock Predictor program:\\n\\n- To run the stock predictor, make sure you are in the current directory and run the command \"python run.py all\". This will scrape data and run the model.\\n- You can also use the commands \"python run.py test\" to run the model on data pulled from a web scraper, and \"python run.py fcn\" to run a fully connected network model.\\n- The command \"python run.py build\" builds the test portion of the code, which requires an internet connection to pull data from Yahoo Finance API.\\n- The configuration file includes parameters such as number of epochs, learning rate, number of hidden features, number of lag days, dataset file path, threshold for node adjacency, and timeframe.\\n- Required packages for running the program include yfinance, pandas-datareader, beautifulsoup4, tensorflow, networkx, numpy, scipy, sklearn, and matplotlib.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_0': ['# Political Popularity of Misinformation\\n- This project looks at the popularity and influence of politicians on Twitter by analyzing the engagement ratios as well as the rolling and cumulative maxes of likes and retweets over time.\\n\\n### Note\\n- To get the data necessary to replicate this project, access to the Twitter API is needed. \\n- You must have configured twarc using `twarc configure` in the terminal with your API credentials in order to run the data pipeline. Additional information on how to do so can be found here, https://github.com/DocNow/twarc.\\n- In addition, to run the data pipeline you must obtain a bearer token from Twitter’s API and store it in a config.py file in the root directory. Information on using and generating a bearer token can be found here, https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens. bearer_token = “...”\\n\\n### Obtaining the txt files\\n- We obtain the tweet IDs that compose our politicians’ timeline found in `src/data` from George Washington University’s TweetSets database found here, https://tweetsets.library.gwu.edu/datasets.\\n- We chose to focus on politicians who served in the 116th United States Congress, which corresponds to two datasets, Congress: Representatives of the 116th Congress and Congress: Senators of the 116th Congress.\\n- After identifying our politicians, we gathered the user IDs for their Twitter accounts using Tweepy, which are then used to query the two Congress datasets. The datasets also contain a file of the House and Senate members along with their user IDs which is an alternative way to obtain these IDs. The files can be found here, [Senate](https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/MBOJNS/8VQVWT&version=1.0) and [Representative](https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/MBOJNS/WXZE5N&version=1.0).\\n- To query the datasets, for each politician, we selected either the Representative or Senator dataset depending on their position and inputted their user ID in the “Contains any user id” box under the “Posted by” section. This process gives us a txt file of tweet IDs for each politician which is then stored in the `src/data/misinformation` and `src/data/scientific` folders depending on the group the politician is assigned to. \\n\\n### Using `run.py`\\n- To get the data, from the project root directory, run `python run.py data`\\n    * This downloads the data using the tweet IDs found in `src/data/misinformation` and `src/data/scientific`.\\n    * This also downloads the engagement metrics needed for ratio analysis using the same tweet IDs.\\n    * The politicians to analyze can be specified in `config/data-params.json`.\\n    * The name of the txt file containing the tweet IDs must match the name specified in `config/data-params.json`.\\n    * The output is a json file for each politician containing their collection of tweets as well as a csv file containing the engagement metrics and text of the tweet ID.\\n\\n- To calculate the ratios for the tweets, from the project root directory, run `python run.py ratio`\\n    * This calculates the ratios for the tweets found in the csv files in `src/data/misinformation` and `src/data/scientific`.\\n    * The politicians to analyze can be specified in `config/data-params.json`.\\n    * Output is an updated csv file for each politician containing the engagement metrics of their tweets and their ratio values.\\n\\n- To calculate the popularity estimate metrics, from the projecy root directory, run `python run.py metrics`\\n    * This will create a JSON file for each estimate metric you wish to analyze. \\n    * The outputs are stored in `src/out`.\\n    * These JSON files will then be used to create visualizations using the visualization target.\\n\\n- To create the visualizations, from the project root directory, run `python run.py visualization`\\n    * This creates visualizations from the JSON files created from the metrics target.\\n    * The outputs are stored in `src/out`.\\n\\n- To run the permutation test on our groups, from the project root directory, run `python run.py permute`\\n    * This will run a permutation test within our two groups as well as between our groups.\\n    * The outputs are stored in `src/out`.\\n\\n- To run all of the above targets, from the project root directory, run `python run.py all`\\n\\n- To run the test target, from the project root directory, run `python run.py test`\\n    * This runs most of the above targets on a small, fake dataset.',\n",
       "  \"This project analyzes the popularity and influence of politicians on Twitter by examining engagement ratios and the likes and retweets of their tweets over time. The data is obtained from the Twitter API and George Washington University's TweetSets database. The project includes various steps such as downloading data, calculating ratios, estimating popularity metrics, creating visualizations, running permutation tests, and running all targets. There is also a test target available for a small dataset.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_1': [\"# The Sentiment of U.S. Presidential Elections on Twitter\\nThis project investigates the public sentiment on Twitter regarding the 2016 and 2020 U.S. Presidential Elections. Political tensions in the United States came to a head in 2020 as people disputed President Donald Trump's handling of various major events that the year brought such as the COVID-19 pandemic and the killing of George Floyd and subsequent racial protests, and we aim to identify if this was quantifiably reflected in people's behavior on social media. To do this, we perform sentiment analysis on tweets related to the elections and conduct permutation testing to analyze how sentiment may differ between the two years and between and within politically left- and right-leaning groups of users. \\n\\n\\n### Running The Project\\n- All commands specified here are to be run from within this project's root directory\\n- To install necessary dependencies, run `pip install -r requirements.txt`\\n- Note: to get the data necessary to replicate this project, access to the Twitter API is needed\\n\\n### Using `run.py`\\n- This project uses publicly available 2016 and 2020 presidential election datasets located at https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/PDI7IN and https://github.com/echen102/us-pres-elections-2020. Given the 2016 dataset is not uniformly structured, you must manually download the txt files of tweet ids from the dataset's website to the directory `data/raw/2016`. The 2020 dataset can be downloaded programmatically using the `data` target, as follows.\\n\\n- To get hydrated tweets for the 2016 and 2020 tweet ids, run the command `python run.py data`\\n    * This samples from the 2016 tweet ids located in `data/raw/2016` and stores them in txt files in `data/temp/2020/tweet_ids/`\\n    * It also directly downloads tweets for the 2020 election from the dataset's GitHub page falling within the date range specified in `config/etl-params.json`, samples from them, and stores them in txt files in `data/temp/2020/tweet_ids/`\\n    * It then rehydrates the tweets using `twarc` and saves the them in jsonl format in the `hydrated_tweets/` directory within each year's respective data directory\\n\\n- To clean the rehydrated tweets, run the command `python run.py clean`\\n    * This takes the rehydrated tweets obtained from running `python run.py data` and creates a single csv file of tweets for each year with fields for the features of interest specified in `config/clean-params.json`\\n    * For the purpose of performing sentiment analysis later, tweets in languages other than English are filtered out.\\n    * The resulting csvs are stored as `clean/clean_tweets.csv` within each year's data directory.\\n\\n- To run the main analysis, run `python run.py compute` from the project root directory\\n    * For each year, this subsets the tweets into left and right leaning, classifies the the different types of dialogue, performs sentiment analysis on the various subsets of data, and conducts permutation testing on the subsets of the data between the two years, producing plots of the results.\\n    \\n- To run the `clean` and `compute` targets on fake test data, run the command `python run.py test`\\n\\n\",\n",
       "  'This project analyzes the sentiment of Twitter users during the 2016 and 2020 U.S. Presidential Elections. It aims to determine if public sentiment on social media reflected the political tensions and major events of those years. The project performs sentiment analysis on election-related tweets and conducts permutation testing to compare sentiment between the two years and among politically left- and right-leaning groups. The project provides instructions for running it, including installing dependencies, accessing necessary data, and executing different commands for data processing and analysis.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_2': ['# Data Science Capstone Project\\n\\n## Info\\n* The data being used here is Tweets from various news sources.\\n## Preqrequisites\\n* Ensure libraries are installed. (pandas, requests, os, gzip, shutil, json, flatten).\\n* Download repo: https://github.com/thepanacealab/covid19_twitter.\\n* Docker container id: tmpankaj/example-docker\\n## How to Run\\n* All parameters are of type str unless specified otherwise\\n* Set twitter API Keys in config/twitter-api-keys.json\\n#### Test\\n* run \\'python run.py test\\' in root directory of repo\\n* look in test/visualizations for the test targets\\n#### Data \\n* Go inside docker container\\n* Add .txt files with Tweet IDs from https://tweetsets.library.gwu.edu/ to some directory where preprocessed data will be stored. (E.g. cnn.txt in /data/preprocessed directory)\\n* Use this hydrator https://github.com/DocNow/hydrator to hydrate these tweets and make sure there is a .csv file in the same directory (E.g. cnn.csv in /data/preprocessed)\\n* Set data parameters in config/data-params.json\\n   * preprocessed_data_path: path to directory of preprocessed data\\n   * training_data_path: path to directory to output training data\\n   * dims (list of str): list of the names of the dimensions that polarities will be eventually calculated on (E.g. [\"moderacy\", \"misinformation\"])\\n   * labels (dict): dictionary with the keys including the news sources and each value being a list with a polarity for each dimension. Every news source that will be used in your data must have a label for every dimension. (E.g. {\"cnn\": [0, 1], \"fox\": [1, 0]})\\n   * user_data_path: path to directory to output user data\\n   * exclude_replies (bool): If true, will exclude replies when collecting user tweets.\\n   * include_rts (bool): If true, will include retweets when collecting user tweets.\\n   * max_recent_tweets (int): maximum recent number of tweets to obtain from a user\\n   * tweet_ids (list of str): list of tweet IDs to collect to analyze flagged vs unflagged retweeters\\n* Make sure paths to directories already exist\\n* run \\'python run.py data\\' in root directory of repo\\n* This will only collect the data\\n#### Train\\n* Go inside docker container and make sure data has been collected\\n* Set train parameters in config/train-params.json\\n   * training_data_path: path to directory of the training data (should be same as data-params)\\n   * model_path: path to directory to output models to be trained\\n   * dims (list of str): list of the names of the dimensions that polarities should be calculated on (E.g. [\"moderacy\", \"misinformation\"])\\n   * fit_priors (list of bools): hyperparemeter for Naive Bayes classifier (1 for each dimension) - Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\\n   * max_dfs (list of floats/ints): hyperparameter for CountVectorizer (1 for each dimension) - When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts.\\n   * min_dfs (list of floats/ints): hyperparameter for CountVectorizer (1 for each dimension) - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. \\n   * n_splits (int): number of folds to use for K-fold cross validation\\n   * outdir: path to directory to output a notebook of the results\\n* Make sure paths to directories already exist\\n* run \\'python run.py train\\' in root directory of repo \\n* Look in the outdir you specified for an html file of the results\\n#### Analysis\\n* Go inside docker container and make sure data has been collected and models have been trained\\n* Set analysis parameters in config/analysis-params.json\\n   * model_path: path to directory of trained models (should be same as train-params)\\n   * user_data_path: path to directory of user data (should be same as data-params)\\n   * dims (list of str): list of the names of the dimensions that polarities should be calculated on (E.g. [\"moderacy\", \"misinformation\"])\\n   * tweet_ids (list of str): list of tweet IDs to analyze\\n   * flagged (dict): dictionary should have a key for every tweet to be analyzed and a boolean for whether or not the tweet was flagged (E.g. {\"123\": true, \"456\": false})\\n   * outdir: path to directory to output a notebook of the results\\n* Make sure paths to directories already exist\\n* run \\'python run.py analysis\\' in root directory of repo\\n* Look in the outdir you specified for an html file of the results\\n#### Results\\n* Go inside docker container and make sure data has been collected, models have been trained, and analysis has been ran.\\n* Set results parameters in config/results-params.json\\n   * user_data_path: path to directory of user data (should be same as data-params)\\n   * dims (list of str): list of the names of the dimensions that results should be calculated on (E.g. [\"moderacy\", \"misinformation\"])\\n   * outdir: path to directory to output a notebook of the results\\n* Make sure paths to directories already exist\\n* run \\'python run.py results\\' in root directory of repo\\n* Look in the outdir you specified for an html file of the results\\n',\n",
       "  'This is a guide for running a Data Science Capstone Project. It provides information on the data being used, prerequisites, and step-by-step instructions on how to run the project. The guide includes sections on testing, data collection, training models, analysis, and viewing the results.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_3': ['## Where to begin?\\n### Begin by uploading your twitter API credentials into a json file as under a new .env director. The file path should look like this: .env/twitter_credentials.json\\n\\nThe json file should be structured as\\n\\n```json\\n{\\n   \"CONSUMER_KEY\":\"your-consumer-key-here\",\\n   \"CONSUMER_SECRET\":\"your-consumer-secret-here\",\\n   \"ACCESS_TOKEN\":\"your-access-token-here\",\\n   \"ACCESS_TOKEN_SECRET\":\"your-access-token-secret-here\"\\n}\\n```\\n\\nIf you do not have twitter API credentials, please visit https://developer.twitter.com/en/docs/twitter-api to apply for a developer account.\\n\\nTo install the dependencies, run the following command from the root directory of the project: pip install ```-r requirements.txt```\\n\\n## How to use run.py:\\nrun.py takes in one argument, a choice between *data*, *eda*, *test*\\n\\n## Directories\\n* A directory titled *data* will be created with 4 subdirectories: *graphs, raw, processed*\\n   * *graphs* will hold any charts from eda functions\\n   * *processed* will hold any statistic data from eda functions\\n   * *raw* will hold raw tweet data\\n* Each of the above directories will be split into two additional subdirectories, *news* and *election*\\n   * *news* will hold any data related to news stations\\n   * *election* will hold any data related to the election dataset\\n\\n## Description of arguments (targets)\\n\\n### data\\n* Your twitter API credentials for use in downloading data to be used in our project.\\n* The target will download all tweets as specified in the config file *news_params.json*\\n\\n### eda\\n* The eda target will generate statistics and visualizations after data has been gathered from the *data* target\\n* Currently we have built a wordcloud visualization that will be stored in *graphs* and a statistic of most common hashtags per news station stored in *processed*\\n\\n### compile and embed\\n* Performs graph embedding calculations as described in the methodology section of the report\\n\\n### test\\n* The test target is designed for grading functionality in the DSC180B capstone course and will test three functionalities:\\n   * *etl_news* checks that test data is available for use\\n   * *eda* generates visualizations and statistics based on the test data, stores in *test/testreport*\\n   * *similarity* will generate similarity hashtag vectors to be used in our main analysis *test/testreport*\\n\\n',\n",
       "  'This text provides instructions on how to begin using the run.py script. It explains how to upload Twitter API credentials, install dependencies, and use the different arguments of the script. It also describes the directory structure and the purpose of each directory. The arguments include \"data\" for downloading tweets, \"eda\" for generating statistics and visualizations, \"compile and embed\" for performing graph embedding calculations, and \"test\" for testing functionality.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_4': ['# Election-Sentiment\\nAn analysis of views towards the US 2020 Presidential election using Twitter data.\\n\\nContained in this repository are a few notebooks that contain our analyses in this investigation on Election Sentiment analysis as well as an investigation into how Twitter impacts election results.\\n\\n### Building the preoject using `run.py`\\n\\nRunning the test target via the command \"python run.py test\" will produce images related to the distribution of discussion levels of the two elections. In order to customize the data that this script is run on, replace the data in the test folder with data of your choice.\\n\\nProvided in the scripts folder, are scripts to donwload tweets from the github repository that we collected tweets on the 2020 election for. The links to the 2020 election and 2016 election tweet ID\\'s are below:\\n\\n2016:\\nhttps://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/PDI7IN\\n\\n2020:\\nhttps://github.com/echen102/us-pres-elections-2020\\n\\nIn order to run the data download scripts, you need to have twarc installed. However, building the project with the provided dockerfile in this repository will download all of the extra packages not native to most Machine Learning and Data Science platforms.\\n\\n\\n## Running the project\\n* To get the data from Twitter, create a developer account and get your developer keys\\n* Configure `twarc`\\n  - On the terminal, run `twarc configure`\\n  - Supply keys made earlier\\n\\n\\n## Groupmate Responsibilities\\n\\n### Chris\\n\\nChris was responsible for the EDA and sentiment analysis and those respective portions of the report. Her work was focused heavily on understanding the sentiment as time progressed and how that related to the individual elections. \\n\\n### Prem\\n\\nPrem was involved heavily in creating the scripts that could be ran by anyone in order to perform ETL on the data. In addition Prem worked on developing the discussion metric that was critical to this investigation.\\n',\n",
       "  'This repository contains notebooks analyzing views towards the US 2020 Presidential election using Twitter data. It also includes scripts for downloading tweets related to the 2020 and 2016 elections. The project can be run by configuring `twarc` with developer keys and running the provided scripts. Chris focused on EDA and sentiment analysis, while Prem worked on ETL scripts and developing the discussion metric.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_5': ['\\n# The Spread of Misinformation on Reddit\\nObserving how different forms of misinformation and conspiracies are spread through social media.\\n\\n## Overview\\nWith the amount of actively spread misinformation circulating popular social media platforms, our goal is to explore if various forms of misinformation follow varying patterns of diffusion. The scope of our project will be limited to two misinformation types -- myth and political misinformation, and focused on Reddit. Our data will be obtained from Reddit archive [pushshift.io](http://pushshift.io).\\n\\n## Contents\\n- `src` contains the source code of our project, including algorithms for data extraction, analysis, and modelling.\\n- `notebooks` contain some examples of the models this code will generate, detailing our findings under the circumstances in which we conducted our testing.\\n- `config` contains easily changable parameters to test the data under various circumstances or change directories as needed.\\n- `run.py` will build and run different the different parts of the source code, as needed by the user.\\n- `references` cite the sources we used to construct this project.\\n- `requirements.txt` lists the Python package dependencies of which the code relies on. \\n\\n## How to Run\\n- Install the dependencies by running `pip install -r requirements.txt` from the root directory of the project.\\n- Alternatively, you may reference our Docker image to recreate our environment, located [here](https://hub.docker.com/r/cindyhuynh/reddit-misinformation).\\n- Due to the open source nature of the PushShift archive, there is no need for any API use or developer account. \\n\\n### Building the project stages using `run.py`\\n- To download the data, run `python run.py data`\\n\\t- This downloads reddit comments from specified subreddits between a certain time period. The subreddits and time period are specified in `config/data_params.json`\\n- To create visualizations of EDA charts, run `python run.py eda`\\n\\t- This creates bar charts representing the dataset we have collected. It also shows visualizes statitics of one-time posters and average number of posts in each category and subreddit.\\n- To get user polarities, run `python run.py user_polarity`\\n\\t- This generates a metric for all users collected in the data, getting filepaths from `config/user_polarity_params.json`\\n- To generate common user matrices, run `python run.py matrices`\\n\\t- This creates two matrices demonstrating, for every possible pair of subreddits, the number and average user polarity of the users in that subset. Filepaths are specified in `config/matrix_params.json`. \\n\\t- NOTE: user_polarities should be run at least once before running `matrices`.\\n- To create visualizations of user polarities and matrices, run `python run.py visualize`\\n\\t- This creates bar charts representing the general user polarity spread, as well as charts showing how users of different types cross into other subreddits. These bar charts will be replaced by heatmaps in a future update for easier visualization. Filepaths are specified in `config/visualize_params.json`. \\n\\t- NOTE: `user_polarities` and `matrices` should be run at least once before running `visualize`.\\n- To run the full pipeline, run `python run.py all`\\n\\t- This will run all the targets. These targets include: `data`, `eda`, `user_polarity`, `matrices`, and `visualize`\\n- To run the full pipeline on test data, run `python run.py test`\\n\\t- This will run all the targets on test data. These targets include: `data`, `eda`, `user_polarity`, `matrices`, and `visualize`\\n',\n",
       "  'This project aims to explore the spread of misinformation on Reddit, specifically focusing on two types of misinformation - myth and political misinformation. The project utilizes data obtained from the Reddit archive pushshift.io. The source code, notebooks, and configuration files are provided for data extraction, analysis, and modeling. The project can be run by installing the necessary dependencies or using a Docker image. Various stages of the project can be executed using the `run.py` script, including downloading data, creating visualizations, generating user polarities and matrices, and visualizing the results. There is also an option to run the full pipeline or test it with test data.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_6': ['# DSC180B-Project\\n\\n## Table of Contents\\n\\n- [Introduction](#introduction)\\n- [Requirement](#requirement)\\n- [Steps for Building](#building)\\n- [Data](#data)\\n- [EDA](#EDA)\\n- [Features and Models](#features_and_models)\\n- [Contributors](#responsibilities)\\n\\n\\n## Introduction\\n\\nCovid-19 changed everyone, from the way we interact, to how we work, and our methods of communication, especially through social media. Under this pandemic period, social media becomes a huge and important part of people’s daily lives. It provides mobile users a convenient way to connect to each other around the world and acquire the updated and trending information about the topic of covid-19. Beside these, people can also express their thoughts and feelings toward certain topics by posting on social media. Throughout the studying of this quarter, we noticed that there are numbers of posts in our Twitter dataset that are related to the topic of covid-19 having some strong emotions and sentiments. In the meantime, a previous study has shown that more people are experiencing negative emotions such as anxiety and panic under this pandemic period. Therefore, we are interested in analyzing the posts that are related to the topic of covid-19 on social media and investigating the underlying causes of the negative emotions implied in these posts.\\n\\n\\n## Requirement\\n\\n- python 3\\n- install the used modules included in the requirements.txt:\\n```\\ncd references\\npip install -r requirements.txt\\n```\\n- In order to rehydrate the twitter data, you need to create a Twitter Developer Api account and get the API(costumer) key, API secret key, Access Token, and Access Token Secret.\\n- Also a Kaggle account with username and key to access the kaggle dataset.\\n- Save these keys into `.env` file in the project root directory. You can modify the `.env.example` with your own information and save it as `.env`.\\n\\n\\n## Building\\n\\nThere are six targets available in order to build the projects:\\n* data\\n    - run **`python run.py data`** for downloading and making datasets (three in total).\\n    - data will be saved in the paths `data/raw` and `data/inteirm`.\\n* analysis\\n    - run **`python run.py analysis`** for cleaning and analyzing the collected data\\n    - results (plots and tables) will be saved in the paths `data/analysis`.\\n    - you can directly view the EDA report in `notebooks/analysis.ipynb`.\\n* feature\\n    - run **`python run.py feature`** for building prediction models by using the pre-trained Kaggle dataset in order to label our own tweet dataset.\\n    - the mean sentiment scores per day will be saved in `data/final`.\\n* model\\n    - run **`python run.py model`** for using time series models to analyze the sentiment scores and daily new cases.\\n    - results will be saved in `data/final`.\\n* test\\n    - run **`python run.py test`** for building steps on our made-up test data.\\n    - this target is equivalent to **`python run.py test-data analysis feature model`**.\\n    - **IMPORTANT**: If you want to run any specific target on the test data, please include the target `test-data` in the command. For example, `python run.py test-data analysis` and `python run.py test-data feature`. And please make sure run the command in this order: test-data, analysis, feature, model.\\n* all\\n    - run **`python run.py all`** for building the complete project.\\n    - this command is equivalent to **`python run.py data analysis feature model`**.\\n\\n\\n## Data\\n\\nWe use data from [thepanacealab](https://github.com/thepanacealab) which gathers COVID-19 twitter data daily. Our data generation script enables us to input a certain date and automatically download the corresponding tweets on that date from the Panacea Lab, unzip the tab-delimited file (tsv), generate a list full of tweets IDs and rehydrate them using twarc, a command line tool and Python library that archives Twitter data.\\n\\nWe also obtain the Covid-19 daily cases dataset from [Our World in Data](https://github.com/owid/covid-19-data/tree/master/public/data) then sum up the daily cases numbers for all countries listed per day. In addition, in order to build efficient machine learning models, we use the tweet text dataset with sentiment labels from [Kaggle](https://www.kaggle.com/kazanova/sentiment140).\\n\\n```\\n.\\n├──src\\n│  ├── data              \\n│      ├── collect_data.sh  # changing directory, unzipping file, curl and twarc\\n│      ├── get_IDs.py   # table processing function that extracts the tweetID and output txt\\n       |—— extract_to_csv.py # convert all the json files into cleaner csv formats\\n       |—— clean_text.py # clean all the tweet text\\n       |—— case_download.py # dowanload and clean the daily cases dataset \\n       |—— train_dataset.py # dowanload and clean the labeled tweet sentiment dataset\\n└── ...\\n```\\n\\nIn order to successfully obtain data from Twitter and Kaggle, you need to have your Twitter and Kaggle Api information saved in the `.env` file. The script will download all of the tweetsIDs and hydrate them using `twarc`. The data will be saved under the `data/raw` folder with date being the subfolder name. Note that the `data/raw` folder is not being version controlled to avoid data corruption. The script will create the raw folder if it does not already exist. All cleaned datasets are saved in `data/interim`.\\n\\n\\n## EDA\\n\\nUse the command `python run.py analysis` to generate statistics and graphs of the twitter data. These files will be generated in the `data/analysis` folder. The `notebooks/report.ipynb` displays a analysis and results report alongside those statistics.\\n\\n\\n## Features and Models\\n\\nBy using the command `python run.py feature`, you can build machine learning models on the pre-labeled Kaggle dataset to label the collected tweet dataset. There are two models that can be used: svc and logreg. In order to change the model options, you can modify the configuration file `config/feature-params.json`: change the value of `model` to either `logreg` or `svc`. The results (mean sentiment score for each day from 03/22 to 11/30) will be saved in `data/final`.\\n\\n\\n## Test\\n\\nAll test data are saved in `test/testdata`. Our test data contains no personal information as all of the details have been replaced. Only some tweets in the test set has hashtags of COVID19 and specific words which ensures it to run efficiently.\\n\\n\\n## Responsibilities\\n```\\n* Jiawei Zheng developed\\n* Yunlin Tang developed\\n* Zhou Li developed\\n```\\n',\n",
       "  'This project is focused on analyzing posts related to COVID-19 on social media and investigating the underlying causes of negative emotions expressed in these posts. The project involves collecting Twitter data, cleaning and analyzing the data, building prediction models using pre-trained datasets, and using time series models to analyze sentiment scores and daily new cases. The project also includes an EDA report and test data for testing purposes. The responsibilities for the project were divided among Jiawei Zheng, Yunlin Tang, and Zhou Li.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_25': ['# DSC180B\\n\\nThere are “wars” going on every day online, but instead of cities, they are defending their options, and perspects. This phenomenon is especially common on the Wikipedia platform where users are free to edit others\\' revisions. In fact, there are “about 12% of discussions are devoted to reverts and vandalism, suggesting that the WP development process is highly contentious.” As Wikipedia has become a trusted source of information and knowledge which is freely accessible, It is important to investigate how editors collaborate and controvert each other in such a platform. This repository will show our coding methods to discuss a new method of measuring controvisality in Wikipedia articles. We have found out that controversiality is highly related to the number of revert edits, the sentiment level among one article comments, and the view counts of that article. Thus we developed a weighted sum formula, which combines those three factors to accurately measure the controversy level within articles in Wikipedia. \\n\\n\\n# Coding part\\nFrom the run.py file, you can notice that we have 2 targets, which is \"All\" and \"Test\". In the following part, We will discuss about the details of those two targets:\\n    \\nFor the \"ALL\" target, it uses datasets from the Wikimedia Data Archives and English Light Dump. Then it used the functions that are listed in the process to generate our final analysis results. There are the purpose for different functions:\\n\\n1. For the get_data.py and deal_withcomment.py, we use those coding files to download XML files from the Wikimedia Data Archives and then convert those raw XML file to dataframe, which is a better form to let us doing the analysis. \\n    \\n2. For the english_lighdump.py, the function for the python file is to download the English Light dump file from the WikiWarMonitor and convert this dataset to a dataframe. It also merge the English Lightdump Dataframe with the comment dataframe that is generated by get_data.py and deal_withcomment.py. \\n    \\n3. For the page_view.py, the function for this python file is to use the titles in the generated dataframe to find the raw description number of views on English Wikipedia of articles in the merged dataframe from those articles\\' start dates to 20210101. \\n    \\n4. For the sentiment_analysis.py, the function for this python file is to use the comments content in the generated dataframe and the Vader model to generate the sentiment score for each comment. Finally, in this python file we will generate a final dataframe that contains M score, page view count, article title, date, comment and sentiment score to use for the future analysis. \\n    \\n5. For the generatefinaldatf.py, the function for this python file is to generate two dataframes that we will use in our analysis part and generate some graphs from analysis. Those two dataframes are dataframe with M is zero and dataframe for all M. \\n    \\n6. For the Analysis.py, the function for this python file is to make some analysis. We generate four graphs in this analysis part:\\n  \\n    a. first one is analysis for corr between M and sentiment score\\n    b. second one is analysis for example of Wooster Ohio\\n    c. third one is relationship between pageview and sentiment score\\n    d. fourth one is view counts with M\\n   For each graph, we save as one figures and use those figures in our report. \\n\\n7. For the Weighted_sum_formula.py, the function for this python file is to generate our final weighed sum formula. And we will use the new scores which are generated by weighted sum formula to make some comparisons with the scores that are generated by M-statistics, and make some analysis on this comparison.  \\n\\n\\nFor the \\'Test\\' target, it mainly runs our test dataset, which means that the result that is generated by our \"test\" target is not representative. And there are some special function for this target, such as generatefinal_dataf_test.py and page_view_test.py, we generate those files because we need to use our test dataset. However, by using this \"test\" target, the analysis result will not be representative. \\n\\n# Notebook\\n\\nFor the content of the notebook, we put our analysis graphs which are generated by \\'ALL\\' target and we will use those graphs in our report. \\n\\n# Resource\\n\\nEnglish light dump data from WikiWarMonitor: http://wwm.phy.bme.hu/light.html\\n\\nXML file from the Wikimedia Data Archives: https://dumps.wikimedia.org/enwiki/20210220/\\n\\nAnd the pageview API from: https://github.com/Commonists/pageview-api\\n\\n# Responsibility\\n\\nCoding: Xingyu Jiang, Xiangchen Zhao\\nNotebook: Xingyu Jiang, Xiangchen Zhao\\nReport: Xingyu Jiang, Xiangchen Zhao and Hengyu Liu\\n',\n",
       "  'This summary discusses the phenomenon of online \"wars\" on platforms like Wikipedia, where users edit and defend their perspectives. The summary highlights the importance of investigating how editors collaborate and disagree on Wikipedia. The coding part of the project involves downloading XML files, converting them to dataframes, merging datasets, analyzing sentiment scores, generating graphs, and developing a weighted sum formula to measure controversy levels in Wikipedia articles. The \"Test\" target runs a test dataset but is not representative. The notebook contains analysis graphs used in the report. The resources used include English light dump data from WikiWarMonitor and XML files from the Wikimedia Data Archives. The responsibility for coding, notebook, and report is shared among Xingyu Jiang, Xiangchen Zhao, and Hengyu Liu.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_26': ['# The Large-Scale Collaborative Presence of Online Fandoms\\n\\nFan communities exist within every industry, and there has been little study on understanding their scale and how they influence the media and their industries. As technology and social media have made it easier than ever for fans to connect with their favorite influencers and find like-minded fans, we’ve seen a rise in fan culture or “fandom”. These individuals form fan groups and communities, which have become increasingly popular online and have rallied behind their favorite artists for different causes.<br><br>\\nThis repository contains library code to explore the similarities and differences in collaboration efforts among fans on two primary online social platforms, Twitter and Wikipedia. It contains methods to quantify the scale, strength, and influence of online fan communities—with a focus on the K-pop fanbase—and how this online collaboration affects outside audiences.\\n\\n## Materials\\n- [Website](https://kyleepeng.github.io/Fandom-Online-Collaboration/)\\n- [Report](https://raw.githubusercontent.com/dliu9999/artifact-directory-template/main/report.pdf)\\n\\n## Usage\\n\\n- Install dependencies (Navigate to the directory you cloned to)\\n`pip install -r requirements.txt`\\n\\n- Run (test) script:\\n`python run.py test` Runs the (all) script on test data found in `test/testdata`. Plots data and aggregate stats to `data/eda`',\n",
       "  'This summary discusses the presence and influence of fan communities in various industries. It highlights the rise of fan culture, facilitated by technology and social media, which has led to the formation of online fan groups and communities. The summary also mentions a repository containing code to explore collaboration efforts among fans on Twitter and Wikipedia, with a focus on the K-pop fanbase. The code helps quantify the scale, strength, and influence of online fan communities and examines how this collaboration affects outside audiences.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_27': ['# DSC180BProject: Wikipedia’s Response to the COVID-19 Pandemic \\n\\n\\nThis is the Wikipedia project working on its performance on providing COVID-19 pandemic information. Most of our data generated can be seen using certain targets, but \\nthere are also some analysis we made through notebook and we will specified those notebooks in the notebooks seciton.\\n\\n### Project Team Members:\\n- Yiheng Ye, yiy291@ucsd.edu\\n- Gabrielle Avila, ggavila@ucsd.edu\\n- Michael Lam, mel157@ucsd.edu\\n\\n### Requirements:\\n- python 3.8\\n- pandas 1.1.0\\n- wordcloud 1.8.1\\n- wikipedia 1.4.0\\n- sklearn 0.24.1\\n- gensim 3.8.3\\n- nltk 3.5\\n\\n### Code, Purpose, and Guideline:\\n\\n- run.py: If target=\\'data\\': Get top 1000 popular articles relating COVID-19 from Wikipedia. Get the pageview data for them in 2020.\\n          If target=\\'eda\\': Get top10 article with top average daily pageview and plot their daily views\\n          If target=\\'revision\": Get revision history for important pages and doing analysis with LDA model on them.\\n          if target=\\'word\\': Generates word cloud for Wikipedia, JHU, and WHO\\n          If target=\\'test\\': Runs test program about data: getting pageview on the test data and eda, getting revision data and doing LDA model on them, and \\n          generating word cloud.\\n- elt.py: the library for the data pipeline, see the documentation for detailed functions of every function writtened. Basically\\n          these functions are used to fulfill the job done in run.py.\\n- eda.py: the library for doing eda on data.\\n- revision.py: the library for analysis revision data\\n- word.py: the library for creating wordcloud\\n- config/data-params.json: it stores the links of the source data as well as the output path for raw data.\\n- code in src/data: the source code to fulfill the functions about processing data. The current usable files are get_data.py(getting top1000 articles\\'\\n  basic information) and get_apipageview.py(getting pageview from given article information csvs)\\n\\n### Notebooks\\nThe notebook file is primary serving as our original test base for code development. Additionally, it also has a notebook called Project EDA Single Webpage.ipynb which we investigate \"COVID-19 pandemic data\" page deeply.\\n\\nThere is also another notebook called \"Word Clouds.ipynb\" which produces word clouds on Wikipedia Coronavirus page, JHU page, and WHO page.\\n\\nThe \"top_model.ipynb\" generated LDA model for the LDA model on article \\'Coronavirus\", and this model needed to be open in a notebook to get visualization.\\n\\n## Responsibilities:\\n- Yiheng Ye set up the structure of the project and the structure of run.py. He also wrote get_data.py and get_apipageview.py and put them into the etl.py. He also \\n  wrote eda.py and eda_pageview.py\\n- Gabrielle Avila constructed our report and made deep analysis into the \"COVID-19 pandemic data\" page. She also made the \"Word Clouds.ipynb\"\\n- Michael Lam made LDA model analysis on the page \"Coronavirus\" and put them into the notebook \"top_model.ipynb\".',\n",
       "  'This is a Wikipedia project focused on providing information about the COVID-19 pandemic. The project team members are Yiheng Ye, Gabrielle Avila, and Michael Lam. The project requires Python 3.8 and various libraries such as pandas, wordcloud, wikipedia, sklearn, gensim, and nltk. The code includes different functionalities such as retrieving popular articles related to COVID-19, analyzing pageviews, analyzing revision history with LDA model, and generating word clouds. There are also notebooks for testing and specific analyses on COVID-19 data and word clouds. Each team member has specific responsibilities in the project.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_23': ['# DSC 180B Final Pipeline\\n---\\n## How to run\\n\\n```\\nusage: python run.py [-h] [-p] [-s] [-r]\\n\\noptional arguments:\\n  -h, --help       show help message and exit\\n  -p, --pages      obtain list of pages to analyze from Wikipedia\\n  -s, --sentiment  run sentiment analysis on pages from list\\n  -r, --results    obtain stats and visuals from sentiment analysis\\n  -t, --test       runs test suite\\n```\\n\\n## Purpose\\nThis program collects wikipedia data from URLs to analyze the sentiment of articles over time.\\n\\n## Config Formats\\nThe configuration .json files in the config folder can be used to change the program operation\\n### Pages\\n* language:      English, Spanish, or Chinese\\n* targets:       Wikipedia categories from which to analyze articles\\n* skip_cats:     Wikipedia categories to skip due to abundance of unnecessary articles\\n* output:        data file to write list of articles to\\n### Sentiment\\n* language:      English, Spanish, or Chinese\\n* infile:        data file to read in from\\n* outfile:       data file to write to\\n### Results\\n* language:      English, Spanish, or Chinese\\n* infile:        data file to read in from\\n* outfile:       data file to write to\\n### Test - Pages\\n* language:      English, Spanish, or Chinese\\n* targets:       Wikipedia categories from which to analyze articles\\n* skip_cats:     Wikipedia categories to skip due to abundance of unnecessary articles\\n* output:        data file to write list of articles to\\n### Test - Sentiment\\n* language:      English, Spanish, or Chinese\\n* infile:        data file to read in from\\n* outfile:       data file to write to\\n### Test - Results\\n* language:      English, Spanish, or Chinese\\n* infile:        data file to read in from\\n* outfile:       data file to write to\\n\\n---\\nYuanbo Shi\\n\\nHenry Lozada\\n\\nParth Patel\\n\\nEmma Logomasini\\n\\nUCSD Winter 2021\\n',\n",
       "  'This program is a final pipeline for DSC 180B. It collects Wikipedia data from URLs and analyzes the sentiment of articles over time. The program has different configurations for pages, sentiment analysis, and results. There are also test configurations for each category. The authors of this program are Yuanbo Shi, Henry Lozada, Parth Patel, and Emma Logomasini from UCSD Winter 2021.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_24': ['# Politics on Wikipedia\\nThis project is focused on detecting political controversy in online communities. We use a bag-of-words model and a party-embed model, trained on the ideological books corpus (Sim et al, 2013) as well as congressional record data (api.govinfo.gov), and attempt to generalize this to Wikipedia articles, validating it on edit comments which explicitly mention reverting bias.\\n\\n\\n## Usage\\n\\nThis code is intended to be run with the dockerfile vasyasha/pow_docker\\n\\nIt relies on data from the ideological books corpus (Sim et al., 2013) with sub-sentential annotations (Iyyer et al., 2014). To download this data please visit https://people.cs.umass.edu/~miyyer/ibc/index.html where you can send an email to the address in order to obtain the full dataset.\\n\\nOnce obtained, please extract the dataset to **/data/full_ibc/**\\n\\nOnce this is done, please alter the config in **/config/get_ibc_params** accordingly.\\n\\nTo run, in terminal type:\\n```\\npython run.py *target*\\n```\\n\\n## Description of Contents\\n\\n### `run.py`\\n\\nMain driver for running the project. The targets and their functions are:\\n* `scrape_anames` : scrapes political article names\\n* `retrieve_anames` : obtains political articles\\n* `ibc` : downloads test IBC data\\n* `interpret_ibc` : runs partyembed model on IBC data\\n* `revision_xmls` : downloads XML files for nine political Wikipedia articles\\n* `partyembed` : runs Rheault and Cochrane model on current-page Wikipedia articles\\n* `partyembed_time` : runs Rheault and Cochrane model on Wikipedia edit histories\\n* `all` : Runs the whole pipeline.\\n* `test`: Runs the pipeline with pre-loaded test data.\\n\\n### `config/`\\n\\n* `get_ibc_params.json` : Input parameters for running the ibc target.\\n\\n* `interpret_ibc_params.json` : Input parameters for running the interpret_ibc target.\\n\\n### `notebooks/`\\n\\n* `Partyembed+IBC_EDA.ipynb` : Jupyter notebook for the exploratory data analysis on Party_embed and IBC.\\n\\n### `src/`\\n\\n* `libcode.py` : Library code.\\n\\n### `src/etl/`\\n\\n* `bias.py` : Preliminary function for extracting bias from Rheault and Cochrane model.\\n* `get_anames.py` : Scrapes relevant article names.\\n* `get_atexts.py` : Scrapes article contents for the list gathered above.\\n* `get_ibc.py` : Downloads sample IBC data. For the full dataset, please see **Usage** above.\\n* `get_revision_xmls.py` : Downloads xml files using Wikipedia API for our time series analysis\\n\\n### `src/models/`\\n\\n* `difflib_bigrams.py` : Finds text difference between two article states\\n* `get_gns_scores.py` : Assigns scores to the article texts according to Gentzkow, Shapiro, Taddy 2019.\\n* `get_x2_scores.py` : Gets x2 scores based on the formula from Gentzkow and Shapiro 2010 from the IBC.\\n* `gns_histories.py` : Applies Gentzkow and Shapiro 2010 approach on edit histories\\n* `loadIBC.py` : This project uses code from (Sim et al., 2013) and (Iyyer et al., 2014). As this was written in a previous version of python, these updated versions replace downloads made during the building process.\\n* `partyembed_current_pages.py` : Applies partyembed model to get scores for the current pages of political Wikipedia articles\\n* `partyembed_ibc.py` : This file extracts from the partyembed .issue() function the ideological leanings of each word in each sentence of the ideological books corpus. After applying an aggregate function on this data, it writes this to a csv.\\n* `partyembed_revisions.py` : Applies partyembed model on edit histories to find change over time.\\n* `treeUtil.py` : This project uses code from (Sim et al., 2013) and (Iyyer et al., 2014). As this was written in a previous version of python, these updated versions replace downloads made during the building process.\\n\\n\\n## Sources\\n\\nPapers Referenced\\n* https://siepr.stanford.edu/sites/default/files/publications/16-028.pdf\\n\\n* https://www.cs.toronto.edu/~gh/2528/RheaultCochraneOct2018.pdf\\n\\nData\\n* https://people.cs.umass.edu/~miyyer/ibc/index.html\\n\\n* https://data.stanford.edu/congress_text\\n\\n* https://dumps.wikimedia.org\\n\\n',\n",
       "  'This project focuses on detecting political controversy in online communities, specifically on Wikipedia. It uses a bag-of-words model and a party-embed model trained on the ideological books corpus and congressional record data. The code provided can be run with the dockerfile vasyasha/pow_docker. The project includes various targets such as scraping political article names, obtaining political articles, downloading test IBC data, running the partyembed model on different datasets, and more. The code also includes configuration files and notebooks for exploratory data analysis. The sources referenced include papers and data used in the project.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_70': [\"Opioids Overdose Genome Analysis\\n==============================\\n\\n## Project Overview\\nOpioids are now one of the most common causes of accidental death in the US. According to statistics, two out of three drug overdose deaths in 2018 involved an opioid, so opioid abuse can not only affect people physically and mentally but can also deprive their lives (https://docs.google.com/document/d/1JXWb1Bla8iqvyKl3EUxJcGAWBWTvqfO6rRhn1kzrOr4/edit#bookmark=id.p3zzn76i4h3). Opioid addiction has a unique background in that a large reason for why people become addicted is that patients in hospitals are often prescribed opioids to treat pain, however these patients wind up misusing their prescriptions and become addicted.\\nThis is a data science project curated by Cathleen Peña, Zhaoyi Guo, and Dennis Wu. This github repo contains the codes that are essential to conduct the explicit visualization on the raw data gather from NCBI. \\n\\n## Website\\n\\nThe website that introduces the project is under https://genetics.denncc.com/\\n\\n## Running the Project \\n\\n### Pull the Docker Image\\nTo test on the project, in DSMLP, simply pull the image we've generated exclusively for this project by inputting:\\n\\n    $ launch-180.sh -i dencc/opioids-od:dw -G B04_Genetics\\n    \\n### Clone the Repository\\nIn the directory you want to run the project in, run\\n\\n    $ mkdir temp\\n    $ cd ./temp \\n    $ git clone https://github.com/denncc/opioids-od-genome-analysis\\n\\nThen you will be able to run the project the project after cloning\\n\\n### Test the project\\nTo test on the project, simply input\\n\\n    $ python run.py test\\n\\nYou will be able to see the testing procedure to run\\n\\n## Project Organization\\n```\\n📦opioids-od-genome-analysis\\n ┣ 📂config\\n ┃ ┣ 📜data_config.json\\n ┃ ┣ 📜feature_config.json\\n ┃ ┣ 📜model_config.json\\n ┃ ┣ 📜submission.json\\n ┃ ┗ 📜test_config.json\\n ┣ 📂data\\n ┃ ┣ 📂external\\n ┃ ┃ ┣ 📂bam\\n ┃ ┃ ┣ 📜.gitkeep\\n ┃ ┃ ┣ 📜GRCh38_latest_rna.fna\\n ┃ ┃ ┣ 📜Log.out\\n ┃ ┃ ┣ 📜SRA_case_table.csv\\n ┃ ┃ ┣ 📜chrLength.txt\\n ┃ ┃ ┣ 📜chrName.txt\\n ┃ ┃ ┣ 📜chrNameLength.txt\\n ┃ ┃ ┣ 📜chrStart.txt\\n ┃ ┃ ┣ 📜gencode.v24.annotation.gff3\\n ┃ ┃ ┣ 📜gencode.v24.annotation.gtf\\n ┃ ┃ ┣ 📜gencode.v24.annotation_mrna.gff\\n ┃ ┃ ┗ 📜genomeParameters.txt\\n ┃ ┣ 📂interim\\n ┃ ┃ ┣ 📜.gitkeep\\n ┃ ┃ ┣ 📜cts.tsv\\n ┃ ┃ ┗ 📜dds_res_before_filter.csv\\n ┃ ┣ 📂processed\\n ┃ ┃ ┣ 📂duplicates_removed\\n ┃ ┃ ┣ 📂htseq\\n ┃ ┃ ┣ 📂kallisto\\n ┃ ┃ ┣ 📂merged\\n ┃ ┃ ┣ 📂sorted\\n ┃ ┃ ┣ 📂temp\\n ┃ ┃ ┣ 📜.gitkeep\\n ┃ ┃ ┣ 📜htseq_cts.csv\\n ┃ ┃ ┣ 📜htseq_cts_1.csv\\n ┃ ┃ ┣ 📜htseq_cts_gene.csv\\n ┃ ┃ ┣ 📜htseq_cts_gene_filtered.csv\\n ┃ ┃ ┣ 📜kallisto_transcripts.idx\\n ┃ ┃ ┗ 📜test_gene_counts.csv\\n ┃ ┣ 📂raw\\n ┃ ┣ 📂test\\n ┃ ┃ ┣ 📂SRR7949794\\n ┃ ┃ ┃ ┣ 📜abundance.h5\\n ┃ ┃ ┃ ┣ 📜abundance.tsv\\n ┃ ┃ ┃ ┣ 📜pseudoalignments.bam\\n ┃ ┃ ┃ ┗ 📜run_info.json\\n ┃ ┃ ┣ 📜SRR7949794_1.fastq.gz\\n ┃ ┃ ┗ 📜SRR7949794_2.fastq.gz\\n ┃ ┗ 📜SRA_case_table.csv\\n ┣ 📂docs\\n ┃ ┣ 📜Makefile\\n ┃ ┣ 📜commands.rst\\n ┃ ┣ 📜conf.py\\n ┃ ┣ 📜getting-started.rst\\n ┃ ┣ 📜index.rst\\n ┃ ┗ 📜make.bat\\n ┣ 📂models\\n ┃ ┗ 📜.gitkeep\\n ┣ 📂notebooks\\n ┃ ┣ 📜.gitkeep\\n ┃ ┣ 📜EDA_python.ipynb\\n ┃ ┣ 📜EDA_r.ipynb\\n ┃ ┣ 📜HTSeq.ipynb\\n ┃ ┣ 📜SRA_eda.ipynb\\n ┃ ┗ 📜htseq_cts.py\\n ┣ 📂references\\n ┃ ┗ 📜.gitkeep\\n ┣ 📂reports\\n ┃ ┣ 📂figures\\n ┃ ┃ ┣ 📜.gitkeep\\n ┃ ┃ ┣ 📜Dist_of_Age.pdf\\n ┃ ┃ ┣ 📜Scatterplot_Matrix_All.pdf\\n ┃ ┃ ┣ 📜Scatterplot_Matrix_Users.pdf\\n ┃ ┃ ┣ 📜cocaine_use_diff_means.pdf\\n ┃ ┃ ┣ 📜cocaine_use_means.pdf\\n ┃ ┃ ┣ 📜diff_group_means.pdf\\n ┃ ┃ ┣ 📜drug_use_pie.pdf\\n ┃ ┃ ┣ 📜group_means.pdf\\n ┃ ┃ ┗ 📜race_pie.pdf\\n ┃ ┗ 📜.gitkeep\\n ┣ 📂src\\n ┃ ┣ 📂__pycache__\\n ┃ ┃ ┣ 📜__init__.cpython-36.pyc\\n ┃ ┃ ┗ 📜__init__.cpython-37.pyc\\n ┃ ┣ 📂data\\n ┃ ┃ ┣ 📂__pycache__\\n ┃ ┃ ┃ ┣ 📜__init__.cpython-36.pyc\\n ┃ ┃ ┃ ┣ 📜__init__.cpython-37.pyc\\n ┃ ┃ ┃ ┣ 📜import_data.cpython-36.pyc\\n ┃ ┃ ┃ ┗ 📜import_data.cpython-37.pyc\\n ┃ ┃ ┣ 📜.gitkeep\\n ┃ ┃ ┣ 📜__init__.py\\n ┃ ┃ ┣ 📜__init__.pyc\\n ┃ ┃ ┣ 📜import_data.py\\n ┃ ┃ ┗ 📜make_dataset.py\\n ┃ ┣ 📂features\\n ┃ ┃ ┣ 📂__pycache__\\n ┃ ┃ ┃ ┣ 📜__init__.cpython-36.pyc\\n ┃ ┃ ┃ ┣ 📜__init__.cpython-37.pyc\\n ┃ ┃ ┃ ┣ 📜build_features.cpython-36.pyc\\n ┃ ┃ ┃ ┗ 📜build_features.cpython-37.pyc\\n ┃ ┃ ┣ 📂r_scripts\\n ┃ ┃ ┃ ┗ 📜main.R\\n ┃ ┃ ┣ 📜.gitkeep\\n ┃ ┃ ┣ 📜__init__.py\\n ┃ ┃ ┗ 📜build_features.py\\n ┃ ┣ 📂models\\n ┃ ┃ ┣ 📂__pycache__\\n ┃ ┃ ┃ ┣ 📜__init__.cpython-37.pyc\\n ┃ ┃ ┃ ┣ 📜build_model.cpython-37.pyc\\n ┃ ┃ ┃ ┗ 📜htseq_cts.cpython-37.pyc\\n ┃ ┃ ┣ 📂r_scripts\\n ┃ ┃ ┃ ┣ 📜deseq2.R\\n ┃ ┃ ┃ ┣ 📜visualization.R\\n ┃ ┃ ┃ ┗ 📜wgcna.R\\n ┃ ┃ ┣ 📂sh_scripts\\n ┃ ┃ ┃ ┗ 📜samtools.sh\\n ┃ ┃ ┣ 📜.Rhistory\\n ┃ ┃ ┣ 📜.gitkeep\\n ┃ ┃ ┣ 📜__init__.py\\n ┃ ┃ ┣ 📜build_model.py\\n ┃ ┃ ┗ 📜htseq_cts.py\\n ┃ ┣ 📂visualization\\n ┃ ┃ ┣ 📜.gitkeep\\n ┃ ┃ ┣ 📜__init__.py\\n ┃ ┃ ┗ 📜visualize.py\\n ┃ ┣ 📜__init__.py\\n ┃ ┗ 📜__init__.pyc\\n ┣ 📜.gitignore\\n ┣ 📜Dockerfile\\n ┣ 📜LICENSE\\n ┣ 📜Makefile\\n ┣ 📜README.md\\n ┣ 📜command-line-htseq.txt\\n ┣ 📜r-bio.yaml\\n ┣ 📜requirements.txt\\n ┣ 📜run.py\\n ┣ 📜setup.py\\n ┣ 📜test_environment.py\\n ┗ 📜tox.ini\\n```\\n\",\n",
       "  'This project focuses on analyzing the genome of opioid overdose cases. Opioid addiction is a major problem in the US, leading to many accidental deaths. The project aims to visualize and analyze data gathered from the National Center for Biotechnology Information (NCBI). The project can be run using a Docker image, and the code is available on GitHub. The website for the project is https://genetics.denncc.com/.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_68': ['# antibiotic-resistance\\n Identifying the genetic basis of antibiotic resistance in E. Coli\\n',\n",
       "  'This article discusses the genetic basis of antibiotic resistance in E. Coli.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_66': ['# RNASeqToolComparison\\nData Science Senior Capstone Project: Comparing RNA Sequencing Differential Gene Expression Analysis Tools\\n\\nIn this project, we want to compare distinct differential gene expression analysis tools on simulated data created with different numbers of genes differentially expressed.\\n\\n## Running the project\\n* Use the command `launch.sh -i buijoseph21/rna-seq-tool-comparison:v1 -m 6 -P Always` in order to have the necessary software from `compcodeR` (e.g., `generateSyntheticData`, `runDiffExp`, `ABSSeq`, `PoissonSeq`, etc.) to generate the synthetic data & perform differential gene expression analysis. The `-m 6` specifies the number of RAM which is needed to run tools that require more memory. \\n\\n## Building the project using `run.py`\\n* Use the command `python run.py build` to generate the synthetic data in `data/data<N>.rds`, where N represents the dataset number, using `generateSyntheticData`\\n* Use the command `python run.py analysis` to perform `DESeq2`, `edgeR.exact`, `NOISeq`, `PoissonSeq`, `ttest`, `ABSSeq`, and `voom.limma` on the synthetic data created in `data` folder which returns the results in `out/data<N>_<tool>.rds`, where N represents the dataset number & tool represents the software. The output of each tool will be organized in its respective `<tool_name><synthetic_data_num>` folders in the `<tool_name>` folders. \\n* Use the command `python run.py graph` to build area under the curves (AUC), type I error rates, accuracy, sensitivity, specificity, and False Discovery Rates (FDR) graphs to compare how well the tools performed with each other. The output of each of the graphs will be stored in `rna_graphs`.\\n* Use the command `python run.py real` to run the whole pipeline mentioned above on the real life dataset: Post-Mortem Molecular Profiling of Schizophrenia, Bipolar Disorder, and Major Depressive Disorder (https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-017-0458-5). \\n\\n## Running the project on test data\\n* `ssh` into dsmlp and `git clone` the repository\\n* Use the command `launch.sh -i buijoseph21/rna-seq-tool-comparison:v1 -m 6` in order to have the necessary software from `compcodeR` (e.g., `generateSyntheticData`, `runDiffExp`, `PoissonSeq`, etc.) to generate the test synthetic data & perform differential gene expression analysis. The `-m 6` specifies the number of RAM which is needed to run tools that require more memory. \\n* Use the command `python run.py test` to create a test synthetic dataset of 100 genes differentially expressed where 50 are differentially expressed in condition 1 and 50 are differentially expressed in condition 2. This test dataset contains 5 samples per condition and is a baseline model with no outliers containing abnormal counts. When running `python run.py test`, it should first create a synthetic dataset which will be stored in the `data` folder and named as `test.rds`. Next, the 7 tools will be performed on the `test.rds` where the outputs will be stored in the `out/test` folder which is created when ran. Each tool will produce different results which is stored in `test_<tool>.rds`. For the next step of our pipeline, we want to produce the area under the curve for this test data, which will be stored in `/out/test/test_auc_plot.png` & the summary of all the metrics in `statistics.csv`.\\n\\n## Group Contributions\\n* Joseph built the dockerfile/container. He also created the starter code for building the synthetic datasets and performing differential expression analysis tools in `compcodeR` such as `DESeq2`, `edgeR`, `NOISeq`, `voom.limma`, and `ttest`. He was able to create an R script that read all the outputs from each tool mentioned previously to write out to a `num_expressed_by_tool.csv`. He created the notebook that illustrates the timings/duration for each of the tools performed on each synthetic dataset. As for the report, he helped write the Abstract, Background, Dataset (Figure 1), Methods (Creating the synthetic data, DESeq2, NOISeq, edgeR.exact), and briefly explained Figure 3. He also looked over other sections to add onto or revise. \\n* Brandon was responsible for creating the `random` outlier synthetic datasets and performing differential expression analysis tools not built-in `compcodeR`, including `ABSSeq` and `PoissonSeq`. Brandon helped write out to the `num_expressed_by_tool.csv` for his outputs produced by `ABSSeq` and `PoissonSeq`. As for the report, he helped write the Dataset and analysis of Figure 3. He also looked over other sections to add onto or revise. Brandon mainly focused on creating the graphs for the comparisons. \\n* Luigi was responsible for creating the `single` outlier and `poisson` synthetic datasets and helped perform `NOISeq` and `voom.limma` on the synthetic datasets. As for the report, Luigi created the citations, made revisions based on comments suggested by Shannon, and briefly described Figure 2. He wrote the Methods section for ABSSeq and PoissonSeq. Luigi was mainly responsible for the creation of the website and running/creating the pipeline for the real life dataset. \\n',\n",
       "  'This project compares different RNA sequencing differential gene expression analysis tools on simulated data with varying numbers of differentially expressed genes. The project includes instructions for running the project, building the project using `run.py`, running the project on test data, and group contributions.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_69': ['# Title: Genetic Overlap between Alzheimer\\'s, Parkinson’s, and healthy patients\\n\\n#### Capstone Project: Data Science DSC180B\\n\\n#### Section B04: Genetics\\n\\n#### Authors: Saroop Samra, Justin Lu, Xuanyu Wu\\n\\n#### Date : 2/2/2021\\n\\n### Overview\\n\\nThis repository code is for the replication project for the paper: Profiles of Extracellular miRNA in Cerebrospinal Fluid and Serum from Patients with Alzheimer’s and Parkinson’s Diseases Correlate with Disease Status and Features of Pathology (https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0094839). The data includes that miRNA sequences from tissues from two biofluids (serum and cerebrospinal fluid), and is from 69 patients with Alzheimer\\'s disease, 67 with Parkinson\\'s disease and 78 neurologically normal controls using next generation small RNA sequencing (NGS).\\n\\n\\n### Running the project\\n\\n•\\tTo install the dependencies, run the following command from the root directory of the project:\\xa0\\n\\n    pip install -r requirements.txt\\n\\n\\n### target: data\\n•\\tTo process the data, from the root project directory run the command:\\n\\n    python3 run.py data\\n\\n•   The data pipeline step takes the .fastq compressed files as input and then applies two transformations: process and align\\n\\n•\\tThis pipeline step also uses an additional CSV file that is the SRA run database, a sample looks like as follows:\\n\\n    Run expired_age    CONDITION    BIOFLUID     \\n    SRR1568567  40  Parkinson\\'s Disease Cerebrospinal \\n\\n\\n\\n•   The configuration files for the data step are stored in config/data-params.json. These include the parameters for the tools as well as the directories used for storing the raw, temporary and output files.\\n\\n    \"raw_data_directory\": \"./data/raw\",\\n    \"tmp_data_directory\": \"./data/tmp\",\\n    \"out_data_directory\": \"./data/out\",\\n\\n•   The configuration also includes an attribute to the SRA run input database (described above), and an attribute of where to store that in the data folder. Additional filter attributes are included for ease of use to avoid processing all patients, if this filter_enable is set it will only process a subset of SRA rows (filter_start_row to filter_start_row + filter_num_rows).\\n\\n    \"sra_runs\" : {\\n        \"input_database\" : \"/datasets/SRP046292/exRNA_Atlas_CORE_Results.csv\",\\n        \"input_database2\" : \"/datasets/SRP046292/SraRunTable.csv\",\\n        \"input_database3\" : \"/datasets/SRP046292/Table_S1.csv\",\\n        \"output_database\" : \"data/raw/exRNA_Atlas_CORE_Results.csv\",\\n        \"filter_enable\" : 0,\\n        \"filter_start_row\" : 120,\\n        \"filter_num_rows\" : 10   \\n    },\\n    \\n\\n•\\tAn optional transformation of the data is \"process\" that uses the following data configuration below that will invoke cutadapt which finds and removes adapter sequences. The attributes include the adapters (r1 and r2) to identify the start and end of pairs are a JSON array. The attribute enable allows to disable this cleaning step, instead it will simply copy the paired files from the source dataset. The arguments attribute allows flexible setting of any additional attribute to the cutadapt process. Finally, we have two wildcard paths that indicate the location of the SRA fastq pair files (fastq1 and fastq2).\\n\\n    \"process\" : {\\n        \"enable\" : 1,\\n        \"tool\" : \"/opt/conda/bin/cutadapt\",\\n        \"r1_adapters\" : [\"AAAAA\", \"GGGG\"],\\n        \"r2_adapters\" : [\"CCCCC\", \"TTTT\"],\\n        \"arguments\" : \"--pair-adapters --cores=4\",\\n        \"fastq1_path\" : \"/datasets/srp073813/%run_1.fastq.gz\", \\n        \"fastq2_path\" : \"/datasets/srp073813/%run_2.fastq.gz\"\\n    },\\n    \\n•   The second transformation of the data is \"aligncount\" that can be set to either use download, STAR or Kallisto. The choice is controlled by the aligncount attribute:\\n\\n    \"aligncount\" : \"download\",\\n\\n•   download step will use the ftp location of the gzip file in the Sra table and download using the curl command and unzips and the extracts the readCounts_gencode_sense.txt which represents thae gene counts for the sample. \\n\\n    \"download\" : {\\n        \"enable\" : 1,\\n        \"tool\" : \"curl\",\\n        \"arguments\" : \"-L -R\",\\n        \"read_counts_file\" : \"readCounts_gencode_sense.txt\"\\n    },\\n\\n•   kallisto uses the index_file attribute is the location of the directory of the reference genome, which for this replication project was GRCh37_E75. The arguments attribute allows flexible setting of any additional attribute to the kallisto process. Including the bootstaro samples.The attribute enable allows to disable this alignment step, this is useful for debugging the process prior step, for example, you can run quality checks on the processed fastq files before proceeding to alignment. \\n\\n    \"kallisto\" : {\\n        \"enable\" : 1,\\n        \"tool\" : \"/opt/kallisto_linux-v0.42.4/kallisto\",\\n        \"index_file\" : \"/datasets/srp073813/reference/kallisto_transcripts.idx\",\\n        \"arguments\" : \"quant -b 8 -t 8\"\\n    },\\n\\n•   STAR uses the gene_path attribute is the location of the directory of the reference genome, which for this replication project was GRCh37_E75 as described in the reference_gene attribute. The arguments attribute allows flexible setting of any additional attribute to the STAR process. Including TranscriptomeSAM in the quantMode arguments will also output bam files. Additionally, the log file gets outputted which has PRUA (percentage of reads uniquely aligned). The attribute enable allows to disable this alignment step, this is useful for debugging the process prior step, for example, you can run quality checks on the processed fastq files before proceeding to alignment. \\n\\n    \"STAR\" : {\\n        \"enable\" : 1,\\n        \"tool\" : \"/opt/STAR-2.5.2b/bin/Linux_x86_64_static/STAR\",\\n        \"reference_gene\" : \"GRCh37_E75\",\\n        \"gene_path\" : \"/path/to/genomeDir\",\\n        \"arguments\" : \"--runMode alignReads --quantMode GeneCounts --genomeLoad LoadAndKeep --readFilesCommand zcat --runThreadN 8\"\\n    },\\n\\n\\n\\n\\n•   The process and align transformation work on each of the samples. After each sample iteration, the temporary fastq files will be deleted to reduce storage requirements.\\n\\n\\n•   Example processing:\\n\\n    python3 run.py data\\n\\n    # ---------------------------------------------------\\n    # Process\\n    # ---------------------------------------------------\\n    # ---------------------------------------------------\\n    # Starting sample # 1 out of 1\\n    # ---------------------------------------------------\\n    # Starting sample # 1 out of 343\\n    curl-proxy -L -R -o ./data/tmp/SRR1568613.tgz ftp://ftp.genboree.org/exRNA-atlas/grp/Extracellular%20RNA%20Atlas/db/exRNA%20Repository%20-%20hg19/file/exRNA-atlas/exceRptPipeline_v4.6.2/KJENS1-Alzheimers_Parkinsons-2016-10-17/sample_SAMPLE_1022_CONTROL_SER_fastq/CORE_RESULTS/sample_SAMPLE_1022_CONTROL_SER_fastq_KJENS1-Alzheimers_Parkinsons-2016-10-17_CORE_RESULTS_v4.6.2.tgz\\n    sh: curl-proxy: command not found\\n    mkdir ./data/tmp/SRR1568613\\n    tar -C ./data/tmp/SRR1568613 -xzf ./data/tmp/SRR1568613.tgz\\n    cp ./data/tmp/SRR1568613/data/readCounts_gencode_sense.txt ./data/tmp/SRR1568613_ReadsPerGene.out.tab\\n    # ---------------------------------------------------\\n    # Starting sample # 2 out of 343\\n    curl-proxy -L -R -o ./data/tmp/SRR1568457.tgz ftp://ftp.genboree.org/exRNA-atlas/grp/Extracellular%20RNA%20Atlas/db/exRNA%20Repository%20-%20hg19/file/exRNA-atlas/exceRptPipeline_v4.6.2/KJENS1-Alzheimers_Parkinsons-2016-10-17/sample_SAMPLE_0427_PD_CSF_fastq/CORE_RESULTS/sample_SAMPLE_0427_PD_CSF_fastq_KJENS1-Alzheimers_Parkinsons-2016-10-17_CORE_RESULTS_v4.6.2.tgz\\n    sh: curl-proxy: command not found\\n    mkdir ./data/tmp/SRR1568457\\n    tar -C ./data/tmp/SRR1568457 -xzf ./data/tmp/SRR1568457.tgz\\n    cp ./data/tmp/SRR1568457/data/readCounts_gencode_sense.txt ./data/tmp/SRR1568457_ReadsPerGene.out.tab\\n    # ---------------------------------------------------\\n\\n\\n### target: merge\\n•   To merge gene count and/or BAM files generated from the data target, from the root project directory run the command:\\n\\n    python3 run.py merge\\n\\n•   The configuration files for the data step are stored in config/count-params.json. These include the parameters for the count merge and bam merge and it\\'s associated arguments.\\n\\n•   The format attrbute informs if to process downlload, kallisto (or STAR) files. The gene counts are merged into a TSV file and as well as a feature table based on the SRA run table. Additional STAR attributes in the JSON allow you to specify skiprows used when processing the  gene count files as well as identifying the column from the  gene matrix file to use as the column used to. There is an additional imputes attribute that allows you to impute any column with missing data. The attributes also include an optional \"filter_names\" gene table used to remove genes as well as removing false-positive genes. Finally, we can rename the feature columns before we save out the feature table.\\n\\n    \"count\" : {\\n        \"enable\" : 1,\\n        \"format\" : \"download\",\\n        \"skiprows\" : 4,\\n        \"column_count\" : 1,\\n        \"skip_samples\" : [\"SRR1568391\"],\\n        \"enable_filter\" : 0,\\n        \"filter_keep_genes\" : \"NM_\",\\n        \"filter_remove_genes\" : [\"chrX\", \"chrY\"],\\n        \"filter_names\" : \"/datasets/srp073813/reference/Gene_Naming.csv\",\\n        \"run_database\" : \"data/raw/exRNA_Atlas_CORE_Results.csv\",\\n        \"imputes\" : [\"TangleTotal\"],\\n        \"features\" : [\"Run\", \"CONDITION\", \"expired_age\", \"BIOFLUID\", \"sex\", \"PMI\", \"sn_depigmentation\", \"Braak score\", \"TangleTotal\", \"Plaque density\", \"PlaqueTotal\"],\\n        \"rename\" : {\"CONDITION\" : \"Disorder\", \"BIOFLUID\" : \"Biofluid\", \"Braak score\" : \"Braak_Score\", \"Plaque density\" : \"Plaque_density\"},\\n        \"replace\" : {\"from\":[\"Parkinson\\'s Disease\", \"Alzheimer\\'s Disease\", \"Cerebrospinal fluid\", \"Healthy Control\"], \"to\":[\"Parkinson\", \"Alzheimer\", \"Cerebrospinal\", \"Control\"]},\\n        \"output_matrix\" : \"data/out/gene_matrix.tsv\",\\n        \"output_features\" : \"data/out/features.tsv\"\\n    },\\n\\n•   For bam merging, which should not be enabled by default, we use the \"samtools\" merge feature that takes all the BAM files and combine them into one merged BAM file. \\n\\n\\n    \"bam\" : {\\n        \"enable\" : 0,\\n        \"output\" : \"data/tmp/merged.bam\",\\n        \"tool\" : \"/usr/local/bin/samtools\",\\n        \"arguments\" : \"merge --threads 8\"\\n    },\\n\\n\\n•   Example processing:\\n\\n    python3 run.py merge\\n\\n    # ---------------------------------------------------\\n    # Merge\\n    Input: SRR3438605_ReadsPerGene.out.tab\\n    Input: SRR3438604_ReadsPerGene.out.tab\\n    Output: data/out/gene_matrix.tsv data/out/features.tsv\\n    # Finished\\n    # ---------------------------------------------------\\n\\n\\n\\n### target: normalize\\n•   To normalize the aligned merge counts, from the root project directory run the command:\\n\\n    python3 run.py normalize\\n\\n•   The configuration files for the data step are stored in config/normalize-params.json. \\n\\n•   We use a custom R script which uses the DESeq2 module to take the input merged gene counts and the experiment features and outputs two normalized counts files. The analysis is done for all samples in the SRA run table. The output_dir sets the output location for the normalized count matrix files. One file is the standard normalized counts using the DESeq2 module, and the second normalized count file is after a Variable Stablization Transform (LRT). We also have a \"max_genes\" attribute that will filter the genes and removes ones that have little to no variance across disorder vesus control.\\n\\n•   The data JSON configuration file also holds an array of samples, a sample looks like as follows:\\n    \\n    {\\n        \"output_dir\" : \"data/out\",\\n        \"DESeq2\" : {\\n            \"Rscript\" : \"/opt/conda/envs/r-bio/bin/Rscript\",\\n            \"source\" : \"src/data/normalize.r\",\\n            \"input_counts\" : \"data/out/gene_matrix.tsv\",\\n            \"input_features\" : \"data/out/features.tsv\",\\n            \"max_genes\" : 8000\\n        },\\n        \"cleanup\" : 0,\\n        \"verbose\": 1\\n    }\\n\\n\\n•   Example processing:\\n\\n    python3 run.py normalize\\n\\n    # ---------------------------------------------------\\n    # Normalize\\n    Rscript  src/data/normalize.r data/out/gene_matrix.tsv data/out/features.tsv data/out/\\n    [1] \"Output data/out/normalized_counts.tsv data/out/vst_transformed_counts.tsv\"\\n    # Finished\\n    # ---------------------------------------------------\\n\\n\\n### target: analysis\\n•   To perform the analysis for the gene counts, from the root project directory run the command:\\n\\n    python3 run.py analysis\\n\\n•   The configuration files for the data step are stored in config/analysis-params.json. \\n\\n•   We use a custom R script which uses the DESeq2 module to take the input merged gene counts and the experiment features and outputs 2 sets of files for each biofluid region. Each biofluid region will compare a disorder versus Control. This will result in a total of 4 sets of files (2 biofluid regions x 2 disorder pair comparisons). Each output set includes a Likelihood Ratio Test (LRT) using the full and reduced model as specified in the attributes below as well as a MA-Plot and Heatmap. The additional attributes include the property of doing parallel processing for DESeq2.\\n    \\n    {\\n        \"output_prefix\" : \"data/out/%biofluid_region%\",\\n        \"DESeq2\" : {\\n            \"Rscript\" : \"/opt/conda/envs/r-bio/bin/Rscript\",\\n            \"biofluid_regions\" : [\"Cerebrospinal\", \"Serum\"],\\n            \"disorders\" : [\"Parkinson\", \"Alzheimer\"],\\n            \"control\" : \"Control\",\\n            \"input_counts\" : \"data/out/pca_normalized_counts.tsv\",\\n            \"input_features\" : \"data/out/features.tsv\",\\n            \"source\" : \"src/analysis/analysis.r\",\\n            \"full\" : \"expired_age+sex+PMI+sn_depigmentation+Braak_Score+TangleTotal+Plaque_density+PlaqueTotal+Disorder\",\\n            \"reduced\" : \"expired_age+sex+PMI+sn_depigmentation+Braak_Score+TangleTotal+Plaque_density+PlaqueTotal\",\\n            \"parallel\" : 0\\n        },\\n        \"cleanup\" : 0,\\n        \"verbose\": 1\\n    }\\n\\n\\n•   Example processing:\\n\\n    python3 run.py analysis\\n\\n    # ---------------------------------------------------\\n    # Analysis\\n    Cerebrospinal x Parkinson vs Control\\n    Rscript src/analysis/analysis.r data/out/Cerebrospinal/Parkinson/gene_matrix.tsv data/out/Cerebrospinal/Parkinson/features.tsv data/out/Cerebrospinal/Parkinson/ full=expired_age+sex+PMI+sn_depigmentation+Braak_Score+TangleTotal+Plaque_density+PlaqueTotal+Disorder reduced=expired_age+sex+PMI+sn_depigmentation+Braak_Score+TangleTotal+Plaque_density+PlaqueTotal charts=1 parallel=0\\n\\n\\n### target: visualize\\n\\n•   The visualize pipeline step can be invoked as follows:\\n\\n    python3 run.py visualize\\n\\n•   The configuration files for the data step are stored in config/visualize-params.json. The output will include multiple sets of charts: Gene Spread Variance Histogram, SRA Linear Correlation between SRA chart, MA-Plot 2x2 chart, Heat Map 2x2 chart, 2x2 Histogram, 4x4 Correlation Matrix and a Disorder Venn Diagram. Each chart type has flexible settings to control the input and layout for the charts as shown below:\\n\\n    \"gene_hist\" : {\\n        \"enable\" : 1,\\n        \"max_genes\" : 8000,\\n        \"nbins\" : 100,\\n        \"title\" : \"Distribution of Genes Based on Spread Metric: All vs Top Genes\"\\n    },\\n    \"missing_plot\" : {\\n        \"enable\" : 1,\\n        \"title\" : \"Percentage of Missing Genes over\"\\n    },\\n    \"sra_lm\" : {\\n        \"enable\" : 1,\\n        \"sra\" : [\"SRR1568567\", \"SRR1568584\"],\\n        \"normalized_counts\" : \"data/out/normalized_counts.tsv\",\\n        \"vst_counts\" : \"data/out/vst_transformed_counts.tsv\",\\n        \"title\" : \"%sra% Regression Log(Norm) v VST counts\"\\n    },\\n    \"ma_plot\" : {\\n        \"enable\" : 1,\\n        \"biofluid_regions\" : [\"Cerebrospinal\", \"Serum\"],\\n        \"disorders\" : [\"Parkinson\", \"Alzheimer\"],\\n        \"src_image\" : \"MAplot.png\",\\n        \"title\" : \"MA Plot: Biofluid Region vs Disorder\"\\n    },\\n    \"heat_map\" : {\\n        \"enable\" : 1,\\n        \"biofluid_regions\" : [\"Cerebrospinal\", \"Serum\"],\\n        \"disorders\" : [\"Parkinson\", \"Alzheimer\"],\\n        \"src_image\" : \"heatmap.png\",\\n        \"title\" : \"Heat Map: Biofluid Region vs Disorder\"\\n    },\\n    \"histogram\" : {\\n        \"enable\" : 1,\\n        \"biofluid_regions\" : [\"Cerebrospinal\", \"Serum\"],\\n        \"disorders\" : [\"Parkinson\", \"Alzheimer\"],\\n        \"title\" : \"Histograms Differential Gene Expression vs Control\",\\n        \"ylim\" : 55\\n    },\\n    \"corrmatrix\" : {\\n        \"enable\" : 1,\\n        \"title\" : \"Spearman Correlations of log2 fold gene expression\"\\n    },\\n    \"venn\" : {\\n        \"enable\" : 1,\\n        \"biofluid_regions\" : [\"Cerebrospinal\", \"Serum\"],\\n        \"disorders\" : [\"Parkinson\", \"Alzheimer\"],\\n        \"pvalue_cutoff\" : 0.05,\\n        \"title\" : \"Venn Diagram Disorders\"\\n    },\\n\\n\\n•   Example processing:\\n\\n    python3 run.py visualize\\n\\n    # ---------------------------------------------------\\n    # Visualize\\n    # Finished\\n    # ---------------------------------------------------\\n\\n\\n### target: qc\\n\\n•   The quality pipeline step can be invoked as follows:\\n\\n    python3 run.py qc\\n\\n•   The configuration files for the data step are stored in config/qc-params.json. These include the parameters for the output directory where the quality HTML reports will be outputted. \\n\\n    \"outdir\" : \"data/out\",\\n    \"inputs\" : \"data/tmp\",\\n\\n•   For fastq files, the quality tool attribute is set to fastqc and that includes attributes to extract reports or keep them in a zip file. To enable this quality check make sure you set the cleanup to 0 in the data configuration pipeline as well as to disable the STAR processing, this will retain the fastq.qz files after the data pipeline step is executed.\\n\\n    \"fastq\" : {\\n        \"enable\" : 1,\\n        \"tool\" : \"/opt/FastQC/fastqc\",\\n        \"extract\" : 1   \\n    },\\n\\n•   For bam files, the quality tool attribute is set to picard and that includes attributes such as collecting alignment summary metrics. To enable this quality check make sure you set the cleanup to 0 in the data configuration pipeline and add \\'TranscriptomeSAM\\' to the arguments for STAR which will then output BAM files that will be retained after the data pipeline step is executed.\\n\\n    \"bam\" : {\\n        \"enable\" : 1,\\n        \"tool\" : \"java\",\\n        \"jar\" : \"/opt/picard-tools-1.88/CollectAlignmentSummaryMetrics.jar\"\\n    },\\n    \\n\\n•   Example processing:\\n\\n    python3 run.py qc\\n\\n    # ---------------------------------------------------\\n    # Quality Check\\n    fastqc data/tmp/out.1.fastq.gz --outdir=data/out --extract\\n    fastqc data/tmp/out.2.fastq.gz --outdir=data/out --extract\\n    java -jar /opt/picard-tools-1.88/CollectAlignmentSummaryMetrics.jar INPUT=data/tmp/SRR3438604_Aligned.bam OUTPUT=data/out/SRR3438604_Aligned.bam.txt\\n    java -jar /opt/picard-tools-1.88/CollectAlignmentSummaryMetrics.jar INPUT=data/tmp/SRR3438605_Aligned.bam OUTPUT=data/out/SRR3438605_Aligned.bam.txt\\n    # Finished\\n    # ---------------------------------------------------\\n\\n\\n### target: report\\n•   To generate the report from the notebook, run this command:\\n\\n    python3 run.py report\\n\\n•   The configuration files for the data step are stored in config/report-params.json. \\n\\n    {\\n        \"tool\": \"jupyter\",\\n        \"args\": \"nbconvert --no-input --to html --output report.html notebooks/report.ipynb\",\\n        \"verbose\" : 1\\n    }\\n\\n\\n### target: clean \\n\\n•\\tTo clean the data (remove it from the working project), from the root project directory run the command:\\n\\npython3 run.py clean\\n\\n\\n### target: all \\n\\n•   The all target will execute the following steps in sequence: data, merge, normalize, analysis and visualize. It can be executed as follows:\\n\\npython3 run.py all\\n\\n\\n### Future Work\\n\\n•\\tNew pipeline step: predict. This step will use the model to predict the classification for a given miRNA sequences on the test data and reporting the classification errors\\n\\n\\n\\n### Major Change History\\n\\n\\nDate:  2/2/2021\\n\\nWork completed:\\n\\n- Created new visualization, Volcano Plot, wrote the code and implemented it into our pipeline in the visualize step \\n- Updated the code pipeline to make the correlation matrix more meaningful by adding color\\n- Finished descriptions for EDA plots\\n\\n\\nDate:  1/19/2021\\n\\nWork completed:\\n\\n- Got all steps in pipeline to work with new data (data, merge, normalize, analysis, visualize) \\n- Used LRT Hypothesis Testing and have updated all previous quarter visualizations to work for our new data set\\n- Compared the outputs of 2 samples that failed FastQC/ERCC quality check with 2 samples that passed\\n- Developed and organized EDA code for gene matrix (missingness, correlation between sequence count and numerical features of the samples) \\n\\n\\nDate:  1/12/2020\\n\\nWork completed:\\n\\n- Created repo, initial version from the DSC180A Genetics project\\n- Added new download step and modified data target to use new SRA\\n- Wrote out background information/introduction sections of the report, researched our diseases (Alzheimer’s/Parkinson’s) and data sources (miRNA, serum/CSF)\\n- Developed and organized EDA code for features in the SRA run table (box plots, histograms, bar plots, etc)\\n\\n\\n\\n### Responsibilities\\n\\n\\n* Saroop Samra, developed the original codebase based on the DSC180A genetics replication project. Saroop ported the code to support the new miRNA dataset including adding a new download step in the data target. She worked on modifying the code and configuration files for the merge, normalize, analysis and visualize targets to process and generate the visualizations from the DSC180A project. She got the new Volcano Plot to work for our dataset and wrote basic descriptions for the visualizations including what significant patterns exists (MA plot, heatmap, histogram, venn diagram, correlation matrix).\\n\\n* Justin Lu, wrote out background information/introduction sections of the report. Justin did the data quality control check with FastQC (focused on the FastQC report outputs that we acquired instead of actually running FastQC since we still do not have access to the raw .fastq data), wrote in descriptions for visualizations and some of the EDA in our final report notebook. He updated the code pipeline with colored correlation matrix.\\n\\n* Xuanyu Wu, generated around 20 EDA plots for features that describe our merged dataset (incl. box plots, histograms, bar plots, etc) Xuanyu created EDA plots to explore the missingness of the gene count matrix and the basic correlation of each sequence with the numerical features we have selected. She also finished the descriptions for EDA plots and analysis.\\n\\n\\n\\n\\n',\n",
       "  \"This repository code is for a replication project that focuses on the genetic overlap between Alzheimer's disease, Parkinson's disease, and healthy patients. The data includes miRNA sequences from tissues from two biofluids (serum and cerebrospinal fluid) and is obtained using next-generation small RNA sequencing. The project includes several pipeline steps such as data processing, merging gene counts, normalization, analysis, visualization, quality control check, and report generation. The code and configuration files are provided for each step of the pipeline. The project also includes future work plans and a change history section detailing the progress made by the authors. The responsibilities of each author are also mentioned.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_9': ['# Live_vs_Vod_Project\\n## Group Name: Live\\n\\nDue to the variety, affordability and convenience of online video streaming, there are more subscribers than ever to video streaming platforms. Moreover, the decreased operation of non-essential businesses and increase in the number of people working from home in this past year has further compounded this effect. More people are streaming live lectures, sports, news, and video calls via the internet at home today than we have ever seen before. Internet Service Providers, such as Viasat, are tasked with optimizing  internet connections and tailoring their allocation of resources to fit each unique customer’s needs. With this increase in internet activity, it would be especially beneficial for Viasat to understand what issues arise when customers stream various forms of video. In general, different internet activities require different resources to optimize the connection. For example, if a customer watches a lot of live video they may prefer a connection with lower latency and higher bandwidth. Although we are able to identify the genre of an activity when a user is not using a VPN, the challenge arises when a user chooses to surf the web through a VPN. When it comes to VPN use cases we can’t identify a user’s unique activity when they experience issues, thus making us unable to successfully troubleshoot  those problems. This is where a tool that could identify various internet activities, specifically live or uploaded video streaming, within a VPN tunnel would be extremely useful for an Internet Service Provider. \\n\\n## Project Report: \\nFound within the **references** folder. The file name is Final_Report.\\n\\n## Guide for Launching this Project:\\nNote: These instructions assume that the user has access to the DSMLP server to be able to run this project. Open terminal, run these commands in the associated order:\\n\\n1.) **ssh user@dsmlp-login.ucsd.edu** (user refers to your school username). Enter credentials.\\n\\n2.) **launch-180-gid.sh -G 100011655 -P Always -i apristin99/live_vs_vod_project**\\n\\n3.) **git clone https://github.com/pristinsky1/live_vs_video_on_demand_VPN_detection.git**\\n\\n4.) **cd live_vs_video_on_demand_VPN_detection**\\n\\nNow you are within the right directory with the environment already set up! Start running the files!\\n\\n5.) For files needed to be predict, you have to put them in the \"data/in\" directory. Acceptable files are files generated network-stats tool provided by Viasat.\\n\\n6.) If you wish to train a new classifier based on new data or new type of model and parameters, you have to change the location of training data and other settings in the \"config/train-parmas.json\". Otherwise, you can directly run **python run.py predict** using the trained model contained in the project.\\n\\n\\n## Guide for Pipeline Testing:\\nRun **python run.py test** and the results will be in **test/out**. Within that folder, it contains the output dataframes, model, and reports of accuracies for the test data.\\n\\n\\n## Contents:\\nThere are three parts of contents:\\n1. src folder - Contains all library code.\\n2. config folder - Contains directory of each target.\\n3. run.py - Main Program for this project.\\n4. notebooks folder - stores notebooks for this project.\\n5. data/out folder - stores results of this project.\\n\\nThe two files to look at for results under data/out:\\n\\ntraining_report.json: Json file contains the report of model\\'s basic information and its performance on validation set.\\npredictions.csv: DataFrame contains basic information of one record generated by network-stats and its prediction result.\\n\\n\\n\\n## How to run it?\\nWarning: The feature, train and predict has to be run in fixed order. If you want to run everything at once, you can use the all script.\\n\\nUse console to run **python run.py eda** as a script. This will run the eda notebook and store the html version for easy accessibility under \"/notebooks\". \\n\\nUse console to run **python run.py feature** as a script. This will create the features. The output dataframe will be stored in \"data/out\" directory in csv format.\\n\\nUse console to run **python run.py train** as a script. This will train the model. The output model and report will be stored in \"data/out\" directory in csv format.\\n\\nUse console to run **python run.py predict** as a script. This will classify the input dataset as live or streaming. The output dataframe will be stored in \"data/out\" directory in csv format.\\n\\nUse console to run **python run.py all** as a script. This will run everything listed above. The output dataframes and model and report will be stored in \"data/out\" directory in csv format.\\n\\n\\n## Description of Each Params Files\\n\"feature-params.json\" -- \"indir: the input directory of training set, outdir: the output directory of generated dataframe, output: 1 means output dataframe containing features information, 0 means only return it as a dataframe(Must be 1 in feature-params.json)\"\\n\\n\"train-params.json\" -- \\n    :param: indir: file directory where extracted features stored.\\n    :param: outdir: file directory where output of this funcition stored.\\n    :param: testsize: the portion of train dataset used for validation.\\n    :param: randomstate: the randomstate number to random split train and valid set.\\n    :param: method: the classifier name used for training.\\n    :param: method_parameters: the parameter used for training.\\n\\n\"predict-params.json\" -- \"indir: the input directory of stored model, indir2: the input directory of testset, outdir: the output directory of test result.\"\\n\\n\\n\\n```\\n### Responsibilities\\n\\n* Da Gong developed the structure of this project.\\n* Zishun Jin worked on the prediction model and the model features of this project.\\n* Tianran Qiu worked on the prediction model and the model features of this project.\\n* Andrey developed the environment and the model feature creation for this project.\\n* Mariam worked on the final report and model feature creation for this project. \\n```\\n\\n\\n\\n\\n### Website\\n\\nLink to the webpage: https://pristinsky1.github.io/live_vs_video_on_demand_VPN_detection/\\n',\n",
       "  'The Live_vs_Vod_Project focuses on understanding the issues that arise when customers stream various forms of video through a VPN tunnel. The project aims to develop a tool that can identify different internet activities, specifically live or uploaded video streaming, within a VPN tunnel. The project report can be found in the references folder with the file name Final_Report. To launch the project, follow the provided instructions assuming access to the DSMLP server. The project includes code files, notebooks, and data folders for testing and running different parts of the project. The website link for more information is https://pristinsky1.github.io/live_vs_video_on_demand_VPN_detection/.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_8': [\"# DANE - Data Automation and Network Emulation Tool\\n\\n<img align='right' src='docs/media/dane-transparent-small.png' height=248>\\n\\nDANE is a hackable dataset generation tool to collect network traffic in a variety of configurable network conditions.\\n\\nIt runs on Windows, Mac, and Linux.\\n\\n**Table of contents**\\n- [Why use DANE?](#why-use-dane)\\n- [Documentation](#documentation)\\n- [Contributing](#contributing)\\n- [Acknowledgements](#acknowledgements)\\n\\n\\n## Why use DANE?\\n\\nDANE provides two core functionalities:\\n\\n1. Automatically collect network traffic datasets in a parallelized manner\\n\\n   Manual data collection for network traffic datasets is a long and tedious process—run the tool and you can easily collect multiple hours of data in one hour of time (magic!) with one or many desired 'user' behaviors.\\n   \\n2. Emulate a diverse range of network conditions that are representative of the real world\\n\\n   Data representation is an increasingly relevant issue in all fields of data science, but generating a dataset while connected to a fixed network doesn't capture diversity in network conditions—in a single file, you can configure DANE to emulate a variety of network conditions, including latency and bandwidth.\\n\\nYou can easily hack the tool to run custom scripts, custom data collection tools, and other custom software dependencies which support your particular research interest.\\n\\n## Documentation\\n\\nFor all documentation, including a [quick start](https://dane-tool.github.io/dane/guide/quickstart.html), details about the [technical approach](https://dane-tool.github.io/dane/guide/approach.html), and [FAQs](https://dane-tool.github.io/dane/guide/faq.html), please consult the [**website 📖**](https://dane-tool.github.io/dane).  \\nhttps://dane-tool.github.io/dane\\n\\n## Contributing\\n\\nSee something you'd like improved? Better yet, have some improvements coded up locally you'd like to contribute?\\n\\nWe welcome you to **submit an Issue** or **make a Pull Request** detailing your ideas!\\n\\n## Acknowledgements\\n\\nThis project was originally created in affiliation with the **Halıcıoğlu Data Science Institute**'s data science program at UC San Diego.  \\nhttps://hdsi.ucsd.edu/, https://dsc-capstone.github.io/\\n\\nDANE was motivated and developed with the generous support of **Viasat**.  \\nhttps://viasat.com/\\n\",\n",
       "  \"DANE is a dataset generation tool that collects network traffic under various network conditions. It can run on Windows, Mac, and Linux. The tool provides automated data collection and the ability to emulate different network conditions. Users can customize the tool by running custom scripts and software dependencies. Documentation, including a quick start guide and FAQs, can be found on the website. Contributions are welcome through issue submissions or pull requests. The project was created in affiliation with the Halıcıoğlu Data Science Institute's program at UC San Diego and was supported by Viasat.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_7': ['# ResRecovery\\n\\nWebsite: https://stephdoan.github.io/ResRecovery/\\n\\n# Table of Contents\\n\\n1. [Abstract](#Abstract)\\n2. [Config Files](#config)\\n   - [`train-params.json`](#train)\\n   - [`model-params.json`](#model)\\n   - [`user-data.json`](#user)\\n   - [`generate-data.json`](#generate)\\n3. [Running the Project](#running)\\n\\n## Abstract\\n\\nVirtual private networks, or VPNs, have seen a growth in popularity as more of the general population has come to realize the importance of maintaining data privacy and security while browsing the Internet. In previous works, our domain developed robust classifiers that could identify when a user was streaming video. As an extension, our group has developed a Random Forest model that determines the resolution at the time of video streaming. Our final model has an overall accuracy of **87%**.\\n\\n<a name=\"config\"></a>\\n\\n## Configuration Files\\n\\n<a name=\"train\"></a>\\n\\n### `train-params.json`\\n\\nAllows users to adjust some parameters of the training data creation process. The main point of focus is the `{interval}` argument. This allows users to adjust how big of a chunk size they would like their model to be trained on. The default is 300 seconds as it allows replication of our project.\\n\\n| Parameter     | Description                                                                                                     |\\n| ------------- | --------------------------------------------------------------------------------------------------------------- |\\n| folder_path   | path to where all of the raw data is stored; please refer to the folder structure below to achieve best results |\\n| interval      | chunk size                                                                                                      |\\n| threshold     | minimum megabit value; used in peak feature creation                                                            |\\n| prominence_fs | sampling rate to find the max peak prominence                                                                   |\\n| binned_fs     | deprecated parameter                                                                                            |\\n\\n##### Data Folder Structure\\n\\nAll of the training data should be stored in an accessible `data` folder. CSV files should be categorized into folders according to their resolution below.\\n\\n```\\n+-- data\\n |\\n +-- 144p\\n +-- 240p\\n +-- 360p\\n +-- 480p\\n +-- 720p\\n +-- 1080p\\n```\\n\\n<a name=\"model\"></a>\\n\\n### `model-params.json`\\n\\nAllows users to adjust hyperparameters of the random forest classifier. The default values are the values we utilized in our original project.\\n\\n| Parameter         | Description                                                              |\\n| ----------------- | ------------------------------------------------------------------------ |\\n| training_data     | path to where training data is stored; data must be stored as a CSV file |\\n| n_estimators      | number of trees in the forest model                                      |\\n| max_depth         | max depth of the tree                                                    |\\n| min_samples_split | minimum number of samples required to split an internal node             |\\n\\n<a name=\"user\"></a>\\n\\n### `user-data.json`\\n\\nAllows users to input their own data to be classified by the model.\\n\\n| Parameter     | Description                                                                                                                     |\\n| ------------- | ------------------------------------------------------------------------------------------------------------------------------- |\\n| path          | path to where all of the raw user data is stored; must be an output of [network-stats](https://github.com/viasat/network-stats) |\\n| interval      | chunk size                                                                                                                      |\\n| threshold     | minimum megabit value; used in peak feature creation                                                                            |\\n| prominence_fs | sampling rate to find the max peak prominence                                                                                   |\\n| binned_fs     | deprecated parameter                                                                                                            |\\n\\n<a name=\"generate\"></a>\\n\\n### `generate-data.json`\\n\\nParameters used by the `generate_data.py` script. Please refer to [Selenium](https://www.selenium.dev/documentation/en/) documentation to install the appropriate `webdriver.exe`. For best use, please configure the `PATH` variable in the `generate-data.py` file to the correct file path of the webdriver. This script was developed using Google Chrome.\\n\\n| Parameter          | Description                                                                                                     |\\n| ------------------ | --------------------------------------------------------------------------------------------------------------- |\\n| network_stats_path | location of network-stats.py                                                                                    |\\n| interface          | user interface to collect from; refer to [network-stats](https://github.com/viasat/network-stats) documentation |\\n| playlist           | link to YouTube playlist                                                                                        |\\n| outdir             | to be implemented                                                                                               |\\n| resolutions        | list of resolutions to be collected                                                                             |\\n\\n<a name=\"running\"></a>\\n\\n## Running the Project\\n\\nThe project is current set to the assumption that users will collect their own training data. There is a repository of available training hosted on the DSMLP server located at `/teams/DSC180A_FA20_A00/b05vpnxray/personal_stdoan/data`. If not accessible, please refer to the [`generate_data.json`](#generate) configurations to automate collection of a training set.\\n\\n#### Running on the DSMLP Server\\n\\nThe project was mean to be run on the UCSD DSMLP server. Below are instructions if user has access to DSMLP resources.\\n\\n1. Open up a terminal and run the command below to log onto the server. Users will need to provide appropriate identification when asked.\\n\\n> `ssh [username]@dsmlp-login.ucsd.edu`\\n\\n2. Launch a docker container to ensure package dependencies are fulfilled by running the command:\\n\\n> `launch-180-gid.sh -G 100011652 -P Always stdoan/viasat-q1`\\n\\n3. Clone this repository.\\n\\n4. Adjust config files as necessary and then run the targets!\\n\\n#### Targets\\n\\n- `python run.py test` will test the various targets to ensure that all methods are running properly.\\n\\n- `python run.py clean` will delete files created from running various targets. The folder and files are deleted from the local machine.\\n\\n- `python run.py features` will create features from data specified in `train-params.json`.\\n\\n- `python run.py predict` will either create training data to create a model or utilize a Pickle\\'d model that we have included. Output is an array of resolution label for each chunk in the data.\\n',\n",
       "  'The ResRecovery project is focused on developing a Random Forest model that determines the resolution of video streaming. The project provides several configuration files (`train-params.json`, `model-params.json`, `user-data.json`, and `generate-data.json`) that allow users to adjust parameters and input their own data for classification. The project can be run on the UCSD DSMLP server, and it offers targets for testing, cleaning, creating features, and predicting video resolutions.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_10': [\"# Data Science Senior Capstone - Viasat VPN Analysis\\n\\n**Table of Contents**:\\n- [Link to Blog page](#blog-page)\\n- [Abstract](#abstract)\\n- [Approach](#approach)\\n- [Running](#running)\\n  - [Setup](#setup)\\n  - [Logging](#logging)\\n  - [Target `data`](#target-data)\\n  - [Target `features`](#target-features)\\n  - [Target `train`](#target-train)\\n- [Report](#report)\\n\\n## Link to Blog page\\nhttps://mhrowlan.github.io/streaming_provider_classifier_inside_vpn/\\n## Abstract\\n\\nWhether to access another country's Netflix library or for privacy, more people are using Virtual Private Networks (VPN) to stream videos than ever before. However, many of the different service providers offer different user experiences that can lead to differences in the network transmissions. This repository contains the implementation of our classifying model to determine what streaming service provider was being used over a VPN. The streaming providers that the model identifies are Amazon Prime, Youtube, Netflix, Youtube Live, Twitch, and an other category consiting of Disney+, Discovery+, and Hulu. This is valuable in understanding the differences in the network work patterns for the different streaming service providers. We achieve an average accuracy of 96.5% on our Random Forest model.\\n\\n## Approach\\n\\nWe utilize Viasat's [`network-stats`](https://github.com/Viasat/network-stats) to collect network traffic on a per-second, per-connection basis while we are connected to a VPN, then engage in either internet browsing or video streaming behavior.\\n\\nUtilizing the output of network-stats, we extract packet-level measurements and engineer features based on the packet sizes, arrival times, and directions.\\n\\nWe leverage these features in a classification model to determine whether or not a network-stats output contains video streaming activity.\\n\\n## Running\\n\\nThe following targets can be run by calling `python run.py <target_name>`. These targets perform various aspects of the data collection, cleaning, engineering, training, and predicting pipeline.\\n\\n### Setup\\n\\nTo leverage the existing dataset, you must be a member of DSMLP and have access to the shared /teams/ directory.\\n\\nLog on to DSMLP via `ssh <username>@dsmlp-login.ucsd.edu`\\n\\nLaunch a Docker container with the necessary components via `launch-180.sh -i jeq004/streaming_provider_classifier_inside_vpn -G B05_VPN_XRAY -c 8 -g 1 -m 64`\\n\\nClone this repository: `git clone https://github.com/mhrowlan/streaming_provider_classifier_inside_vpn.git`\\n\\nNavigate to this repository `cd streaming_provider_classifier_inside_vpn`\\n\\nNow, you are ready to configure targets for our project build. Details are specified below.\\n\\n### Logging\\n\\nLogging behavior can be configured in `config/logging.json`.\\n| Key | Description |\\n| --- | --- |\\n| produce_logs | Boolean. Whether or not to write to the log file. Default: `true` |\\n| log_file | Path to the log file. Default: `data/logs/project_run.log` |\\n\\n### Target `data`\\n\\nLoads data from a source directory then performs cleaning and preprocessing steps on each file. Saves the preprocessed data to a intermediate directory.\\n\\nIf on DSMLP with the proper group membership, this target will symlink existing data from the shared /teams/ directory.\\n\\nSee `config/data-params.json` for configuration:\\n| Key | Description |\\n| --- | --- |\\n| source_dir | Path to directory containing raw data. Default: `data/raw/` |\\n| out_dir | Path to store preprocessed data. Default: `data/preprocessed/` |\\n\\n### Target `features`\\n\\nEngineers features on the preprocessed data with configurable parameters and saves to an output directory.\\n\\nSee `config/features-params.json` for configuration:\\n| Key | Description |\\n| --- | --- |\\n| source_dir | Path to directory containing preprocessed data. Default: `data/preprocessed/` |\\n| out_dir | Path to directory to store feature engineered data. Default: `data/features/` |\\n| chunk_size | Milliseconds. We split our variable length data into smaller chunks with consistent time spans. Default: `90000` |\\n| rolling_window_1 | Milliseconds. We generate a smoothed mean using rolling windows of multiple lengths, this is the first length. Default `10000` |\\n| rolling_window_1 | Milliseconds. This is the second length for our smoothed means. Default `60000` |\\n| resample_rate | Time offset. We resample our packet measurements to produce a consistent sample-rate signal for spectral analysis. Default `500ms` |\\n| frequency | Hertz. We compute a power spectral density feature on a signal of this sample rate. Default `2.0` |\\n\\n### Target `train`\\n\\nTrains a classifier based on the new features and outputs the accuracy between the predicted and true labels. In other words, it prints out the percentage of cases that were correctly classified as streaming.\\n\\nSee `config/train-params.json` for configuration:\\n| Key | Description |\\n| --- | --- |\\n| source | Path to csv containing feature engineered data. Default: `data/features/features.csv` |\\n| out | Path to .pkl file which will save the trained model. Default: `data/out/model.pkl` |\\n| validation_size | Proportion. This amount of training data will be withheld to evaluate the performance of the trained classifier. Default: `0.3` |\\n| classifier | String name of scikit-learn classifier to use. One of 'RandomForest', 'KNN', or 'LogisticRegression'. Default: `RandomForest` |\\n| model_params | Scikit-Learn hyperparameters for the chosen model. See scikit-learn documentation. |\\n\\n### Target `all`\\n\\nRuns `data`, `features`, `train` in order.\\n\\n### Target `test`\\n\\nRuns `data`, `features`, `train` with configuration found in `test/config/`.\\n\\nCan optionally specify targets after test to only run that target. For example `python run.py test data` will only run the data target with the test config.\\n\\n## Report\\n\\nAn academic report on the exploration and model built in this repository can be found at [`SPICIVPN_report.pdf`](SPICIVPN_report.pdf)\\n\",\n",
       "  'This is a summary of the \"Data Science Senior Capstone - Viasat VPN Analysis\" project. The project focuses on analyzing Virtual Private Networks (VPN) used for streaming videos. The goal is to classify the streaming service provider being used over a VPN, such as Amazon Prime, Youtube, Netflix, Youtube Live, Twitch, and others. The project achieves an average accuracy of 96.5% using a Random Forest model. The approach involves collecting network traffic data using Viasat\\'s `network-stats` tool and extracting packet-level measurements to engineer features for classification. The project provides instructions for running different targets related to data collection, cleaning, engineering, training, and predicting. Additionally, an academic report on the project can be found in the provided link.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_65': ['## Interpreting Higgs Interaction Network with Layerwise Relevance Propagation\\n\\n#### Abstract\\n\\nWhile graph interaction networks achieve exceptional results in Higgs boson identification, GNN explainer methodology is still in its infancy. To introduce GNN interpretation to the particle physics domain, we apply layerwise relevance propagation (LRP) to our existing Higgs boson interaction network (HIN) to calculate relevance scores and reveal what features, nodes, and connections are most influential in prediction. We call this application HIN-LRP. The synergy between the LRP interpretation and the inherent structure of the HIN is such that HIN-LRP is able to illuminate which particles and particle features in a given jet are most significant in Higgs boson identification. The resulting interpretations are ultimately congruent with extant particle physics theory, with the model demonstrably learning the importance of concepts like the presence of muons, characteristics of secondary decay, and salient features such as impact parameter and momentum.\\n\\n#### [READ PAPER](report.pdf)\\n<hr>\\n\\n## Description of contents\\n\\n\\n* `notebooks`:\\n    * `relevance_heatmap.ipynb`: notebook that contains example plots of this project\\n* `src`:\\n    * `model`\\n        * `GraphDataset.py`\\n        * `InteractionNetwork.py`\\n    * `sanity_check`\\n        * `make_data.py`\\n    * `util`: utility functions such as I/O or plotting\\n        * `copy.py`\\n        * `data_io.py`\\n        * `model_io.py`\\n        * `plot.py`\\n    * `LRP.py`: core component of this project\\n\\n* `run.py`: Entry point for running different targets of this project\\n* `test`: directory for storing Dev data\\n    * `test.root`: the generated root file for testing purpose\\n* `data`\\n    * `model`: contains a trained IN state dictionary to start with\\n    * `definitions.yml`: contains metadata definition of the data used in this project\\n<hr>\\n\\n## Build Environment\\n* [Docker image](https://hub.docker.com/repository/docker/shiro0x19a/higgs-interaction-network) used for this project\\n\\n<hr>\\n\\n\\n## Usage\\nTo use `run.py`, a list of supported arguments are provided below\\n\\nFor sanity check of the explanation,\\n```\\npython run.py sc <arguments>\\n```\\n|arguments|purpose|\\n|-|-|\\n|`all`|build all targets, equivalent to using [`data` `train` `plot`] as argument|\\n|`data`| generate dummy data for sanity check|\\n|`train`| train a dummy IN on the sythesized data|\\n|`plot`|create static heatmap plots of precomptued relevance scores|\\n\\n\\n\\nFor explaining a pre-trained Higgs boson Interaction Network,\\n```\\npython run.py <arguments>\\n```\\n|arguments|purpose|\\n|-|-|\\n|`test`| build all targets, equivalent to using [`data` `train` `plot`] as argument on Dev data|\\n|`all`| similar to `test`, but build on actual data|\\n|`explain`| generate relevance score for given data|\\n|`plot`| create static heatmap plots of precomptued relevance scores|\\n\\n<br>\\n\\n',\n",
       "  'This paper introduces the application of layerwise relevance propagation (LRP) to interpret a Higgs boson interaction network (HIN) in particle physics. The method, called HIN-LRP, calculates relevance scores to identify the most influential features, nodes, and connections in predicting Higgs boson identification. The interpretations align with existing particle physics theory and highlight the importance of concepts like muons, secondary decay characteristics, impact parameter, and momentum. The paper provides code and instructions for running different targets of the project.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_64': ['# Particle Physics Result Replication\\n## Checkpoint #1\\n\\n#### Brief Description of Structure\\n\\n##### config\\n###### data-params.json\\nThis file includes the parameters for data extraction.\\n\\n##### src/data\\n###### etl.py\\nThis file contains the functions necessary for data extraction.\\n\\n##### run.py\\nThis file successfully extracts the data.\\n\\n#### In order to run the file run.py, execute the following command:\\n##### python run.py data',\n",
       "  'This is a summary of the structure and instructions for running the file \"run.py\" in the context of particle physics result replication. The file \"run.py\" successfully extracts data using the parameters specified in \"data-params.json\". To run \"run.py\", execute the command \"python run.py data\".'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_72': ['## Project MAI\\n### Project Description\\nMAI refers to Microservice-based Auto Infrastructure, a serverless architecture.\\nThis project aims to build a lightweight, effective, and robust system that helps with the automations of several workflows of the campus COVID detection team.\\n\\n### Underlying Architecture\\n![alt text](mai.png)\\n\\n### Supported Functions\\n#### AUM(auto-update microservice)\\n1. Automate excel parsing => cross-reference => update the sheet \\n2. All in one script(rewrote excel parse used to parse raw cases data)\\n3. One upload and a POST request\\n4. Improved code quality\\nfor more information, go to [repository](https://github.com/CrisZong/AUM)\\n\\n\\n#### statsTool\\n1. Prediction (autoregression model by buildings)\\n2. Autocorrelations (autocorrelation by building)\\n3. Stats(Cases by building sorted by number of cases)\\nfor more information, go to [repository](https://github.com/CrisZong/statsTool)\\n\\n#### Manhole Graph\\nthe project supports creating a graph of the manhole downstream that can be consumed by the campus notification team and running standard graph routines for paths searching or manipulations.\\n\\n#### Functions for graph\\nTo build a graph and see the resulting csv, please run `python run.py graph`\\n\\n### Dataset\\nThe data used was provided by the Knight Lab team. One data source is the daily wastewater data on the Google Spreadsheet. The other data source is the network folder which contains all the shape files about sewer and manhole connections on campus.\\n\\n',\n",
       "  'Project MAI is a serverless architecture called Microservice-based Auto Infrastructure. It aims to automate workflows for the campus COVID detection team. The project includes an auto-update microservice, a stats tool for prediction and analysis, and a manhole graph feature. The data used for the project is provided by the Knight Lab team, including daily wastewater data and shape files about sewer and manhole connections on campus.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_60': [\"# AutoBrick: A system for end-to-end automation of building point labels to Brick turtle files\\n\\n![alt text](https://github.com/Advitya17/AutoBrickify/blob/main/autobrick_workflow.png?raw=true)\\n\\n## Setup\\n\\nClone the repository and cd into the root directory.\\n\\n`git clone https://github.com/Advitya17/AutoBrickify` & `cd AutoBrickify`\\n\\nThen run the command below to setup the tool environment.\\n\\n`python run.py env-setup` (alternatively you can build with the Dockerfile!)\\n\\nThis'll print a message to the console at the end to confirm setup.\\n\\n#### `python run.py test` (Only for 180B Submission, can ignore otherwise)\\n\\n## Instructions\\n\\n### Step 1\\nSpecify your configurations in config/data-params.json. \\n\\nDetailed instructions are available in the `config/README.md` file in this repository.\\n\\n\\n### Step 2\\nRun the project from the root directory.\\n\\n`python run.py`\\n\\nYour Turtle object file (`output.ttl`) will be generated in the root directory!\\n\",\n",
       "  'AutoBrick is a system that automates the process of building point labels to Brick turtle files. To set up the tool environment, clone the repository and run the provided command. Then, specify your configurations in the `config/data-params.json` file. Finally, run the project using the command `python run.py` and your Turtle object file will be generated in the root directory.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_61': [\"## Infection Risk App\\nIn this project, We propose an application which estimates infection risk of COVID-19 in buildings. The application accepts building data and a set of parameters regarding occupants and infection rates of the surrounding community. Code and assumptions made in the algorithm are clearly explained to users for transparency, which those explainations are included in the project in our last quarter. \\n\\nOpening up the project\\nThe source codes for the calculator are located in /src/calculator. To see the notebook containing the underlying logic and sample runnings for the calculator, open presentation in /notebooks. Note there are actual codes in those notebooks, because the purpose of this project is to create an infection estimation algorithm that's clear to users and easily understanable by users. It's important to show what the algorithm does for each steps. To see our UI demo, open Website in /notebooks. To visit our website, visit https://hinanawits.github.io/DSC-180B-Presentation-Website/ Notice we are constantly updating our website with newest data so if the Website notebooks and the website itself are inconsistant it means we updated something. \\n\\nTo use run.py in command line, input python run.py [targets].\\n\\nWe currently have the following targets available:\\n\\ntest: which runs the calculator using sample parameters.\\nMore information about those sample runs can be found in the report notebooks mentioned above.\\n\\nResponsibilities\\n\\nEtienne Doidic built the structure and underlying logic of the calculator, and also the notebooks for walk through.\\n\\nNicholas Kho helped developing the application and migrated the codes to src and project structure.\\n\\nZhexu Li added features to the application migrating the codes, updated the project structure and developed configs and run.py.\\n\",\n",
       "  'This project proposes an application that estimates the risk of COVID-19 infection in buildings. The application takes building data and parameters related to occupants and infection rates into account. The algorithm used in the application is transparent, with clear explanations provided to users. The source codes for the calculator can be found in the /src/calculator directory, along with notebooks that explain the underlying logic and provide sample runs. A UI demo is also available in the /notebooks directory, and the website for the project can be accessed at https://hinanawits.github.io/DSC-180B-Presentation-Website/. The run.py file can be used in the command line with various targets, such as \"test\" which runs the calculator using sample parameters. The responsibilities of the team members involved are also mentioned.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_34': [\"# Project: User Wait\\n\\n## Introduction\\n\\nActivity monitor in the computer visualizes the system performance, but we don't know when our system gets slow or halted. If we are able to predict the mouse wait time, users could terminate their processes ahead of time to avoid waiting. Currently, there is little research conducted on the mouse wait prediction.\\n\\n## Running the project\\n\\n- To get the data run the following command line located inside run.py file.\\n  This will call specific dll files to collect the data regard to your laptop.\\n\\n- To get the data, from the project root dir, you should see the data folder.\\n  Inside the data folder, you should see a new database file being generated.\\n\\n## Type Of Data\\n\\nData provided by Intel includes 14,534,433 rows with 29,587 unique GUID within the 2020 interval. Since the complete dataset is fairly large, we sample 1/14 of the dataset, which leaves 27,014 GUID for our model. For the target, we divide the wait time into 0-3s, 3-5s, 5-7s, and 7+s as a preparation for the classification model. After exploring the correlation between potential features and the mouse wait time, we find that dynamic features, including CPU utilization, disk utilization, hard page faults, and static features, including the number of cores, RAM, model type, etc, could influence the mouse wait time. These features are then used in the model.\\n\\nThe data set we use is ”mousewaitall.csv001”,which is provided by the Intel teams. This data setrecords kinds of system usage before and after mousewait happens. Each feature in this data set consists ofprefix, infix and suffix. Prefix has ”before” and ”after”.It represent is this feature recorded before or after mousewait event. Infix has ”CPUUtil”, ”harddpf”, ”diskutil”and ”networkutil”. This represents what kind of systemusage this feature records. Suffix has ”min”, ”max” and”mean”. This represents the way this feature computesstatistics.\\n\\nThe second data set we use is ”system sysinfo uniquenormalized.csv000”, which is provided by the Intel teams. It contain 32 different features and 100,000unique systems.This data set provides informationabout the system hardware like CPU model, GPU,ram, etc.\\n\",\n",
       "  'This project focuses on predicting mouse wait time in order to allow users to terminate processes ahead of time and avoid waiting. The project uses data provided by Intel, including features such as CPU utilization, disk utilization, hard page faults, number of cores, RAM, and model type. The dataset used includes 14,534,433 rows with 29,587 unique GUID within the 2020 interval. The data is sampled to include 27,014 GUID for the model. Another dataset called \"system sysinfo uniquenormalized.csv000\" is also used to provide information about system hardware.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_33': ['# DSC180B-Capstone-Project\\nDSC Capstpne Project: A Prediction Model for Battery Remaining Time\\n## Usage Instructions\\nWe provided 4 targets for your usage. They are `data`, `eda`, `model`, and `test`.\\n\\n`test` would run our project on test data, which provides a miniature of what we have done on a smaller dataset. \\n\\nAn exmple for running our project through terminal is `python run.py data`, which will show our main dataset. To run other branches, just replace `data` with `eda`, `model` or`test`.\\n\\n\\n## Description of Contents\\n```\\nPROJECT\\n├── config\\n    └── data-params.json\\n    └── inputs.json\\n├── notebooks\\n    └── DSC180B_Presentation.ipynb\\n├── references\\n    └── README.md\\n└── src\\n    ├── EDA\\n        └──  feature_selection.py\\n    ├── data\\n        ├── Loading_Data.py\\n        ├── minimini_battery_event2.csv\\n        ├── minimini_battery_info2.csv\\n        ├── minimini_device_use1.csv\\n        ├── minimini_device_use2.csv\\n        ├── minimini_hw1.csv\\n        ├── minimini_hw2.csv\\n        ├── minimini_process1.csv\\n        └──minimini_process2.csv\\n    └── model\\n        └──hypothesis_testing.py\\n├── .gitignore\\n├── README.md\\n└── run.py\\n└── submission.json\\n```\\n\\n\\n### `config/`\\n* `data-params.json`: It contains the file path for our dataset\\n* `inputs.json`: It contains the argument inputs\\n\\n### `notebooks/`\\n* `DSC180B_Code.ipynb`: EDA, Hypothesis Testing and Visual Presentation on our project\\n\\n### `references/`\\n* `README.md`: External sources\\n\\n### `src/`\\n* `EDA/feature_selection.py`: code for selecting desired features for our prediction model\\n* `data/Loading_Data.py`: code for load our dataset\\n* `data/minimini_[DIFFERENT FILE NAME]_csv`: sample dataset\\n* `model/hypothesis_testing.py` : code for prediction model and hypothesis testing\\n\\n### `run.py`\\n* Main driver for this project.\\n',\n",
       "  'This is a description of the contents of the DSC180B-Capstone-Project. It includes information about the project structure, such as the `config/`, `notebooks/`, `references/`, and `src/` directories. It also provides details about specific files, such as `data-params.json`, `inputs.json`, `DSC180B_Presentation.ipynb`, and others. The main driver for the project is `run.py`.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_35': [\"# DSC180B_Project checkpoint_1\\n\\n# Background\\nIn the PC industry, there are different computer setups for omnifarious PC users. Different types of customers have different needs and budget for their computers. For instance, we think that gamers prefer desktops or laptops with high-end GPU and CPU while office users prefer the ones with decent CPU and long battery life. Hence, being aware of the different needs from different customers could help computer retailers dramatically with marketing and resource allocation. With this background, we decided to build a machine learning model that can predict a users’ persona based on the information of their computers.\\n\\n# How To Run: Classification of Your Own Audio Files\\n1. Clone this repository.\\n2. On the command line, navigate to this repository locally.\\n3. on the command line, use\\n\\n    *python run.py test*     runs the pipeline with the test-project target. This will run the test build classifier. THIS PROCESS WOULD LAST HOURS!!!\\n    \\n    *python run.py build-classifier*    to build all classifier. THIS PROCESS WOULD LAST HOURS!!!\\n    \\n    *python run.py hypo-test*      to run the hypothesis test on column 'ram' with other columns.\\n    \\n    *python run.py chi-square-test*     to run the hypothesis test on column 'ram' with other columns.\\n\",\n",
       "  \"This project aims to build a machine learning model that can predict a user's persona based on their computer information. The project provides instructions on how to run the classification of audio files and various tests using Python.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_36': ['# Prediction Task: Utilizing CPU Statistics and Application Usage to Predict a User’s Persona\\n\\n\\n\\n## Homepage\\nhttps://vlw003.github.io\\n\\n## Medium Blog\\nhttps://predicting-persona-b09group04.medium.com/\\n\\n## Usage\\n```\\ngit clone https://github.com/jonxsong/DSC180AB-Capstone.git\\ncd DSC180AB-Capstone\\npython run.py test\\n```\\n\\n\\n\\n## Files\\n\\n**./config/data-params.json** - directory where data should be output to\\n\\n**./config/hw-metric-histo-data-params.json** - description of the dataset and features we utilize\\n\\n**./config/systems-sysinfo-unique-normalized-data-params.json** - description of the dataset and features we utilize\\n\\n**./config/ucsd-apps-execlass-data-params.json** - description of the dataset and features we utilize\\n\\n**./config/frgnd_backgrnd_apps-data-params.json** - description of the dataset and features we utilize\\n\\n**./notebooks/eda.ipynb** - notebook containing data explorations from DSC180B\\n\\n**./notebooks/dsc180a-notebook.ipynb** - notebook containing data explorations from DSC180A\\n\\n**./src/data_exploration.py** - file containing relevant methods for data exploration\\n\\n**./src/model.py** - file containing relevant methods for data modelling\\n\\n**./requirements.txt** - required packages\\n\\n**./run.py** - call run.py to run data analysis\\n\\n\\n\\n## Data/Output Files\\n\\n**./data/out/...** - this location should hold all the outputted pictures generated from methods\\n\\n**./data/raw/...** - this location should hold all the datasets downloaded below\\n\\n\\n\\n## Link to download the datasets:\\nhttps://drive.google.com/drive/folders/1nNpwhzrbKUJd0ZwbCYLGQH49CKkKLTQ4?usp=sharing\\n\\nThe datasets we are using are too large for github. The datasets should be stored in /data/raw/.\\n\\n\\n\\n## Sources\\nLink to Project Report: https://docs.google.com/document/d/1IpWfuG2IxurT5LOMyudWpn3UOLsKYKdjbbwqNhPGlYk/edit?usp=sharing\\n\\n\\n\\n## Responsibilities:\\n\\nJon:\\n    - Report + main ideas\\n    - data analysis - code breakdown\\n    - repository structuring\\n    - notebook outlining\\n    - script writing\\n\\nVince:\\n    - data modeling\\n    - Report + targets\\n    - data cleaning\\n    - data explorations\\n    - classifications\\n    - Visual Presentation Checkpoint\\n    - Website\\n    - Final Report\\n    - Slides\\n\\nKeshan:\\n    - data preparation\\n    - tabled data\\n    - key notes all throughout notebook\\n    - graphs + graph analysis\\n    - ATL work\\n',\n",
       "  \"This document provides information about a project that aims to predict a user's persona using CPU statistics and application usage. It includes links to the project homepage and Medium blog, as well as instructions for cloning the project repository and running the code. The document also lists various files and their descriptions, including data exploration notebooks and relevant source code files. Additionally, it mentions the location of the data/output files and provides a link to download the datasets used in the project. Finally, it outlines the responsibilities of each team member involved in the project.\"],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_32': [\"Scale model Group\\nThe purpose of this project is to provide a testing ground for the transmission code in SchoolABM. By scaling down the parameters to a single room of students, effects of elements such as distance and ventilation can be seen more easily.\\nThis also provides a testing environment that has a significantly faster runtime and has been useful in verifying our math for transmission both through droplets and aerosols\\n\\nTest Simulation can be run using: python -m run 'test'\\n\\nVisualization can be run using: python -m run 'visualize'\\n\\nVisualization includes:\\n-Color viz indicating which students are infected\\n-Distribution plot of transmission rates post-processing (for each 5 minute step and unique infected individual)\\n-Time series plot of when infection occurs\\n\\nTODO:\\nDocker Image (due thursday)\\nAiravata (due wednesday)\\n\\nWell-Mixed room (due thursday)\\n\",\n",
       "  'The scale model group project aims to test the transmission code in SchoolABM by scaling down parameters to a single room of students. This allows for easier observation of the effects of distance and ventilation. The project also provides a faster testing environment and helps verify the math for transmission through droplets and aerosols. The project includes test simulation and visualization options, such as color visualization indicating infected students, distribution plot of transmission rates, and time series plot of infection occurrences. There are also upcoming tasks including creating a Docker image, using Airavata, and implementing a well-mixed room by Thursday.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_31': [\"# Capstone Project dsc180B Covid-19 Transmission in Buses\\n\\nIn order to install mesa-geo and rtree, we have copied Johnny Lei's README file from our domain repo. Here it is:\\n\\n### Install MESA\\n#### Install ABM package MESA with:\\n\\n> pip install mesa\\n\\n#### Install MESA-geo\\n  If you are using a Mac Machine:\\n    Install rtree FIRST with conda (there seems to exist a distribution issue with pip specificly to Mac):\\n\\n> conda install rtree\\n\\n  Install geospatial-enabled MESA extension mesa-geo with:\\n\\n> pip install mesa-geo\\n\\nend of credit to J.L\\n\\n\\n### How to run\\n\\nOpen terminal, change to the directory of the code, then type in the following command \\n\\n> python run.py\\n\\nor if you want to run the code with your own parameters in config/test.json\\n\\n> python run.py test\\n\\n\\n## Logistics\\n\\nWe are using Agent-Based-Modeling to simulate covid-19 transmission in Buses.\\nIn order to run this simulation, you could change the parameters in config/test.json to simulate different scenarios.\\n\\nThis simulation start with an empty bus. The bus is scheduled to stop at a certain number of stops at certain times, picking up certain number of students. \\nAll these parameters are adjustable. As the bus continues, the sick students in the bus transmit the virus by normal activities like talking and breathing, and also coughing, sneezing, etc. This model stops the simulation at the end of a trip where the bus reaches the school (destination). Then this model creates a graph of the number of healthy and sick(if they received the virus) students every minute. There is also a gif of the simulation that shows the position of each student, the time since start, and the layout of the bus.\\n\",\n",
       "  'This text is about a capstone project on Covid-19 transmission in buses. It provides instructions on how to install the required packages and run the simulation. The simulation uses Agent-Based Modeling to simulate the transmission of the virus in buses. The parameters can be adjusted to simulate different scenarios, and the model creates a graph and a gif of the simulation.'],\n",
       " 'https://github.com/leonkuoDSC/Policing_and_Income': ['# Policing_and_Income\\nNotebooks include work on the car makes and models, using the San Diego Open Policing database, and getting the data by service area.\\n\\nUsing run.py with target test will add the car make_model, age and price to the tx_50.csv test dataset and create a new test.csv file with the added columns.\\n',\n",
       "  'The work on car makes and models using the San Diego Open Policing database is summarized in this notebook. The data is obtained by service area. By running the \"run.py\" script with the target \"test\", the car make_model, age, and price are added to the tx_50.csv test dataset. A new file called test.csv is created with the added columns.'],\n",
       " 'https://github.com/lizheng1226/DSC180B_Project_AutoFiNews': [\"# Model Analysis of Stock Price Trend Predictions based on Financial News\\n\\nThis is a data analysis project that aims to use different methods to predict the change of stock price from financial news and compare the performances of those methods.\\n\\nWe build following models:\\n- Baseline model: use Bag-of-Words to extract frequent words in news and determine their attitudes.\\n- AutoPhrase model: use AutoPhrase to extract high quality phrases and determine their attitudes.\\n- Doc2vec and LSTM model: use Doc2vec to create numberical representation of documents and use LSTM to predict the result.\\n- BERT model: use BERT to make prediction.\\n\\nThe code and results of different methods can be found in this repository.\\n\\n## Local Run\\n```\\n$ python run.py [test] [eda] [etl] [doc2vec_lstm_model] [doc2vec_lstm_model2] [baseline_model] [autophrase_model] [bert_model]\\n```\\n\\n### `test` target\\nThis target runs a subset of pre-trained AutoPhrase dataset on the Word2vec model using the data in `test/testdata` and test the configurations in `config`.\\n### `eda` target\\nThis target generate the EDA analysis and wordclouds of the positive and negative word bank and on the Apple stock price, and the results are saved to `data/eda_data`\\n### `etl` target\\nThis target cleans and outputs the training dataset from the Apple stock price and corresponding financial news release dates, and the results are saved to `data/etl_data`\\n### `doc2vec_lstm_model` target\\nThis target runs the doc2vec and lstm model to give prediction on Apple's stock price movement, and the model and results are saved to `data/d2v_lstm_model_data`\\n### `doc2vec_lstm_model2` target\\nThis target runs the doc2vec and lstm model to give the sentiment analysis labels prediction based on the financial news title. The model and results are saved to `data/d2v_lstm_model_data2`\\n### `baseline_model` target\\nThis target runs the baseline_model using the Bag-of-Words model.\\n### `autophrase_model` target\\nThis target runs the Autophrase model to predict the stock price changes.\\n### `bert_model` target\\nThis target runs the BERT model to predict the stock price changes.\\n\\n\\n## Docker\\n- The docker repository is `lizheng1226/dsc180_autofinews`.\\n- Please use the following command to run a DSMLP container using docker:\\n```\\nlaunch.sh -c 2 -m 4 -g 1 -i lizheng1226/dsc180_autofinews:latest\\n```\\n\\n## Webpage\\n* https://aponyua991.github.io/AutoFiNews/\\n\\n\\n## Group Members\\n- Liuyang Zheng\\n- Mingjia Zhu\\n- Yunhan Zhang\",\n",
       "  'This project aims to predict stock price trends based on financial news using various methods. The models built include a baseline model using Bag-of-Words, an AutoPhrase model, a Doc2vec and LSTM model, and a BERT model. The code and results for each method can be found in the repository. The project also includes targets for local run, such as testing the AutoPhrase dataset, generating EDA analysis and wordclouds, cleaning and outputting the training dataset, running the Doc2vec and LSTM models for stock price prediction, running the baseline model, running the AutoPhrase model, and running the BERT model. The project is also available as a Docker container and has a webpage for reference. The group members involved in this project are Liuyang Zheng, Mingjia Zhu, and Yunhan Zhang.'],\n",
       " 'https://github.com/jamesjaeyu/autophrase_over_time': [\"# Utilizing AutoPhrase on Computer Science papers over time\\n### DSC 180B Capstone Project\\n\\n### Group Members: Cameron Brody, Jason Lin, James Yu\\n\\n### Link to website: https://jamesjaeyu.github.io/autophrase_over_time/\\n\\n<br />\\n\\n[Link to DBLP v10 dataset download](https://lfs.aminer.cn/lab-datasets/citation/dblp.v10.zip)\\n\\n- Direct download link from [aminer.org/citation](https://www.aminer.org/citation)\\n- zip file size is 1.7 GB, 4.08 GB when extracted\\n\\nFile & folder descriptions:\\n- `config` folder: Contains configuration files for run.py\\n\\n- `data` folder: Directory for full data when running 'all' target on run.py\\n\\n- `docs` folder: Contains GitHub Pages files for the visual presentation website\\n\\n- `results` folder: Contains results of EDA figures and AutoPhrase\\n\\n- `src` folder: Contains .py files for processing datasets, performing EDA, and model generation\\n\\n- `test` folder: Contains `testdata` and `testresults` for running 'test' target on run.py\\n\\n- `run.py`: Run file for the project. Targets are 'data', 'eda', 'model', 'analysis'\\n    - 'all' will run all targets with full dataset\\n    - 'test' will run 'data', 'model', and 'analysis' with test data. 'eda' will run the same\\n\",\n",
       "  'This is a summary of the project \"Utilizing AutoPhrase on Computer Science papers over time\" by the DSC 180B Capstone Project group members Cameron Brody, Jason Lin, and James Yu. The project focuses on using AutoPhrase to analyze computer science papers. The project website can be found at https://jamesjaeyu.github.io/autophrase_over_time/. The project utilizes the DBLP v10 dataset, which can be downloaded from https://www.aminer.org/citation. The dataset is 1.7 GB in size when zipped and 4.08 GB when extracted. The project includes various folders such as \\'config\\', \\'data\\', \\'docs\\', \\'results\\', \\'src\\', and \\'test\\' which contain different files and scripts for data processing, EDA, and model generation. The \\'run.py\\' file is used to run different targets such as \\'data\\', \\'eda\\', \\'model\\', and \\'analysis\\'.'],\n",
       " 'https://github.com/YongqingLi14/codenames-ai-analysis': ['# codenames-ai-analysis\\n\\n## Introduction\\nThis project includes the experiments and analysis on the performance of Codenames AI, a system capable of replacing human efforts in the game Codenames.\\n* To play the game, please visit https://github.com/XueweiYan/codenames-game-ai\\n* For more details on the project, please visit https://xueweiyan.github.io/codenames-ai-website/\\n\\n\\n## Instructions\\n* Download the datasets from the below link and store them on the same level as this repository in a file named \"AI_dataset\":\\n  * https://drive.google.com/drive/folders/1FqEHYL_uTQDQ_MFw8T4gdD4VHaa2r0jv?usp=sharing\\n* Clone the game repo and place it on the same level as this repository:\\n  * https://github.com/XueweiYan/codenames-game-ai\\n* Resulting set up should look like this:\\n\\n<p align=\"center\">\\n  <img src=\"https://github.com/YongqingLi14/codenames-ai-analysis/blob/main/file_organization.png\" />\\n</p>\\n\\n\\n## Running the Repository\\n* Run the following docker image inside a container: \\n  * yongqingli/codenames_ai\\n* To test the pipeline of the project: \\n  * `python3 run.py test`\\n* To view full results (total time will take ~15 hours): \\n  * `python3 run.py all` \\n* To revert the repo back to its original state: \\n  * `python3 run.py clean` \\n* Results will be availale in Report.ipynb and Report.html\\n\\n\\n## Datasets\\nWe put 3 datasets to the test in this experiment:\\n* GloVe: \\n  * pretrained word embeddings from Wikipedia\\n  * cosine similarity\\n* Word2Vec\\n  * word embeddings trained from the English Simple Wiki using the gensim word2vec model\\n  * cosine similarity\\n* WordNet\\n  * word embeddings from the WordNet dataset\\n  * Wu-Palmer similiarity\\n\\n\\n## Testing Metrics\\n* Average turns taken to finish the game\\n* Number of assassins triggered\\n* Accuracy in correctly guessing the intended words\\n',\n",
       "  'This project analyzes the performance of Codenames AI, a system designed to replace human players in the game Codenames. The project includes instructions for setting up the necessary files and running the repository. The datasets used in the experiments are GloVe, Word2Vec, and WordNet. The testing metrics include average turns taken to finish the game, number of assassins triggered, and accuracy in correctly guessing the intended words.'],\n",
       " 'https://github.com/jonathantanoto/spam_detection_180B/': [\"# Spam Detection Using Natural Language Processing\\n\\nBuilding a spam detection algorithm by utilizing Natural Language Processing to extract features associated with spam emails. Deep Learning methods as well as word-to-vector transformation are used to create a spam email classifier.\\n\\n## Usage Instructions\\n\\nPotential run.py arguments:\\n* data: downloads and populates data folder from source.\\n* build: feature extraction and bi-LSTM model building, requires data to be run.\\n* predict: runs script to predict phrases whether it is a spam or not, requires data and build to be run.\\n\\n## Project Contents\\n\\n```\\nROOT FOLDER\\n├── .gitignore\\n├── .gitmodules\\n├── AutoPhrase (forked submodule repository)\\n├── run.py\\n├── README.md\\n├── data (populated by calling data argument of run.py)\\n├── models (populated by calling build argument of run.py)\\n│   ├── model\\n│   │   └── ...\\n│   └── tokenizer.pickle\\n├── notebooks\\n│   └── report.ipynb\\n└── src\\n    └── generate_dataset.py\\n    └── process_build.py\\n    └── spam_or_not.py\\n```\\n\\n### `src`\\n\\n* `generate_dataset.py`: Code that pulls dataset used for training from data source, and combines data into a dataframe.\\n* `process_build.py`: Code that extracts features from data, processes data to be used for training deep learning models. Trains Bidirectional Long Short-Term Memory model.\\n* `spam_or_not.py`: Code that loads model from process_build and runs a script to predict input text's probability of being a spam message.\\n\\n\\n### `notebooks`\\n\\n* Jupyter notebooks for Reports and line-by-line executed code.\\n* Final Report in PDF form.\\n\\n\\n## Links\\n\\n* Website: https://jonathantanoto.github.io/spamdetection/\\n* Presentation Video: https://youtu.be/wkc7R0J4_VM\\n\",\n",
       "  'This project focuses on building a spam detection algorithm using Natural Language Processing (NLP). The algorithm utilizes deep learning methods and word-to-vector transformation to create a spam email classifier. The project includes code for data generation, feature extraction, model building, and prediction. The project structure consists of a root folder with subfolders for data, models, notebooks, and source code. The \"src\" folder contains code for dataset generation, feature extraction, and model training. The \"notebooks\" folder contains Jupyter notebooks for reports and code execution. Links to the project website and presentation video are also provided.'],\n",
       " 'https://github.com/mcelz/MedCoin-Authorization-Smart-Contract': [\"# MedCoin Authorization Contract\\n## Overview\\nEHR(Electronic Health Records) was never intended to manage and preserve the complications of cross-institutional and lifelong medical records: Medical information for a patient comes from a variety of places, and different pieces of that information must be put together for clinicians to make efficient healthcare decisions. Because of storage constraints, EHRs frequently store health data at a single location for a few years rather than keeping all-time records for patients. EHR systems used by different hospitals are frequently incompatible. Patients who seek medical treatment at several locations must frequently retype their personal information and request data transfers across these health providers, and they encounter considerable issues accessing their reports, correcting incorrect information, and authorizing medical data. <br />\\n<br />\\nAnother concern in this area is the permission of medical records. To regulate the health industry, patient data protection procedure protocols such as HIPAA and EPHI were established, and different medical information sources have distinct authorization requirements that must be met before patient data can be shared with someone else. Sensitive data, such as the patient's gender, name, residence, zip code, and age, should not be leaked to a third party without authority; similarly, generally non-sensitive medical data should be examined with caution. No information can simply be aired or made available to the general public. Often, a physician will have all of the information they require, as well as others that they may not be aware of but are necessary to care for. <br />\\n<br />\\nWe propose to implement the authorization smart contracts for EHR-related medical blockchains. This auditing layer of medical blockchain has featured a security design that would prevent data breaches in the medical records and separate sensitive data from non-sensitive data: We separate sensitive and non-sensitive data when patients log their medical information and we would show legal statements (HIPAA, EPHI) to notify the patients when they are authorizing their private data to a third party. Then, the smart contracts would allow different levels of authorization of access to data. The smart contracts would also allow for the protection of private data while delivering useful medical data to health providers and doctors. \\n<br /><br />\\nThis authorization contract would therefore provide the medical record requesters(doctors, researchers, providers) with stratified access to medical information for research use, clinical use, or kept private; Patients could choose different authorization levels for people to access their medical records enabling a distributed system that provides layered and use for info users and info providers. This authorization smart contract would be the core construct in our medical data encryption, authorization stratification, and medical records transfer aggregation pipeline.\\n<br /><br />\\n\\n## Contributors\\nYifei Wang and Ruiwei Wan\\n\\n## Directory Structure\\n<pre>\\n├── README.md\\n├── notebooks/\\n│\\xa0\\xa0 ├── MedCoin White Paper.pdf\\n│\\xa0\\xa0 └── report.pdf\\n├── references/\\n│\\xa0\\xa0 └── README.md\\n├── report.pdf\\n└── src/\\n    ├── smart contract/\\n    │\\xa0\\xa0 ├── Patient_Registry.sol\\n    │\\xa0\\xa0 └── Relationship_Management.sol\\n    └── test/\\n        ├── Patient_Registry_Test.sol\\n        └── Relationship_Management_Test.sol\\n</pre>\\n\\n## Data Use Explanation\\nSince our project is not concerned with data ingestion and preprocessing,\\nwe are only concerned with authorization application in the smart contract,\\nwe therefore does not include the data in the data part.\\nFor testing purposes, we only rely on remix website, which is https://remix.ethereum.org/, \\nbecause implementing blockchain infrastructure would take months or even years.\\nSo if our test solidity file and smart contract solidity files were put on the remix and use the test resources there,\\nit would suffice the testing purpose of our contract.\\n\\n## Docker Use Explanation\\nWe also will not use docker, because it is hard to deploy docker for bloakchain project.\\n\\n## Testing website\\nFor testing purposes, we only rely on remix website, which is https://remix.ethereum.org/\\n\\nFirst we should download the solidity files in the src folder:\\n\\n- `Patient_Registry.sol`: the contract which is in charge of register new patients for authorization\\n- `Relationship_Management.sol`: the contract which manages the authorization pairs of patients and third parties\\n- `Patient_Registry_Test.sol`: the test contract for Patient_Registry.sol\\n- `Relationship_Management_Test.sol`: the test contract for Relationship_Management.sol\\n\\nThen we put the source files and test files onto remix website(https://remix.ethereum.org/) to test. \\nWe also connected to MetaMask Ropsten testnetwork, however, due to the limitations of blockchain infrastructure, our project stops here.\\n\\n## Project Report\\nWe also include the Project report in the notebook folder.\\nOur project website is: https://ellawan.github.io/.  \\n\",\n",
       "  'The MedCoin Authorization Contract aims to address the challenges of managing and preserving cross-institutional and lifelong medical records. The contract proposes the implementation of authorization smart contracts for EHR-related medical blockchains, which would ensure data security and separate sensitive data from non-sensitive data. The smart contracts would allow different levels of authorization for accessing medical data, providing layered access for doctors, researchers, and providers. The project includes source files and test contracts that can be tested on the Remix website. However, due to limitations in blockchain infrastructure, the project is currently not fully implemented.'],\n",
       " 'https://github.com/DSC180A-A04/spatiotemporal_analysis': ['# Spatiotemporal Analysis\\n\\n## Webpage\\nhttps://dsc180a-a04.github.io/spatiotemporal_analysis/\\n\\n\\nThe analysis in this repo uses uncertainty quantification feature that we have developed in [torchTS](https://github.com/Rose-STL-Lab/torchTS).\\n\\nExample output:\\n\\nUse conformal prediction to construct a conformal confidence band consisting of quantiles `[0.025, 0.5, 0.975]` for a 95% confidence level.\\n![uncertainty_quantification](./static/uncertainty_quantification.png)\\n\\n## Getting Started\\n\\n1. Create a virtual environment\\n\\n```bash\\npython3 -m venv venv\\n```\\n\\n2. Activate the virtual environment\\n\\n```bash\\nsource venv/bin/activate # for mac\\n```\\n```bash\\nvenv/Scripts/activate # for windows\\n```\\n3. Install dependencies\\n\\n```bash\\npip install -r requirements.txt\\n```\\n\\n4. Train models and make predictions. This will generate a `conformal_prediction.png` plot in the root directory.\\n\\n```bash\\npython run.py\\n```',\n",
       "  'This webpage provides information on spatiotemporal analysis using uncertainty quantification. The analysis utilizes a feature developed in torchTS. The example output demonstrates the use of conformal prediction to construct a conformal confidence band with quantiles [0.025, 0.5, 0.975] for a 95% confidence level. The webpage also includes instructions on getting started, such as creating a virtual environment, activating it, installing dependencies, and running the code to train models and make predictions.'],\n",
       " 'https://github.com/dhaar-data/DSC180B-project': [\"# DSC180B-project\\nThis is an exploration of the bootstrap post-prediction inference approach introduced by Wang et al. in [Methods for correcting inference based on outcomes predicted by machine learning](https://www.pnas.org/content/117/48/30266/tab-article-info) through a study of tweets and their corresponding political alignment in the US. When we look at a tweet, what are the kinds of words or phrases that most strongly indicate the tweet's alignment to Democrats or Republicans? In today's political climate, what topics or figures are most heavily scrutinized by one party or another? We seek to find these key figures, phrases, and topics through a statistical analyses of political tweets.\\n\\nAs said above, this statistical analyses also functions as an investigation of the bootstrap post-prediction inference approach. Post-prediction--or postpi, as Wang et al. calls it--is the use of predicted outcomes in lieu of observed outcomes during inferential analysis. If postpi is conducted without accounting for the use of predicted outcomes as is often the case, this leads to issues with bias and standard , among other things. While the aforementioned bootstrap post-prediction inference approach corrects these issues for a wide range of datasets, we seek to study its applicability specifically towards text data, particularly in political science. \\n\\n## Build Instructions\\nThis project has already provided pre-scraped Twitter data in `data/out/raw`.\\n* Data Collection: To clean and split the data into train-test-validation sets, run `python run.py data`. The datasets will be stored in `data/out/clean` in separate csvs for covariates and outcomes. \\n* EDA: To conduct EDA on the data, run `python run.py data eda`. The output will be three figures in `results/figures` as .png files.\\n* Prediction: To build the prediction model and predict the outcomes of the datasets, run `python run.py data predict`. The predicted values will be stored in `data/out/predicted` in txt files.\\n* Relationship: To build the relationship model based on the predicted and observed outcomes from the test dataset, run `python run.py data predict rel`.\\n* Inference/Bootstrapping: To conduct the bootstrap post-prediction inference on selected features, run `python run.py data predict rel inference`, equivalent to running all targets (see last bullet point). Features you want to conduct inference for can be adjusted in `config/inference-params.json`. This will output estimators, standard errors, and t-statistics of the features in a csv file located at `results/inference`.\\n* Run all:\\n    * To run the entire process on the dataset specified in `config/data-params.json`, run `python run.py all`\\n    * To run the entire process on test data in `test/testdata`, run `python run.py test`\\n    \\n## Running the Project\\n1. Clone this repo\\n    ```\\n    git clone https://github.com/dhaar-data/DSC180B-project.git\\n    ```\\n2. Build and run the docker image\\n    ```\\n    docker build -t ##\\n    docker run --rm -it ## /bin/bash.\\n    ```\\n3. Run the project according to build instructions above. As an example:\\n    ```\\n    python run.py all\\n    ```\\n    \\n## Project Website\\n```\\nhttps://dhaar-data.github.io/DSC180B-project/\\n```\\n\",\n",
       "  'This project explores the bootstrap post-prediction inference approach introduced by Wang et al. It focuses on analyzing tweets and their political alignment in the US to identify key words, phrases, and topics that indicate alignment to Democrats or Republicans. The project also investigates the applicability of the bootstrap post-prediction inference approach to text data in political science. The project provides instructions for data collection, exploratory data analysis, prediction modeling, relationship modeling, inference/bootstrapping, and running the entire process. The project repository and website are available for more information.'],\n",
       " 'https://github.com/duha-aldebakel/DSC180B-LDACode': ['# Overview & Setup\\n\\nThis repository serves as the central user code repository for Group A06\\'s DSC180B Capstone project.\\n\\n## Title:\\nExploration of Variational Inference and Monte Carlo Markov Chain Models for Latent Dirichlet Allocation of Wikipedia Corpus\\n## Abstract:\\nTopic modeling allows us to fulfill algorithmic needs to organize, understand, and annotate documents according to the discovered structure. Given the vast troves of data and the lack of specialized skillsets, it is helpful to extract topics in an unsupervised manner using Latent Dirichlet Allocation (LDA). LDA is a generative probabilistic topic model for discrete data, but unfortunately, solving for the posterior distribution of LDA is intractable, given the numerous latent variables that have cross dependencies. It is widely acknowledged that inference methods such Markov Chain Monte Carlo and Variational Inference are a good way forward to achieve suitable approximate solutions for LDA. In this report, we will explore both these methods to solve the LDA problem on the Wikipedia corpus. We find that better performance can be achieved via preprocessing the data to filter only certain parts-of-speech via lemmatization, and also exclude extremely rare or common words. We improved on the Expectations-Maximization (EM) Algorithm used for variational inference by limiting the number of iterations in the E step even if sub-optimal. This leads to benefit of faster runtimes and better convergences due to fewer iterations and avoidance of local minima. Finally, we explore early stopping runtimes on under-parameterized LDA models to infer the true dimensionality of the Wikipedia vocabulary to solve for topics. While the English language has around a million words, our findings are that it only takes around fifteen thousand words to infer around twenty major topics in the dataset.\\n\\n# To get it up and running:\\n## 1) Set up python environment\\n### Option 1: (Easiest) Pulling our Docker from Dockerhub\\n- Just run \"`docker run -it --rm daldebak/dsc180b bash`\"\\n- If the docker is not on your machine locally, the command should pull it from docker hub automatically. Here is how to force the action, \"`docker pull daldebak/dsc180b`\"\\n- Run the docker using \\'docker run -it --rm daldebak/dsc180b bash \\'\\n### Option 2: Rebuilding a Docker\\n- Build from Dockerfile using the docker CLI command\\n- Type \"`docker build -t <image-fullname> .`\" and hit \\\\<Enter\\\\>, notice the \"period/dot\" at the end of the command, which denotes the current directory. Docker will then build the image in the current directory\\'s context. The resulting image will be labeled `<image-fullname>`. Monitor the build process for errors.\\n- For example, a command could be \\'docker build -t daldebak/dsc180b .\\'\\n- Run the docker using \\'docker run -it --rm daldebak/dsc180b bash \\', or replace with your own <image-fullname>\\n### Option 3: If using DSMLP\\n- SSH to `dsmlp-login.ucsd.edu`. (Note if working outside the school, you would need to first connect via VPN)\\n- Run \"`launch.sh -i daldebak/dsc180b:latest`\"\\n### For local development:\\n- Make sure, preferably, you have python3.7+ installed and assuming you have configured the `PATH` and `PATHEXT` variables upon installation:\\n- `python3.7 -m venv env`\\n- `source env/bin/activate`\\n- `pip install -r requirements_pip.txt`\\n\\nTo interact with jupyter notebooks (make sure virtual environment is activated and requirements_pip.txt are installed):\\n- `cd DSC180B-LDACode`\\n- `ipython kernel install --user --name=env` (assuming you named your virtual environment `env`)\\n-  `jupyter notebook`\\n- When notebook server is running: Navigate to `Kernel` > `Change kernel` > select `env`\\n  \\n## 2) Getting the repository from Github\\n- \"`git clone https://github.com/duha-aldebakel/DSC180B-LDACode.git`\"\\n- \"`cd DSC180B-LDACode`\"\\n- \"`python run.py test-gensim`\" to run gensim on test data\\n- \"`python run.py gensim`\" to run gensim on production data \\n- \"`python run.py test-lda-cgs`\" to run gibbs on test data\\n- \"`python run.py lda-cgs`\" to run gibbs on production data \\n- \"`python run.py onlineldavb`\" to run blei MFVI LDA on production data   \\n',\n",
       "  \"This repository is for Group A06's DSC180B Capstone project on exploring Variational Inference and Monte Carlo Markov Chain Models for Latent Dirichlet Allocation (LDA) of the Wikipedia corpus. The report discusses the challenges of solving LDA and the use of inference methods such as Markov Chain Monte Carlo and Variational Inference. Preprocessing techniques, improved algorithms, and early stopping runtimes are explored to achieve better performance. The setup instructions include options for setting up the Python environment using Docker or locally, as well as getting the repository from GitHub. Various commands are provided to run different models on test or production data.\"],\n",
       " 'https://github.com/889884m/DSC180_Project': ['# Locating Sound with Machine Learning\\n\\nSite URL: https://889884m.github.io/DSC180_Project/\\n\\nMost of the project code was actually built using Jupyter Notebooks, so the latest working code would be found in the `Notebooks` folder. Here the latest figures and hyperparameter tuning can be found. Demonstrations of the Neural Net, Support Vector Machine, and Random Forest are here.\\n\\nThe code is run via the command `python run.py test`. This runs the baseline model on the test data, which is simply the normal data but randomized.\\n\\nProject code with working models can be found in `src` folder. Here the code for data generation and the test data can be found. This is also where the model code can be found which is in the `prediction` folder. Here, the code for the feed-forward Neural Net and the SVM can be found.\\n\\nTo build the `docker build -t <tag_name> .` which gives a local docker container with the libraries scipy, numpy, pandas, pytorch, and sklearn.',\n",
       "  'The project code for locating sound using machine learning is available on the provided site URL. The latest working code can be found in the \"Notebooks\" folder, which includes figures and hyperparameter tuning. Demonstrations of the Neural Net, Support Vector Machine, and Random Forest are also available there.\\n\\nTo run the code, use the command `python run.py test`, which runs the baseline model on randomized test data.\\n\\nThe source code for data generation, test data, and models can be found in the \"src\" folder. The model code is located in the \"prediction\" folder, where you can find the feed-forward Neural Net and SVM implementations.\\n\\nTo build a local docker container with necessary libraries (scipy, numpy, pandas, pytorch, and sklearn), use the command `docker build -t <tag_name> .`.'],\n",
       " 'https://github.com/sunqiaochen/NeurlPS_2022_DSC180': ['# NeurlPS_2022_DSC180\\nIn this project, dataset we used: https://drive.google.com/drive/u/0/folders/10gwW55-9xQyAvJh7KHR1uJqYSpDBW0iL\\nhttps://drive.google.com/file/d/1TmPrp74d2y77FCu3Fv0u_vlOP7HvLxkX/view?usp=sharing\\n\\nNowadays, human activities such as wildfires and hunting have become the largest factor that would have serious negative effects on biodiversity. In order to deeply understand how anthropogenic activities deeply affect wildlife populations, field biologists utilize automated image classification driven by neural networks to get relevant biodiversity information from the images. However, for some small animals such as insects or birds, the camera could not work very well because of the small size of these animals. It is extremely hard for cameras to capture the movement and activities of small animals. To effectively solve this problem, passive acoustic monitoring (PAM) has become one of the most popular methods. We could utilize sounds we collect from PAM to train certain machine learning models which could tell us the fluctuation of biodiversity of all these small animals. The goal of the whole program is to test the biodiversity of these small animals (most of them are birds). However, the whole program could be divided into plenty of small parts. I and Jinsong will pay attention to the intermediate step of the program.\\n\\nThe goal of our project is to generate subsets of audio recordings that have higher probability of vocalization of interest, which could help our labeling volunteer to save time and energy. The solutions could help us reduce down the amount of time and resources required to achieve enough training data for species-level classifiers. We perform the same thing with AID_NeurIPS_2021. Only the data is different between these two github. For this github, we use the peru data instead of Coastal_Reserve data.\\n\\n\\n',\n",
       "  'The project aims to understand how human activities impact wildlife populations, particularly small animals like insects and birds. To overcome the limitations of cameras in capturing their movements, passive acoustic monitoring (PAM) is used to collect sounds for training machine learning models. The project focuses on generating subsets of audio recordings with a higher probability of vocalization of interest, reducing the time and resources needed for species-level classifiers. The dataset used is different from a previous project, using Peru data instead of Coastal Reserve data.'],\n",
       " 'https://github.com/EdmundoZamora/TweetyNet_CUDA_GPU_Adaptation': ['### Binary classifies bird vocalizations in a wav files\\n\\n### Stores wav and csv data in data/raw/ it outputs results in data/out/\\n\\n### Takes in raw wave files, trains and outputs best weights and performance and data evaluation(labeling). \\n\\n### Run entire project with: python run.py data features model evaluate nips  : deletes data directory and recreates each time the above command is ran. To supress future dependent library version warnings : python -W ignore run.py data features model evaluate nips :\\n\\n### If data is already downloaded, spare your self the wait using : python run.py data skip features model evaluate nips: including skip in the targets skips the data downloading step. To supress future dependent library version warnings : python -W ignore run.py skip features model evaluate nips :\\n\\n\\nwebsite: [DSC180-A09-Eco-Acoustic-Event-Detection](https://edmundozamora.github.io/DSC180-A09-Eco-Acoustic-Detection)\\n\\nlink to CPU Model Repository: [CPU Model Repo](https://github.com/EdmundoZamora/Q1-Project-Code)\\n',\n",
       "  'This project is about binary classification of bird vocalizations in WAV files. It stores the WAV and CSV data in the \"data/raw/\" directory and outputs the results in the \"data/out/\" directory. The project takes raw wave files as input, trains a model, and outputs the best weights, performance, and data evaluation. The entire project can be run using the command \"python run.py data features model evaluate nips\". If the data is already downloaded, it can be skipped using the command \"python run.py data skip features model evaluate nips\". More information about this project can be found on the website [DSC180-A09-Eco-Acoustic-Event-Detection](https://edmundozamora.github.io/DSC180-A09-Eco-Acoustic-Detection). The CPU Model Repository can be accessed at [CPU Model Repo](https://github.com/EdmundoZamora/Q1-Project-Code).'],\n",
       " 'https://github.com/UCSD-E4E/Pyrenote': [\"## Pyrenote, The E4E Manual Audio Labeling System\\n\\nThis project, Pyrenote, creates moment to moment or strong labels for audio data. Pyrenote and much of this README are based on heavily on [Audino](https://github.com/midas-research/audino) as well as [Wavesurfer.js](https://github.com/katspaugh/wavesurfer.js). The name is a combination of Py, Lyrebird, and note (such as making a note on a label).\\n\\nIf you want to use Pyrenote, use the following to get started!\\n\\n**NOTE**: Before making any changes to the code, make sure to create a branch to safely make changes. Never commit directly to main or production branch.\\n**Read github_procedures.md for more detailed information before contributing to the repo.** \\n\\n## Usage\\n\\n*Note: Before getting the project set up, message project leads for env file. This file should be put in `/audino`. **Make sure the file is never pushed to the github***\\n\\nPlease install the following dependencies to run `Pyrenote` on your system:\\n\\n1. [git](https://git-scm.com/) *[tested on v2.23.0]*\\n2. [docker](https://www.docker.com/) *[tested on v19.03.8, build afacb8b]*\\n3. [docker-compose](https://docs.docker.com/compose/) *[tested on v1.25.5, build 8a1c60f6]*\\n\\n### Clone the repository\\n\\n```sh\\n$ git clone https://github.com/UCSD-E4E/Pyrenote.git\\n$ cd audino\\n```\\n\\n**Note for Windows users**: Please configure git to handle line endings correctly as services might throw an error and not come up. You can do this by cloning the project this way:\\n\\n```sh\\n$ git clone https://github.com/UCSD-E4E/Pyrenote.git --config core.autocrlf=input\\n```\\n\\n### For Development (Note this is the one we will test on and use)\\n\\nSimilar to `production` setup, you need to use development [configuration](./docker-compose.dev.yml) for working on the project, fixing bugs and making contributions.\\n**Note**: Before proceeding further, you might need to give docker `sudo` access or run the commands listed below as `sudo`.\\n\\n**To build the services (do this when you first start it), run:**  \\n**Note**: Remember to cd into audino before starting\\n```sh\\n$ docker-compose -f docker-compose.dev.yml build\\n```\\n\\n**To bring up the services, run:**\\n```sh\\n$ docker-compose -f docker-compose.dev.yml up\\n```\\nThen, in browser, go to [http://localhost:3000/](http://localhost:3000/) to view the application.\\n\\n**To bring down the services, run:**\\n\\n```sh\\n$ docker-compose -f docker-compose.dev.yml down\\n```\\n## Troubleshooting for starting docker\\n\\n1) Docker containers do not even get a chance to start\\n  - Make sure docker is set up properly\\n  - Make sure docker itself has started. On Windows, check the system tray and hover over the icon to see the current status. Restart it if necessary\\n2) Backend crashes\\n  - For this error, check the top of the log. It should be complaining about /r characters in the run-dev.sh files\\n  - The backend will crash if the endline characters are set to CRLF rather than LF\\n  - On VSCode, you can swap this locally via going into the file and changing the CRLF icon in the bottom right to LF\\n  - Do this for `frontend/scripts/run-dev.sh` and `backend/scripts/run-dev.sh`\\n3) Database migration issues\\n  - If the backend complains about compiler issues while the database migration is occurring go into `backend/scripts/run-dev.sh`\\n  - On line 25, check and make sure that the stamp command is pointing to the right migration for the database\\n      - Ask for help on this one\\n\\n## Getting Started\\n\\nAt this point, the docker should have gotten everything set up. After going to [http://localhost:3000/](http://localhost:3000/) you should be able to log into the docker\\n\\nTo access the site, sign in with the username of **admin** and password of **password**. On logging in, navigate to the admin-portal to create your first project. Make sure to make a label group and some labels for the project!\\n\\nAfter creating a project, get the API key by returning to the admin portal. You can use the API key to add data to a project. Create a new terminal (while docker is running the severs) and cd into `audino/backend/scripts`. Here use the following command:\\n\\n```\\npython upload_mass.py --username admin.test --is_marked_for_review True --audio_file C:\\\\REPLACE\\\\THIS\\\\WITH\\\\FOLDER\\\\PATH\\\\TO\\\\AUDIO\\\\DATA --host localhost --port 5000 --api_key REPLACE_THIS_WITH_API_KEY\\n```\\nMake sure to have a folder with the audio data ready to be added. For testing purposes, get a folder with about 20 clips. \\n\\nOnce that runs, you are ready to start testing!\\n\\n\\n### For Production (Don't use on windows)\\n\\nYou can either run the project on [default configuration](./docker-compose.prod.yml) or modify them to your need.  \\n**Note**: Before proceeding further, you might need to give docker `sudo` access or run the commands listed below as `sudo`.  \\n**Note**: Remember to cd into audino before starting  \\n\\n**To build the services, run:**\\n\\n```sh\\n$ docker-compose -f docker-compose.prod.yml build\\n```\\n\\n**To bring up the services, run:**\\n\\n```sh\\n$ docker-compose -f docker-compose.prod.yml up\\n```\\n\\nThen, in browser, go to [http://0.0.0.0/](http://0.0.0.0/) to view the application.\\n\\n**To bring down the services, run:**\\n\\n```sh\\n$ docker-compose -f docker-compose.prod.yml down\\n```\\n\\n### For Dev Team:\\nFeatures should be turned on and off by admins for individual projects. When adding a new feature to either a project's data page or\\nannotation page, make sure to do the following:\\n1) Go to `.\\\\audino\\\\frontend\\\\src\\\\containers\\\\forms\\\\featureForm.js`\\n2) Add a new item in the featuresEnabled directory. This will be the name of the `feature_toggle` variable. \\n3) Return to the page you are working on. \\n  - For example, if you are working on the annotation page, navigate to the `componentDidMount()` method\\n  - about 20 lines down in the `setState` callback, add to the list `SOME_VAR: response.data.features_list['VARIABLE_NAMED_IN_STEP_2']`.\\n\\n  \",\n",
       "  'Pyrenote is an audio labeling system that creates moment to moment or strong labels for audio data. It is based on Audino and Wavesurfer.js. To use Pyrenote, you need to install git, docker, and docker-compose. You can clone the repository and set up the project using the provided instructions. There are separate setups for development and production environments. Troubleshooting tips are also provided. Once the setup is complete, you can access the application in your browser and log in with the username \"admin\" and password \"password\". You can create projects, label groups, and labels in the admin portal. To add data to a project, you need to use an API key and run a Python script provided in the instructions. The summary also mentions how features can be enabled or disabled for individual projects by admins.'],\n",
       " 'https://github.com/RuojiaTao/Covid-Misinformation-Spread-on-Tweets': [\"# Covid-Misinformation-Spread-on-Tweets\\nAnalyze the misinformation about covid spreading in tweet\\n\\nOur website: https://ruojiatao.github.io/Covid-Misinformation-Spread-on-Tweets/\\n\\nAbstract: \\nSpread of misinformation over social media posts challenges to daily information intake and exchange. Especially under current covid 19 pandemic, the disperse of misinformation regarding to covid 19 diseases and vaccination posts threats to individuals' wellbeings and general publich health. This project seeks to invertigate the spread of misinformation over social media (Twitter) under covid 19 pademic. The first topic is the effect of bot users on the spread of misinformation, and the second topic is to examine users' attitude towards misinformation. These two topics are analyze under the social structure (connected social graphs) created through user's interactions on Twitter. This project also seeks to invertigate the change in proportion of bot users and users' attitude towards misinformation as it's approaching to the center of the social network. \\n\\n\\n\\n - The `run.py` file run all part of code\\n - `Data_Pre` folder contains: \\n     - `hydrate_tweets_twarc.py` : hydrating the data\\n     - `EDA.py`: explore the raw data and try to find interesting points\\n - `K_Core` folder contains:\\n     - `K_Core_Degree.py`: set up a graph and calculate the K-Core degree , assign K Core degree back to all data, and plot out the spread of K-Core.\\n     - `sample_dataset.py` : sample a small dataset for analzye\\n     - `Tests_result.py` : run a linear machine learning model to predict the negative sentiment based on different parapmeters. Base on the coefficient of ML model to find the relstionship between parpameters and negative sentiment.\\n - `Bot_User` folder contains: \\n     - `bot_detection.py`: use Botomerter to predict if a user is bot or not\\n     - `Bot_merge.py`: merge predictions to K Core results, generating graphs for percentage of bot useres in each K core degrees\\n - `NLP` folder contains:\\n     - `NLP.py`: get the sentiment scores of each tweets, and plot out overall trends\\n     - `NLP_ab_testing`: run a 2 sample t-test to see if there is an sigificant difference between negative sentiments in different pair of groups that has different K-Core degrees\\n - `data` folder contains:\\n     - `iffy.csv` used for checking misinformation links\\n\\n\\n\",\n",
       "  \"This project aims to analyze the spread of misinformation about COVID-19 on Twitter. It focuses on two main topics: the impact of bot users on the spread of misinformation and users' attitudes towards misinformation. The analysis is conducted using social structure data created from user interactions on Twitter. The project also investigates the change in proportion of bot users and users' attitudes as they move closer to the center of the social network. The code files in different folders perform various tasks such as data preprocessing, graph analysis, bot detection, sentiment analysis, and hypothesis testing.\"],\n",
       " 'https://github.com/davamini/DSC180B_Reddit_MisInfo_Capstone': ['# DSC180B Subreddit Misinformation Capstone Project\\nAnalyzing the spread of misinformation in the Reddit platform.<br>\\nWebsite URL: https://davamini.com/dsc180_project.github.io/index.html\\n<br>\\n### Build Instructions:\\n```sh\\npip install pandas praw gspread oauth2client\\n```\\n> Must create conf.json and google_sheets_creds.json in the <b>config directory</b>.<br>\\n> * conf.json must contain values for client_secret, and client_id, which are aquired from Reddit after creating an application.<br>\\n> * google_sheets_creds.json is aquired from Google Cloud after enabling APIs for google sheets.<br>\\n#### Run either:\\n```sh\\npython run.py\\n```\\n#### Or:\\n```sh\\npython run.py test\\n```\\n',\n",
       "  'This project analyzes the spread of misinformation on the Reddit platform. To build and run the project, you need to install pandas, praw, gspread, and oauth2client using pip. Additionally, you must create conf.json and google_sheets_creds.json files in the config directory. The conf.json file should contain values for client_secret and client_id obtained from Reddit after creating an application. The google_sheets_creds.json file is obtained from Google Cloud after enabling APIs for Google Sheets. To run the project, use either \"python run.py\" or \"python run.py test\".'],\n",
       " 'https://github.com/fieryashes/DSC180B_Misinformation_Project': ['# DSC180B Misinformation Project\\nProject Website: https://anaaamika.github.io/DSC180B-Misinformation/\\n\\n## Build Instructions\\nEnsure that you have twarckeys.py and youtubekeys.py files in your secrets folder. The twarckeys.py file should contain the variables consumer_key, consumer_secret, access_token, access_token_secret populated from the project keys and tokens from [the Developer Portal for the Twitter API](https://developer.twitter.com/en/portal/dashboard). The youtubekeys.py file should contain the variable api_key populated with a API key for [the YouTube Data API v3](https://developers.google.com/youtube/v3). \\n\\nTo build the project run the following commands: \\n`python run.py data` This will populate the *data* folder with tweet datasets, YouTube video ids dataset, and datasets with YouTube metadata. \\n\\nThen run `python run.py analysis` to view preliminary analysis on the YouTube captions and metadata.\\n\\nFinally, run `python run.py model` to build a misinformation detection model based on the YouTube transcripts. \\n',\n",
       "  'The DSC180B Misinformation Project provides a project website with build instructions. To build the project, ensure that you have the necessary files in your secrets folder and run specific commands using Python. The commands include populating the data folder with tweet datasets, YouTube video ids dataset, and datasets with YouTube metadata, viewing preliminary analysis on YouTube captions and metadata, and building a misinformation detection model based on YouTube transcripts.'],\n",
       " 'https://github.com/Bryan-Az/DSC-Capstone-Result-Replication': [\"## DSC-Capstone-Result-Replication\\n### DSC180A Quarter 1 and 2\\n\\nWebsite Link: http://www.bambriz.me/wp-content/uploads/2022/03/dsc180b.html\\n\\nWebsite code is included in the 'docs' directory of this repo.\\n\\nTo run the GENConv GNN model, create docker pod using jmduarte/capstone-particle-physics-domain:latest and group key to access data folder.\\n\\nThen clone this repo and cd into root folder.\\n\\nIn the root folder, you can use python run.py [train, test] to produce a dataframe with predictions and other data useful for visualization in the src/analysis/evaluation.ipynb notebook. \\n\\nTo visualize, you can simply run all cells in the eval. notebook.\\n\\nPls. ignore the loading bar in terminal it doesn't mean anything - but training it will take a while. \\n\\nThere is really no need to run the training since the testing script loads presaved weights (stored in github) to initialize the pre-trained model for testing.  \\n\",\n",
       "  'This is a summary of the instructions for replicating the results of the DSC-Capstone project. The website link provided contains the necessary code and documentation. To run the GENConv GNN model, a docker pod needs to be created using a specific image. After cloning the repository and navigating to the root folder, running a Python script will generate a dataframe with predictions and other data for visualization. The evaluation notebook can be used to visualize the results. It is mentioned that the loading bar in the terminal does not indicate progress, and training may take some time. However, running the training script is not necessary as pre-trained weights are already available for testing.'],\n",
       " 'https://github.com/UCSDJLEE/DSC180B-A11-Project': ['# DSC180B-A11-Project\\n## Particle Jet Mass Regression\\n\\n<img src=\\'https://raw.githubusercontent.com/isacmlee/particle-physics-visuals/main/images/cern_atlas.jpeg\\'>\\n\\n\\nWith jet data collected from proton-proton collision, the contents in this repository aim to predict the mass of particle jet, which is an important feature in classifying the types of those jets.\\nThe model is implemented based on PyTorch Neural Network APIs and gets fitted to training sets available on DSMLP server.\\n\\n#### Repository Structure:\\n\\n - `conf`: directory in which all necessary variables needed for configuration in development are stored\\n - `data`: temporary directory that explains the source of our data\\n - `notebooks`: directory in which .ipynb notebooks for EDA and model evaluation is stored\\n - `src`: source directory for all library codes used for this project\\n - `run.py`: main script to run within properly set-up environment using `python3 run.py train` for model training or `python3 run.py test` for model assessment\\n - `simplenetwork_best.pt`: PyTorch-based file that stores weights of optimized, or best \"fitting,\" model\\n',\n",
       "  'This repository aims to predict the mass of particle jets using jet data collected from proton-proton collision. The model is implemented using PyTorch Neural Network APIs and is fitted to training sets available on the DSMLP server. The repository structure includes directories for configuration variables, data sources, notebooks for exploratory data analysis and model evaluation, source code, and a main script for training and testing the model. The repository also includes a file that stores the weights of the optimized model.'],\n",
       " 'https://github.com/shonepatil/GNN-Spotify-Recommender-Project': [\"# Graph Neural Networks for Song Recommendation on Spotify Playlists\\n\\nThis project tackles the task of creating meaningful and accurate song recommendations to Spotify Playlists by using Graph Neural Networks. The goal is to better capture the characteristics of songs by analyzing co-occurence of song pairs across thousands of playlists in the form of a graph.\\n\\nTo obtain the spotify playlist data, visit https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge and put the files in `data/playlists`. You will have to create these subfolders. Confirm data paths for future files in `config/data-params`. To add song features and create graph from scratch if you only have the playlist data, set `create_graph_from_scratch` to be `True` in data-params. For obtaining song features using the Spotify API, put secret key and secret id in `spotifyAPI_script.py` within `/src/api`. Also within data-params, we suggest setting `playlist-num` to be 1000 such that the Spotify API requesting only takes 15-20 minutes. If you set it to 10000 for a larger dataset, the code may take 4 hours.\\n\\nTo run the GraphSAGE based model on the Spotify Playlist data, first use this command from the root folder: `python run.py data model`. You can leave out the `model` argument if you don't want to train the model from scratch. Next to see and create recommendations open and run through the notebook `Recommender Demonstration.ipynb` from within `src/dgl_graphsage/`.\\n\\nTo customize the model parameters, edit `config/model-params`.\\n\",\n",
       "  'This project focuses on using Graph Neural Networks to improve song recommendations on Spotify playlists. The approach involves analyzing the co-occurrence of song pairs across thousands of playlists in the form of a graph. To obtain the necessary data, visit a specific website and follow the instructions for downloading and organizing the files. Additionally, there are suggestions for setting certain parameters to optimize performance. To run the model, use a specific command from the root folder, and to customize the model parameters, edit a specific configuration file.'],\n",
       " 'https://github.com/yangshengaa/dynamic_stock_industry_classification': [\"# Dynamic Stock Industrial Classification\\n\\nUse graph-based analysis to re-classify stocks and experiment different re-classification methodologies to improve Markowitz portfolio optimization performance in the low-frequency quantitative trading context.\\n\\nNote that for strategy confidentiality, many files are hidden.\\n\\nTo accommodate speedy development, the current code structure simplicity is sacrificed. This will be addressed in later versions.\\n\\nProject Website: [Dynamic Stock Industrial Classification](https://yangshengaa.github.io/dynamic_stock_industry_classification/)\\n\\n## Module Breakdown\\n\\nThis project contains the following six modules:\\n\\n- [data ingestion](src/data_ingestion): address finance data I/O and handle storage of intermediate results;\\n- [factor generation](src/factor_generation): compute and store factors alpha factors and risk factors for low-frequency trading;\\n- [backtest](src/backtest): low-frequency backtest framework (both factors and signals). Factors have continuous values on each cross section whereas signals have only -1, 0, and 1 overall;\\n- [factor combination](src/factor_combination): combine factors using ML models;\\n- [portfolio optimization](src/portfolio_optimization): Markowitz portfolio optimization, with turnover, industrial exposure, style exposure, and various other constraints.\\n- [graph cluster](src/graph_cluster): experiment different graph-based clustering on stocks.\\n\\n## Data\\n\\nChina A-Share stocks, the corresponding major index data (sz50, hs300, zz500, zz1000), and the member stock weights from 20150101 to 20211231, provided by [Shanghai Probability Quantitative Investment](http://www.probquant.cn/).\\n\\n## Experiment Results\\n\\nWith a fixed predicted ML results, we go through the optimization pipeline to optimize each trained classification.\\n\\n**Stock Pool**: zz1000 member stocks  \\n**Benchmark**: zz1000 index  \\n**Time Period**: 20170701 - 20211231  \\n\\n| Model | AlphaReturn (cumsum) | AlphaSharpe | AlphaDrawdown | Turnover |\\n| ----- | :---------------------: | :----------: | :-----------: | :------: |\\n| LinearRegressor | 71.58 | 1.92 | -19.84 | **1.01** |\\n| LgbmRegressor | 145.64 | **3.65** | **-11.58** | 1.21 |\\n| LgbmRegressor-opt | **146.73** | 2.96 | -29.79 | 1.11 |\\n| .. | .. | .. | .. | .. | .. |\\n| 40-cluster PMFG Unfiltered Spectral | 154.45 | 3.15 | **-22.69** | 1.11 |\\n| 10-cluster PMFG Filtered Average Linkage | 160.95 | **3.32** | -26.77 | 1.11 |\\n| 30-cluster AG Unfiltered Sub2Vec | 160.96 | 3.24 | -23.05 | 1.10 |\\n| 5-cluster MST Unfiltered Sub2Vec | 163.26 | 3.27 | -27.39 | 1.11 |\\n| **20-cluster PMFG Filtered Node2Vec** | **164.68** | 3.30 | -27.06 | 1.11 |\\n\\nCompared to the original optimization result, we observe a 12.23% improvement in excess return and 12.16% improvement in excess Sharpe ratio.\\n\\nSince factors based on price and volume lost their predictive power staring from 20200701, we also look at the performances before that time.\\n\\n**Time Period**: 20170701 - 20200701\\n\\n| Model | AlphaReturn (cumsum) | AlphaSharpe | AlphaDrawdown | Turnover |\\n| ----- | :---------------------: | :----------: | :-----------: | :------: |\\n| LgbmRegressor | 150.64 | 6.06 | **-4.59** | 1.23 |\\n| LgbmRegressor-opt | **170.31** | 5.43 | -6.76 | 1.12 |\\n| .. | .. | .. | .. | .. | .. |\\n| 10-cluster PMFG Filtered Sub2Vec | 173.10 | 5.49 | **-5.51** | 1.12 |\\n| 5-cluster MST Filtered Sub2Vec | 182.89 | 5.78 | -7.14 | 1.12 |\\n| 10-cluster AG Filtered Sub2Vec | 181.50 | 5.64 | -7.40 | 1.12 |\\n| **20-cluster PMFG Filtered Node2Vec** | **184.21** | **5.85** | -6.42 | 1.12 |\\n\\nIn this period, we observe a 8.16% improvement in excess return and a 7.73 improvement in excess Sharpe ratio, compared to the original optimization result.\\n\\nFor a complete list of results, check out [summary_20170701_20211231.csv](out/res/signal_test_file_20220305_long_experiment/summary.csv) and [summary_20170701_20200701.csv](out/res/signal_test_file_20220305_short_experiment/summary.csv). And more details are discussed on the project website listed above.\\n\\n## Environment\\n\\nTo run codes in this project, it is recommended to create an environment listed in the [environment.yml](environment.yml). If conda is installed, run:\\n\\n```bash\\nconda env create -f environment.yml\\nconda activate finance-base\\n```\\n\\nAlternatively, one could also pull the corresponding docker image from [yangshengaa/finance-base](https://hub.docker.com/repository/docker/yangshengaa/finance-base) and then activate the finance-base environment using the latter conda command.\\n\\n## Quick Start\\n\\nIt's very easy to use this platform!\\n\\nTips:\\n\\n- run each module at a time, and run the following command sequentially;\\n- change config for corresponding module in respective files (file location indicated inside [run.py](run.py));\\n- detailed running instructions, including a walkthrough of parameters in each modules, are in README of each module.\\n\\nTo run each module, in current directory:\\n\\nFactor Generation:\\n\\n- factor generation: `python run.py gen`\\n\\nBacktest:\\n\\n- backtest factor: `python run.py backtest_factor`\\n- backtest signal: `python run.py backtest_signal`\\n\\nFactor Combination:\\n\\n- factor combination: `python run.py comb`\\n\\nPortfolio Optimization:\\n\\n- generate factor returns: `python run.py opt_fac_ret`\\n- estimate covariance matrices: `python run.py opt_cov_est`\\n- adjust weight: `python run.py opt_weight`\\n\\nGraph Clustering:\\n\\n- train graph clustering: `python run.py cluster_train`\\n\\nTo run each submodules, in current directory:\\n\\n- generate pairs factors: `python run.py pairs`\\n- generate risk factors: `python run.py gen_risk`\\n\\nCurrently risk attribution module is very slow and suboptimal. To be addressed later.\\n\\n## Acknowledgement\\n\\nSpecial thanks to coworkers and my best friends at [Shanghai Probability Quantitative Investment](http://www.probquant.cn/): Beilei Xu, Zhongyuan Wang, Zhenghang Xie, Cong Chen, Yihao Zhou, Weilin Chen, Yuhan Tao, Wan Zheng, and many others. This project would be impossible without their data, insights, and experiences.\\n\\n## For Developer\\n\\nLog known issues here:\\n\\n- signals given by factor test could not give the same alpha returns (slightly less) as in signal test\\n  - examine output holding stats\\n- plain risk attribution\\n\",\n",
       "  'The Dynamic Stock Industrial Classification project aims to improve the performance of Markowitz portfolio optimization in low-frequency quantitative trading. It uses graph-based analysis to re-classify stocks and experiment with different re-classification methodologies. The project consists of six modules, including data ingestion, factor generation, backtesting, factor combination, portfolio optimization, and graph clustering. The data used in the project includes China A-Share stocks, major index data, and member stock weights. The experiment results show improvements in excess return and excess Sharpe ratio compared to the original optimization results. The project provides an environment setup guide and quick start instructions for running the modules. Acknowledgements are given to coworkers and friends who contributed to the project.'],\n",
       " 'https://github.com/MarthaY01/hdsi_faculty_tool/tree/main': ['# HDSI Faculty Exploration Tool\\n\\nThis repository contains project code for experimenting with LDA for Faculty Information Retrieval System.\\n\\nThis tool is now deployed @ https://hdsi-faulty-tool.herokuapp.com/\\n\\n## Running the Project\\n* All of the below lines should be run within a terminal:\\n\\n* Before running any of the below commands, launch the docker image by running `launch.sh -i duxiang/dsc180a:latest`\\n\\n* To get the preprocessed data file, run `python run.py process_data`\\n* To get the fitted sklearn.lda model, run `python run.py model`\\n* To prepare/update the dashboard, run `python run.py prepare_sankey`\\n* To run the live dashboard, run `python run.py run_dashboard`\\n\\n## Using the Dashboard\\n* When executing `run_dashboard`, it will launch dash with a locally hosted port.\\n* It would require port-forwarding on a remote server.\\n\\n# Web Link\\nWebsite: https://marthay01.github.io/hdsi_faculty_tool/\\n',\n",
       "  'This repository contains project code for experimenting with LDA for a Faculty Information Retrieval System. The tool is deployed at https://hdsi-faulty-tool.herokuapp.com/. To run the project, launch the docker image using `launch.sh -i duxiang/dsc180a:latest` and then run various commands such as `python run.py process_data` to get the preprocessed data file, `python run.py model` to get the fitted sklearn.lda model, `python run.py prepare_sankey` to prepare/update the dashboard, and `python run.py run_dashboard` to run the live dashboard. The dashboard can be accessed by executing `run_dashboard`, which will launch dash with a locally hosted port. Port-forwarding may be required on a remote server. The website for this tool can be found at https://marthay01.github.io/hdsi_faculty_tool/.'],\n",
       " 'https://github.com/amuamushu/adv_avod_ssn': ['# AVOD for Single Source Robustness Against Adversarial Attacks.\\nThis project is worked on by Amy Nguyen ([@amuamushu](https://github.com/amuamushu)) and Ayush More ([@ayushmore](https://github.com/ayushmore)) over the course of 20 weeks under the mentorship of Lily Weng. \\n\\nVisual Presentation: https://ayushmore.github.io/2022-03-07-improving-robustness-via-adversarial-training/\\n\\nOral Presentation Slides: https://docs.google.com/presentation/d/1DSoFa1vUQBcjghLfds6ce4sIG0qkBvtL1c2ixlEs7qc/edit?usp=sharing\\n\\n## Setup\\n### Container Setup\\nTo mimic our environment, please build a docker container using the image `amytn/avod-adv:latest`. To prevent memory errors, please ensure your container has **at least 16 GB of RAM** available. For faster runtime, including a GPU may be useful.\\n\\n### Cloning the repository\\nSince this reposity contains a submodule, cloning would require an additional commandline argument. \\n\\nMake sure to clone the reposity in your home directory so the Python paths in the Docker image match up. If the repository is cloned elsewhere, please set up the python path yourself (see [Setting up Necessary Python Paths](#setting-up-necessary-python-paths)).\\n\\n```\\ngit clone --recurse-submodules https://github.com/amuamushu/adv_avod_ssn.git\\n```\\n\\n### The dataset\\nThe dataset we will be using is the [KITTI dataset](http://www.cvlibs.net/datasets/kitti/). For the dataset and mini-batch setup, please follow the download steps listed in the [AVOD repository](https://github.com/kujason/avod#dataset).\\n\\nFor the dataset, we will be follow a slightly different setup to the one on the AVOD repository.\\n\\nIn your home directory, the layout should look like this:\\n\\n```\\nhome\\n..\\\\avod_data\\n....\\\\Kitti\\n......\\\\object\\n........\\\\testing\\n........\\\\training\\n..........\\\\calib # camera calibration (can ignore)\\n..........\\\\image_2 # the 2D images\\n..........\\\\label_2 # true labels and bounding boxes\\n..........\\\\planes \\n..........\\\\velodyne # the lidar point clouds\\n....... train.txt # list of sample names to use for training\\n....... val.txt # list of sample names to use for validation\\n..\\\\adv_avod_ssn # this repository!\\n  \\n```\\nMore information about the true labels can be found here: https://github.com/kujason/avod/wiki/Data-Formats\\n\\n## Run\\n```\\npython3 run.py [clean] [test] [clean-model] [adv-model] [ssn-model]\\n```\\n\\nThese targets can be run one-by-one in the order provided.\\n\\n### `clean` target: \\nDeletes all files in the `outputs` folder.\\n\\n### `test` target: \\nRuns training and inference on test data found under `test/testdata` and writes the predictions and AP scores to the `outputs/<checkpoint_name>` directory. For this target, the checkpoint name is `test_data`.\\n- Note: If you plan on only using the test data and not downloading the full dataset, in `scripts/offline_eval/kitti_native/eval/run_eval.sh`, please update `$prev/avod_data/Kitti/object/training/label_2/` to be `$repo/test/testdata/Kitti/object/training/label_2/`. `run_eval.sh` is used for computing the AP Scores after running inference. \\n\\n### `clean-model` target:\\nRuns training for a clean model and adversarial inference on the full dataset found at `home/avod_data` and writes the predictions and AP scores to the `outputs/<checkpoint_name>` directory. For this target, the checkpoint name is `pyramid_cars_with_aug_simple`.\\n\\n### `adv-model` target:\\nRuns training for an adversarial model and all three types of inference (clean, adversarial, SSN) on the full dataset found at `home/avod_data` and writes the predictions and AP scores to the `outputs/<checkpoint_name>` directory. For this target, the checkpoint name is `test_adv`.\\n- **NOTE**: Since the adversarial model only fine-tunes the clean model, running this target requires the clean model to be completely trained.\\n\\n### `ssn-model` target:\\nRuns training for a clean model and adversarial inference on the full dataset found at `home/avod_data` and writes the predictions and AP scores to the `outputs/<checkpoint_name>` directory. For this target, the checkpoint name is `trainsin_pyramid_cars_with_aug_simple_rand_5`.\\n- **NOTE**: Since the single-source-noise model only fine-tunes the clean model, running this target requires the clean model to be completely trained.\\n\\nNote: `pyramid_cars_with_aug_simple` and `trainsin_pyramid_cars_with_aug_simple_rand_5` are the same checkpoint names that our previous work author Taewan Kim had so we kept it for ease of comparison.\\n\\n## The Shell Script\\nEach shell script is responsible for running the entire experiment for one model. Details about the specific experiments are covered in the paper.\\n\\n**Arguments for training, inference, and evaluation:**\\n\\n`pipeline_config`: Specifies the path to the experiment configurations (batch size, number of steps, learning rate, number of iterations, dataset path, etc).\\n\\n`data_split`: Can be `train`, `val`, or `test`. Whichever keyword is specified determines which samples are used.\\n\\n`output_dir`: Path to where the results and AP scores are written to.\\n\\n`ckpt_indices`: This value specifies which model checkpoint to use for inference. \\n  - Every couple of steps during training, the current trained model is saved to a checkpoint. \\n\\n#### Configuration Files: \\n\\n\\n## Experimental Configurations\\nEach shell script references multiple `.config` files: one for training the model and one or more for inference. These are found under `./avod/configs`.\\n\\n### `model_config`:\\nContains configurations for the object detection model. Notable configurations are:\\n- `model_name`: Should be `avod_model` for all the experiments we run.\\n- `checkpoint_name`: The name of the experiment checkpoint. This determines what folder the outputs are saved to.\\n- `is_adversarial`: A boolean value defaulted to False. Train the model adversarially and runs adversarial inference if True, otherwise train the model normally.\\n- `adv_epsilon`: The epsilon to use for our perturbation. Only used if the model is being trained or running inference adversarially.\\n\\n### `train_config`: \\n- `pretrained_ckpt`: What checkpoint to continue training from. This would be used and should be updated as the model is being fine-tuned.\\n\\n### `eval_config`:\\n- `dataset_dir`: The dataset used for training and infernece. \\n- `pretrained_chkpt`: What trained model checkpoint to use for inference. This is useful if the inference `.config` file is different from the training `.config` file. \\n\\n\\n### Viewing Results\\n#### AP Scores: \\nThe AP scores can be found under `outputs/<checkpoint_name>/offline_eval/results` where the 3 values provided for each test corresponds to easy, medium, and hard respectively.\\n\\n**Here is an exmaple of what the AP Scores look like**:\\n```\\ncar_detection AP: 89.973267 87.620293 80.301704\\ncar_detection_BEV AP: 89.292168 86.383148 79.538277\\ncar_heading_BEV AP: 89.177887 85.891479 78.938744\\ncar_detection_3D AP: 77.332970 67.945251 66.929985\\ncar_heading_3D AP: 77.288345 67.749474 66.543098\\n```\\n\\nAfter inference, if the AP scores are not saved properly, they can be manually calculated and saved again using this command:\\n```\\nbash scripts/offline_eval/kitti_native_eval/run_eval.sh ./outputs/<checkpoint_name>/<prediction_type>/kitti_native_eval/ 0.1_val <training_step> <checkpoint_name>\\n```\\n- `checkpoint_name`: name of the checkpoint as specified in the `.config` file\\n- `prediction_type`: would be ...\\n   - `prediction` if inference results on clean data is wanted\\n   - `prediction_adv` if inference results on adversarial data is wanted\\n   - `predictions_sin_rand_5.0_5` if inference results on SSN data is wanted\\n\\n\\n#### Visualization with bounding boxes, IoU scores, and confidence level: \\nRun the below code to generate bounding boxes on top of the images used during inference. Images will be saved to `outputs/<checkpoint_name>/predictions/images_2d`\\n\\n```\\npython3 demos/show_predictions_2d.py <checkpoint_name>\\n```\\n\\n**Here is an example of a generated image**:\\n![000152](https://user-images.githubusercontent.com/35519361/152208158-833ca90f-911a-4ab5-a846-e167cfc2e1a3.png)\\n\\n\\n\\n## References\\nThis project builds off of Kim, Taewan and Ghosh, Joydeep\\'s work on \"Single Source Robustness in Deep Fusion Models.\" In their GitHub repository (https://github.com/twankim/avod_ssn), they incorporate single source noise into the inputs of the AVOD 3D object detection model. We expand on their work and code by incorporating adversarial noise, rather than Gaussian noise, into the input images.\\n```\\n@inproceedings{kim2019single,\\n  title={On Single Source Robustness in Deep Fusion Models},\\n  author={Kim, Taewan and Ghosh, Joydeep},\\n  booktitle={Advances in Neural Information Processing Systems},\\n  pages={4815--4826},\\n  year={2019}\\n}\\n```\\n\\nWe also relied on the documentation of the original AVOD code (https://github.com/kujason/avod) for setting up the model and data as well as understanding our results.\\n```\\n@article{ku2018joint, \\n  title={Joint 3D Proposal Generation and Object Detection from View Aggregation}, \\n  author={Ku, Jason and Mozifian, Melissa and Lee, Jungwook and Harakeh, Ali and Waslander, Steven}, \\n  journal={IROS}, \\n  year={2018}\\n}\\n```\\n\\nWe referred to the KITTI dataset website (http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d) and its related papers to better understand our dataset.\\n```\\nA. Geiger, P. Lenz and R. Urtasun, \"Are we ready for autonomous driving? The KITTI vision benchmark suite,\" \\n2012 IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 3354-3361, doi: 10.1109/CVPR.2012.6248074. \\nhttp://www.cvlibs.net/publications/Geiger2012CVPR.pdf\\n```\\n\\n\\n## Appendix\\n### Setting Up Necessary Python Paths\\nRun these two commands to set the Python paths for `avod_ssn` and `wavedata`:\\n```\\nexport PYTHONPATH=$PYTHONPATH:\\'<path to avod>\\'\\nexport PYTHONPATH=$PYTHONPATH:\\'<path to wavedata>\\'\\n```\\n\\n### Pretrained Models:\\nSince training takes many hours, we included our pretrained clean model here:\\nhttps://drive.google.com/drive/folders/18U7t-4gU4sXvAD33GEuVnwSwUSItHdPX?usp=sharing\\n',\n",
       "  'This project, called \"AVOD for Single Source Robustness Against Adversarial Attacks,\" was worked on by Amy Nguyen and Ayush More over the course of 20 weeks under the mentorship of Lily Weng. The project aims to improve the robustness of object detection models against adversarial attacks. The code and documentation for the project can be found on their respective GitHub repositories. The project utilizes the KITTI dataset and provides instructions on how to set up the dataset and run the code. It also includes pretrained models that can be used for inference. The project builds upon previous work by Kim and Ghosh on single source robustness in deep fusion models. The paper and code of their work are referenced in this project.'],\n",
       " 'https://github.com/Maderlime/DSC180_Q1_Code': [\"# STEPS TO RUN CODE ON ADVERSARIAL ROBUST TRAINING (QUARTER 2 - DSC 180B)\\n\\n## How to SSH into the DSMLP server\\nssh [user]@dsmlp-login.ucsd.edu\\n\\n## How to build the Docker file\\ndocker build -t test .\\ndocker run -it --rm test /bin/bash\\n  \\n## Deploy a pod with GPU support\\nlaunch-scipy-ml-gpu.sh\\n\\n# LOADING IN DATA\\n\\nLoad in the data from the following source: https://www.dropbox.com/sh/tg6xij9hhfzgio9/AADqu6BMq3Rko7U7-q6vwmMFa?dl=0\\n\\nWe will use the following files for each dataset:\\n- val_test_x_preprocess.npy\\n- val_test_y.npy\\n\\nMake a folder in for the test and train data for each dataset. Within each of these folders, create two subfolders titled as '0' and '1'. \\n\\nFrom here, go to the file at DSC180_Q1_Code/patch_attacks/data/cxr/make_fast_adversarial_documents.ipynb and run the code in these cells. This will load in the images as Numpy files and partition them into training and test sets. Set the output writing dirctories to the folders you created above. Use a 70/30 split in the ranges in the code based on the size of the dataset. \\n\\n# MAKING ADJUSTMENTS TO THE CODE\\n\\nYou can edit hyperparameters for the FGSM training model in the train_fgsm.py file in src/test. You can edit attack parameters in the evaluate_pgd method on the utils.py file in src/model. You can select the datasets you want to load in from the ones you created above. \\n\\n\\n# Command line prompt to run the code\\npython run.py test\\n\\n# STEPS TO RUN CODE ON ADVERSARIAL ATTACKS (QUARTER 1 - DSC 180A)\\n\\n# Build Overview\\n### Command line prompt to run the code\\n#### python run.py test\\n### Run the larger test/analysis code with\\n#### python run.py analysis\\n\\n\\n## How to SSH into the DSMLP server\\nssh <user>@dsmlp-login.ucsd.edu\\n\\n## How to build the Docker file\\ndocker build -t test .\\ndocker run -it --rm test /bin/bash\\n\\n<!-- docker run -it --rm mjtjoa/dsc180a_quarter1_code bash -->\\n\\n\\ndocker tag test mjtjoa/dsc180a_quarter1_code\\ndocker push mjtjoa/dsc180a_quarter1_code:latest\\n\\n# Cleaning docker\\ndocker system prune\\ndocker system prune -a\\nsudo rm -rf /var/lib/docker\\n\\n### Launching the Docker File  \\nlaunch-scipy-ml-gpu.sh -i mjtjoa/dsc180a_quarter1_code:latest\\n  (if this doesn't work, add -P Always)\\n### Relevant Links\\n  Project Report: https://docs.google.com/document/d/1iQ0lZ_wpxqQXRwwjwKANrwNn6FRDwvasISHf9vpNIHw/edit?usp=sharing\\n  \\n  Project Proposal for Q2: https://docs.google.com/document/d/1d4Z4yS0aSyCMxht0NaaEf_mMxH5KFPg2B8b3rKXyRwk/edit?usp=sharing\\n  \\n  Source Report: https://arxiv.org/pdf/1804.05296.pdf\\n  \\n  Source Repository: https://github.com/sgfin/adversarial-medicine\\n\",\n",
       "  'This document provides steps to run code on adversarial robust training for Quarter 2 of DSC 180B. It includes instructions on how to SSH into the DSMLP server, build the Docker file, deploy a pod with GPU support, and load in data from a Dropbox source. It also explains how to make adjustments to the code, edit hyperparameters and attack parameters, and select datasets. The command line prompt to run the code is provided as well.\\n\\nAdditionally, it briefly mentions steps to run code on adversarial attacks for Quarter 1 of DSC 180A. This includes instructions on how to SSH into the DSMLP server, build the Docker file, and launch the Docker file. Relevant links to project reports and repositories are also provided.'],\n",
       " 'https://github.com/Actionable-Recourse/recourse-api': ['# Recourse API\\n\\nhttps://recourse-api.herokuapp.com/recourse\\n\\nFrom the link above, you can check a list of actions a person can take to be accepted by a Credit Scoring algorithm.\\n\\n## How to set up the environment\\n\\n```pip install virtualenv```\\n```virtualenv venv``` to create your new environment\\n```. venv/bin/activate```\\n```pip install -r requirements.txt``` to install all the required packages\\n\\n## How to run\\n\\nActivate the python environment.\\n```\\n. venv/bin/activate\\n```\\n\\n```\\npython app.py\\n```\\n\\n**To run this in WSL, you might need to run ```wsl --shutdown``` in Powershell. Otherwise, localhost will not be accessible.**',\n",
       "  'The Recourse API provides a list of actions that can help a person be accepted by a Credit Scoring algorithm. To set up the environment, you need to install virtualenv and create a new environment. Then, activate the environment and install the required packages. To run the API, activate the Python environment and run the app.py file. If running in WSL, you may need to run \"wsl --shutdown\" in Powershell for localhost to be accessible.'],\n",
       " 'https://github.com/freebreadstix/capstone_B02': ['# Group B02 Capstone project repo\\n\\nTo run:\\n\\n\\nIf running from DSMLP cluster:\\n```\\nssh user@dsmlp-login.ucsd.edu\\nlaunch-scipy-ml.sh -i freebreadstix/q1-replication\\n```\\nElse be sure to run in container: https://hub.docker.com/repository/docker/freebreadstix/q1-replication\\n\\nThen:\\n```\\ngit clone https://github.com/freebreadstix/capstone_B02.git\\ncd capstone_B02\\n```\\nIf not merged to main, make sure to switch to branch with run.py\\n```\\ngit checkout lucas-runpy\\n```\\n\\nConfigure config yaml with appropriate parameters. You can make your own .yml using config.yml as reference, just pass it as the argument on CLI\\n\\nRun run.py w/ config yaml corresponding to configuration you are running. For testing this is test_config.yml\\n```\\npython3 run.py test_config.yml\\n```\\nLink to Presentation Website\\n```\\nhttps://micmiccitymax.github.io/dsc180b02-site/\\n```\\n\\nExplainations of Config.yml output options\\n```\\nnum_words: how many words are in the \"important words\" for the models\\nsave_predictions: saves output of predictions to a file\\nprint_results: prints results of evaluations to terminal\\nprint words: Prints important words of each model in terminal\\nintersections: computes the important words similarity of all combinations of model and topics, USE ONLY WHEN YOU HAVE ALL MODELS MADE\\ndecision_tree_model: outputs a plotting of decision tree to a figure\\nwordcloud: outputs an important word wordcloud to a figure in the figures folder\\n```\\n\\n**Note**: if you are using intersections, decision_tree_model, or wordcloud options, make sure data is saved as \\'data/processed/general.csv\\' and has columns \\'Original Article Text\\' as the document text, \\'Verdict\\' as \\'TRUE\\' or \\'FALSE\\', \\'Category\\' as category, or change code within old_utils.py\\n\\nProject Organization\\n------------\\n\\n    ├── LICENSE\\n    ├── Makefile           <- Makefile with commands like `make data` or `make train`\\n    ├── README.md          <- The top-level README for developers using this project.\\n    ├── data\\n    │\\xa0\\xa0 ├── external       <- Data from third party sources.\\n    │\\xa0\\xa0 ├── interim        <- Intermediate data that has been transformed.\\n    │\\xa0\\xa0 ├── processed      <- The final, canonical data sets for modeling.\\n    │\\xa0\\xa0 └── raw            <- The original, immutable data dump.\\n    │\\n    ├── docs               <- A default Sphinx project; see sphinx-doc.org for details\\n    │\\n    ├── models             <- Trained and serialized models, model predictions, or model summaries\\n    │\\n    ├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\\n    │                         the creator\\'s initials, and a short `-` delimited description, e.g.\\n    │                         `1.0-jqp-initial-data-exploration`.\\n    │\\n    ├── references         <- Data dictionaries, manuals, and all other explanatory materials.\\n    │\\n    ├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\\n    │\\xa0\\xa0 └── figures        <- Generated graphics and figures to be used in reporting\\n    │\\n    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\\n    │                         generated with `pip freeze > requirements.txt`\\n    │\\n    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported\\n    ├── src                <- Source code for use in this project.\\n    │\\xa0\\xa0 ├── __init__.py    <- Makes src a Python module\\n    │   │\\n    │\\xa0\\xa0 ├── data           <- Scripts to download or generate data\\n    │\\xa0\\xa0 │\\xa0\\xa0 └── make_dataset.py\\n    │   │\\n    │\\xa0\\xa0 ├── features       <- Scripts to turn raw data into features for modeling\\n    │\\xa0\\xa0 │\\xa0\\xa0 └── build_features.py\\n    │   │\\n    │\\xa0\\xa0 ├── models         <- Scripts to train models and then use trained models to make\\n    │   │   │                 predictions\\n    │\\xa0\\xa0 │\\xa0\\xa0 ├── predict_model.py\\n    │\\xa0\\xa0 │\\xa0\\xa0 └── train_model.py\\n    │   │\\n    │\\xa0\\xa0 └── visualization  <- Scripts to create exploratory and results oriented visualizations\\n    │\\xa0\\xa0     └── visualize.py\\n    │\\n    └── tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io\\n\\n\\n--------\\n\\n<p><small>Project based on the <a target=\"_blank\" href=\"https://drivendata.github.io/cookiecutter-data-science/\">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>\\n',\n",
       "  'This is a summary of the Group B02 Capstone project repo:\\n\\n- To run the project, you can either run it from the DSMLP cluster or in a container.\\n- If running from the DSMLP cluster, use the provided command to launch the project.\\n- If running in a container, use the provided Docker image.\\n- Clone the project repository and navigate to it.\\n- If not merged to main, switch to the branch with run.py.\\n- Configure the config.yml file with appropriate parameters.\\n- Run run.py with the corresponding config yaml file.\\n- The presentation website can be accessed at a provided link.\\n- The config.yml file has options for output settings and explanations are provided for each option.\\n- The project organization is described in detail in the README file.'],\n",
       " 'https://github.com/aavelasq/dsc180-Q2sentiment': ['# The Effect of Cancel Culture on Sentiment Over Time\\n\\nThis project analyzes the change in public sentiment towards musicians\\nwho were cancelled on English-speaking Twitter due to socially unacceptable behavior.\\nSpecifically, we looked at how the \\ntype of issue, the background of the artist, and the strength of their\\nparasocial relationship with their fans affected sentiment towards them over time. \\nFor our analysis, we chose to focus on music artists from three different genres: \\nK-Pop, Hip-Hop, and Western Pop. \\nTo measure sentiment, we utilized the \\n[Google Perspective API](https://www.perspectiveapi.com/). \\n\\n## Running the Project\\n- Install dependencies using `pip install -r requirements.txt`\\n\\n- To run the project using test data: run `python run.py test`\\n\\n- To scrape Twitter data: run `python getTweets.py`\\n    - In order to run this script, must obtain valid Twitter API keys and save to a \\n    file named `twitterkeys.py`\\n    - To change artist and timeframe of tweets, change the query attribute in the `query_params` variable\\n    - Saves scraped Twitter data with columns `id, text, author_id, created_at` to `data\\\\raw`\\n\\n- To run the project using real data: run `python run.py data`\\n    - This calls `etl.py` and retrieves data stored in `data\\\\temp` folders. The directory where data is stored can be changed in `data-params.json`\\n\\n- The different sentiment API and library scripts are found in `run.py`.\\n    - To run Google Perpsective API script on Twitter data: run \\n    `python run.py data toxicity`\\n        - Before runnning, must obtain Google Developer API keys to run API script.\\n        - Outputs a dataframe containing toxicity, severe toxicity, insult, \\n        and profanity probability scores and saves to `data\\\\temp`\\n    - To run TextBlob library script on Twitter data: run \\n    `python run.py data polarity`\\n        - Outputs a dataframe with sentiment polarity values and saves to `data\\\\out`\\n    - To run Vader library script on Twitter data: run \\n    `python run.py data vader`\\n        - Outputs a dataframe with sentiment polarity values and saves to `data\\\\out`\\n\\n- To calculate some exploratory statistics and visualizations: run\\n    `python run.py data eda`\\n    - Runs on each individual artist, both canceled and control\\n        - Saves a dataframe containing the number of tweets collected per day \\n        to `data\\\\out`\\n        - Saves visualizations of user activity, toxicity, and polarity over time to `data\\\\out`\\n\\n- To smooth out short-term trends and compute rolling average of sentiment data:\\n    run `python run.py data preprocessing`\\n    - Saves rolling average dataframe to `data\\\\temp`\\n\\n- To generate results for first sub-question (type of issue): \\n    - run `python run.py data typefOfIssue`\\n        - Saves dataframes used later to generate visuals to `data\\\\temp\\\\rq1_type`\\n    - run `python run.py visuals_ti`\\n        - Saves visualizations to `data\\\\out\\\\rq1_type`\\n\\n- To generate results for second sub-question (background of artist): run `python run.py data background`\\n    - Saves dataframes used later to generate visuals to `data\\\\temp\\\\rq_bg2`\\n    - Saves visualizations to `data\\\\out\\\\rq_bg2`\\n\\n- To generate results for third sub-question (parasocial relationships): \\n    - run `python run.py data parasocial`\\n        - Saves dataframes used later to generate visuals to `data\\\\temp\\\\rq3_ps`\\n    - run `python run.py ps_visuals`\\n        - Saves visualizations to `data\\\\out\\\\rq3_ps`',\n",
       "  'This project analyzes the change in public sentiment towards musicians who were cancelled on English-speaking Twitter due to socially unacceptable behavior. The analysis focuses on the type of issue, the background of the artist, and the strength of their parasocial relationship with their fans. The project utilizes the Google Perspective API to measure sentiment and includes steps for scraping Twitter data and running different sentiment analysis scripts. It also generates exploratory statistics, visualizations, and results for specific sub-questions related to the type of issue, background of the artist, and parasocial relationships.'],\n",
       " 'https://github.com/18anguyen9/Single_Cell_Coupled_Autoencoders': ['# Single-Cell-Coupled-Autoencoders\\n\\n&emsp; &emsp; In this project, we implement a coupled autoencoder for working with single-cell data, which includes data sets on DNA, mRNA and protein data.\\n\\n## About\\n\\n&emsp; &emsp; Historically, analysis on single-cell data has been difficult to perform, due to data collection methods often resulting in the destruction of the cell in the process of collecting information. However, an ongoing endeavor of biological data science has recently been to analyze different modalities, or forms, of the genetic information within a cell. Doing so will allow modern medicine a greater understanding of cellular functions and how cells work in the context of illnesses. The information collected on the three modalities of DNA, RNA, and protein can be done safely and because it is known that they are same information in different forms, analysis done on them can be extrapolated understand the cell as a whole. Previous research has been conducted by Gala, R., Budzillo, A., Baftizadeh, F. et al. to capture gene expression in neuron cells with a neural network called a coupled autoencoder. This autoencoder framework is able to reconstruct the inputs, allowing the prediction of one input to another, as well as align the multiple inputs in the same low dimensional representation. In our paper, we build upon this coupled autoencoder on a data set of cells taken from several sites of the human body, predicting from RNA information to protein. We find that the autoencoder is able to adequately cluster the cell types in its lower dimensional representation, as well as perform decently at the prediction task. We show that the autoencoder is a powerful tool for analyzing single-cell data analysis and may prove to be a valuable asset in single-cell data analysis.\\n\\n## How to run this project\\n\\n1. Clone this repository onto your local machine with `git clone https://github.com/18anguyen9/Single_Cell_Coupled_Autoencoders.git` and change into the directory.\\n\\n2. Launch the Docker image for the project with the following line: `launch.sh -i alandnin/method3:latest`\\n\\n3. The code is ran with command line arguments:\\n\\n    * `test`: (`python3 run.py test`) Due to the size of the full data set, this will perform a test run on a much smaller subset of our data sets to simulate the output of our project. \\n    \\n    *  `test-full`: (`python3 run.py test-full`) This a full run of training the coupled autoencoder with the entire data set. This will take much longer (expect 20-30 minutes) than `test`, but will contain meaningful outputs compared to the simulated `test`. However, you will first need to download the entire data set using the following command line argument `aws s3 sync s3://openproblems-bio/public/ $HOME/data/ --no-sign-request` and moving the `cite_gex_processed_training.h5ad` and `cite_adt_processed_training.h5ad` files into the `/data` file of this directory.\\n    \\n    *  `clear-cache`: The training computation is memory expensive. `python3 run.py clear-cache` can be run in the case that your machine runs out of memory. This will only happen when `test-full` is ran.\\n\\n## Related\\n\\nWe based our project off of this NeurIPS competition: https://openproblems.bio/neurips_docs/about/about/\\n\\nOur website: https://18anguyen9.github.io/DSC_180_website/\\n    \\n    \\n    \\n',\n",
       "  'This project implements a coupled autoencoder for analyzing single-cell data, specifically DNA, mRNA, and protein data. The goal is to gain a better understanding of cellular functions and their relationship to illnesses. The autoencoder framework used in this project can reconstruct inputs and align multiple inputs in a low-dimensional representation. The project successfully clusters cell types and performs well in the prediction task. The code can be run using command line arguments for different levels of testing or full training with the entire dataset. The project is based on a NeurIPS competition and more information can be found on their website.'],\n",
       " 'https://github.com/zwcolin/T5_SQuAD_Prompt_Tuning': [\"# On Evaluating the Robustness of Language Models with Tuning\\nAuthors: Colin Wang, Lechuan Wang, Yutong Luo\\n\\nWebsite: https://rachelluoyt.github.io/T5_SQuAD_Prompt_Tuning/\\n## Pipeline for DSC 180B (not normally used by us, but for DSC 180B which asserts a certain format)\\nBuild a container using `zwcolin/180_method5:latest` docker. Clone the repo, then at the root folder, run `python run.py test`. Warning: lots of time may be spent on downloading the data, pretrained model, tokenizer, and preparation. The testing itself may take ~30 minutes (not including downloading and building the dataset) to output evaluation metrics (we've modified the script for you so it just measures the first 10 examples, which may take around 30 seconds to intialize and process). If you do want to see some results, you may want to wait for quite a bit. Alternatively, some existing train/testing logging has been provided inside the the `prompt_tuning` folder. You can take a look at that instead of actually running the code.\\n\\n## Internal Pipeline\\n### Manipulating Model in `run.py`\\n#### Training & Testing\\n- Simply do `bash experiment.sh` and modify any experiment meta info as well as hyperparameter as necessary. The pipeline has been built to suit single/multi GPU configurations under a single server instance.\\n\\n## Deployment\\nA `Dockerfile` has been provided in the root folder to set up a docker environment. Note that this dockerfile has only been experimented at UCSD's DataHub. Use it with caution.\\n\\n## DSC 180B Specific Instructions\\nWe don't strictly follow the structure of the given suggestions, with a `test` folder and a `testdata` folder inside it. It's too rigid. Instead, all the training and test data will be store inside the `data` folder and `experiment.sh` contains all the necessary code to build the model or to test the model for running the model. We don't like the way that you need to run `test.py` with some arguments in the command line because it's obviously not suitable for a deep learning project where there are way many possible arguments (i.e. you will likely have to type `test.py -xx -xx -xx -xx`, repeating `-` for dozens of times).\\n\\n## Reference\\nThe script is based on the following paper:\\n@misc{lester2021power,\\n      title={The Power of Scale for Parameter-Efficient Prompt Tuning}, \\n      author={Brian Lester and Rami Al-Rfou and Noah Constant},\\n      year={2021},\\n      eprint={2104.08691},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n\",\n",
       "  'The authors discuss the evaluation of the robustness of language models with tuning. They provide instructions for building a container and manipulating the model for training and testing. They also mention specific instructions for DSC 180B and provide a reference to the paper they based their script on.'],\n",
       " 'https://github.com/zhw005/DSC180B-Project': ['# DSC180B: Explainable AI\\nThis is a repository that contains code for DSC180B section B06\\'s Q2 Project: Explainable AI.\\n\\n\"build-script\": \"zhw005/dsc180b-project\"\\n\\n## Authors\\n- [Jerry (Yung-Chieh) Chan](https://github.com/JerryYC)\\n- [Apoorv Pochiraju](https://github.com/apochira)\\n- [Zhendong Wang](https://github.com/zhw005)\\n- [Yujie Zhang](https://github.com/yujiezhang0914)\\n\\n## Introduction\\nIn our project, we will be focusing on using different techniques from causal inferences and explainable AI to interpret various machine learning models across various domains. In particular, we are interested in three domains - healthcare, banking, and the housing market. Within each domain, we are going to train several machine learning models first:XGBoost, LightGBM, TabNet, and SVM. And we have four goals in general: \\n1) Explaining black-box models both globally and locally with various XAI methods; \\n2) Assessing the fairness of each learning algorithm with regard to different sensitive attributes; \\n3) Explaining False Negative and False Positive predictions using Causal Inference;\\n4) Generating recourse for individuals - a set of minimal actions to change the prediction of those black-box models.\\n\\n## Running the project\\n\\n target | config | experiment |\\n| :---: | :---: | :---: |\\n| airbnb_features | \\'config/FeatureEng-params-airbnb.json\\' | Do feature engineering for airbnb dataset |\\n| loan_features | \\'config/FeatureEng-params-loan.json\\' | Do feature engineering for loan dataset |\\n| diabetes_features | \\'config/FeatureEng-params-diabetes.json\\' | Do feature engineering for diabetes dataset |\\n| fairness | \\'config/Fairness-example.json\\' | Do fairness evaluation |\\n| FN_FP | \\'config/FN_FP-example.json\\' | Do False Negative and False Positive explanation |\\n| model_explanations | \\'config/Model_Explanations_Example_loan.json\\'| Do model explanations - loan data example|\\n| recourse | \\'config/Recourse-example.json\\'| Generate recourse explanation - loan data example|\\n',\n",
       "  \"This repository contains code for the DSC180B section B06's Q2 Project on Explainable AI. The project focuses on using techniques from causal inferences and explainable AI to interpret machine learning models in healthcare, banking, and the housing market domains. The goals include explaining black-box models, assessing fairness, explaining false negative and false positive predictions, and generating recourse for individuals. The project can be run with different configurations for feature engineering, fairness evaluation, model explanations, and generating recourse explanations.\"],\n",
       " 'https://github.com/TanveerMittal/Feature_Type_Inference_Capstone': [\"# Feature Type Inference Capstone\\n\\n### Team Members: Tanveer Mittal & Andrew Shen\\n### Mentor: Arun Kumar\\n\\n## Resources:\\n- [Torch Hub Release of Pretrained Models](https://github.com/TanveerMittal/BERT-Feature-Type-Inference)\\n    - Allows anyone to load our models in a single line of code using the th PyTorch Hub API\\n- [Tech Report](https://tanveermittal.github.io/capstone/)\\n    - Provides detailed methodology and results of our experiments\\n- [ML Data Prep Zoo](https://github.com/pvn25/ML-Data-Prep-Zoo)\\n    - Provides benchmark data and pretrained models for Feature Type Inference\\n- [Project Sortinghat](https://adalabucsd.github.io/sortinghat.html)\\n\\n## Overview:\\n\\nOne of the first steps in automated data prepration in AutoML platforms is to identify the feature types of individual columns in input data. This information then allows the software to understand the data and then preprocess it to allow machine learning algorithms to run on it. Project Sortinghat frames this task of Feature Type Inference as a machine learning multiclass classification problem. As an extension of Project SortingHat, we worked on applying Bidirectional Encoding Representation Transformer(BERT) models to this task and did further investigations on the effects of adjusting the feature set for input with a random forest model. Our BERT CNN models currently outperform all existing tools currently benchmarked against SortingHat's ML Data Prep Zoo.\\n\\nThis repository includes code for architecture and feature experiments for the transformer models. The results of our 2 released models can be seen in the tables below:\\n\\n- BERT CNN with Descriptive Statistics:\\n    - 9 Class Test Accuracy: **0.934**\\n\\n| Data Type | numeric | categorical | datetime | sentence | url   | embedded-number | list  | not-generalizable | context-specific |\\n|-----------|---------|-------------|----------|----------|-------|-----------------|-------|-------------------|------------------|\\n| **Accuracy**  |   0.983 |       0.972 |        1 |    0.986 | 0.999 |           0.997 | 0.994 |             0.968 |            0.967 |\\n| **Precision** |   0.959 |       0.935 |        1 |    0.849 | 0.969 |           0.989 |  0.96 |             0.848 |             0.87 |\\n| **Recall**    |   0.996 |       0.943 |        1 |    0.859 | 0.969 |           0.949 | 0.842 |             0.856 |            0.762 |\\n\\n- BERT CNN without Descriptive Statistics:\\n    - 9 Class Test Accuracy: **0.929**\\n\\n| Data Type | numeric | categorical | datetime | sentence | url   | embedded-number | list  | not-generalizable | context-specific |\\n|-----------|---------|-------------|----------|----------|-------|-----------------|-------|-------------------|------------------|\\n| Accuracy  |   0.981 |       0.967 |    0.999 |    0.987 | 0.999 |           0.997 | 0.994 |             0.966 |            0.968 |\\n| Precision |   0.958 |       0.917 |    0.993 |    0.853 | 0.969 |            0.99 | 0.959 |             0.869 |            0.854 |\\n| Recall    |   0.992 |       0.941 |        1 |     0.88 | 0.969 |            0.96 | 0.825 |             0.805 |            0.789 |\\n\",\n",
       "  'The team members for the Feature Type Inference Capstone project are Tanveer Mittal and Andrew Shen, with Arun Kumar as their mentor. They have developed a set of resources including pretrained models, a tech report, benchmark data, and a project called Sortinghat. The goal of the project is to identify the feature types of columns in input data for automated data preparation in AutoML platforms. They have applied Bidirectional Encoding Representation Transformer (BERT) models to this task and experimented with adjusting the feature set using a random forest model. Their BERT CNN models outperform existing tools in terms of accuracy. The results of their released models show high accuracy, precision, and recall for various data types.'],\n",
       " 'https://github.com/amelia-kawasaki/dsc_capstone': ['# DSC Capstone: Group B08\\n## Exploring Noise in Data: Applications to ML Models\\n### Models Supported:\\nKernel machines, Random Forests, k-Nearest Neighbor Classification\\n### Building the Project:\\nPlease build the project using the Docker container located at the DockerHub repo in submission.json\\n### Running the Project:\\nTo run on all data:\\n> python3 run.py\\n\\nTo run on all data with a custom config file:\\n> python3 run.py all [json config file]\\n\\nTo run the code on test section of data:\\n> python3 run.py test\\n\\nTo clean all output files:\\n> python3 run.py clean\\n\\nWebsite for an introduction to our project:\\nhttps://amelia-kawasaki.github.io/dsc_capstone/\\n',\n",
       "  'This is a summary of the DSC Capstone project by Group B08. The project explores noise in data and its applications to machine learning models. The supported models include Kernel machines, Random Forests, and k-Nearest Neighbor Classification. The project can be built using the Docker container located at the DockerHub repo mentioned in submission.json. To run the project, you can use the provided commands in the summary. Additionally, there is a website available for an introduction to the project: https://amelia-kawasaki.github.io/dsc_capstone/'],\n",
       " 'https://github.com/edinhluo/DSC180-Capstone-Project': ['# COVID-19 Group Testing Strategies\\n\\n## Abstract\\n\\nThe COVID-19 pandemic that has persisted for more than two years has been combated by efficient testing strategies that reliably identifies positive individuals to slow the spread of the pandemic. Opposed to other pooling strategies within the domain, the methods described in this paper prioritize true negative samples over overall accuracy. In the Monte Carlo simulations, both nonadaptive and adaptive testing strategies with random pool sampling resulted in high accuracy approaching at least 95% with varying pooling sizes and population sizes to decrease the number of tests given. A split tensor rank 2 method attempts to identify all infected samples within 961 samples, converging the number of tests to 99 as the prevalence of infection converges to 1%.\\n',\n",
       "  'This paper discusses COVID-19 group testing strategies that prioritize true negative samples and aim for high accuracy. Monte Carlo simulations show that both nonadaptive and adaptive testing strategies with random pool sampling can achieve at least 95% accuracy. A split tensor rank 2 method is also proposed to identify infected samples within a large population, reducing the number of tests needed.'],\n",
       " 'https://github.com/pnair7/ml-fairness': ['# Patterns of Fairness in Machine Learning\\n\\n[Website Link](https://annemxu.github.io/ml-fairness/)\\n\\n[Report Link](https://raw.githubusercontent.com/pnair7/artifact-directory-template/main/report.pdf)\\n\\nAn empirical analysis of machine learning fairness using a variety of metrics, models, and datasets.\\n\\n## Instructions\\n`python run.py` for full output matrix\\n\\n`python run.py test` to run on just test data\\n\\n### Adding your own data\\nRaw datasets sit in the `rawDatasets/` folder, and can be processed by a script in the `preprocessing/` folder. The output should add a folder to `cleanedDatasets/` containing two elements: a JSON config file (see folder for example format) and a CSV file of the cleaned dataset. Input columns should all be numerical, and output columns should be 0/1 for binary classification. (If your data is already cleaned to these specifications, it can be placed directly in the `cleanedDatasets` folder, no need to upload the raw dataset or use a preprocessing script.)\\n\\n#### Example Config File\\n\\n```\\n{\\n    \"y_col\": \"refer\",                                    # which column are you predicting?\\n    \"X_cols\": [                                          # which columns are the predictor variables?\\n        \"dem_age_band_18-24_tm1\",\\n        \"dem_age_band_25-34_tm1\",\\n        \"dem_age_band_35-44_tm1\",\\n        ...\\n        \"trig_max-high_tm1\",\\n        \"trig_max-normal_tm1\",\\n        \"gagne_sum_tm1\"\\n    ],\\n    \"group_cols\": [                                      # which column is the protected attribute? is a list for consistency, but just one element\\n        \"race\"\\n    ],\\n    \"prediction_type\": \"binary\",                         # only binary is implemented (not used)\\n    \"dataset_name\": \"Obermeyer Health Dataset\",          # display name for dataset\\n    \"data_path\": \"rawDatasets/obermeyer_data.csv\",       # path to raw dataset (not used)\\n    \"data_script\": \"preprocessing/obermeyer.py\"          # script to preprocess raw data (not used)\\n}\\n```\\n\\n### Adding your own models or metrics\\nAdding your own models or metrics is a little more involved, but still quite simple. \\n\\nCurrently, models are contained in `models/sklearn_models.py`. First, you must write a model function that takes in the same parameters as the other models, which can be located in the same file, or in a different file in the `models` directory. Next, in the `utils/utils.py` file, you must import the model, and add the model to the `run_models` function, assigning it a string name for the model to map to the function. Finally, you must add the string name of the model to the list of models at the top of `run.py`.\\n\\nThe process is nearly identical for metrics. Create a metric function that takes in the same parameters as the other metric functions, add the metric to the `apply_metric` function in `utils/utils.py` with its own string name, and add that string name to the list of metrics in `run.py`.\\n',\n",
       "  'This is a summary of the information provided:\\n\\nThe article discusses an empirical analysis of machine learning fairness using various metrics, models, and datasets. It provides instructions on how to run the analysis and add your own data, models, or metrics. The article also includes an example configuration file for preprocessing raw datasets.'],\n",
       " 'https://github.com/mglevitt/Medical-Disparity-Causal-Analysis': ['## Quality of Life Causal Analysis Project\\n\\nIn this project, we aim to establish causality between various socioeconomic variables and life expectancy outcomes in  roughly 166 different countries, noting the strongest connections between economic and political factors with the length of life expectancy. \\n\\n### How to Use\\n\\nTo run our project just run the following two lines of code in a Unix shell, we utilized Unbuntu 20.04 LTS but it should work for others.\\n```\\ndocker pull mglevitt/world_happiness_project:run_project\\ndocker run mglevitt/world_happiness_project:run_project\\n```\\nThe pull should not take too long, but to run the code may take upwards of half an hour as a result of the many computations being made with PC. The end output should be a dictionary of the relations we found that will be printed to your terminal and all the graphs of the relations we found will pop up on your device as the code is run. Feel free to consult us if you run into any difficulties with getting our code to run properly.\\n\\n#### Run PC on Your Own Data\\n\\nThe pipeline we developed is flexible to work with any data in the correct format to find causal relations present in the data. You can follow the step by step instructions below to run PC on your own datasets. \\n\\n1. In a local terminal, navigate to where you would like to place the repository of our code\\n2. Clone this repository on to your local machine in the destination of your choice and navigate into the repository of our code with: \\n```\\ngit clone https://github.com/mglevitt/Medical-Disparity-Causal-Analysis.git\\ncd .\\\\Medical-Disparity-Causal-Analysis\\\\\\n```\\n3. Install pipenv on your local machine and use it to to install the dependencies needed to run our code with: \\n```\\npip install pipenv\\npipenv install\\n```\\n4. Save the data you would like to reformat to src/data as a csv or xlsx file. \\n5. When adding in your own datasets there are a few prior cleaning steps that may have to be done. For any of our provided datasets that you want to use, this step can be skipped. First, the data must be global time series data identified by country names or country name and year. All datasets must have a column with country names that has \"country\" in the column name. The country names also must follow the same naming conventions as other data that you are merging your data with. If you wish to have an included column with years, then \"year\" must be in the column\\'s name. When combining datasets with a year column, the pipeline will only include years that are in both datasets, so make sure the years are overlapping in their span. \\n6. Open the code of src/scripts/download_data.py with whatever method works from you local terminal or file exploror. Edit the 2 variables after the line \"# Add your own data here\" towards the bottom of the script to have the correct names for your file names in the list for variable name \"new_file_names\", including the extension .csv or .xlsx, and the name for your output file before .csv. Once these variable are edited, delete the \"#\" from before the last line of code then to save the changes to this file.\\n7. Run the code to have your new data file added to src/final_data with your inputted file name with: \\n```\\npipenv run .\\\\src\\\\scripts\\\\download_data.py\\n```\\n8. After all these steps, a table of relations with the most common relations at the top should be outputted in your terminal. \\n9. This step is not neccesary, but for further exploration into the causal relations present you can adjust the signifigance level of PC. Higher signifigance will lead to more relations being present and vice versa. The default signifigance is .2 and this value must be between 0 and 1. The signifigance can be adjusted by editing the value of alpha in the last line of code. \\n',\n",
       "  'This project aims to establish causality between socioeconomic variables and life expectancy outcomes in approximately 166 countries. The strongest connections between economic and political factors with life expectancy are noted. To run the project, two lines of code need to be executed in a Unix shell. The code may take some time to run due to the computations involved. The output will be a dictionary of the found relations, and graphs will be displayed. The pipeline developed for this project can also be used with other datasets by following the provided instructions.'],\n",
       " 'https://github.com/GogoHYX/DSC180_sleep_apnea': [\"# DSC180 Capstone Project\\n## [Project Website](https://gogohyx.github.io/DSC180_sleep_apnea/)\\n### Build instructions\\n\\nIn the home directory, running `python run.py --targets` builds the requisite files.\\n\\n#### Targets:\\n\\n1. `features`: reads raw data files, builds features, joins the files, and stores the columns of resulting dataframe in different files under `data/out`.\\n2. `model`: trains the voting models and saves them under `results/` directory. **RUNNING THIS TAKES A WHILE FOR THE FIRST TIME BECAUSE OF THE IMPORT STATEMENT. AFTER THAT IT'S INSTANTANEOUS.**\\n3. `predict`: loads the saved models, makes predictions, stores them under `results/` directory, and also stores the model recall values in a `.txt` file.\\n4. `test_rnn` test the rnn model on a subset of test files and save the output under `data/out`.\\n5. `test`: provides the functionality of `1` `2` `3` and `4` targets combined, on the test raw data.\\n6. `all`: provides the functionality of `1` `2` `3` and `4` targets combined, on the real raw data\\n\\n### Directory Map\\n\\n1. `config/`: contains the configuration `.json` files.\\n\\n    a. `create-test-data-params.json`: config file to build test-data.\\n    \\n    b. `data-params.json`: config file for cleaning data in the future.\\n    \\n    c. `eda-params.json`: config file for any EDA figures that will be generated.\\n    \\n    d. `features-params.json`: config file for building features from raw data.\\n\\n    e. `test-features-params.json`: config file for building features from test raw data.\\n    \\n    f. `model-params.json`: config file for model parameters.\\n\\n    g. `test-model-params.json`: config file for test model parameters.\\n    \\n    h. `test-rnn-params.json`: config file for testing rnn model.\\n\\n2. `data/`: contains the raw data files and data files after feature engineering. \\n\\n    a. `raw/`: contains the raw data files downloaded from source. _Does not include anything in the repo because raw data is confidential_. \\n    \\n    b. `out/`: contains the data after feature generation.\\n    \\n3. `notebooks/`: notebooks with some EDA and experimentation.\\n4. `references/`: this contains acknowledgement for any models or results that we use to build our project off of.\\n5. `results/`: running the build script populates this directory with the trained models (`.pkl`) and a `.txt` file which outlines the model performance. The `actual/` directory has the results of our actual model. The 'test/' directory will be created on running the `test` target and will contain results of our model on the test data.\\n6. `src/`: contains all the script `.py` files.\\n    \\n    a. `features/`: `build_features.py` performs feature engineering on clean data and populates `data/out` with files that can be used to train and test the models.\\n    \\n    b. `models/`: `train_model.py` trains the voting models and saves them in a `.pkl` file ; `test_model.py` makes predictions using the saved models and saves the performance metric in a `.txt` file. Both files are stored in `results/` directory.\\n    \\n    c. `helper_functions.py`: library of functions that are used to perform common tasks.\\n    \\n7. `test/`: contains `testdata/` which has the artificially generated test data.\\n8. `dockerfile`: creates a container with the necessary libraries and packages to run all the scripts.\\n9. `run.py`: running this script builds the requisite files.\\n10. `submission.json`: contains the dockerhub-id for building the container and build-script command to build the targets.\\n\",\n",
       "  'The DSC180 Capstone Project is a project focused on sleep apnea. The project website provides build instructions for running the project. The build instructions include different targets such as building features, training models, making predictions, and testing the models. The directory map of the project includes folders for configuration files, raw data files, notebooks, references, results, source code files, test data, and a Dockerfile for creating a container with the necessary libraries and packages. The run.py script is used to build the requisite files.'],\n",
       " 'https://github.com/chinkevin/DSC180_sleep_apnea': [\"# DSC180 Capstone Project\\n\\nSleep apnea is a sleep disorder where breathing starts and stops intermittenly. It can cause many issues while sleeping and even increases the risk of strokes and heart attacks. Traditionally, sleep research relies on human visual scoring. However with the advancement of machine learning, sleep research can be become a highly automated process. The purpose of this respoitory is to automatically classify sleep stages specifically for people with sleep apnea. Using signals from polysomnography data, such as EEG, EMG, EOG, and ECG, we can score sleep records using a Light Gradient Boosted Machine classifier into five stages: wake state, REM, N1, N2, and N3.\\n\\n### Building the project stages using `run.py`\\n\\n* To get the data, from the project root dir, run `python run.py data features`\\n  - This fetches the data, then creates features (defined in\\n    `src/features.py`) and saves them in the location specified in\\n    `features-params.json`.\\n* To include ECG features, run 'python run.py data features_ecg'\\n  - This builds the same features as before with additional ECG features.\\n* To build a model, from the project root dir, run `python run.py data\\n  features model`\\n  - This fetches the data, creates the features, then trains a lgbm classifier\\n    (with parameters specified in `config`).\\n* To predict and validate a model, from the project root dir, run `python run.py predict validate`\\n  - This runs the model on validation data, analyzes its performance, and creates visualizations.\\n\",\n",
       "  \"This project focuses on using machine learning to automatically classify sleep stages for people with sleep apnea. The project involves using signals from polysomnography data and a Light Gradient Boosted Machine classifier to score sleep records into five stages: wake state, REM, N1, N2, and N3. The project stages are built using the `run.py` script, which fetches the data, creates features, builds a model, and predicts and validates the model's performance.\"],\n",
       " 'https://github.com/a2lu/CAPSTONE_WILDFIRE': ['# CAPSTONE_WILDFIRE\\n\\n## Usage\\n```\\ngit clone https://github.com/a2lu/CAPSTONE_WILDFIRE.git\\ncd CAPSTONE_WILDFIRE\\npython run.py test\\n```',\n",
       "  'The given code is for a project called CAPSTONE_WILDFIRE. It can be used by cloning the repository and running the `run.py` file with the `test` argument.'],\n",
       " 'https://github.com/LauraDiao/Anomaly_Detectives': ['# Anomaly Detectives\\nAn in depth approach to detecting significant real-time shifts in network performance indicating network degradation. Building on the data generation process behind [DANE](https://github.com/dane-tool/dane) and Viasat\\'s [network stats](https://github.com/Viasat/network-stats), we build a classification system that determines if there are substantial changes to packet loss rate and degree of latency. Please visit [our webpage](https://lauradiao.github.io/Anomaly_Detectives) for a more comprehensive view of this project.\\n\\n<br>\\n\\n# Quick Links\\n- [Modified DANE](https://github.com/jenna-my/modified_dane)\\n- [network-stats](https://github.com/Viasat/network-stats)\\n\\n<br>\\n\\n## To generate data for this project:\\n\\n1. Generate data using our [modified fork of DANE](https://github.com/jenna-my/modified_dane)\\n    - ```make```, ```docker.io```, and ```docker-compose``` are required on your machine to run modified_dane properly.\\n    - a recursive flag is required to properly install modified_dane: <br>```git clone https://github.com/jenna-my/modified_dane --recursive```\\n\\n2. Clone this branch of the repository\\n   ```\\n   git clone https://github.com/LauraDiao/Anomaly_Detectives\\n   ```\\n\\n3. Place all raw DANE csv files within the directory ```data/raw``` of this repository. If the directory has not been created, run the command ```run.py``` once to generate all relevant directories.\\n\\n<br>\\n\\n## To use this repository: \\nEach of these targets implements a core feature of the repository within ```run.py```. All code can be executed with the run.py according to various targets specified below. <br>\\nExample call: ```python run.py data inference```\\n### Target List:\\n- ```data```: generates features from unseen and seen data\\n- ```eda```: Generates visualizations used in exploring which features to use for the model\\n- ```train```: prints results of model performance tested on training (\"seen\") data with four different models with varying architectures: decision tree, random forest, extra trees, and gradient boost\\n- ```inference```: (deprecated) prints results of model performance tested on testing (\"unseen\") data with the same exact models.\\n- ```clean```: Removes files generated by targets in commonly used output directories\\n- ```test```: Verifies target functionality by running the targets ```data```,```eda```, ```train```, and ```inference``` with a subset of the original model training data.\\n- ```all```: runs all targets except ```test```\\n\\n<br><br>\\n\\nOur modified version of DANE creates csv files with a naming scheme in the following format: \\n> *datevalue*_*latency*-*loss*-*deterministic*-*laterlatency*-*laterloss*-iperf.csv\\n\\ne.g. ```20220117T015822_200-100-true-200-10000-iperf.csv```\\n\\nthis format is crucial for the model to train on the proper labels.\\n\\n## Configuration Files\\n### eda.json\\n\\n- `lst`: [1, 2], # list of runs to compare side by side made by plottogether() inside of eda.py\\n- `filen1`: \"combined_subset_latency.csv\", - subset of the processed data to make eda\\n- `filen2`: \"combined_t_latency.csv\", - features generated from processed data\\n- `filen3`: \"combined_all_latency.csv\" - all processed - \\n\\n### model.json\\n\\n- `n_jobs`: -1 - number of cores the model training is done on\\n- `train_window`: 20 - number of seconds that the model will aggregate on for training window size\\n- `pca_components`: 4 - number of components for PCA, we determined 4 was optimal for our model\\n- `test_size`: 0.005 - model validation set size (train _test_ split)\\n- `threshold`: -0.15 - threshold for loss anomaly detection\\n- `emplosswindow`: 25 - rolling window aggregation of empirical loss, set at 25 seconds\\n- `pct_change_window`: 2 - how many seconds the anomaly detection system looks back for determining change.\\n- `verbose`: \"True\" - whether terminal output should be verbose or not. For debugging purposes.\\n \\n\\n',\n",
       "  'The text describes a project called \"Anomaly Detectives\" that focuses on detecting real-time shifts in network performance indicating network degradation. The project builds on the data generation process of DANE and Viasat\\'s network stats. It includes a classification system that determines substantial changes in packet loss rate and latency. The text provides links to the project webpage and related repositories. It also provides instructions for generating data and using the repository, along with information about configuration files used in the project.'],\n",
       " 'https://github.com/tatummaston/anomaly_network_detection': ['# anomoly_network_detection',\n",
       "  'Anomaly network detection refers to the process of identifying and flagging unusual or abnormal behavior within a network.'],\n",
       " 'https://github.com/arjunsawhney1/intel-capstone-project': ['# Intel Telemetry: Data Collection & Time-Series Prediction of App Usage\\n## Abstract\\nDespite advancements in hardware technology, PC users continue to face frustrating app launch times, especially on lower end Windows machines. The desktop experience differs vastly from the instantaneous app launches and optimized experience we have come to expect even from low end smartphones. We propose a solution to preemptively run Windows apps in the background based on the app usage patterns of the user. \\n\\nOur solution is two-step. First, we built telemetry collector modules in C/C++ to collect real-world app usage data from two of our personal Windows 10 devices. Next, we developed neural network models, trained on the collected data, to predict app usage times and corresponding launch sequences in python. We achieved impressive results on selected evaluation metrics across different user profiles. \\n\\n## Usage\\nDue to the nature of our project, we have two distinct predictive tasks.\\n\\nThe project pipeline for our HMM model may be run as follows:\\n```\\nlaunch-scipy-ml.sh -i arjunsawhney1/intel-telemetry:latest\\ngit clone git@github.com:arjunsawhney1/intel-capstone-project.git\\ncd intel-capstone-project/src/models/HMM\\npython run.py\\n```\\n\\nThe project pipeline for our LSTM model may be run as follows:\\n```\\nlaunch-scipy-ml.sh -i arjunsawhney1/intel-telemetry:latest\\ngit clone git@github.com:arjunsawhney1/intel-capstone-project.git\\ncd intel-capstone-project/src/models/LSTM\\npython run.py\\n```\\n\\nOutputs for both models can be located in the outputs folder.\\n\\n## Project Website\\nhttps://arjunsawhney1.github.io/intel-capstone-project/\\n\\n## Docker Image\\narjunsawhney1/intel-telemetry:latest\\n',\n",
       "  'This article discusses Intel Telemetry, a solution to improve app launch times on lower-end Windows machines. The solution involves collecting real-world app usage data and using neural network models to predict app usage times and launch sequences. The article provides instructions for running the project pipeline for both the HMM and LSTM models, as well as a link to the project website and the Docker image.'],\n",
       " 'https://github.com/andydo1998/dsc180-data-analysis': ['# DSC 180 Section B14 Project\\n\\n## Project Introduction\\nIn an effort to reduce app wait time, the time it takes for an application to launch, we collected data on application use and app wait time for a single user over several weeks. With this data we plan to build a series of models to predict which application a user will open with an emphasis on when and for how long. The focus is currently on our foreground app data which shows us which app is in the foreground of the user’s computer with timestamp. Using this data, we created a single chain Hidden Markov Model (HMM) to predict the next app the user opens based on their current one. With not enough amount of layers, we implemented a Long Short-Term Memory (LSTM) model to predict the amount of time a user will use an application.\\n\\n## Overview\\nWe collected foreground windows with our data collection library for 2 months on a Windows laptop. The Jupyter Notebook contains the code to read in and combine each database file, data cleaning/preprocessing, the HMM model, and the LSTM model. The run.py file is a streamline version of our notebook that reads in the files, creates a Hidden Markov Model, and outputs prediction on every unique application that was present in our data collection.\\n\\n## How to Use\\n1. Pull the repo to obtain all necessary files to run test\\n2. With a terminal, navigate onto overarching folder (dsc180-data-analysis)\\n3. Run the command: \\n```\\npython run.py test\\n```\\n4. When finished, the terminal should report an accuracy of the model and outputs all possible predictions onto outputs/outputs.txt\\n\\n## More Information\\nFor more information, please read our report, the pdf file on the repository, for a more in depth explanation of the process.\\n\\nA visual presentation can also be viewed here: https://www.youtube.com/watch?v=2h5k6alz3WU\\n\\n(note that to retrieve the most amount of information, please view the report, as the video only provides a summary of our process)\\n',\n",
       "  'The project aims to reduce app wait time by predicting which application a user will open and for how long. The data collected includes application use and app wait time for a single user over several weeks. The project uses a Hidden Markov Model (HMM) to predict the next app based on the current one, and a Long Short-Term Memory (LSTM) model to predict the amount of time a user will use an application. The code is available in a Jupyter Notebook and can be run using the provided run.py file. The accuracy of the model is reported in the terminal, and all predictions are outputted to outputs/outputs.txt. More information can be found in the report and a visual presentation is also available.'],\n",
       " 'https://github.com/cgorlla/intel-sur': [\"# INTELli*next*: A Fully Integrated LSTM and HMM-Based Solution for Next-App Prediction With Intel SUR SDK Data Collection\\n# Intel DCA x HDSI UCSD System Usage Reporting Research\\n\\nCyril Gorlla, Jared Thach, Hiroki Hoshida. [INTELli*next*: A Fully Integrated LSTM and HMM-Based Solution for Next-App Prediction With Intel SUR SDK Data Collection.](https://github.com/cgorlla/intel-capstone-submission/blob/main/report.pdf) *Halıcıoğlu Data Science Institute Capstone Showcase, March 11, 2022*\\n\\nAs the power of modern computing devices increases, so too do user expectations for them. Despite advancements in technology, computer users are often faced with the dreaded spinning icon waiting for an application to load. Building upon our previous work developing data collectors with the Intel System Usage Reporting (SUR) SDK, we introduce INTELli*next*, a comprehensive solution for next-app prediction for application preload to improve perceived system fluidity. We develop a Hidden Markov Model (HMM) for prediction of the k most likely next apps, achieving an accuracy of 70% when k = 3. We then implement a long short-term memory (LSTM) model to predict the total duration that applications will be used. After hyperparameter optimization leading to an optimal lookback value of 5 previous applications, we are able to predict the usage time of a given application with a mean absolute error of ~45 seconds. Our work constitutes a promising comprehensive application preload solution with data collection based on the Intel SUR SDK and prediction with machine learning.\\n\\n\\nThis repository contains the code for our research at UCSD on predicting PC user behavior in collaboration with Intel Corporation.\\n\\nYou can read about the development of the Input Libraries that collected the data used to predict in the paper below.\\n\\nCyril Gorlla, Jared Thach, Hiroki Hoshida. Development of Input Libraries With Intel XLSDK to Capture Data for App Start Prediction. 2022. ⟨[hal-03527679](https://hal.archives-ouvertes.fr/hal-03527679)⟩\\n\\n## Repository Overview\\n- `config\\\\`: contains configuration files for various scripts, such as data and output locations\\n- `notebooks\\\\`: contains EDA with visualizations and other helpful Jupyter Notebooks to better understand the data\\n- `src\\\\`: contains the main data loading, analysis, and model building scripts\\n- `main.py`: Python script to execute data parsing, data cleaning, training, and testing\\n\\n## `run.py`\\nThis Python file contains the necessary code to parse and clean data from the Input Libraries detailed in the above paper, as well as to build the models in the project. These include:\\n- First Order Hidden Markov Model for Next-App Prediction\\n- LSTM for Next-App Prediction\\n- LSTM for App Duration Prediction\\n\\n### Building `run.py`\\nTo run: `python run.py {data} {analysis} {model}`\\n\\nTo just build the model: `python run.py data model`\\n\\nTo test: `python run.py test` \\n\\nThis will load in test data in `test\\\\testdata` and build the HMM and LSTM prediction models off of it. The predictions of the test model will be stored in `data\\\\out\\\\test_{model}.csv`, which you may verify against the provided files named `prov_{model}.csv` to ensure the model is functioning as expected. Note that due to the inherently probabilistic nature of the models your outputs may not be the same as the provided files, but they provide a sanity check.\\n\\n## `src\\\\model\\\\model.py`\\n\\nThis file contains the:\\n\\n- First order Hidden Markov model classfor predicting future foreground applications. After splitting the data and fitting the training set to a `first_order_HMM` instance using `fit`, the model keeps track of the prior and posterior probabilities of the training set's foreground applications. When inputting an observation, `X`, to `predict`, the function returns a list of foregrounds, (of size `n_foregrounds`, with default value of 1) with the highest conditional probability given `X`'s inputted foreground application and the trained model's posterior probabilities. `accuracy` returns the accuracy of the `y_test` on `y_pred` by taking each true foreground application in `y_test` and checking whether or not it appears in its respective list of foregrounds in `y_pred`.\\n\\n- The next-app prediction LSTM model using a “look-back” value of one previous foreground application in order to predict one future foreground application, where a “look-back” is defined as the number of previous events a single input will use in order to generate the next output\\n\\n- The duration prediction LSTM using a look-back value of five. In other words, the model uses the previous five data points to predict the next. \\n\\nBoth LSTM models' architecture is similar, with the four layers in the same order. \\n\\n\\n\\n## Docker\\nA dockerfile is included and will create a Docker environment that allows for the successful execution of all code in this repository.\\n\",\n",
       "  'The research paper titled \"INTELli*next*: A Fully Integrated LSTM and HMM-Based Solution for Next-App Prediction With Intel SUR SDK Data Collection\" introduces a comprehensive solution for next-app prediction to improve system fluidity. The paper discusses the development of a Hidden Markov Model (HMM) for predicting the next apps with an accuracy of 70% when considering the top 3 likely apps. It also presents a long short-term memory (LSTM) model for predicting the duration of app usage with a mean absolute error of approximately 45 seconds. The research utilizes data collected with the Intel System Usage Reporting (SUR) SDK and applies machine learning techniques for prediction.\\n\\nThe repository contains code for predicting PC user behavior in collaboration with Intel Corporation. It includes configuration files, notebooks for exploratory data analysis, and scripts for data loading, analysis, and model building. The main script, `run.py`, allows for parsing and cleaning data from input libraries and building the HMM and LSTM models. The `model.py` file contains classes for the first-order Hidden Markov Model and LSTM models used in the project.\\n\\nA Docker environment is also provided to facilitate the execution of the code in the repository.'],\n",
       " 'https://github.com/wolftossH/DSC--180AB-escrow': ['# DSC--180AB-escrow\\n<!-- Improved compatibility of back to top link: See: https://github.com/othneildrew/Best-README-Template/pull/73 -->\\n<a name=\"readme-top\"></a>\\n<!--\\n*** Thanks for checking out the Best-README-Template. If you have a suggestion\\n*** that would make this better, please fork the repo and create a pull request\\n*** or simply open an issue with the tag \"enhancement\".\\n*** Don\\'t forget to give the project a star!\\n*** Thanks again! Now go create something AMAZING! :D\\n-->\\n\\n\\n\\n<!-- PROJECT SHIELDS -->\\n<!--\\n*** I\\'m using markdown \"reference style\" links for readability.\\n*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).\\n*** See the bottom of this document for the declaration of the reference variables\\n*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.\\n*** https://www.markdownguide.org/basic-syntax/#reference-style-links\\n-->\\n[![Contributors][contributors-shield]][contributors-url]\\n[![MIT License][license-shield]][license-url]\\n<!-- [![Forks][forks-shield]][forks-url]\\n[![Stargazers][stars-shield]][stars-url]\\n[![Issues][issues-shield]][issues-url] -->\\n\\n\\n\\n<!-- PROJECT LOGO -->\\n<br />\\n<div align=\"center\">\\n  <a href=\"https://github.com/othneildrew/Best-README-Template\">\\n    <img src=\"new_client/images/final_logo.png\" alt=\"Logo\" width=\"200\" height=\"200\">\\n  </a>\\n\\n  <h3 align=\"center\">Best-README-Template</h3>\\n\\n  <p align=\"center\">\\n    An awesome README template to jumpstart your projects!\\n    <br />\\n    <a href=\"https://github.com/othneildrew/Best-README-Template\"><strong>Explore the docs »</strong></a>\\n    <br />\\n    <br />\\n    <a href=\"https://escryptow.net/\">View Webiste</a>\\n\\n  </p>\\n</div>\\n\\n\\n\\n<!-- TABLE OF CONTENTS -->\\n<details>\\n  <summary>Table of Contents</summary>\\n  <ol>\\n    <li>\\n      <a href=\"#about-the-project\">About The Project</a>\\n      <ul>\\n        <li><a href=\"#built-with\">Built With</a></li>\\n      </ul>\\n    </li>\\n    <li>\\n      <a href=\"#getting-started\">Getting Started</a>\\n      <ul>\\n        <li><a href=\"#prerequisites\">Prerequisites</a></li>\\n        <li><a href=\"#installation\">Installation</a></li>\\n      </ul>\\n    </li>\\n    <li><a href=\"#usage\">Usage</a></li>\\n    <li><a href=\"#roadmap\">Roadmap</a></li>\\n    <li><a href=\"#contributors-and-contact\">Contributors and Contact</a></li>\\n    <li><a href=\"#acknowledgments\">Acknowledgments</a></li>\\n  </ol>\\n</details>\\n\\n\\n\\n<!-- ABOUT THE PROJECT -->\\n## About The Project\\n\\n[![Product Name Screen Shot][product-screenshot]](https://example.com)\\n\\n\\n\\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\\n\\n\\n\\n### Built With\\n<p>\\n\\n[![React][React.js]][React-url] \\\\\\n[![Node][Node.js]][Node-url] \\\\\\n[![Solidity][solidity]][solidity-url] \\\\\\n[![Vite][vite]][vite-url] \\\\\\n[![Tailwind][tailwind]][tailwind-url] \\\\\\n[![Hardhat][hardhat]][hardhat-url] \\\\\\n![HTML][html] \\\\\\n![CSS][css]\\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\\n\\n<!-- GETTING STARTED -->\\n## Getting Started\\n\\nThis is an example of how you can use the website\\n\\n### Prerequisites\\n\\nThis is an example of how to list things you need to use the software and how to install them.\\n\\n\\n<!-- USAGE EXAMPLES -->\\n## Usage\\n\\nEscrow shop for all users\\n<div align=\"center\">\\n  <a href=\"https://github.com/othneildrew/Best-README-Template\">\\n    <img src=\"images/cart.png\" alt=\"Logo\" width=\"500\" height=\"300\">\\n  </a>\\n</div>\\n\\n\\n\\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\\n\\n\\n\\n<!-- ROADMAP -->\\n## Roadmap\\n\\n- [x] Design logo\\n- [x] Added ipfs\\n\\n\\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\\n\\n\\n\\n<!-- LICENSE -->\\n## License\\n\\nDistributed under the ....... License. See `LICENSE.md` for more information.\\n\\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\\n\\n\\n\\n<!-- CONTACT -->\\n## Contributors and Contact\\n\\nHuy Trinh - [![LinkedIn][linkedin-shield]][linkedin-url-huy]\\n\\nAntoni Liria-Sala - [![LinkedIn][linkedin-shield]][linkedin-url-antoni]\\n\\nWilliam Li - [![LinkedIn][linkedin-shield]][linkedin-url-william] \\n\\nGuangyu Yang - [![LinkedIn][linkedin-shield]][linkedin-url-irvin] \\n\\n\\n\\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\\n\\n\\n\\n<!-- ACKNOWLEDGMENTS -->\\n## Acknowledgments\\n\\nUse this space to list resources you find helpful and would like to give credit to. I\\'ve included a few of my favorites to kick things off!\\n\\n* [Choose an Open Source License](https://choosealicense.com)\\n* [Img Shields](https://shields.io)\\n* [Font Awesome](https://fontawesome.com)\\n* [React Icons](https://react-icons.github.io/react-icons/search)\\n\\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\\n\\n\\n\\n<!-- MARKDOWN LINKS & IMAGES -->\\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\\n[contributors-shield]: https://img.shields.io/github/contributors/wolftossH/DSC--180AB-escrow.svg?style=for-the-badge\\n[contributors-url]: https://github.com/wolftossH/DSC--180AB-escrow/graphs/contributors\\n[forks-shield]: https://img.shields.io/github/forks/othneildrew/Best-README-Template.svg?style=for-the-badge\\n[forks-url]: https://github.com/othneildrew/Best-README-Template/network/members\\n[stars-shield]: https://img.shields.io/github/stars/othneildrew/Best-README-Template.svg?style=for-the-badge\\n[stars-url]: https://github.com/othneildrew/Best-README-Template/stargazers\\n[issues-shield]: https://img.shields.io/github/issues/othneildrew/Best-README-Template.svg?style=for-the-badge\\n[issues-url]: https://github.com/othneildrew/Best-README-Template/issues\\n[license-shield]: https://img.shields.io/github/license/othneildrew/Best-README-Template.svg?style=for-the-badge\\n[license-url]: https://github.com/othneildrew/Best-README-Template/blob/master/LICENSE.txt\\n[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555\\n\\n[linkedin-url]: https://www.linkedin.com/feed/\\n[linkedin-url-huy]: https://www.linkedin.com/in/huy-trinh-9868ba194\\n[linkedin-url-antoni]: https://www.linkedin.com/in/antoniliriasala/\\n[linkedin-url-william]: https://www.linkedin.com/in/tianyangwillli/\\n[linkedin-url-irvin]: https://www.linkedin.com/in/irvinyang/\\n\\n[product-screenshot]: images/website_main_pic.jpg\\n[Next.js]: https://img.shields.io/badge/next.js-000000?style=for-the-badge&logo=nextdotjs&logoColor=white\\n[Next-url]: https://nextjs.org/\\n[React.js]: https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB\\n[React-url]: https://reactjs.org/\\n[Vue.js]: https://img.shields.io/badge/Vue.js-35495E?style=for-the-badge&logo=vuedotjs&logoColor=4FC08D\\n[Vue-url]: https://vuejs.org/\\n[Angular.io]: https://img.shields.io/badge/Angular-DD0031?style=for-the-badge&logo=angular&logoColor=white\\n[Angular-url]: https://angular.io/\\n[Svelte.dev]: https://img.shields.io/badge/Svelte-4A4A55?style=for-the-badge&logo=svelte&logoColor=FF3E00\\n[Svelte-url]: https://svelte.dev/\\n[Laravel.com]: https://img.shields.io/badge/Laravel-FF2D20?style=for-the-badge&logo=laravel&logoColor=white\\n[Laravel-url]: https://laravel.com\\n[Bootstrap.com]: https://img.shields.io/badge/Bootstrap-563D7C?style=for-the-badge&logo=bootstrap&logoColor=white\\n[Bootstrap-url]: https://getbootstrap.com\\n[JQuery.com]: https://img.shields.io/badge/jQuery-0769AD?style=for-the-badge&logo=jquery&logoColor=white\\n[JQuery-url]: https://jquery.com \\n[html]: \\thttps://img.shields.io/badge/HTML5-E34F26?style=for-the-badge&logo=html5&logoColor=white\\n[css]: https://img.shields.io/badge/CSS3-1572B6?style=for-the-badge&logo=css3&logoColor=white\\n[solidity]: https://img.shields.io/badge/Solidity-e6e6e6?style=for-the-badge&logo=solidity&logoColor=black\\n[solidity-url]: https://soliditylang.org/\\n[vite]: https://img.shields.io/badge/Vite-B73BFE?style=for-the-badge&logo=vite&logoColor=FFD62E\\n[vite-url]: https://vitejs.dev/\\n\\n[tailwind]: https://img.shields.io/badge/Tailwind_CSS-38B2AC?style=for-the-badge&logo=tailwind-css&logoColor=white\\n[tailwind-url]: https://tailwindcss.com/\\n\\n[Node.js]: https://img.shields.io/badge/Node.js-339933?style=for-the-badge&logo=nodedotjs&logoColor=white\\n[node-url]: https://nodejs.org/en/\\n\\n[hardhat]: https://hardhat.org/_next/static/media/hardhat-logo.5c5f687b.svg\\n[hardhat-url]: https://hardhat.org/\\n',\n",
       "  'This is a README template for a project called \"DSC--180AB-escrow\". It provides information about the project, including its purpose, built technologies, how to get started, usage examples, roadmap, contributors and contact information, and acknowledgments. The template also includes various badges and links to relevant resources.'],\n",
       " 'https://github.com/matin-g/Q2-DSC180B-A02': ['# Most up to date web app code in this folder --> front-end-webApp\\n# Most up to date contract code is final_purchase.sol\\n\\n<br>\\n\\n# Q2 capstone project \\nA decentralized exchange via ethereum smart contracts intergrated within a website (Dapp) in order to create a decentralized peer-to-peer ecommerce platform. Please refer to our [report](https://github.com/matin-g/DSC180a-Q1-final-code/blob/main/report.pdf) (note: the future work section explains what we are doing now in this repository)\\n\\n# Please see Q1 code and report here: \\nhttps://github.com/matin-g/DSC180a-Q1-final-code\\n\\n\\n# Instruction to run on local machine\\nAfter pulling all the codes within front-end folder to your local machine ---\\n\\nMake sure you have node.js (npm) installed (https://nodejs.org/en/download/)\\ninstall express package: open up terminal, input \"npm install express --save\"\\ncd into the front-end folder, and input \"node server.js\", and you will see \"port is open on 8082\"\\nopen chrome - \"http://127.0.0.1:8082/\"\\n',\n",
       "  'The folder \"front-end-webApp\" contains the most up-to-date web app code. The file \"final_purchase.sol\" contains the most up-to-date contract code. The Q2 capstone project aims to create a decentralized exchange using Ethereum smart contracts integrated within a website. More information can be found in the provided report and code links. Instructions for running the project on a local machine are also provided.'],\n",
       " 'https://github.com/crvander/capstoneproj2023': ['<h1 align=\"center\">\\n  Transformers for Sentiment Analysis on Financial Text\\n</h1>\\n\\n<h4 align=\"center\">\\n  Fine-tuned Models based on pretrained Hugging Face Transformers\\n</h4>\\n\\n## Usage\\n\\n### From Command Line\\n```bash\\n# this will install necessary packages\\npip install -r requirements.txt\\n\\n# this will run the whole pipeline consists of downloading full dataset, generate data, \\n# downloading our finetuned models from google drive, unzip model folders,\\n# predict sentiments on testing dataset\\n\\npython run.py generate_data download_models test\\n\\n# trainning process based on pretrained models from HuggingFace\\npython run.py generate_data train test\\n\\n# for predict based on tweets data from twitter API\\npython run.py download_models predict\\n\\n# for default testing run (submission for Quater1), \\n# test run will download, unzip our finetuned models from google drive,\\n# predict on dummy testdata(3 samples) and output predicted labels\\n\\npython run.py testing\\n```\\n\\n### File structure and configuration\\nAll configuration will be read from config folder, **data-params.yml** will be read when generating data, **model_config.yml** will be read when downloading, unzipping finetuned models, **train-params.yml** will be read as training hyperparameters as well as io path, **test-params.yml** consists io for test dataset and testrun dummy dataset.\\n\\nRaw dataset will be scraped from data sources and saved in data/raw, processed data will be saved in data/temp predictions will be saved in data/out, all datafiles will be saved in .csv file.\\n\\nFinetuned models, no matter directly output from training process, or download from google shared drive, all will be saved in results folder **train.py** will download pretrained models from Hugging Face hub, and read data from data/temp, finally save finetuned model to result folder.\\n\\n**test.py** will take two argument, test_target and test_lines. test_target can be specified as testing, which will generate prediction on testing data\\nor default as test to predict on testrun dummy data. All prediction will be saved in data/out.\\n\\n```\\nFinTech Project                      //\\n├─ config                            //\\n│  ├─ config.json                    //\\n│  ├─ data-params.yml                //\\n│  ├─ model-config.yml               //\\n│  ├─ test-params.yml                //\\n│  └─ train-params.yml               //\\n├─ data                                                            //\\n│  ├─ kaggle.json                                                  //\\n│  ├─ out                                                          //\\n│  │  ├─ model.joblib                                              //\\n│  │  └─ preds.csv                                                 //\\n│  ├─ raw                                                          //\\n│  ├─ temp                                                         //\\n│  │  ├─ test.csv                                                  //\\n│  │  └─ train.csv                                                 //\\n├─ myapp.log                                                       //\\n├─ README.md                                                       //\\n├─ run.py                                                          //\\n├─ spacy                                                           //\\n│  ├─ .DS_Store                                                    //\\n│  └─ create_model.py                                              //\\n├─ reports                           //\\n│  ├─ abstract.md                    //\\n│  ├─ demo.md                        //\\n│  ├─ discussion.md                  //\\n│  ├─ figures                        //\\n│  │  └─ logo_png.png                //\\n│  ├─ intro.md                       //\\n│  ├─ introduction.md                //\\n│  ├─ methods.md                     //\\n│  ├─ requirements_jb.txt            //\\n│  ├─ results.md                     //\\n│  ├─ _config.yml                    //\\n│  └─ _toc.yml                       //\\n├─ src                                                             //\\n│  ├─ data                                                         //\\n│  │  └─ make_dataset.py                                           //\\n│  ├─ test.py                                                      //\\n│  ├─ train.py                                                     //\\n│  ├─ utils                                                        //\\n│  │     └─ download_models.py                                     //\\n├─ submission.json                                                 //\\n├─ test                                                            //\\n│  └─ testdata                                                     //\\n│     └─ test.csv                                                  //\\n├─ twitter                                                         //\\n│  ├─ pull_tweets.py                                               //\\n│  └─ twitter_credentials.py                                       //\\n├─ _requirements.txt                                               //\\n└─ _run.py                                                         //\\n```\\n\\n',\n",
       "  'This is a project that focuses on using Transformers for sentiment analysis on financial text. The project uses fine-tuned models based on pretrained Hugging Face Transformers. The usage of the project can be done from the command line, with various options available such as generating data, downloading models, training, testing, and predicting sentiments. The file structure and configuration of the project are also explained.'],\n",
       " 'https://github.com/nathansng/fintech_library': ['# FinDL - Financial Deep Learning Library\\n\\n## DSC 180 Project\\n\\nThe goal of this project is to create a deep learning and machine learning library that allows users to easily create and deploy machine learning models for finance related tasks, such as future stock forecasting. This repo contains a data loader, data preprocessing functions, time series forecasting models, and loss visualization functions to provide an end-to-end machine learning and visualization pipeline. This project is made in parallel with the finance library\\'s NLP group. \\n\\n#### Project Website\\nLink to FinDL\\'s project website: [https://nathansng.github.io/fintech_library](https://nathansng.github.io/fintech_library/)\\n\\nThe project website code can be found in the finDL_website branch of this repository.\\n\\nLink to FinDL\\'s documentation: [https://fintech-library.readthedocs.io/en/latest/code/overview.html](https://fintech-library.readthedocs.io/en/latest/code/overview.html)\\n\\nThe project documentation website code can be found in the docs folder of the main branch. \\n\\n### Downloading Data\\n\\nOur data is downloaded from Kaggle.com. The dataset that we used for our experiment is the Stock Exchange Data created by Cody in 2018. The dataset is available at: https://www.kaggle.com/datasets/mattiuzc/stock-exchange-data. The dataset we used from this Kaggle dataset is \"indexProcessed.csv\". Save the csv file in the path `./data/raw/`. \\n\\n### Models \\n\\nThe following models have been implemented in the current implementation of the library. \\n\\n- CNN\\n- LSTM\\n- GRU\\n- TreNet\\n\\nThe Convolutional Neural Network (CNN) takes raw data points as input, extracts and learns the local feature information, and outputs the predicted local feature. The tuneable parameters of CNN are as follows:\\n\\n- number of layers\\n- convolutional layer size \\n- filter size\\n- dropout\\n- output size\\n\\nThe Long Short Term Memory takes the trends\\' slope and duration, which are extracted by using linear approximation approach on the raw data points, learns the trend dependencies, and outputs the predicted slope and duration of the next trend. The tunable parameters of LSTM are as follows:\\n\\n- input size\\n- hidden size\\n- number of layers\\n- output size\\n\\nThe Gated Recurrent Network works similar to LSTM and uses an update and reset gate. It is also used to extract information from the trends\\' slope and duration. The tunable parameters of GRU are as follows: \\n\\n- input size\\n- hidden size\\n- number of layers\\n- output size\\n\\nTreNet takes the predicted results from both CNN and LSTM, and combines them using a fully connected layer to generate the predicted output. The tunable parameters of TreNet are as follows: \\n\\n- Hyperparameters of LSTM \\n- Hyperparameters of CNN\\n- size of feature fusion layer\\n- output size \\n\\n\\n### Running Code\\n\\n*Note*: Running the code on a gpu will make the program run significantly faster than only cpu. The code also benefits with more RAM, as too low of memory will kill the process. \\n\\nTo run the code, run `python run.py [target]` to run the corresponding target. The available targets and their description are listed below: \\n\\n- `all`: Runs all targets using actual data, data path can be specified in `./config/data_params.json` file\\n\\n- `test`: Runs all targets using test data, test data can be found in `./test/testdata/testdata.csv`, test path can be specified in `./config/test_data_params.json` file\\n\\n- `data`: Runs the data and feature loading code, which opens a dataset and converts the dataset into trend durations and slopes\\n  - Configure parameters in `./config/data_params.json`\\n\\n- `features`: Runs the preprocessing code for the trends and stock data, normalizes all data, and splits data into training and testing sets for machine learning model use \\n  - Configure parameters in `./config/feature_params.json`\\n\\n- `model`: Runs the machine learning model training code \\n  - Configure model parameters in `./config/model_params.json`\\n  - Configure training parameters in `./config/training_params.json`\\n\\n- `visual`: Runs the loss plot visualization code which stores a line plot of the loss per epoch to a path \\n  - Configure parameters in `./config/visual_params.json`\\n\\nYou can also specify the model to run by specifying the model name in addition to any targets listed above by using `python run.py [targets] [model]`. The default model used if no model is specified is TreNet. The available models and their description are listed below: \\n\\n- `trenet`: Runs the TreNet model. Processes 1 dimensional time series data into a sequence of linear regressions encoded by the regressions slope and duration. Uses trend slope and duration data to predict future trends. \\n\\n- `lstm`: Runs an LSTM model. Trains on 1 dimensional time series data to predict future time series data. \\n\\n- `cnn`: Runs the CNN stack from the TreNet model. Trains on 1 dimensional time series data to predict future time series data. \\n\\n\\n',\n",
       "  'The FinDL project is a deep learning and machine learning library for finance-related tasks, such as stock forecasting. It includes data loading, preprocessing functions, time series forecasting models, and loss visualization functions. The project also has a website and documentation. The library supports models like CNN, LSTM, GRU, and TreNet. The code can be run with different targets to perform various tasks like data loading, feature preprocessing, model training, and loss visualization. The default model used is TreNet, but other models like LSTM and CNN can also be specified.'],\n",
       " 'https://github.com/vineettalla/Servicechain.io': ['This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\\n\\n## Getting Started\\nFirst, Make sure to install the dependencies in the package.json file by running:\\n\\n```bash\\nnpm install\\n```\\n\\n\\nNext(get it?), run the development server:\\n\\n```bash\\nnpm run dev\\n# or\\nyarn dev\\n# or\\npnpm dev\\n```\\n\\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\\n\\nYou can start editing the page by modifying `pages/index.js`. The page auto-updates as you edit the file.\\n\\nThe `pages/api` directory is mapped to `/api/*`. Files in this directory are treated as [API routes](https://nextjs.org/docs/api-routes/introduction) instead of React pages.\\n\\nYou now should be able to work with our current codebase!\\n\\n### Editing Smart Contract \\nIf you wish to make changes to the smart contract \\n```bash\\ncd ./ethereum/contracts\\n```\\nIn here you will see the solidity file that has the smart contract logic.<br>\\nOnce you have made changes you will need to compile and deploy the factory contract once again in order to get the ABI and Byte Code<br>\\nWe can do this by running:\\n```bash\\nnode compile.js\\nnode deploy.js\\n```\\nAfter the deploy command you will be give an address in the terminal window similar to the image below:\\n\\n![deployment](https://user-images.githubusercontent.com/80795080/225056996-8a1e5df9-6f87-4a60-aa08-c4a0bf53ee1d.png)\\n<br>Copy the contract address and paste it into the factory.js file located at \\n```bash\\ncd ./ethereum/factory.js\\n```\\nYou will replace the preexisting \"addressOfDeployedFactory\" with the one you pasted at the location shown below:\\n![image](https://user-images.githubusercontent.com/80795080/225057653-dcd111f9-0691-4b3a-9a37-d2b95f58ea92.png)\\n<br>\\nNow you have a fresh new smart contract that is connected to the frontend of ServiceChain.io! \\n\\n### Changing Backend\\nIf you are editing the smart contract you must also change the configuration of the firebase backend to connect to your own! In order to do so go to:\\n```bash\\ncd ./config/firebase.js\\n```\\nAll you have to do is login to [firebase](https://firebase.google.com/), create a project, enable authentication and firestore, then just change the firebase config variable with your own! \\n\\n## Deployed Version \\nYou can also checkout the actual deployed webpage [here](https://servicechain-io.vercel.app/).\\n\\n## Using the App\\n\\nCurrently our app only works if you have a metamask account since this is a way to interact with the ethereum blockchain with little to no work.\\n1. Download [Metamask](https://metamask.io/download/) \\n2. Create an Account\\n3. You should be prompted to a screen like this.\\n<br><img src =\"https://user-images.githubusercontent.com/80795080/225046985-9b79bf0b-86fd-4da8-9023-0908b620ea22.png\" width =\\'200\\' height =\\'300\\'><br> \\n**Make sure to switch your network to the Goerli Test Network**\\n<br>\\n5. Optionally if you wish to use actions in our app like sending ratings, tips, etc. You must load your account with test ether. In order to do so go to to a [faucet](https://goerlifaucet.com/) and paste your public address from metamask which is the highlighted value in the image below.\\n<br> <br><img src =\"https://user-images.githubusercontent.com/80795080/225049758-e570310c-452a-4a9b-98ce-92e9aa570ba1.png\"><br> \\n6. Head to our [website](https://servicechain-io.vercel.app/) and click signup to create your own account and use the app! \\n\\n## Test Accounts\\nIf you wish to just browse the site, we have an account for each user type. Note that functionalities like tipping **will not work** unless you have completed step 5.\\n<br>\\n<br>\\nUser Type: Employee<br>\\nUsername: je@gmail.com<br>\\nPassword: password<br>\\n<br>\\nUser Type: Manager<br>\\nUsername: jm@gmail.com  <br>\\nPassword: password<br>\\n<br>\\nUser Type: Customer<br>\\nUsername: sr@gmail.com  <br>\\nPassword: password<br>\\n<br>\\n## Extras\\nI hope you enjoy playing around and improving our dapp. Feel free to point out any flaws or inefficiencies that can be improved upon. After all the beauty of a DAPP is that its all open source!\\n\\n\\n\\n',\n",
       "  'This is a Next.js project bootstrapped with create-next-app. To get started, install the dependencies in the package.json file by running \"npm install\". Then, run the development server using \"npm run dev\". The project can be accessed at http://localhost:3000. The page can be edited by modifying pages/index.js. The pages/api directory is used for API routes instead of React pages. If you want to make changes to the smart contract, navigate to ./ethereum/contracts and modify the solidity file. After making changes, compile and deploy the factory contract using \"node compile.js\" and \"node deploy.js\". Replace the addressOfDeployedFactory in ./ethereum/factory.js with the new contract address. To change the backend configuration, go to ./config/firebase.js and update the firebase config variable with your own credentials. The deployed version of the webpage can be found at https://servicechain-io.vercel.app/. The app requires a Metamask account to interact with the Ethereum blockchain. Download Metamask, create an account, switch to the Goerli Test Network, and load your account with test ether from a faucet if needed. Test accounts are available for each user type on the website.'],\n",
       " 'https://github.com/scottasut/dsc180b-project': ['<h1 align=\"center\">\\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/18/UCSD_Seal.png\", width=150, height=150>\\n<img src=\"https://avatars.githubusercontent.com/u/71526309?s=280&v=4\", width=150, height=150>\\n<img src=\"https://logodownload.org/wp-content/uploads/2018/02/reddit-logo-16.png\", width=150, height=150>\\n\\nInteraction Graph-Based Community Recommendation on Reddit\\n</h1>\\n\\n#### Group Members\\n\\n- Scott Sutherland (sasuther@ucsd.edu)\\n- Ryan Don (rdon@ucsd.edu)\\n- Felicia Chan (f4chan@ucsd.edu)\\n- Pravar Bhandari (psbhanda@ucsd.edu)\\n\\n## Overview:\\n<br/>\\n<div align=\"center\">\\n<img src=\"https://user-images.githubusercontent.com/55766484/224842781-a9657aef-54d5-4305-8a09-fce7112693a1.png\"  width=\"600\" height=\"300\">\\n</div><br/>\\n\\nThis capstone project focuses on graph-based recommender systems for the social media platform Reddit. Users can choose to comment, subscribe, or otherwise interact in different online communities within Reddit called subreddits. Utilizing the graph database and analytics software TigerGraph, we create a recommendation model that recommends subreddits to users based on a variety of different interaction-related features.\\n\\nThe source code for the project is broken up as follows:\\n- `src/dataset`: files which handle data downloading and parsing it into a heterogeneous graph representation.\\n- `src/features`: files which handles the non-graph feature generation process for our graph data (users/subreddits).\\n- `src/models`: our baseline and final models which actually make recommendations for users as well as an evaluation handler class. `src/models/baselines.py` contains the non-graph baseline models while `src/models/models.py` contains the graph-based final models. `src/models/evaluator.py` handles evaluation of recommendations via precision@k calculation given a testing interaction set.\\n\\nThe website associated to this project can be found [here](https://scottasut.github.io/dsc180b-project/).\\n\\nDue to this project\\'s reliance on TigerGraph\\'s tools, our models cannot be run via a test target without access to a cluster. For a quick demo that our code which makes recommendations for a user, please refer to [this video](https://www.youtube.com/watch?v=fD63_7fDCcM). Additionally, you may refer to `notebooks/model_testing.ipynb` to see the evaluation of the models.\\n\\n## Project Structure:\\n```\\ndsc180b-project/\\n├─ docs/\\n│  ├─ css/\\n│  ├─ images/\\n│  ├─ _config.yml\\n│  ├─ index.html\\n├─ notebooks/\\n│  ├─ eda.ipynb\\n│  ├─ model_testing.ipynb\\n│  ├─ network_stats.ipynb\\n│  ├─ test.ipynb\\n├─ src/\\n│  ├─ dataset/\\n│  │  ├─ create_dataset.py\\n│  │  ├─ generate_dataset.py\\n│  │  ├─ make_dataset.py\\n│  ├─ features/\\n│  │  ├─ build_features.py\\n│  ├─ models/\\n│  │  ├─ baselines.py\\n│  │  ├─ evaluator.py\\n│  │  ├─ model.py\\n│  │  ├─ models.py\\n│  ├─ util/\\n│  │  ├─ logger_util.py\\n│  │  ├─ tigergraph_util.py\\n├─ .gitignore\\n├─ Dockerfile\\n├─ README.md\\n├─ poster.pdf\\n├─ report.pdf\\n├─ run.py\\n├─ submission.json\\n```\\n\\n## Prerequisites:\\n\\nBeyond the packages outlines in `requirements.txt`, there are a few tools needed for this project. Namely:\\n- [wget](https://www.gnu.org/software/wget/): In order to download the data, you will need wget installed on your system.\\n  - If you do not meet this requirement, [here](https://www.jcchouinard.com/wget/) is a useful guide on how you can get it.\\n- [TigerGraph](https://www.tigergraph.com/): To get this setup for this project there are quite a few steps. Let\\'s walk through them:\\n\\n### Working with TigerGraph:<a name=\"workingwithtigergraph\"></a>\\n\\n#### Setting up a TGCloud account\\nIn order to leverage graph-based machine learning techniques and TigerGraph\\'s suite of tools in particular, we need to set up a TGCloud instance to work from:\\n\\n1. Got to https://tgcloud.io/.\\n2. Select \\'sign up\\'.\\n3. Fill in the requested information on the sign up page. The organization name can be anything you like, but you will need it to log in.\\n4. Log in using the information you just provided.\\n5. Select \\'Clusters\\' on the left hand side menu bar, and then select \\'Create Cluster\\' in the upper right of the interface.\\n6. From here, you can choose how to configure your cluster. We were able to achieve all the goals of this project using a free cluster with the following specifications: Version: `3.8`, Instance type: `4 vCPU, 7.5GB Memory`, Storage: `50GB`, Number of nodes: `1 Node, Partition Factor 1, Replication Factor 1`.\\n\\n#### Defining a Graph and Graph Schema\\nNext, within the GraphStudio tool, we need to create a graph schema to hold our data:\\n\\n1. Nagivate to \\'Tools\\' > \\'GraphStudio\\' and select the cluster you created.\\n2. Follow [these](https://youtu.be/Z48cjYuJXX4) steps to create the required schema:\\n\\n\\n#### What is a graph schema?\\n\\nA graph schema is a kind of blueprint that defines the types of nodes and edges in the graph data structure, as well as the relationships and constraints between them. TigerGraph has a graphical user interface called GraphStudio that can be used to set up the initial schema and data mapping/loading. [Here](https://docs.tigergraph.com/gsql-ref/current/ddl-and-loading/defining-a-graph-schema#:~:text=A%20graph%20schema%20is%20a,(properties)%20associated%20with%20it) is a useful link that goes more in depth in terms of defining and loading a graph using TigerGraph. [This](https://www.youtube.com/watch?v=Q0JUkiU0lbs) is another short video demonstration showing how to create a schema in GraphStudio.\\n\\n#### Loading our Data:\\n*This step requires that the data has been downloaded and processed, please refer to the [Usage](#usage) section*\\n\\nWithin GraphStudio, you can follow [these](https://www.youtube.com/watch?v=7sg6Cw7BuWw) steps\\n\\nOnce you have TigerGraph up and running, you need to be able to authenticate yourself when using it. In this project, you can do so by creating `configs/tigergraph_config.json` in this directory which contains the following: \\n```\\n{\\n    \"host\": \"<The Host for your TigerGraph Cluster>\",\\n    \"graphname\": \"<The Name of Your Graph>\",\\n    \"username\": \"<Your TigerGraph Username>\",\\n    \"password\": \"<Your TigerGraph Password>\",\\n    \"gsqlSecret\": \"<Your Secret Key>\",\\n    \"certPath\": \"<The location of your my-cert.txt>\"\\n}\\n``` \\nWhile we worked in TigerGraph, we needed to have a file `my-cert.txt` located in our local machine\\'s root directory `~`. Please refer to [this](https://dev.tigergraph.com/forum/t/tigergraph-python-connection-issue/2776) thread for information on how to get that file.\\n\\n## Usage:<a name=\"usage\"></a>\\nIn order to run the different components of the project, you will interact with the `run.py` file. There are two main \\'targets\\' or arguments you can pass to the script when running it to work with the project: `data`, `features`. Due to the nature of the project and reliance on running TGCloud instance, testing targets are not available out of the box.\\n\\n- `data`: downloads the raw data and parses it into a heterogeneous graph format\\n- `features`: generates necessary features for final model from raw data. Depends on the `data` target.\\n\\nTargets can be called as follows `python run.py data features`.\\n\\n\\n#### Important Usage Notes:\\n- Your TigerGraph cluster must be on when calling any of the functions here which use `pyTigerGraph` otherwise a connection will not be able to be established. If you are experiencing connection errors, ensure that the cluster you are using is indeed turned on.\\n- The `data` and `feature` target processes can be configured in a couple of ways via a mandatory file `configs/setup.json` which contains the following where `year`, `month`, `test_year`, `test_month` specify the years and months which training and testing data should be pulled from Reddit respectively and `keywords` specifies the number of keywords we save from a user\\'s comment history (and by extension the size of their keyword embeddings). *Note that more recent data within Reddit is larger and will increase the computational needs for almost every aspect of the project. To see where the data is pulled from and see the file sizes, please refer [here](https://files.pushshift.io/reddit/comments/)*.\\n```\\n{\\n    \"year\": \"2010\",\\n    \"month\": \"12\",\\n    \"test_year\": \"2011\",\\n    \"test_month\": \"03\",\\n    \"keywords\": 25\\n}\\n```\\n\\n## Resources:\\n- [Course Site](https://dsc-capstone.github.io/)\\n- [Project Specifications](https://dsc-capstone.github.io/assignments/projects/q2/)\\n- [TigerGraph](https://www.tigergraph.com/)\\n- [TigerGraph Cloud](https://tgcloud.io/)\\n- [Reddit Comment Datasets](https://files.pushshift.io/reddit/comments/)\\n- [TigerGraph Community ML Algos](https://docs.tigergraph.com/graph-ml/current/community-algorithms/)\\n',\n",
       "  'This project focuses on graph-based recommender systems for the social media platform Reddit. The goal is to recommend subreddits to users based on their interactions. The project utilizes the graph database and analytics software TigerGraph. The source code is organized into different directories for data handling, feature generation, and models. The project also includes a website and a video demonstration of the recommendation model. To run the project, prerequisites include installing wget and setting up a TigerGraph cluster. The usage involves running the `run.py` file with the `data` and `features` targets.'],\n",
       " 'https://github.com/KazumaYamamoto2023/DSC180B-Q2-Project': ['# Graph-Based Deep Learning for Fraud Detection in Ethereum Transaction Networks\\n\\nThis project aims to compare graph-based to non-graph based algorithms for fraud detection in Ethereum transaction networks. We will predict whether a given Ethereum wallet in the transaction graph is fraudulent or non-fraudulent, given the wallet\\'s transaction history in the network.\\n\\nGraph exploration, analysis, and model building will be conducted using [TigerGraph](https://tgcloud.io/), an enterprise-scale graph data platform for advanced analytics and machine learning. \\n\\nModel performance was determined by taking the average classification accuracy on the testing set over 10 model runs. The resulting classifier performance for this prediction task are as follows:\\n\\n* Support Vector Machine (~60.5%)\\n* K-Nearest Neighbors (~74.6%)\\n* XGBoost (~81.6%)\\n* Graph Convolutional Network (~79.6%)\\n* Graph Attention Network (~78.5%)\\n* GraphSAGE (~81.9%)\\n* Node2Vec (~76.6%)\\n* Topology Adaptive Graph Convolutional Network (~82.2%)\\n\\n## Getting Started\\n1. Launch a docker container with the following command\\n```bash\\ndocker run -it srgelinas/dsc180b_eth_fraud:latest\\n```\\n2. Clone the repository and `cd` to the project directory:\\n```bash\\ngit clone https://github.com/KazumaYamamoto2023/DSC180B-Q2-Project.git\\n```\\n\\n3. Create a [TigerGraph](https://tgcloud.io/) account and launch an \"ML Bundle\" database cluster. Save the cluster\\'s domain name in `config/tigergraph.json`.\\n\\n4. Open [GraphStudio](https://tgcloud.io/app/tools/GraphStudio/) and create a new graph named \\'Ethereum\\'\\n\\n5. Open [AdminPortal](https://tgcloud.io/app/tools/Admin%20Portal/) and navigate to the \"Management\" tab and select \"Users.\" Generate a secret alias and secret value, and save the secret value in `config/tigergraph.json`.\\n\\n6. Run the following command to connect to the TigerGraph database instance, build the graph schema, load the dataset, and evaluate the models\\n    * This process is detailed in `notebooks/tg_data_loading.ipynb`\\n```bash\\npython run.py eth\\n```\\n\\n## Project Structure \\n```bash\\n├── config\\n│   └── tigergraph.json\\n├── data\\n│   └── visuals \\n│       ├── tagcn_feat_imp.png\\n│       └── tagcn_subgraph.png\\n│   ├── edges.csv\\n│   └── nodes_train_test_split.csv\\n├── gsql\\n│   ├── build_schema.gsql\\n│   ├── get_degrees.gsql\\n│   ├── load_data.gsql\\n│   └── summarize_ammounts.gsql\\n├── notebooks\\n│   ├── tagcn_model_validation.ipynb\\n│   └── tg_data_loading.ipynb\\n├── src\\n│   ├── baseline.py\\n│   ├── connect.py\\n│   ├── gnn_models.py\\n│   ├── node2vec.py\\n│   ├── ta_gcn.py\\n│   └── visualize.py\\n├── Dockerfile\\n├── README.md\\n└── run.py\\n```\\n\\n## File Descriptions\\n\\n`root`\\n* `run.py:` Python file with main method to run the project code\\n* `Dockerfile:` Dockerfile with dependencies to bulid docker image to deploy containerized environment\\n\\n`gsql`\\n* `build_schema.gsql:` GSQL query to create transaction network graph schema in TigerGraph\\n* `get_degrees.gsql:` GSQL query to add indegree/outdegree as node features\\n* `load_data.gsql:` GSQL query to load dataset into TigerGraph database\\n* `summarize_amounts.gsql:` GSQL query to add summary statistics of sent/received ETH as node features\\n\\n`notebooks`\\n* `tagcn_model_validation.ipynb:` Notebook comparing models, visualizing node feature importance and fraudulent wallet subgraphs\\n* `tg_data_loading.ipynb:` Notebook documenting graph schema design and data upload to TigerGraph\\n\\n`src`\\n* `baseline.py:` Python file containing baseline models: Support Vector Machine, K-Nearest Neighbors, XGBoost\\n* `connect.py:` Python file to connect to TigerGraph database, add node features, upload and retreive data\\n* `gnn_models.py:` Python file containing Graph Neural Network models (GCN, GAT, GraphSAGE)\\n* `node2vec.py:` Python file containing Node2Vec model\\n* `ta_gcn.py:` Python file containing TAGCN model\\n* `visualize.py:` Python file to generate visualizations for node feature importance and fraudulent wallet subgraphs\\n\\n\\n## Data Source\\nThis dataset contains transaction records of 445 phishing accounts and 445 non-phishing accounts of Ethereum. We obtain 445 phishing accounts labeled by [Etherscan](etherscan.io) and the same number of randomly selected unlabeled accounts as our objective nodes. The dataset can be used to conduct node classification of financial transaction networks. \\n\\nWe collect the transaction records based on an assumption that for a typical money transfer flow centered on a phishing node, the previous node of the phishing node may be a victim, and the next one to three nodes may be the bridge nodes with money laundering behaviors, as figure shows. Therefore, we collect subgraphs by [K-order sampling](https://ieeexplore.ieee.org/document/8964468) with K-in = 1, K-out = 3 for each of the 890 objective nodes and then splice them into a large-scale network with 86,623 nodes and 106,083 edges. \\n\\n![A schematic illustration of a directed K-order subgraph for phishing node classification.](https://s1.ax1x.com/2020/03/27/GCZGmd.md.jpg)\\n\\n[XBlock](http://xblock.pro/#/dataset/6) collects the current mainstream blockchain data and is one of the blockchain data platforms with the largest amount of data and the widest coverage in the academic community.\\n```\\n@article{ wu2019tedge,\\n  author = \"Jiajing Wu and Dan Lin and Qi Yuan and Zibin Zheng\",\\n  title = \"T-EDGE: Temporal WEighted MultiDiGraph Embedding for Ethereum Transaction Network Analysis\",\\n  journal = \"arXiv preprint arXiv:1905.08038\",\\n  year = \"2019\",\\n  URL = \"https://arxiv.org/abs/1905.08038\"\\n}\\n```\\n---\\n[Project Website](https://srgelinas.github.io/dsc180b_eth_fraud/)\\n\\n[Demo Video](https://youtu.be/WStx_VLHuNk)',\n",
       "  'This project focuses on fraud detection in Ethereum transaction networks using graph-based deep learning algorithms. The goal is to predict whether a given Ethereum wallet is fraudulent or non-fraudulent based on its transaction history. The project compares the performance of various algorithms, including Support Vector Machine, K-Nearest Neighbors, XGBoost, Graph Convolutional Network, Graph Attention Network, GraphSAGE, Node2Vec, and Topology Adaptive Graph Convolutional Network. The highest performing algorithm achieved an accuracy of 82.2%. The project provides instructions for getting started and includes a detailed project structure and file descriptions. The dataset used in the project contains transaction records of phishing and non-phishing accounts in Ethereum.'],\n",
       " 'https://github.com/Barry0121/graph-neural-net-benchmark': ['# DSC180 - Graph Neural Network\\n\\nAuthor: Barry Xue\\n\\nThis repository contains the materials and codes from the exploration of the topic of Graph Neural Network.\\n\\n# Option 1: Train WGAN network with GraphRNN, Discriminator, and Inverter with `main.py`\\n`main.py` will call the training function, and then it will call the visualization function to generate the graphs.\\n* All user defined parameters can be modified in `src/models/args.py` file.\\n\\n# Option 2: Test GCN and GCN-AE with `run.py`\\n`run.py` can run node classification task and edge prediction task with GCN Models (Multi-layer GCN for node classification, and GCN-AE for edge prediction).\\n* Use the `--task` flag to choose between the two tasks.\\n    - Example: python run.py --name \\'expt_test\\' --dataset \\'Cora\\' --task \\'Edge Prediction\\'\\n\\nThere are also two dataset to run either task upon: Cora, CiteSeer, and PubMed.\\n* Use the `--dataset` flag to choose between the two datasets.\\n    - Example: python run.py --name \\'expt_test\\' --dataset \\'CiteSeer\\' --task \\'Node Classification\\'\\n\\nOther options:\\n1. `--epochs`: change the number of training epochs.\\n2. `--hidden_size`: change the first layer hidden layer size.\\n3. `--encode_size`: change the encoding size/final hidden layer size.\\n4. `--train`/`--validation`/`--test`: specify number of training sample per classes, validation size, and testing data size.\\n\\n## Note on installing `pytorch_geometric`\\n* Normally, the command: `conda install -c pyg pyg` will work on MacOS, Windows, and any Linux distro with anaconda/miniconda installed.\\n* There are two known cases where this wouldn\\'t work:\\n1. If the pytorch isn\\'t installed, or the installed version in the environment is <1.12.x, y9ou might need to look into alternative installation method (ex: building from source, use pip, etc).\\n2. The newest MacOS Ventura (v13.0.1) has installation issue, due to the lack of support for M1 Macbooks (pytorch scatter doesn\\'t support \\'mps\\' device yet). One way to get around this issue is to follow this post: https://github.com/rusty1s/pytorch_scatter/issues/241.\\n\\n### Note on our codebase \\n* If you are interested in the \"Discovering Continuous Latent Space Representation of Graph\", everything is in the `src` directory. \\n* If you are interested in simple GNNs, everything is in `legacy_code/model` and `legacy_code/features`. \\n',\n",
       "  'This repository contains materials and codes related to the exploration of Graph Neural Networks. There are two options available for training and testing different models. Option 1 involves training a WGAN network with GraphRNN, Discriminator, and Inverter using the `main.py` file. Option 2 allows testing GCN and GCN-AE models using the `run.py` file, with the ability to choose between node classification and edge prediction tasks. The codebase is organized into different directories depending on the specific area of interest.'],\n",
       " 'https://github.com/hblyx/CommunityDetection': [\"# DSC180 Project 2 - Performance Evaluation of Community Detection on Neural Networks\\n#### Yaoxin Li, Justin Nguyen, Vivek Rayalu\\n\\n### Introduction\\nAs our previous work, we explored the traditional solutions of community detection, including Louvain, Girvan-Newman, and etc, we are also explring the solutions with neural networks. For this project, we explore the community detection solutions with machine learning, deep learning in particular. We specifically explore the performance of neural networks on task of community detection by implementing and training different neural networks, including Multiple Layers Perceptrons and Graph Neural Networks, to test whether neural networks can be a solution for community detection.\\n\\nThis repository contains all code for all findings and attempt related to this project.\\n\\n### Files\\n* `checkpoints` contains best models' stats in format of PyTorch's `.pt`.\\n* `config` contains parameters used for models.\\n* `notebooks` contains notebooks which illustrates data analysis, the result,  and training process.\\n* `outputs` contains the training plots including loss and score plots of models.\\n* `references` contains all reference.\\n* `src` contains all source code used for data analysis, models, and training. \\n    * `data` contains source code used for generate random and test data, data analysis, loading data for models, and some code for read specific format of data.\\n    * `features` contains code for feature engineering.\\n    * `models` contains the code for models, algorithms, and training.\\n* `test` contains the test data for `run.py`. Specifically, the test data are stored in `/test/testdata/`.\\n* `requiremetnts.txt` speficy the requirements of running this project.\\n* `run.py` can run a simple test for this project.\\n* `submission.json` contains information of Docker image for this project.\\n\\n### Requirements\\n`submission.json` contains the Docker image which have all packages needed. Meanwhile, the specific requirements are in `requirements.txt`. The Docker image contains all needed environment exclude `torch` and `torch_geometric`. Specifically, since `torch` and `torch_geometric` requires specific version according to the device, CUDA version, we choose to leave them. Therefore, to reproduce our results, `torch` and `torch_geometric` needed to be installed correctly according to CUDA version or CPU only version. The details of installing `torch` and `torch_geometric` can be found in https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html and https://pytorch.org/get-started/locally/ .\\n\\n### Run\\nTo run the project, we left the test with `run.py`. However, since this project is  more about to explore the solutions for community detection, the content is relatively mass. It is really difficult to re-run all analysis, model training, and algorithms implemented in a short period of time. Therefore, in `test` of `run.py`, it will run our naive traditional community detection algorithm which depends on the number of common neighbors on the test dataset. It will just make sure the graph/network enviroment has been set correctly with test data. In addition, since the entire environment of the running models of Graph Neural Network depends on the hardware environment. Specifically, since the training process requires to specify the GPU/CPU, it must corporate with the correct version of PyTorch, `torch`, and `torch_geometric` which need to specify whether use CPU only or specific CUDA version. Since the device run these code might have different hardware environment, we leave this part free to change. However, all training results and process are reproducible in the notebooks and code.\\n\\nInstead of presenting our results and code in `run.py`, we choose notebooks to show the results and process. Specifically `/notebooks` folder contains all attempts, models, and results we did, built, and ran. \\n\\n### Website\\n\\nThe project [website](https://hblyx.github.io/CommunityDetection/) and its source code is under the [gh-pages branch](https://github.com/hblyx/CommunityDetection/tree/gh-pages) of the same repository.\\n\",\n",
       "  'This project focuses on the performance evaluation of community detection on neural networks. The researchers explore traditional solutions like Louvain and Girvan-Newman, as well as solutions using machine learning and deep learning. They implement and train different neural networks, such as Multiple Layers Perceptrons and Graph Neural Networks, to test their effectiveness in community detection. The repository contains code, notebooks, and data related to the project. The requirements for running the project are specified in the `requirements.txt` file. The project can be run using the `run.py` script, but it mainly serves as a test for graph/network environment setup. The results and process are presented in the notebooks found in the `/notebooks` folder. Additionally, there is a website associated with the project that showcases its findings and includes the source code.'],\n",
       " 'https://github.com/stassinopoulosari/dsc180b-a15-q2project': ['# DSC 180B WI23-A15-2: Community Detection on Twitter\\n\\n## Abstract\\n\\nRecent work examined the vast unfolding of communities in large networks, in which it was shown that the Louvain Algorithm was the most effective at identifying and dividing communities into clusters. The growth of social media networks in the modern world nurtures the growth and identification of similarities between groups of people. These similarities between groups can be identified more formally as communities. While the number and types of communities grow, the identification and classification of these communities becomes more challenging. To define the scope of the project, we will be utilizing public data from the social media network Twitter. Specifically, we will look at the followers and followings of users throughout twitter. In this paper, we utilize the Louvain Algorithm to explore communities within the social media platform twitter. The results of the study allowed us to uncover and analyze distinct communities based on our seed account of a modern day rap musician named Dessa (@DessaDarling). Further research is needed to determine the potential applications of these algorithms in the field of community detection in social media since we only used one platform’s data.\\n\\n## Running our code\\n\\nWe recommend using [this docker repository](https://hub.docker.com/repository/docker/stassinopoulosari/dsc180b-wi23-a15-2/general). To start the code, use the following entry point command:\\n\\n`python run.py data test`',\n",
       "  'This paper discusses the use of the Louvain Algorithm to identify and analyze communities on Twitter. The study focuses on the followers and followings of a rap musician named Dessa (@DessaDarling). The results reveal distinct communities within the platform. Further research is needed to explore the potential applications of these algorithms in community detection on social media. To run the code, it is recommended to use a specific docker repository and execute a command.'],\n",
       " 'https://github.com/darehunt/DSC180B-Project2': ['# DSC 180B [WI 23] Project 2:<br> Community Detection of Music Genres\\n*** Classifying Spotify Artists through Community Detection and Clustering ***\\n\\n<!--This site was built using [GitHub Pages](https://pages.github.com/).-->\\n\\n## Data Background\\n<!-- TODO -->\\nOur primary source of data is a dataset of Spotify playlists collected by Andrew Maranhão, which is freely available on Kaggle [(link)](https://www.kaggle.com/datasets/andrewmvd/spotify-playlists). This dataset was collected using a subset of users who published their #nowplaying tweets via Spotify. This tabular dataset lists a row for each song that was tweeted out, containing the name of the song, the artist of the song, and the playlist that song was playing from. While the data also contained the user IDs of each person who tweeted the track they were listening to, our model does not take personal information as input.\\n\\n## Deployment\\n\\nClone the project\\n\\n```bash\\n  git clone https://github.com/darehunt/DSC180B-Project2\\n```\\n\\nAfter running Docker and logging into your account, pull and launch the docker image: (note that this requires more RAM than the usual to run)\\n```bash\\n  launch.sh -i anmokhta/DSC180B-proj2:latest -m 32\\n```\\n\\nFrom there, copy in the directory, change directories into the project and run commands from bash using run.py:\\n\\n```bash\\n  cp DSC180B-Project2 .\\n  cd DSC180B-Project2\\n  python run.py all\\n```\\n## Commands\\n\\nThe build script can be run directly from bash `python run.py`\\n\\n| Command | Description |\\n| --- | --- |\\n| `clean`  | Clears reminents of previous networks or community detection  |\\n| `data`  | Downloads, extracts, and prepares data network from Kaggle dataset  |\\n| `model`  | Runs community detection to attempt to group artists by genre |\\n| `all`  | equivalent of running `data model`  |\\n\\n## Authors\\n\\n- [@darehunt](https://www.github.com/darehunt)\\n- [@anmokhta](https://www.github.com/anmokhta)\\n- [@Btran206](https://www.github.com/Btran206)\\n- [@jul016](https://www.github.com/jul016)\\n\\n',\n",
       "  'This project focuses on community detection and clustering of Spotify artists to classify music genres. The primary source of data is a dataset of Spotify playlists collected from #nowplaying tweets via Spotify. The deployment process involves cloning the project, running Docker, and executing commands using run.py. The available commands include cleaning previous networks or community detection, downloading and preparing data network from the Kaggle dataset, running community detection to group artists by genre, and running all commands. The authors of this project are darehunt, anmokhta, Btran206, and jul016.'],\n",
       " 'https://github.com/gordonhu608/Revisit_CLIP': ['# Revist CLIP: Multi-perspective improvements on Vision-Language Model\\n\\n\\n> [Wenbo Hu](https://gordonhu608.github.io/), [Johnny Liu](https://github.com/jawkneeLoo)\\n\\n[![Website](https://img.shields.io/badge/Project-Website-87CEEB)](https://gordonhu608.github.io/Revisit_CLIP/)\\n[![Temporary paper](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://gordonhu608.github.io/files/revisitclip.pdf)\\n\\n\\n<hr />\\n\\n# :rocket: Highlights\\n\\n![main figure](docs/main_figure.png)\\n> **<p align=\"justify\"> Abstract:** *Large-scale contrastive vision-language pre-training\\n> has shown significant progress in visual representation\\n> learning. Unlike traditional visual systems trained by a\\n> fixed set of discrete labels, a new paradigm was introduced\\n> in CLIP to directly learn to align images with raw texts in\\n> an open-vocabulary setting. On downstream tasks, a carefully designed text prompt is employed to make zero-shot\\n> predictions. To avoid non-trivial prompt engineering, context optimization has been proposed to learn continuous vectors\\n> as task-specific prompts with few-shot training  examples. Instead of learning the input prompt token,\\n> an orthogonal way is learning the weight distributions of\\n> prompt, which is also very effective. An alternative\\n> path is fine-tuning with a light-weight feature adapter\\n> on the visual branch The most recent work introduces multimodal prompt learning, which uses a synergy function\\n> to simultaneously adapt language and vision branches for\\n> improved generalization. In our work, we revisit recent improvements in CLIP from different perspectives and propose\\n> an optimal way of combining the model’s architecture. We\\n> demonstrate that Data Augmentation (DA) and Test-Time\\n> Augmentation (TTA) are important for few-shot learning\\n> (FSL). We propose an end-to-end few-shot learning pipeline\\n> (DA + MaPLe + Adapters + TTA) that can be referenced for\\n> all downstream tasks. Compared with the state-of-the-art\\n> method ProDA  in FSL, our model achieves an absolute\\n> gain of 6.33% on the 1-shot learning setting and 4.43% on\\n> the 16-shot setting, averaged over 10 diverse image recognition datasets.* </p>\\n\\n## Main Contributions\\n\\n1) **Standard workflow for few-shot learning:** We combined current state-of-the-art models from different perspectives and achieved better performance.\\nWe gained an average of 5.28% absolute improvement for [1,2,4,8,16] shots learning over 10 diverse image\\nrecognition datasets than the best baseline model.\\n2) **Data and Test Time Augmentation:** We employed optimal Data Augmentation and Test Time Augmentation (TTA) and demonstrates TTA’s\\nimportance in few-shot learning and thus should be used as a convention in future few-shot learning tasks\\n\\n\\n## :ballot_box_with_check: Supported Methods\\n\\n| Method                    | Paper                                         |                             Configs                             |          Training Scripts          |\\n|---------------------------|:----------------------------------------------|:---------------------------------------------------------------:|:----------------------------------:|\\n| MaPLe                     | [arXiv](https://arxiv.org/abs/2210.03117)    | [link](configs/trainers/MaPLe/vit_b16_c2_ep5_batch4_2ctx.yaml)  |       [link](scripts/maple)        |\\n| CoOp                      | [IJCV 2022](https://arxiv.org/abs/2109.01134) |                  [link](configs/trainers/CoOp)                  |        [link](scripts/coop)        |\\n| Co-CoOp                   | [CVPR 2022](https://arxiv.org/abs/2203.05557) |                 [link](configs/trainers/CoCoOp)                 |       [link](scripts/cocoop)       |\\n| Ours                    | [arXiv](https://gordonhu608.github.io/files/revisitclip.pdf) |                  [link](configs/trainers/Ours)                  |        [link](scripts/ours)        |\\n<hr />\\n\\n## Results\\n### Our method in comparison with existing methods\\nResults reported below show accuracy for few shot training on [1,2,4,8,16] for across 10 recognition datasets averaged over 3 seeds.\\n\\n| Name                                                      | 1-shot | 2-shot |   4-shot     | 8-shot |  16-shot | \\n|-----------------------------------------------------------|:---------:|:----------:|:---------:|:------:|:------:|\\n| [CLIP](https://arxiv.org/abs/2103.00020)                  |   36.13   |   47.83    |   58.52  |   66.24   |   72.03    |   \\n| [CoOp](https://arxiv.org/abs/2109.01134)                  | 59.95 |   63.74    |    67.18  |  70.52  |  74.03  | \\n| [CLIP-Adapter](https://arxiv.org/abs/2110.04544)                |   62.13   |   65.64    |   69.07   |   72.55  |   76.  |  \\n| [ProDA](https://arxiv.org/abs/2205.03340)                |   65.19   |   68.59    |   71.4   |   74.21   |   76.78   | \\n| [Ours](https://gordonhu608.github.io/files/revisitclip.pdf)          |   **71.94** | **74.12**  | **78.48** |   **80.2**    |   **82.64**   | \\n\\n## Installation \\nFor installation and other package requirements, please follow the instructions detailed in [INSTALL.md](docs/INSTALL.md). \\n\\n## Data preparation\\nPlease follow the instructions at [DATASETS.md](docs/DATASETS.md) to prepare all datasets.\\n\\n## Training and Evaluation\\nPlease refer to the [RUN.md](docs/RUN.md) for detailed instructions on training, evaluating and reproducing the results using our pre-trained models.\\n\\n\\n<hr />\\n\\n## Contact\\nIf you have any questions, please create an issue on this repository or contact at w1hu@ucsd.edu\\n\\n\\n## Acknowledgements\\n\\nOur code is based on [Co-CoOp, CoOp](https://github.com/KaiyangZhou/CoOp) and [MaPLe](https://github.com/muzairkhattak/multimodal-prompt-learning) repository. We thank the authors for releasing their code. If you use our model and code, please consider citing these works as well.\\n\\n',\n",
       "  \"The paper discusses improvements on the CLIP (Contrastive Language-Image Pre-training) model from different perspectives. The authors propose an optimal way of combining the model's architecture and demonstrate the importance of Data Augmentation (DA) and Test-Time Augmentation (TTA) in few-shot learning. They also provide a standard workflow for few-shot learning and achieve better performance compared to baseline models. The results show that their method outperforms existing methods on various recognition datasets. The paper provides installation instructions, data preparation guidelines, and training/evaluation details for reproducing the results.\"],\n",
       " 'https://github.com/GSam789/DSC180B-Capstone-Network-Dissection': ['# Improving Network Accuracy through Network Manipulation\\n\\n\\n## To run our code:\\nSimply run ```python3 test.py``` to find the accuracies of our 3 trained VGG16 models on the first 100 test data points of CIFAR-100.\\n\\n## Project Background & Context\\nDeep learning has been growing rapidly over the past couple decades due to its ability in solving extremely complex problems. However, this machine learning\\nmethod is often considered as a \"black box\" since it is unclear how the neurons of a deep learning model work together to arrive at the final output. A recently found\\nmethod called Network Dissection has solved this interpretability issue by coming up with a visual that shows what each neuron looks for and why. Given that we have \\ninformation regarding neuron activities, we want to investigate if we can use this information to further improve our network\\'s performance.\\n\\nThus, in this project, we explored 3 methods:\\n1. Network Dissection Intervention\\n2. FocusedDropout\\n3. Input Gradient Regularization\\n\\nWe implemented these network dissection intervention on VGG16 that was pre-trained on Places365.\\nWe implemented FocusedDropout and Input Gradient Regularization separately on VGG16 on CIFAR-100 dataset. This was done due to our limited resources since training on \\nPlaces365 would take 50 days.\\n\\n## Rundown of Folders & Files\\nIn this Github repository, you will find the following folders and files:\\n\\n- Folders:\\n  - models: Contains all the state dictionaries of the trained models\\n  - utils: Contains all the model skeletons and FocusedDropout implementation\\n\\n- Files:\\n  - dataloader.py: Loads the CIFAR-100 dataset into a DataLoader\\n  - plotting.py: Plots the resulting accuracies and losses over epochs\\n  - test_data.pt: The first 100 test data points of CIFAR-100\\n  - test.py: Runs our 3 VGG16 on CIFAR-100 models (Baseline, VGG16 with FocusedDropout, VGG16 with Input Gradient Regularization) on test_data.pt \\n  - train_baseline_focuseddropout.py: When run, trains either baseline model (Plain VGG16) or VGG16 with FocusedDropout from scratch based on selected model in file and \\nsaves final model\\'s state dictionary into the models/ folder\\n  - train_input_grad.py: When run, trains VGG16 with Input Gradient Regularization and saves final model\\'s state dictionary into the models/ folder\\n\\n',\n",
       "  'This project aims to improve the accuracy of deep learning networks through network manipulation. Three methods were explored: Network Dissection Intervention, FocusedDropout, and Input Gradient Regularization. The implementation was done on VGG16 models trained on CIFAR-100 dataset. The code can be run using `python3 test.py` to obtain accuracies for the first 100 test data points of CIFAR-100. The repository contains folders for models and utils, as well as files for data loading, plotting, and training different models.'],\n",
       " 'https://github.com/agupta01/ml-theory-capstone': ['# Benchmarking Kernel Machines on Text Datasets\\nData Science Capstone Project advised by Mikhail Belkin.\\n\\n## Scaling Tests\\nIf you would like to run the expriments used in the report \"On Feature Scaling of Recursive Feature Machines\", follow the steps below:\\n\\n1. Have a GPU and a GPU-enabled Pytorch v1.13 environment (feel free to use the environment.yml file in the repo, although this contains a lot of other stuff).\\n2. Navigate to `src` within the project root (`cd src`)\\n3. Run the following:\\n```shell\\npython scaling.py --name=<PROVIDE A NAME HERE> \\\\\\n\\t--noise=<specify noise to add to dataset here> \\\\\\n\\t--N_runs=<set to 100 for 100 trials> \\\\\\n\\t--N=<number of examples in dataset> \\\\\\n\\t--target_fn=<cubic for the default function, randmat for the random matrix function> \\\\\\n\\t--baseline=<True to run baseline (Laplacian) kernel, False to train full RFM)\\n```\\n4. After the experiment is run (100 trials takes about 30 minutes when N=1000 on a RTX 2060), you can find result files in `<project root>/results/arrays/scaling_results`. Every run will generate two numpy arrays, named `train_MSEs_<name>.npy` and `test_MSEs_<name>.npy`, appended with \"\\\\_baseline\" if a baseline run was used. Each array has shape (N_runs, len(d_range)), where d_range are the feature sizes attempted ([5, 6, 7, ..., 99] + [100, 110, 120, ..., 2000] in the base experiment in the original paper).\\n\\n## How to use\\n```shell\\nusage: run.py [-h] [--verbose] {test,test-data,mnist,cifar10,fashionmnist}\\n\\npositional arguments:\\n  {test,test-data,mnist,cifar10,fashionmnist}\\n                        task to run\\n\\noptional arguments:\\n  -h, --help            show this help message and exit\\n  --verbose             Set logging to DEBUG\\n```\\n\\nExample:\\n\\n```shell\\n$ python run.py mnist --verbose\\nDec 04 2022 11:38PM [DEBUG] \\t Logging set to DEBUG\\nDec 04 2022 11:38PM [DEBUG] \\t Using 1259 training samples and 210 test samples\\nDec 04 2022 11:38PM [DEBUG] \\t x_train shape: (1259, 784)\\nDec 04 2022 11:38PM [DEBUG] \\t y_train shape: (1259,)\\nDec 04 2022 11:39PM [INFO] \\t TRAIN MSE: 0.000 | Accuracy: 1.000\\n TEST MSE: 0.019 | Accuracy: 0.995 | Precision: 0.995 | Recall: 0.992\\n```\\n\\nThe corresponding config files for each dataset can be found in `config/<dataset>.json`. Below is an explanation of the options:\\n\\n```json\\n{\\n    \"kernel_type\": \"laplace\", # can be either laplace or gaussian\\n    \"print_result\": true, # to print result to log\\n    \"data\": {\\n        \"dataset\": \"mnist\", # dataset to use, can be \"mnist\", \"cifar10\", or \"fashionmnist\"\\n        \"subset\": 0.1, # subset of dataset to use. See note below.\\n        \"pos_class\": 1, # class label for positive class (1)\\n        \"neg_class\": 8 # class label for negative class (-1)\\n    },\\n    \"model\": {\\n        \"gamma\": 0.00128, # kernel bandwidth, see src/utils.py for further details\\n        \"return_metrics\": true # return metrics rather than predictions after training kernel\\n    }\\n}\\n```\\n\\n## A note on not breaking your computer\\nThis code produces pairwise distance kernels for use in kernel machines for binary classification. If your dataset\\nhas $n$ examples, your kernel will be $n \\\\times n$! My 2018 Macbook Pro with 16GB RAM can only handle $n \\\\approx 1000$ \\nbefore it starts to freeze up on itself. Use the `subset` parameter in the config file, do the math, and you may \\navoid bricking your laptop for 5 minutes.\\n\\nIf you\\'re using DSMLP, subset = 0.01 should work for most datasets.\\n',\n",
       "  'This text provides instructions and information about benchmarking kernel machines on text datasets. It includes steps for running scaling tests, usage instructions for the code, and configuration options for different datasets. The text also includes a note on avoiding computer performance issues when working with large datasets.'],\n",
       " 'https://github.com/jimzers/DSC180B-A08': [\"# DSC180B A08: Imitating Behavior to Understand the Brain\\n\\nScott Yang, Daniel Son, Akshay Murali, Adam Lee, Eric Leonardis, Talmo Pereira\\n\\n#### Repos combined:\\n\\n- https://github.com/danielcson/dsc_capstone_q1\\n- https://github.com/scott-yj-yang/180A-codebase\\n- https://github.com/akdec00/DSC-180A\\n- https://github.com/jimzers/dsc180-ma5\\n\\n#### DSMLP Spawning Script\\n\\nTo train the model using UC San Diego's Data Science & Machine Learning Platform (DSMLP), you can setup a training\\nenvironment in the following commands\\n\\n```bash\\n# on dsmlp's bash\\nIDENTITY_PROXY_PORTS=1 launch-scipy-ml.sh -b -j -g 1 -m 32 -c 10 -i scottyang17/dm:latest\\n```\\n\\nExplanation: `IDENTITY_PROXY_PORTS=1` allow DSMLP's proxy port forwarding another empty port for the sake of record\\nkeeping interfaces such as `tensorboard`. `-b` means run the pod in background mode, `-j` means launch Jupyter notebook\\nserver within container (default), `-i` means custom docker image name.\\n\\n#### Training Procedures\\n\\nEntrypoint: Train expert SAC agent with `train_cheetah.py`\\n\\n```bash\\npython train_cheetah.py --automatic_entropy_tuning=True\\n```\\n\\nExtract activations\\n\\n```bash\\npython src/run/extract_activations.py --model_path data/models/sac_checkpoint_cheetah_123456_10000 --env_name HalfCheetah-v4 --num_episodes 1000 --save_path data/activations/cheetah_123456_10000\\n```\\n\\nCollect expert data\\n\\n```bash\\npython src/run/collect_expert.py --model_path data/models/sac_checkpoint_cheetah_123456_10000 --env_name HalfCheetah-v4 --num_episodes 15 --save_path data/rollouts/cheetah_123456_10000\\n```\\n\\nTrain behavioral cloning agent\\n\\n```bash\\npython src/run/train_bc.py --rollout_path data/rollouts/cheetah_123456_10000/rollouts.pkl --save_path data/bc_model/cheetah_123456_10000 --epochs 10 --batch_size 32 --lr 3e-4\\n```\\n\\nTODO: Run analysis\\n\\n```bash\\npython run_analysis --policy=path/to/policy --analysis_path=path/to/analysis \\n```\\n\\n\\n### Fire and forget bash scripts\\n\\n```bash\\nbash scripts/collect_expert.sh\\nbash scripts/train_bc.sh\\nbash scripts/collect_activations.sh\\nbash scripts/collect_activations_bc.sh\\n```\\n\",\n",
       "  \"This is a summary of the DSC180B A08 project on imitating behavior to understand the brain. The project involves multiple contributors and repositories. The DSMLP Spawning Script is provided to set up a training environment using UC San Diego's Data Science & Machine Learning Platform. The training procedures include training an expert SAC agent, extracting activations, collecting expert data, and training a behavioral cloning agent. There are also fire and forget bash scripts available for convenience.\"],\n",
       " 'https://github.com/jmryan19/DSC180': ['# DSC180\\nHelpers contatins python files of helper code written to make experimentation faster\\nMost exploring is done in 1-3_NAME.ipynb.\\n\\nUnfortunately, github cannot hold the weights of the final model, as the file is too large. \\n\\nIn order to run:\\nMust be run with a GPU!\\npython run.py [test or train]\\n\\nTrain will go through the training process of the four omodels utilized in the paper.\\nTest will run all statistics on models used on paper and regenerate figures.\\n\\nThe website corresponding to this project is located at https://jmryan19.github.io/DSC180.io/\\n',\n",
       "  'The DSC180 project includes helper code in Python files to speed up experimentation. Most exploration is done in the 1-3_NAME.ipynb notebook. However, the weights of the final model cannot be stored on GitHub due to their large size. To run the project, a GPU is required. The \"run.py\" script can be used with either \"test\" or \"train\" as arguments. Running \"train\" will go through the training process of the four models used in the paper, while running \"test\" will generate statistics and figures for all models used in the paper. The corresponding website for this project can be found at https://jmryan19.github.io/DSC180.io/.'],\n",
       " 'https://github.com/ddav118/DSC-180B': [\"# UC San Diego, DSC-180B, Winter 2023 <br>\\nPredicting Pulmonary Edema Using Deep Learning and Image Segmentation <br>\\nTeam Members: David Davila-Garcia, Marco Morocho, Yash Potdar\\n\\nNote: All data was deidentified but is not publicly available\\n\\n├── README.md          <- The top-level README for developers using this project.<br>\\n├── Final_Report.pdf<br>\\n├── Final_Poster.pdf<br>\\n├── models             <- Contains the outputs from trained models: Losses and Test Set Predictions<br>\\n│   ├── Losses         <- Training and Validation MAE Losses by Epoch.<br>\\n│   ├── Test Set Preds <- NT-proBNP predictions on test set using the best model (minimize MAE valid loss).<br>\\n├── 1 - Preprocessing.ipynb                               <- Cleaning the original x-rays + clinical data, excluding rows with missing data/no image available<br>\\n├── 2 - Transfer Learning Training & Evaluation.ipynb     <- (Not used in project) Provided by UCSD AIDA Lab, shows training of U-Net segmentation model.<br>\\n├── 3 - Predicting Unannotated.ipynb                      <- Used the U-Net segmentation model from the UCSD AIDA Lab to create binary masks (lungs, heart, clavicles, spinal column) for each radiograph in our dataset. Saved the segmentations to an hdf5 file. <br>\\n├── 4 - Creating Masks.ipynb                              <- Uses the binary masks created in '3 - Predicting Unannotated.ipynb' to produce the segmentation inputs for our model <br>\\n├── 5 - CNN Models.ipynb                                  <- Contains all code for training and testing models. <br>\\n├── model.py           <- Contains modified ResNet152 architectures, extends the Pytorch ResNet152 implementation <br>\\n├── train.py           <- Contains model training and testing functions; different inputs called for different architectures<br>\\n\\n\\nAcknowledgements: Thank you to our incredible mentor Albert Hsiao, MD, PhD for his guidance, and Amin Mahmoodi for providing the U-Net segmentation network.\\n\",\n",
       "  'This is a summary of a project conducted by a team at UC San Diego. The project focuses on predicting pulmonary edema using deep learning and image segmentation. The team members are David Davila-Garcia, Marco Morocho, and Yash Potdar. The project includes various files such as a final report, final poster, and trained models. The team used preprocessing techniques, transfer learning, and CNN models for training and testing. They also utilized a U-Net segmentation model provided by the UCSD AIDA Lab. The project was guided by mentor Albert Hsiao, MD, PhD, and the U-Net segmentation network was provided by Amin Mahmoodi.'],\n",
       " 'https://github.com/shivsakthivel/CNN-Multilabel-Classification': ['# Exploring the viability of Convolutional Neural Networks (CNNs) on a multi-label classification task to detect radiographic outliers\\n\\n## Task\\nAn implementation of a Convolutional Neural Network (CNN) multi-label classifier that takes in chest radiograph images as and outputs their corresponding predicted labels for detecting pulmonary edema and pleural effusion.\\n\\n## Retrieving the Data for this project\\nThe data available for this project came in the form of DICOM files stored on a Google Cloud instance (credentialed access only), with the entire database being of size 4 TB. The required credentialing can be obtained [here](https://physionet.org/content/mimic-cxr/2.0.0/). For the purposes of this project and accessing the DSMLP resources, the source radiograph images had to be manually downloaded in batches and transferred onto the teams drive on DSMLP. \\n\\nThe scripts associated with this repository, therefore, assume that the user has the required access to the data files, with the required filepaths relative to the directory in which they were developed. However, this GitHub repository contains exploratory notebooks, walking through the data access and model training process, and covers examples of the obtained results. Specifically, the notebook `Single-Var-Model-Edema.ipynb` is a comprehensive exploration of one of the single label binary classifiers developed for this project. The model build and evaluation techniques for the other models developed in this project largely follow a similar process.\\n\\n## Build and Run\\n- To run the single label Pulmonary Edema classifier run `python main.py edema`.\\n- To run the single label Pleural Effusion classifier run `python main.py effusion`.\\n- To run the multi-label classifier run `python main.py multilabel`.\\n- To run the multi-class classifier run `python main.py multiclass`.\\n\\n## Requirements\\nThe dependencies required for this project can be installed by running `pip install -r requirements.txt`.\\n\\n## Other Notes\\nIf running the code on this repository, the DSMLP instance should be launched with GPU to ensure that the files run efficiently (The project was developed using tensorflow).\\n',\n",
       "  'This project explores the use of Convolutional Neural Networks (CNNs) for a multi-label classification task to detect radiographic outliers. The goal is to develop a CNN classifier that can predict labels for pulmonary edema and pleural effusion in chest radiograph images. The data for this project is stored as DICOM files on a Google Cloud instance, with a total size of 4 TB. The required credentials can be obtained from the provided link. The scripts in this repository assume that the user has access to the data files and have the necessary filepaths relative to the development directory. The repository also includes exploratory notebooks and examples of model training and evaluation. To run the classifiers, specific commands are provided in the \"Build and Run\" section. The project requires certain dependencies, which can be installed using the provided requirements.txt file. It is recommended to run the code on a DSMLP instance with GPU for efficient execution, as TensorFlow was used for development.'],\n",
       " 'https://github.com/Angela-Wang111/Pneumothorax_classification': ['# DSC-180B-Project\\n**Project Topic:** classification of penumothorax dataset [CANDID-PTX](https://pubs.rsna.org/doi/10.1148/ryai.2021210136) using classification models, segmentation models, and cascade models.\\n\\nGROUP NAME: AC/DS :metal: (Angela + Cecilia -> AC, Data Science -> DS, AC/DC -> AC/DS) :fist_right::fist_left:\\n\\n**Brave Angela Not Afraid of Error :partying_face:**\\n\\n**Be Calm and Write Code Cecilia :innocent:**\\n\\n## Goal :pray:\\n- [x] Birthday (Angela) :birthday:\\n- [x] Classification Final Results\\n- [x] Segmentation Final Results\\n- [x] Cascade Final Results\\n- [x] Poster [03/09 DDL]\\n- [x] Website/Report/Code [03/14 DDL]\\n- [ ] Presentation/Birthday (Cecilia) :birthday: [03/15 DDL]\\n\\n## Website\\nIf you just want to have an idea of what this project is about without seeing all these codes (which I understand :stuck_out_tongue_winking_eye:), click here :point_right: https://angela-wang111.github.io/Pneumothorax_classification/\\n\\n## Documentation\\nThe useful files for checkpoint testing phase are: run.py, submission.json, config.json, src(folder), test(folder), and outpout(folder). \\n\\n:heavy_exclamation_mark:Execution instruction (in terminal):\\n1. `ssh <username>@dsmlp-login.ucsd.edu`\\n2. `launch.sh -i angela010101/pneumothorax:latest -c 8 -m 64 -g 1` at least 1 GPU and 64 GB memory is needed\\n3. `git clone https://github.com/Angela-Wang111/Pneumothorax_classification` (first time execution only)\\n4. `cd Pneumothorax_classification`\\n5. `python run.py <model type>` model type has to be one of \"classification\", \"segmentation\", or \"cascade\"\\n6. :crossed_fingers:\\n### submission.json\\nContains the URLs for this Github Repository and the DockerHub Repository used for building a docker image for this pipeline.\\n### config.json\\nContains the hyperparameters used for model training.\\n### run.py\\nThis is the main .py file for executing the whole pipeline from data preprocessing to training and test classification models, segmentation models, cascade models. To run it, just run `python run.py <model type>` in the terminal (and hope everything goes fine :crossed_fingers: ). Model type has to be in the following three formats: \"classification\", \"segmentation\", \"cascade\" (all in lowercase). Example of the full terminal command: `python run.py classification`.\\n### src\\n#### data_preprocessing.py\\nThis file contains functions to decode the RLE encoded pixels from the source \"test/testdata/Pneumothorax_reports_small.csv\" file and to save both positive and negative masks into test/testdata/masks, so they could be used for the segmentation model training/testing.\\n#### generate_train_val_test_csv.py\\nThis file contains functions to generate \"test/testdata/train.csv\", \"test/testdata/train_pos.csv\", \"test/testdata/train_neg.csv\", \"test/testdata/validation.csv\", and \"test/testdata/test.csv\" for model training/validation/test.\\n#### create_dataloader.py\\nThis file contains functions to create dataframe from .csv file like \"test/testdata/validation.csv\", create custermized Dataset, and create DataLoader. The function to create customized Dataset is modified to read .png formatted original images. The full version code is written to read DICOM format images stored in the team group folder.\\n#### build_model.py\\nThis file contains functions to build customized pytorch pretrained ResNet 34 model and pretrained EfficientNet-B3 model, and train/validate classification models & segmentation models & cascade models.\\n#### evaluate_test.py\\nThis file contains fucntions to plot, print, and save metrics based on the test set to evaluate all models.\\n#### save_model_imgs.py\\nThis file contains functions to save predicted masks from pre-trained segmentation models. Mainly used for preparing images to be input to the classification models during the cascade model training.\\n#### run_model.py\\nThis file contains functions to execute the entire pipeline to run classification, segmentation, and cascade models. It automatically run all models within the same structure.\\n- We currently disabled save_model() function to avoid saving large files on github. To enable it, please uncomment the lines `save_model(cla_model, file_name)` in the run_class() function and `save_model(seg_model, file_name)` in the run_seg() function.\\n\\n### test\\nAll the data here are just a small portion of CANDID-PTX (100/19237) for pipeline testing purpose only since the original data size is ~30GB.\\n*All empty folders currently store the outputs after test trials.*\\n#### testdata\\n- *Pneumothorax_reports_small.csv*: source test .csv file. Includes 100 penumothorax samples (15 positive, 85 negative) with **SOPInstanceUID** to identify each sample, and **EncodedPixels** to specify the penumothorax region (in RLE encoded format if positive, -1 if negative).\\n- *images*: folder contains the original X-Ray images (1024x1024). Named in the format \"\\\\<SOPInstanceUID>.png\". The original images are in DICOM format, but are changed to .png format for the pipeline testing purpose. \\n- *masks*: empty folder to store the decoded binary masks.\\n- *intermediate_data*: empty folder to store the intermediate images generated by the segmentation models. These images will be used as input for classification models in the cascade structure.\\n\\nAfter runing the pipeline, the following files would be created :point_down:\\n- *masks*: folder contains the decoded binary masks (1024x1024). Named in the format \"\\\\<SOPInstanceUID>.png\" if positive, \"negative_mask.png\" if negative.\\n- *train.csv*: training set with 80 samples (12 postive, 68 negative).\\n- *train_pos.csv*: all positive samples in the training set\\n- *train_neg.csv*: all negative samples in the training set\\n- *validation.csv*: validation set with 10 samples (2 positive, 8 negative).\\n- *test.csv*: test set with 10 samples (1 positive, 9 negative).\\n\\n#### saved_model\\nThis is the folder where saved models will be located if the function is enabled.\\n\\n### output\\nThis should be an empty folder before executing the pipeline. After executing the pipeline, the following metrics plots will be created :point_down:\\n- auc-roc plot inside *auc-roc* folder.\\n- train/val losses inside *both_loss* folder.\\n- confusion matrix inside *confusion_matrix* folder.\\n',\n",
       "  'The project topic is the classification of the penumothorax dataset using classification models, segmentation models, and cascade models. The group name is AC/DS, consisting of Angela and Cecilia. The goal includes completing various tasks such as final results for classification, segmentation, and cascade models, creating a poster and website/report/code, and giving a presentation. The documentation provides instructions for executing the project pipeline and describes the various files and folders involved.'],\n",
       " 'https://github.com/styyxofficial/DSC180B-Quarter-2-Project': ['# Processing Electrophysiology Data to Extract Neural Trajectories\\n\\nRaw electrophysiology data is very high dimensional and contains a lot of noisy, spiky, activity. Due to this, it must be heavily processed before the accurate neural trajectories can be extracted.\\nWe utilized Variational Latent Gaussian Process in our study to reduce its dimensions and smooth our data.\\n\\nUsing this dimensionality reduced smooth data, we created a classifier to predict mouse behavior.\\n\\n## Variational Latent Gaussian Process\\n\\nIn a variational latent Gaussian process (VLGP), the observed data, y, is modeled as a Gaussian process, with mean function, f(x), and covariance function, k(x, x\\'). The underlying structure in the data is captured by latent variables, z, which are treated as random variables. The prior distribution over the latent variables is modeled as a Gaussian distribution.\\n\\nThe goal of the VLGP is to infer the posterior distribution, q(z|x), over the latent variables given the observed data. This is done using variational inference by minimizing the objective function, also known as the evidence lower bound (ELBO), given by:\\n$ELBO = -D_{KL}(q(z|x) || p(z)) + E_{q(z|x)}[log(p(y|z,x))]$\\nwhere $D_{KL}$ is the Kullback-Leibler divergence, which measures the difference between two distributions, and E is the expected value. The first term in the ELBO encourages the approximate posterior, q(z|x), to be close to the prior, p(z), while the second term represents the negative log-likelihood of the data given the latent variables. [1]\\n\\nThe optimization problem can be solved using gradient-based optimization algorithms, such as gradient descent or conjugate gradient. The solution provides estimates of the latent variables, which can be used to reconstruct the hidden patterns in the data. For the purposes of our project, vLGP is used to extract neural trajectories, which are the underlying patterns in neural activity that reflect how the brain processes information.\\n# To Run\\n`python run.py <config_name>.json`\\n\\nConfig files are .json files stored in \"config/\". They contain the hyperparameters of the model, as well as the PID, EID, and probe of the mouse, which determines what data will be analyzed. You can go to [the IBL website](https://viz.internationalbrainlab.org/app) to get different data. If the data you want is not already in \"data/raw/ONE/\", then in run.py remove the `mode=\\'local\\'` flag when instantiating ONE.\\n\\nOutputs of run.py will be stored in \"output/<exp_name>/\"\\n',\n",
       "  'This text discusses the processing of electrophysiology data to extract neural trajectories. The data is high dimensional and noisy, so it needs to be processed before accurate trajectories can be obtained. The authors used a method called Variational Latent Gaussian Process (VLGP) to reduce the dimensions and smooth the data. They then created a classifier to predict mouse behavior using this processed data. The VLGP model infers the posterior distribution over latent variables given the observed data using variational inference. The optimization problem is solved using gradient-based optimization algorithms. The solution provides estimates of the latent variables, which can be used to reconstruct hidden patterns in the data. To run the code, a config file with hyperparameters and mouse information needs to be provided. The outputs will be stored in a designated folder.'],\n",
       " 'https://github.com/somet3000/1kgp-coverage-analysis': ['# 1KGP Coverage Analysis\\nExpression quantitative trait loci (eQTL) and fine-mapping analysis of a cohort from the 1000 Genomes Project using both lower coverage and higher coverage (30x) data.\\n\\nThe raw VCF data can be obtained from the 1000 Genomes Project: https://www.internationalgenome.org/data\\n\\nThe gene expression data can be obtained from the BioStudies website for the RNA-sequencing 1KGP paper: https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-GEUV-1/sdrf?full=true\\n\\nThe analysis is a little bit different when running on at-scale versus test data, leading to different files between them. To run this code on the test data: ```python run.py test```. The test code will produce the QQ-plots and fine-mapping plots from the test data in the repository directory. The QQ plot will be in ```output_qqplot_test.pdf``` and the fine-mapping plots will be in the ```Rplots.pdf``` file. To run this code on the real data: ```python run.py all```\\n\\nThis repository uses plink (https://www.cog-genomics.org/plink/1.9/), Matrix eQTL (http://www.bios.unc.edu/research/genomic_software/Matrix_eQTL/), susieR (https://github.com/stephenslab/susieR), and UCSC LiftOver (https://genome.ucsc.edu/cgi-bin/hgLiftOver). Check them out! \\n\\nThanks for stopping by this repository! :)\\n\\n',\n",
       "  \"This is a summary of the #1KGP Coverage Analysis. It involves the analysis of expression quantitative trait loci (eQTL) and fine-mapping using data from the 1000 Genomes Project. The raw VCF data can be obtained from the project's website, while gene expression data can be obtained from the BioStudies website. The analysis can be run on both test data and real data, with different files being generated for each. The code uses various tools such as plink, Matrix eQTL, susieR, and UCSC LiftOver.\"],\n",
       " 'https://github.com/jacquelinekclee/twas-dsc180-a17': [\"# Application of Transcriptome-Wide Association Studies for Identifying Genes Associated with Inflammatory Bowel Disease\\n\\nFind the capston project website, including the full report and summary of the project's background, analysis, and findings, [here](https://notsamzhou.github.io/twas/).\\n\\n## Running the analysis\\n\\nThis repository provides a pipeline to perform a TWAS analysis.\\n\\nThis analysis requires the DockerHub repository at `notsamzhou/twas:latest`\\n\\nTo run the analysis, run `python run.py all`\\n\\nIf running another analysis with the same gene expression and variant data but different GWAS summary statistics, we do not need to recompute weights for each gene. Just run  `python run.py assoc` with an updated data-params.json\\n\\nTo run the respository on test data, run `python run.py test`\\n\\n## Obtaining raw data\\n\\nThe primary vcfs used in the analysis can be downloaded from [here](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20110521/ALL.chr22.phase1_release_v3.20101123.snps_indels_svs.genotypes.vcf.gz) and [here](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20110521/ALL.chr22.phase1_release_v3.20101123.snps_indels_svs.genotypes.vcf.gz.tbi). This analysis used the Chromosome 22 vcfs from the 1000 Genomes Project.\\n\\nThe gene expression data can downloaded from [here](https://www.ebi.ac.uk/biostudies/files/E-GEUV-1/E-GEUV-1/analysis_results/GD462.GeneQuantRPKM.50FN.samplename.resk10.txt.gz).\\n\\nThe population data can be downloaded from [here](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20110521/phase1_integrated_calls.20101123.ALL.panel).\\n\\nVarious summary statistic files can be downloaded from [here](https://github.com/TiffanyAmariuta/TCSC/tree/main/sumstats) based on a disease of interest.\\n\\n[This file](https://drive.google.com/uc?export=download&id=1gd6FP4qlteo1dBoAH8zGkXzbZvs2PPt4), which provides IDs and locations for various genes, is also required for plotting purposes.\\n\\nAll of these files should be placed directly in data/raw\\n\",\n",
       "  'The provided text is a set of instructions and links related to the application of Transcriptome-Wide Association Studies (TWAS) for identifying genes associated with Inflammatory Bowel Disease. It includes information on running the analysis, obtaining raw data, and accessing additional files needed for plotting purposes.'],\n",
       " 'https://github.com/Med-Dash/Med-Dash.github.io': ['# Medical Dashboarding - Quarter 2 Project\\n',\n",
       "  'The summary of the project titled \"Medical Dashboarding - Quarter 2\" is not provided.'],\n",
       " 'https://github.com/BradPowell23/First-and-Second-Level-Brain-Analysis': ['Dataset Link:\\nhttps://openneuro.org/datasets/ds003338/versions/1.1.0\\n\\nLink to download the pre-processed data needed for analysis:\\nhttps://app.globus.org/file-manager?origin_id=dc43f461-0ca7-4203-848c-33a9fc00a464&origin_path=%2Fr8b8-k094%2F\\n\\n`notebooks`: code and analysis in the form of Jupyter notebooks stored\\n<br>\\n`references`: nilearn tutorials\\n',\n",
       "  'The dataset link provided is for a specific dataset on OpenNeuro. The pre-processed data needed for analysis can be downloaded from the given link. The dataset includes Jupyter notebooks with code and analysis, as well as references to nilearn tutorials.'],\n",
       " 'https://github.com/mzh4ng/DSC180B_Q2_Project': ['# Evaluating Fungal Feature Importance in Predicting Life Expectancy for Cancer Patients\\nThis is the repository for DSC180B Section B18-1\\'s Project consisting of Benjamin Sacks, Ethan Chan, and Mark Zheng.\\nThis project is an extension of a study on the classification of cancer types using fungal mycobiome counts which can\\nbe found here: https://www.cell.com/cell/fulltext/S0092-8674(22)01127-8.\\n\\nThis project consists of two main machine learning models based upon the data presented in the previously mentioned\\nstudy as well as additional metadata collected about each sample that was not used in prior models. The first is a \\nregression model to predict the \"days to death\" continuous metadata variable measuring when the patient died in days\\nafter their sample was taken. The second is a classification model which aims to distinguish between different cancer\\nstages(I-IV) as opposed to cancer types in the original study.\\n\\n\\nINSTRUCTIONS:\\n\\nTo run these models, run the run.py file with 1 argument, the name of the config file for the desired model. \\nEx. \"run.py default-cancer-stage.json\".\\nAdditionally, there is a notebook in the path notebooks/run.ipynb that can be used to run this program in \\nJupyter Notebook if desired.\\n\\nDifferent models can be selected and run using the config files. Config files are json files in the \"config\" directory. \\nThey can be edited to change the parameters of the experiment as well as the type of experiment run. Each experiment\\nonly has 1 config file that it uses to increase the customization of experiments without flooding the folder with \\ntoo many config files.\\n\\nIn each config file, there are 3 subcategories: dataset, preprocessing, and model.\\n<br /> Dataset specifies information about the raw feature tables including which column is the target variable.\\n<br /> Preprocessing specifies the parameters of the preprocessing including what transformations to apply to each column. \\n    Preprocessing can also be turned off if data is already preprocessed with \"do_preprocessing\".\\n<br /> Model specifies the parameters of the model as well as cross validation. These are model specific and will vary\\n    based upon which type of model is being used.\\n\\nAdditionally, these are some important keys in the config file:\\n<br /> experiment_name: Specifies the unique id of the experiment. This is important for separating plots in figures.\\n<br /> experiment_title: Title of the experiment that will be displayed on the graphs\\n<br /> experiment_type: internal parameter telling the pipeline which class of model to use (classification or regression)',\n",
       "  'This project is an extension of a study on the classification of cancer types using fungal mycobiome counts. It consists of two main machine learning models: a regression model to predict the \"days to death\" variable measuring when the patient died after their sample was taken, and a classification model to distinguish between different cancer stages. To run these models, the \"run.py\" file should be executed with the name of the config file for the desired model as an argument. The config files can be edited to customize the parameters and type of experiment. The important keys in the config file include experiment_name, experiment_title, and experiment_type.'],\n",
       " 'https://github.com/Amandoj/DSC180-Q2-Project': ['# MULTI-LABEL DISEASE PREDICTION BASED ON GUT MICROBIOME\\nAbstract: In this study, we will be exploring the gut microbiome of Latin American immigrants to determine what factors of their gut microbiome affect metabolic diseases. The goal of our project is to determine what metabolic diseases/disorders an individual has based on their gut microbiome and other supporting information on the individual. To achieve our goal, we will be exploring machine learning and data analysis techniques to summarize the key points of the data and understand the patterns and relationships in the data.\\n\\n\\n## Retrieving the data locally:\\n(1) Download the data files from the following Google Drive: https://drive.google.com/drive/folders/1cpUvpXbh3YEHHaW4jmeKhL8DYfE7tE5V?usp=sharing\\n\\n(2) Place files in `data/raw` directory\\n\\n## Activating Qiime2\\nAfter launching container, open terminal and type in the following command before running `run.py`:\\n\\n`conda activate qiime2-2022.11`\\n\\nTo use within jupyter notebook also type in the following commands: \\n\\n`pip install -–user ipykernel`\\n\\n`python -m ipykernel install -–user -–name=qiime2-2022.11`\\n\\nthen refresh jupyter hub\\n\\nand select the qiime2 kernel\\n\\n## Running the Project:\\n* To revert to a clean repository, from the project root dir, run `python run.py clean`\\n  * This deletes all built files\\n* To run the entire project on test data, from the project root dir, run `python run.py test`\\n  * This fetches the test data, creates features, cleans the data, performs permanova tests, creates pcoa plots, creates machine learning model and model performance graphs\\n  for given disease types\\n* To run the entire project on the real data, from the project root dir, run `python run.py all`\\n  * This fetches the original data, creates features, cleans the data, performs permanova tests, creates pcoa plots, performs UMAP, creates machine learning model and model performance graphs\\n  for given disease types\\n  \\n## Model Performance\\nTo view model performance graphs, permanova tests, and pcoa plots after running `run.py`, download `.qzv` files from `data/out` and upload to https://view.qiime2.org/\\n\\nCollaborator: Amando Jimenez, Emerson Chao, Renaldy Herlim\\n\\nFor more information visit: https://renaldyh27.github.io/Capstone-Website/\\n',\n",
       "  'This study aims to explore the gut microbiome of Latin American immigrants and identify the factors that contribute to metabolic diseases. The researchers will use machine learning and data analysis techniques to analyze the data and determine the relationship between the gut microbiome and metabolic diseases. The project includes steps for retrieving the data, activating Qiime2, and running the project on test or real data. Model performance graphs, permanova tests, and pcoa plots can be viewed by downloading `.qzv` files from `data/out` and uploading them to https://view.qiime2.org/. Collaborators for this project are Amando Jimenez, Emerson Chao, and Renaldy Herlim. More information can be found at https://renaldyh27.github.io/Capstone-Website/.'],\n",
       " 'https://github.com/ZixinMa27/DSC180-Aerosol-Flow-Modeling-and-Simulation-in-a-Classroom-with-Mobile-Sensors': [\"## Modeling and Simulation of Aerosol Flow in a Classroom Environment with Mobile Sensors\\n#### Team Members: Zixin Ma, Jiali Qian, Yidan Wang \\n#### Mentors: Professor Tauhidur Rahman, PhD Tanjid Hasan Tonmoy\\n\\n### Overview:\\nIn light of the significant impact of COVID-19, it is crucial for individuals to assess the safety of indoor environments effectively and accurately. Although there are existing apps that monitor factors such as air quality and temperature, they fail to consider the concentrations of respiratory aerosols or other contaminants. To address this issue, we aim to develop a mobile application that utilizes built-in sensor data  and machine learning models to simulate aerosol flow and forecast the safety of indoor environments. Our app will not only serve as a tool for assessing COVID-19 safety, but also for other illnesses and purposes. \\n\\n**This project is also related to an ongoing doctorate research project, thus some of the repos are kept private per professor's request.**\\n**Based on the condition above, this repo is created for the purpose of showing our current progress, so the commits are a little compact. Further proof can be provided to show that our effort on the project is consistent.**\\n### Resources:\\n1. Data Collection APP https://github.com/tanjidt/hdsi-capstone-project [private]\\n2. https://github.com/tanjidt/aerosol-models [private]\\n3. Thermal Image Analysis: https://github.com/kavetinaveen/Thermal_Image_Processing \\n\\n### Build instruction:\\nTo avoid path conflicts, run.py is not located in the root directory.\\nPlease run `python src/models/compartment_model/run.py test` instead.\\n\",\n",
       "  'The team aims to develop a mobile application that uses sensor data and machine learning models to simulate aerosol flow and predict the safety of indoor environments. This app will be useful for assessing COVID-19 safety as well as other illnesses. The project is related to an ongoing doctorate research project and some repositories are kept private. The build instructions suggest running a specific Python file instead of the root directory file to avoid path conflicts.'],\n",
       " 'https://github.com/Brian96086/STNP_RL': ['## Accelerating STNP with Reinforcement Learning \\n\\nOverview: This repository implements the improvement of brute-force parameter search with DeepQ Network(DQN). In the repository, there will \\n\\n\\n### Notes to DSC180A TA\\'s\\n\\n### Instructions - Conda Virtual Environment\\nIn this section, you\\'ll execute the code with the below steps:\\n1. Create a conda environment with python version 3.9 `conda create --name placeholder_name python=3.9`. Note the \"placeholder_name\" is the environment name that you desire\\n2. Activate the conda environment `conda activate placeholder_name`. \\n3. Within the environment, install the python packages by running `pip install -r requirements.txt`\\n4. By this stage, the conda environment should contain all of the required packages. To execute the code, run `python main.py` (or `python3 main.py`)\\n\\n## Repository Structure\\n- The repository currently contains config folder, models folders, and utils folder. \\n- The config folder will store all of the hardcoded constants and allows one to tune and perform the experiments/hyperparameters. In particular, make sure to change the snapshot parameter to the location you want output files to be stored. \\n- The models folder contain the original source code from the STNP model. It is seperated into two files - seir(the actual simulation) and dcrnn (the surrogate model)\\n- The utils folder contains the core components that assist the STNP model to select parameters, namely the reinforcement-related files. It is further categorized into agents, env, exploration_strategies, trainer, which are standard RL modules/abstractions.\\n- engine.py provides helper methods that run the training procedure and wraps complicated logic into each method \\n- main.py performs the execution of the code. Therefore, you\\'ll be compiling on main.py\\n\\nREADME update date: March 14th, 2023',\n",
       "  'This repository implements the improvement of brute-force parameter search with DeepQ Network (DQN). It contains config, models, and utils folders. The config folder stores constants for tuning experiments/hyperparameters. The models folder contains the original source code for the STNP model. The utils folder includes reinforcement-related files for parameter selection. The engine.py file provides helper methods for running the training procedure, and main.py executes the code.'],\n",
       " 'https://github.com/3XiangyiKong3/DSC180AB_code': ['# Optimization of DeepGLEAM on Flu Forecasting Time-Series Data\\nThe current COVID-19 pandemic and common flu highlight the importance of time-sensitive information in biomedical institutions, politics, and economics. The application of data science in creating real-time predictive models is crucial to help researchers and world leaders better understand disease spread and take preventative measures.\\n\\n## GLEAM Prediction before and after Interpolation\\n![GLEAM Before Interpolation](./references/beforeinterp.png)\\n![GLEAM After Interpolation](./references/afterinterp.png)\\n\\n## ARIMA and ETS\\n![ARIMA](./references/arima_validation_plot1.png)\\n![ETS](./references/ets_validation_plot1.png)\\n## Prediction\\nFour weeks ahead Flu prediction residual between groundtruth and prediction for 10 states\\n![uncertainty_quantification_flu_residual_washingtion](./references/10_states_4_weeks_prediction.png)\\n\\n## Result Comparison \\n![MAE result](./references/Combied_result.png)\\n\\n## Setup, Model training and Model Testing\\n \\n1. Requirements\\n```bash\\n>>> pip install -r requirements.txt\\n```\\n2. Train models and make prediction (Model already trainned in the submission)\\n```bash\\n>>> python3 run.py --config_filename=data/model/dcrnn_cov.yaml\\n```\\n3. For Test, run the following command\\n```bash\\n>>> ./test.sh\\n```\\n- Visualization \\n\\n  - After running the command for test, a new folder named plot_weeknumber_result will appear containing [0.025, 0.5, 0.975] residual predictions the .npz files \\n  - Select the one with lowest MAE score \\n  - Run the flu_forecast_result_plot notebook\\n\\n## Docker\\n\\n```bash\\n>>> docker build -f ./Dockerfile -t Dockerfile .\\n>>> docker run --rm -it Dockerfile /bin/bash\\n>>> launch.sh -i xiangyikong/dsc180a:latest #Use this command below to launch the image in DSMLP\\n```\\n',\n",
       "  'This document discusses the optimization of DeepGLEAM for flu forecasting using time-series data. It emphasizes the importance of real-time predictive models in understanding disease spread and taking preventative measures. The document includes visualizations of GLEAM prediction before and after interpolation, as well as ARIMA and ETS models. It also presents a comparison of results and provides instructions for setup, model training, testing, and visualization. Docker instructions are also provided.'],\n",
       " 'https://github.com/apatankar22/hier-neural-proc': [\"\\n# Capstone Project (DSC 180B): Active Learning with Neural Processes for Epidemiology Modeling\\n## [Report](https://drive.google.com/file/d/1Mk2uujYlSpMKpOzAgYlZWoz1AOed6XPl/view), [Poster](https://drive.google.com/file/d/1m3Gy5ldjGqiTkYX6XV3meAU44MSHP9dL/view), [Website](http://apatankar22.github.io/hier-neural-proc/) <br>\\nAuthors: Amogh Patankar <br>\\nMentors: Rose Yu, Yian Ma\\n\\n## [SIR Neural Process and Gaussian Process Data](https://drive.google.com/drive/folders/1osXBkuDuzSmB8__2r3lLoOLHIXqju3G2)\\nSIR_GP: Save <code>nargp_data</code> and <code>sfgp_data</code> on the same level as <code>src</code>. \\n\\nSIR_NP: Save <code>mfnp_nested, nested, and nonnested</code> on the same level as <code>src/sir_np/</code>. \\n\\n## Packages\\nInstalled packages through pip. To replicate, run <code> pip install -r requirements.txt.</code> <br>\\nAlternatively install along as you go, the Gaussian processes don't require all the same packages as the neural processes. \\n\\n## How to Run\\nFor Gaussian Processes, simply change to <code>src/sir_gp</code> and run the two jupyter notebooks.<br>\\nFor Neural Processes, simply change to <code>src/sir_np/MA</code>, then go into the desired process and run train.py. This should train and also test + evaluate the model.\\n\",\n",
       "  'This is a summary of the instructions for the Capstone Project (DSC 180B) on Active Learning with Neural Processes for Epidemiology Modeling. The project report, poster, and website are available. The authors are Amogh Patankar and the mentors are Rose Yu and Yian Ma. The project involves SIR Neural Process and Gaussian Process Data. There are instructions on how to save data for SIR_GP and SIR_NP. The required packages can be installed using pip or installed as needed. To run Gaussian Processes, go to src/sir_gp and run the jupyter notebooks. To run Neural Processes, go to src/sir_np/MA, choose the desired process, and run train.py to train, test, and evaluate the model.'],\n",
       " 'https://github.com/Grizlucks/DSC180B-CapstoneFinalProject': ['# DSC180B-CapstoneFinalProject\\n\\nEvaluating gender bias within images generated by DallE-2.\\n\\nRequirements:\\n\\nThis project makes use of OPENAI and requires a \\'config.py\\' file\\nwith the below line. \\n\\nOPENAI_API_KEY = \"YOUR_API_KEY_HERE\"\\n\\nIf running the test target with a valid OpenAI key, it will generate ten images\\n(256x256) on that account to be charged (or with the user\\'s remaining free\\ncredits if any).\\n\\n## Instructions and Project Guide\\n\\n### run.py\\n\\nCall `python run.py test` to run the project using test data. \\n\\nTo run the project on the data used for the actual report, call `python run.py` however this will take a bit of time.\\n\\nThe run.py is setup to work with seperated targets that can be called by using the format `python run.py target1 target2 ...`:\\n\\n- `power` Performs analysis on BLS like data to give information on the sample size necessary at 1% statistical power for our statistical testing.\\n- `images` Generates images based on the occupations specified in the main or test `params.json` files. Requires a `config.py` file to be setup as specified.\\n- `analysis` Performs the statstical tests for the report while generating informative visualizations. Requires labeled image data in the format found under `test/raw/labeled_results.csv`. \\n\\n## notebooks/\\n\\nIncluded for reference, these notebooks were used in the hand-labeling \\nprocess. \\n\\n## src/ \\n\\nContains all source code in generating images and selecting occupations\\nwithin a specified range (of a 50/50 gender split). Currently, only tester\\nimages can be generated. ',\n",
       "  'This project aims to evaluate gender bias in images generated by DallE-2. It requires an OpenAI API key and provides instructions on how to run the project using test data or actual report data. The project includes targets for performing analysis on BLS-like data, generating images based on specified occupations, and performing statistical tests. The \"notebooks\" folder contains notebooks used in the hand-labeling process, while the \"src\" folder contains the source code for generating images and selecting occupations.'],\n",
       " 'https://github.com/ptse8204/airlinedatabias': [\"# Welcome to the Airline Pricing Model Bias Repositary\\n\\n## Introduction\\nThis repositary showcase how we investigate on how airline price discriminate on certain protected groups, including but not limit to:\\n* Race\\n* Income\\n* Geo areas\\n\\nWe understand that airfare pricing is a business decision that was driven by revenues. However, by investigating such factors, it may also drives airline's bottom line as the result could be useful for more attractive pricing for passengers.\\n\\n## Methodology\\n### Dataset\\nWe uses the Airline Origin and Destination Survey from the USDOT. The main reason why we chose to use such a dataset, instead of the advertising fare of the flight is because the data point represent a fare that is actually purchase by a customers. \\n\\nThe detail descrption and the data of the dataset is available on: https://www.transtats.bts.gov/tables.asp?QO_VQ=EFI&QO_anzr=Nv4yv0r\\n\\n### How does finding bias work?\\nWe aim our investigation (mainly) in 2 directions:\\n* Investigate whether there is price discrepency in protected groups on existing dataset\\n* Feed the data on to our custom build models, and see whether the model would generate results that showcase strong bias. In especially models that are extremely accuracte, and has a hard time to correctly identity areas that has a strong influence with protected groups.\\n\\n## Expected Goals and Outcomes\\n### Goals\\n* Discover bias, if any\\n* Creating an accurate model, that is both accurate and unbias, using various accuracy measurments, and bias mitogation techniques.\\n* Discover any trend shift of airfare pre-pandemic and post-pandemic\\n\\n### Outcomes\\n* An unbias model for extimatting the fair price that customers pay\\n* An indicator allow consumers to know whether they are price dscriminated and whether they are paying the fair price\\n\\n## Credits\\nTba :)\\n\\n## Notes\\nWe will keep updating the repo alongside with our progress.\\nreadme file last update: Feb 12, 2023\\n\",\n",
       "  'This repository investigates how airlines discriminate against certain protected groups, such as race, income, and geographic areas, in their pricing models. The dataset used is the Airline Origin and Destination Survey from the USDOT. The investigation aims to discover any bias in pricing and develop an accurate and unbiased model using various accuracy measurements and bias mitigation techniques. The expected outcomes include an unbiased model for estimating fair prices and an indicator for consumers to determine if they are being price discriminated against. The repository will be regularly updated with progress.'],\n",
       " 'https://github.com/NicoloWX/CausalTreeInference': ['# DSC180 Capstone Project\\n- `run.R`: a demo of the tree-based method\\n\\nFor models in `src/models/`:\\n\\nTo replicate the table[2][3][4] in the report\\n- `ageTest.R`\\n- `yearTest.R`\\n- `genderTest.R`\\n\\nTo replicate the tree graphs in the report\\n- `ageTest.R`\\n- `yearTest.R`\\n- `genderTest.R`\\n\\nTo replicate the boxplots in the report\\n- `boxplotGenAge.R`\\n- `boxplotGenYear.R`\\n- `boxplotGenGender.R`\\n\\nTo replicate table[5][6][7] in the report\\n- `forest-TestAge.R`\\n- `forest-TestYear.R`\\n- `forest-TestGender.R`\\n',\n",
       "  'The `run.R` file is a demo of the tree-based method for the DSC180 Capstone Project. There are several files in the `src/models/` directory that can be used to replicate different parts of the report. \\n\\nTo replicate the table[2][3][4] in the report, you can use the `ageTest.R`, `yearTest.R`, and `genderTest.R` files. \\n\\nTo replicate the tree graphs in the report, you can also use the `ageTest.R`, `yearTest.R`, and `genderTest.R` files. \\n\\nTo replicate the boxplots in the report, you can use the `boxplotGenAge.R`, `boxplotGenYear.R`, and `boxplotGenGender.R` files. \\n\\nLastly, to replicate table[5][6][7] in the report, you can use the `forest-TestAge.R`, `forest-TestYear.R`, and `forest-TestGender.R` files.'],\n",
       " 'https://github.com/alecpanattoni/MissingnessFairnessAnalysis': ['When in the repo directory, in order to produce the test results, one can use target \"test\". In order to produce results with the downloaded data, target name \"all\" can be used. To make use of these targets, simply cd into the repo\\'s director and input \"python3 run.py <target>\"\\n\\nThe project directory contains the following:\\n\\ndata folder contains test data, the notebook for generating the test data, as well as the complete allegations data\\n\\nDockerfile from which necessary packages will be installed to run project\\n\\nreport folder contains the overleaf report pdf\\n\\nrun.py is the code in which results are produced (main coding file)\\n\\nsrc folder contains python code for data cleaning and generation of missing data (methods called in run.py). It also contains the models for ensuring that the model is fair and producing predictions so that fairness notion measurements can be produced. ',\n",
       "  'To produce test results in the repository directory, use the \"test\" target. To produce results with downloaded data, use the \"all\" target. To utilize these targets, navigate to the repository directory and run \"python3 run.py <target>\". The project directory includes a data folder with test data and a notebook for generating it, as well as complete allegations data. The Dockerfile installs necessary packages. The report folder contains the Overleaf report PDF. The main coding file is run.py, which produces results. The src folder contains Python code for data cleaning, generating missing data, ensuring model fairness, and producing predictions for fairness measurements.'],\n",
       " 'https://github.com/ejsong37/Trustworthy-Recommender-Systems-Capstone': ['# Trustworthy Recommender Systems via Bayesian Bandits Capsone\\n\\n### Team Members: Eric Song, Xiqiang Liu, Hien Bui, Vivek Saravanan\\n\\n### Mentor: Yuhua Zhu\\n\\n## About\\nRecommender systems have emerged as a simple yet powerful framework for the suggestion of relevant items to users. However, a potential issue arises when recommender systems overly recommend or spam undesired products to users in which the model loses the trust of the user. We propose a constrained bandit-based recommender system. We show this model outperforms Upper Confidence Bound (UCB) and Thompson sampling in terms of expected regret and does not lose the trust of the users. This work was presented at the Halıcıoğlu Data Science Institute Capstone Showcase on March 15th, 2023 at UC San Diego.\\n\\n## Resources\\n- [Website](https://hi3nb1.github.io/capstone/)\\n- [Poster](https://drive.google.com/file/d/1BjS6ZcwmB4TsGctyS56vNxZsTev8F0zF/view)\\n- [Report](https://drive.google.com/file/d/10VEKJZ_TWxqBKimkeTWUmmYjivagMGZJ/view)\\n\\n## Running Experiments\\n```bash\\npython run.py all  # run all experiments\\n\\npython run.py etc  # run Explore-Then-Commit (ETC) experiments\\npython run.py ucb  # run UCB experiments\\npython run.py ts  # run Thompson Sampling experiments\\npython run.py optimal  # run Bayesian Optimal Policy experiments\\n\\npython run.py linucb  # run LinUCB experiments\\npython run.py lints  # run Linear Thompson Sampling experiments\\n```\\n\\nTo run experiments related to Trustworthy Recommender Systems, run code in experiments.ipynb in TrustworthyMAB folder.\\n\\nAll the results are going to be saved in `results/` sub-directory.\\n\\n## Visualize Results\\n\\nNotebooks to visualize collected results could be found in `notebooks/` sub-directory.\\n',\n",
       "  'This project is about a trustworthy recommender system that addresses the issue of recommending undesired products to users. The proposed model outperforms other methods in terms of expected regret and does not lose the trust of the users. The project was presented at the Halıcıoğlu Data Science Institute Capstone Showcase at UC San Diego. You can find more information, including a website, poster, and report, in the provided links. There are also instructions for running experiments and visualizing results in the code.'],\n",
       " 'https://github.com/abhianish0105/DSC180B-Q2-Project': ['# DSC180B-Q2-Project\\n\\n# Twitter Sentiment Analysis on Gun Control\\n\\n## Introduction\\nThis project is on the sentiment analysis of tweets regarding the topic of gun control. In recent years in the United States, there have been an outburst of several horrific events as a result of guns getting in the hands of the wrong people. In 2023, the number of mass shootings in the US has already reached triple digits. With these incidents, we believe it would be an interesting study to do further research into the sentiment and beliefs that people have toward these issues and study this using automation and machine learning. Twitter is a very vocal platform and with the use of our new knowledge regarding sentiment analysis, there are possibly very many interesting discoveries to be made, such as how user sentiment toward one topic may differ from another. We hypothesize that because of the many gruesome events that have occurred in recent years, we will observe a mostly positive sentiment toward gun control, in that people support more regulation of weapon distribution rather than less. Our proposed project period of 10 weeks can be attributed to the fact that we are looking to improve upon our previous work by expanding our knowledge on the Astra Streaming features, and working on more efficient implementation of our architecture to ensure that we receive enough data for sufficient analysis.  We will also look to go further by integrating new features into our project, including displaying results from our visualizations on a website in addition to streaming data to a feature store or database. As we work towards these goals, we recognize that we may come across technical issues that may require us to be more flexible and adjust our project structure, so we would like to stay open minded in regards to what additional features will be added.\\n\\n\\n## Running the Repository\\n* The code in this repository runs our sentiment analysis code on a small test set of test tweets, the same code that was ran on our collected database of tweets retrieved from the Twitter API.\\n* Run the following docker image inside a container: \\n  * tepatel/test_q2\\n* To test the model on the test data: \\n  * `python3 run.py test`\\n\\n\\n\\n',\n",
       "  \"This project focuses on analyzing the sentiment of tweets related to gun control. The goal is to understand people's beliefs and attitudes towards this issue using automation and machine learning. The project aims to explore how user sentiment may differ on different aspects of gun control. The project period is 10 weeks, during which the team plans to improve their previous work, expand their knowledge on Astra Streaming features, and implement a more efficient architecture. They also plan to integrate new features such as visualizations on a website and streaming data to a feature store or database. The repository provides code for running sentiment analysis on a small test set of tweets.\"],\n",
       " 'https://github.com/DSC-180A/spam.detector.github.io': ['# Q2-Project\\n\\n## How Spam Affects the Sentiment of Tweets\\n\\n## Contributors\\nLucas Lee, Tyson Tran, Yi (Skylar) Li\\n\\n\\n## Objective\\nThe objective of our research is to develop a pipeline that filters spam content to model noise-reduced sentiments towards abortion on Twitter in real-time and analyze how spam content affects said sentiments. We compared the use of both Naive Bayes and a transfer learning model based on BERT to do spam filtration, and analyzed the impact of spam on sentiment distribution results to gain a deeper understanding of the role it plays in shaping public opinion on social media platforms.\\n\\n\\n## Pipeline\\nThis project is produced as part of the DSC180B Capstone at UCSD, working with mentors from DataStax. In this project, we utilized Apache Pulsar through Astra Streaming to create a pipeline that ingests live stream of abortion related tweets, incoporate spam-filtering ML models, and performs sentiment analysis. This is primarily done through the following steps:\\n<img src=\"visuals/Untitled drawing (2).jpg\" width=350 height=600> \\n  \\n1. A producer makes Twitter API calls to request a stream of tweets through the FilteredStreamV2 endpoint \\n2. The producer then publishes each incoming tweet (stringified Json) to a pulsar topic — Raw Tweet Topic.\\n3. Pulsar consumers subscribes to the Raw Tweet Topic and\\n   a) Consumer 1 performs sentiment analysis directly.\\n   b) Consumer 2 deploys Naive Bayes Model to label spam tweets, then performs SA.\\n   c) Consumer 3 deploys BERT Model to label spam tweets, then performs SA.\\n4. Consumers then update data to a data source.\\n5. Grafana visualizes the finding on <a href=\"https://skylar1013.grafana.net/d/_ztsas0Vz/capstone?orgId=1&from=1675065600000&to=1676188799000\">dashboards</a>\\n\\n\\nWith the above described pipeline, we now have a real-time stream of tweets being classified. We used Google Spreadsheet <a href=\"https://docs.google.com/spreadsheets/d/1fZ6MsCqtPXHWekonx2QGst0-eGei9ABzMG5LFDMEFbA/edit#gid=0\">Google Spreadsheet</a> to collect real-time tweets with producer running.\\n\\n## Requirements\\n- `requirements.txt` provided with all dependencies needed to run the code below\\n- `capstone_googlesheet_key.json` provided necessary credentials to update tweets to database.\\n- Astra Streaming account\\n- Twitter developer account\\n\\n### Required Envronmental Variables\\nRunning the producers and consumers require Astra Streaming topic keys. Setting up topics and creating an account is free, and more information can be found at the tutorial [here](https://docs.google.com/document/d/1VS31dXTIAmEkIh9o_9FcAhD-rVvcmnTo_Zm1zSMgCmY/edit).\\n\\nThe shell envrionmental variables are used within the `Producer` and `Consumer` classes within `producer.py` and `consumer.py`.\\n- `ASTRA_STREAMING_TOKEN`\\n- `ASTRA_STREAMING_URL`\\n- `ASTRA_TOPIC`\\n\\n\\n## How to run\\n`run.py test`. This runs a test pipeline on test data.\\nThis will then run:\\n`python src/producer.py`. This makes requests to the Twitter API, publishing remote-work related tweets to a pulsar topic. In its current test tag, it will run the `src/producer_offline.py`, which is encouraged to be used to test.\\n`python src/consumer_*.py`. This captures the cleaned tweets, utilizes spam detection ML models, performs sentiment analysis, alters it so that it feeds your needs for downstream analysis. Note that there are three such consumers that will be simultanouesly run in the background. Please use `ps` to check the list of processes and `kill [pid]` to end the processes. This is fully intended as producer and consumers are long running jobs in the background.\\n\\n\\n\\n## Usage\\n* Since the publishing time of the tweet is currently calculated by when the consumer receives the tweet from the topic, it\\'s recommended to use concurrently run both `producer.py` and `consumer.py` simultaneously\\n* Keep in mind that this requires the setup of the Astra Streaming dashboard, a tutorial is available [here](https://docs.google.com/document/d/1VS31dXTIAmEkIh9o_9FcAhD-rVvcmnTo_Zm1zSMgCmY/edit#heading=h.3znysh7)\\n* With the above setup, the only thing left to change is the topic that the `Consumer` subscribes to, which is in its constructor.\\n* The path and nameof the generated CSV is modifiable in the constructor of a `Consumer` in `consumer.py`\\n\\n## Files\\n- `src/producer.py`: Main driver class for fetching tweets\\n- `src/consumer_*.py`: Main driver class for filtering spams and performing sentiment analysis.\\n- `requirements.txt`: Required dependencies in Python\\n\\n',\n",
       "  'This project aims to develop a pipeline that filters spam content in real-time and analyzes how spam affects sentiments towards abortion on Twitter. The pipeline uses Naive Bayes and BERT models for spam filtration and performs sentiment analysis. The project also visualizes the findings using Grafana dashboards. The code requires certain environmental variables and dependencies to run. There are producer and consumer classes for fetching tweets, filtering spams, and performing sentiment analysis.'],\n",
       " 'https://github.com/DSC180A/transaction_categorization': ['# transaction_categorization\\n\\n## Rethinking Credit Scores: Ensuring Fair Lending through NLP for Transaction Categorization\\n\\n### By Kyle Nero, Chung En (Shawn) Pan, Nathan Van Lingen, Koosha Jadbabaei\\n\\n#### Industry Mentor: Brian Duke (Petal)\\n#### Faculty Mentor: Berk Ustun\\n\\nThe invention of credit has reshaped modern personal finance. For most, a credit card provides an opportunity to buy something you can\\'t afford in full, such as a car or a home. However, for others, credit is seen as a barrier. Because of the nature of how credit scores are calculated, certain groups, often referred to as the \"credit invisible\" are neglected. These individuals may be credit invisible for any number of reasons– they may be a young adult, or may have recently immigrated to a new country. Regardless, this group of \"credit invisible\" individuals often struggle to be approved for loans because they have no credit history.\\n\\nIn our project, we aim to cater to this \"credit invisible\" group by creating a tool that is able to classify transaction memos from an individual\\'s checking history. We are working with industry partner Petal, a financial services company who uses an alternative metric, called the \"CashScore\", as opposed to a conventional credit score. Being able to accurately classify transaction memos is important because it allows Petal to more precisely calculate a CashScore for each credit applicant, which will over time increase the amount of people who qualify for credit– especially those with no credit history.\\n\\nTo reproduce this analysis, simply run the run.py file using the included docker image.\\n',\n",
       "  'The article discusses the importance of credit scores and how they can exclude certain groups of people, known as the \"credit invisible.\" The authors propose a tool that can classify transaction memos to help financial services company Petal calculate a more accurate metric called the \"CashScore\" for credit applicants. The article also provides instructions on how to reproduce the analysis.'],\n",
       " 'https://github.com/kkw002/Quarter2Project': [\"# DSC180B Quarter 2 Project: Prediction of Transaction Types using NLP analysis.\\nThis project attempts to identify which phrases, among other features that are generated from given data from Petal are used to predict the categorization of transaction made by a user. Currently, the model in the files only represents the 'base' model in which the features are relatively basic, as well as the model that was used to generate the accuracy.\\n\\n## Accessing Data\\nThe data needs to be accessed through ``` https://drive.google.com/file/d/10JH-rN5c1cMXIEXgPkPGImWSGOzC19Kx/view?usp=share_link ```.\\n1) After downloading the data, replace the ``` testdata.parquet ``` file with the downloaded file.\\n2) In the run.py file, replace ```getData('data/testdata.parquet')``` with ```getData('data/DSC180B.parquet')```\\n\\n## Viewing Results\\nTo see the accuracy score of the model on the dataset run ``` python run.py test ```\\n\",\n",
       "  'This project aims to predict transaction types using NLP analysis. The model uses phrases and other features generated from Petal data to categorize user transactions. The data can be accessed through a specific link, and the accuracy score of the model on the dataset can be viewed by running a specific command.'],\n",
       " 'https://github.com/NJMIXI98/DSC180_Q2PROJECT': ['# Auditing race inequality based on gender in data science related job market\\n\\nData Science Capstone Project advised under Stuart Geiger\\n\\n# Authors\\n\\nNancy Jiang and Sahil Wadhwa\\n\\n# Overview\\n\\nIn the twentieth century, one of sociology’s findings is that race and gender matter in the job market. Jobs were segregated by race and gender with whites earning more than other people of color and men earning more than women. Race inequality in the job market has been a long-standing interest of scholars. Notably, some research indicates that racial gaps are more significant for women than for men. Women face the unique challenges of lower wages and lower rewards in the global workforce. However, as women’s relative share in occupations grows nowadays, the gender inequality gap narrows in most job markets(Stier et al.,2014). Besides, a finding shows that race-based discrimination is weaker in high-paid jobs. Back in 2012, the Harvard Business Review acclaimed data science as “the sexiest job of the 21st century”. Some may pose the question of whether the statement still holds today. According to the U.S. BUREAU of Labor Statistics, employment in data science is projected to grow 36% from 2021 to 2031, much faster than the average for all occupations, which means employers will create more than 13,500 new data science related job opportunities each year on average, over the decade(Bureau of Labor Statistics 2022). Thus, we are curious about the gender-based race inequality in the data science related job market. \\n\\n# Code Instruction\\nUse Docker image: \"ucsdets/datahub-base-notebook:2022.3-stable\"\\nRunning python run.py test\\n',\n",
       "  \"The authors of this project, Nancy Jiang and Sahil Wadhwa, are auditing race inequality based on gender in the data science job market. They discuss how race and gender have historically affected job opportunities and wages, with women facing unique challenges. However, as women's representation in occupations increases, the gender inequality gap is narrowing. The authors also mention that race-based discrimination is weaker in high-paid jobs. They are interested in exploring the gender-based race inequality specifically in the data science job market, which is projected to grow significantly in the coming years. The code instruction suggests using a specific Docker image and running a Python script for testing purposes.\"],\n",
       " 'https://github.com/brianjhuang/CryptoWho': ['# CryptoWho\\nData Science Capstone Project advised under Stuart Geiger\\n\\n## Authors\\n#### Brian Huang and Lily Yu\\n\\n## Overview\\nIn 2020, Bitcoin, other Cryptocurrencies and blockchain investments (NFTs) experienced a major boom. With the endorsements of many major companies and what seemed to be a large scale adoption on the horizon, the Crypto craze had started kicking off.\\n\\nFor many, cryptocurrency and NFTs were the first time they encountered an \\'investment\\'. Many proclaimed crypto/NFT traders had no prior experience with any investing (stock market, retirement accounts, etc.) and looked to crypto/NFT as a get rich quick path. However, with the influx of new and young traders it was only a matter of time before the crypto/NFT scams began to pray on it\\'s new consumers.\\n\\nWhile crypto/NFT is not inherently a scam, many bad actors began to manipulate the influx of young and inexperienced investors. Many schemes akin to pump and dumps began to appear and with the popularity of crypto/NFT throughout social media, many inexperienced investors were quick to turn a blind eye to scams that seasoned investors may recognize immediately.\\n\\nCases like this have occured with traditional markets as well, where investors pour money into an asset based off of hype with no sound reasoning ($GME or GameStop). Crypto/NFTts however were especially susceptible to this with the combination of a relatively new asset, large demographic of young investors, and many social media influencers promoting these assets (Logan Paul, Doja Cat, etc)\\n\\nWith the most recent scandals in the cryptocurrency/NFT world (FTX, Logan Paul\\'s CryptoZoo), it\\'s more important than ever to investigate the platforms that many of these assets are promoted on. While platforms like YouTube and TikTok may not be intentionally promoting this content, it\\'s important that they\\'re aware of if their algorithms do indeed promote this type of content. To clarify, we do not provide an opinion on whether these scams are run by the founders of these products (FTX\\'s Sam Bankman-Fried and CryptoZoo\\'s Logal Paul) but rather emphasize that the scams have occured.\\n\\n![crypo_losses](references/figures/crypto_losses.png)\\n\\nAccording to the FTC, over a billion dollars has been lost to these types of scams since 2021. Half of which have orginated directly from social media. The most susceptible group of people? Young individuals.\\n\\nThe goal of this project is to investigate YouTube\\'s recommendation algorithm to provide insight on the types of investment recommendations being provided to users across a variety of age groups. We assume that all individuals should be receiving the same proportion of recommendations based on their search trends (within a margin of error) regardless of age. This implies that a user who is younger and searching for general investment advice should not be receiving more crypto/NFT recommendations than someone who is older with similar watch history. By conducting audits on YouTube, we hope to gain valuable insight on YouTube and it\\'s role in propogating this type of content on their platform (whether intentional or not).\\n\\n### What does this repository offer?\\n\\nThe repository offers all tools used to conduct the audit. Helper functions and classes help download video and metadata from YouTube, run headless browsers to watch seed videos, and query the GPT-3 API for video sentiment classification.\\n\\nAll code can be found in the `src` folder and imported respectively. For any changes in filepath or settings, please look through the `config` folder.\\n\\n## Table of Contents\\n\\n- [Overview](#overview)\\n- [Methodology](#methodlogy)\\n- [Installation](#installation)\\n- [Downloading YouTube Data](#downloading-youtube-video-data)\\n- [GPT Prompting and Fine-Tuning](#gpt-prompting-and-fine-tuning)\\n- [Conducting the Audit](#conducting-the-audit)\\n- [Inference](#running-inference)\\n- [Analyzing Audit Results](#analyzing-audit-results)\\n- [Acknowledgements](#acknowledgements)\\n\\n## Methodology\\n\\nOur project is split into three parts:\\n\\n**Part 1: Collecting Seed Videos and Creating Persona Users**\\n\\nSeed videos function as a way for us to evaluate our GPT model and the videos that users watch to build watch history. Given the time we had, we could only collect and label 140 videos (20 for each age, 40 for traditional investments, 40 for blockchain investments, 5 for mixed, and 15 for edge cases where the video discusses money but is actually unrelated).\\n\\nThese seed videos fall into one of four labels:\\n\\nBlockchain, Traditional, Mixed, and Unrelated.\\n\\nUsing our seed videos, we create six persona users with different watch behaviors.\\n```\\nA young individual (18-23) who:\\n\\nWatches blockchain\\n\\nWatches traditional\\n\\nWatches mixed\\n```\\n```\\nA old individual (55-60) who:\\n\\nWatches blockchain\\n\\nWatches traditional\\n\\nWatches mixed\\n```\\nBy comparing the recommendations across these users, we can determine if YouTube is fairly and evenly recommending this content to all age groups. \\n\\n**Part 2: Creating a Prompt that peforms the best for our task at hand**\\n\\nCreating a prompt is important when using GPT as a classifier. More info about our prompt can be found in the sections that follow.\\n\\n**Part 3: Running the audit**\\n\\nUsing selenium, we can mimic the behavior of the six persona users above. By having a headless browser click and watch each video, we can collect the recommendations of each user. \\n\\nEach user watches 50% of the video by default. This and other config can be changed in the `config/Audit.py` file. \\n\\nThe diagrams below illustrate the general pipeline for our audit:\\n\\n![method](references/figures/methods.png)\\n![audit](references/figures/audit.png)\\n\\n## Installation\\n\\nWe are using Python version [3.8.5](https://www.python.org/downloads/release/python-385/).\\n\\nPlease install this version of Python, as gensim summarization is still supported.\\n\\nVersions between 3.8 and 3.9 should work, however we recommend you install the same version we use.\\n\\nWe recommend using [Anaconda](https://www.anaconda.com/) environments to do so.\\n\\n**Once you\\'ve installed Anaconda, please run the following to create your environment.**\\n\\n```bash\\nconda create --name <env_name> python=3.8.5\\n```\\n\\n**Activate your conda environment like so:**\\n```bash\\n# Linux/Mac\\nsource activate <env_name>\\n\\n# Windows\\nactivate <env_name>\\n```\\n\\n**Install required packages**\\n\\n```bash\\npip install -r requirements.txt\\n```\\n\\n**If you have Python 3.8.5 working outside of Conda (or any other version of Python that works with gensim) you can create a normal environment if you prefer**\\n\\n```bash\\npython3 -m venv .venv\\n\\nsource .venv/bin/activate\\n```\\n\\n**Install required packages**\\n\\n```bash\\npip install -r requirements.txt\\n```\\n\\n### Headless Browser Setup\\n\\nWe use Selenium with the Firefox Webdriver to conduct our audit and gather YouTube video recommendations.\\n\\nNote: More recent versions of FireFox will just launch if you have the browser installed. Please install the Firefox browser. If it does not work, install the driver.\\n\\nTo run the scraper, you will need to install the Firefox Webdriver, which can be downloaded [here](https://github.com/mozilla/geckodriver/releases).\\n\\nTo install, place your OS-appropriate executable in a directory locatable by your PATH.\\n\\n### API Key Setup\\n\\nOur codebase uses the YouTube Data API to download video metadata, comments, and for many other purposes like searching YouTube and grabbing recommendations. We use the OpenAI API to provide snippets and retrieve classification labels for our downloaded videos.\\n\\nYou can enable the YouTube Data API for your Google account and obtain an API key following the steps <a href=\"https://developers.google.com/youtube/v3/getting-started\">here</a>.\\n\\nThe key can be found after you set up your cloud console.\\n\\nYou can fetch your OpenAI API key from <a href=\"https://platform.openai.com/\">here</a>.\\n\\nThe key can be found in your profile under ***View API Keys***.\\n\\nOnce you have both API keys, please set the ```YOUTUBE_DATA_API_KEY``` and ```OPENAI_API_KEY``` variable in your environment:\\n\\nYou can do so by going to your home directory and running the following command:\\n\\n**Mac OS and Linux**\\n\\n```\\nnano .bash_profile\\n\\n# Note Mac Users using zsh shell users should also set their keys in their zsh_profile\\nnano .zsh_profile\\n```\\n\\nInside your bash profile, you can go ahead and set this at the top:\\n\\n```\\n# YOUTUBE API KEY\\nexport YOUTUBE_DATA_API_KEY=\"YOUR_API_KEY\"\\nexport OPENAI_API_KEY=\"YOUR_API_KEY\"\\n```\\n\\nClose out of your terminal and your code editor to see changes occur.\\n\\n**Check that updates have been made**\\n```\\necho $YOUTUBE_DATA_API_KEY\\necho $OPENAI_API_KEY\\n```\\n\\nThe following tutorials cover how to do this as well:\\n\\nhttps://www.youtube.com/watch?v=5iWhQWVXosU&t=1s (Mac/Linux)\\n\\nhttps://www.youtube.com/watch?v=IolxqkL7cD8 (Windows)\\n\\nIf you are not seeing updates, your `bash_profile` may not be sourced. To resolve this, add the following line to your `.bashrc`:\\n\\n```\\n. ~/.bash_profile\\n\\n# Note Mac Users using zsh shell users should do this in .zshrc\\n. ~/.zsh_profile\\n```\\n\\nThis can be anywhere, but we\\'ve put ours at the very bottom. Use the following command to enter your `.bashrc`.\\n\\n```\\nnano .bashrc\\n# Note Mac Users using zsh shell users should do this\\nnano .zshrc\\n```\\n\\nNow within Python you can access your API key by doing the following:\\n```\\nimport os\\n\\nyoutube_key = os.environ.get(\"YOUTUBE_DATA_API_KEY\")\\nopenai_key = os.environ.get(\"OPENAI_API_KEY\")\\n```\\n\\n### Connecting to a VPN\\n\\nTo ensure all things are constant, we connected to the UC San Diego VPN for all of our audits and downloads. We recommend you connect to a VPN in the same location as well. There are many different VPN providers.\\n\\n### Preserving User Agent\\n\\nWe recommend the audits be done on the same device, and if that is not possible, the same operating system. All of our audits were conducted on a Ubuntu device, however the platform you choose to audit with should not matter as long as they are consistent.\\n\\n## Running The Entire Project Pipeline (Not Recommmended)\\n\\nOur code uses a `run.py` file to help you run our code out of the box. You have the option to run the entire project pipeline using the folliwng command:\\n\\n`python run.py all`\\n\\nThis command will download all seed videos, create snippets and evaluate your seed videos using the classifier, conduct the audit and download all videos, and classify the videos from the audit.\\n\\nWe **DO NOT** recommend you run this. The entire pipeline may take multiple days to run depending on the size of your seed videos. (Note: With 140 seed videos, our entire pipeline takes three days to run.)\\n\\nIf you would like to validate that the pipeline and it\\'s sub-targets work, we instead recommended using:\\n\\n`python run.py test`\\n\\nWhich will run the pipeline end to end on a much smaller subset of data found in `test/youtube`. This takes around an hour.\\n\\nYou can continue reading below for more details, but here is an overview of all targets in the run.py file. They are listed in the order we recommend you run them. Running each target gives you finer control over what is downloaded, avoiding repeated efforts/wasted API calls.\\n```\\nseed - The following target downloads and processed seed data. Target will prompt user for what they want to download and if they want to process the seed data (create video snippets.) Video snippets for context is a concatenation of the video title + summarized transcript (via TextRank) + top ten video tags (via TF-IDF)\\n\\nclassify-seed - The following target classifies the seed videos, providing a baseline classification report of your prompt. The confusion matrix is saved in references/figures. Note that the classification report is printed in terminal and not saved.\\n\\naudit - The following target conducts the audit on a Firefox headless browser. It allows you to run a single or multiple audits. Config for each audit can be found in config/Audit.py\\n\\ndownload-audit - The following target allows you to download the results of your audit. You can choose to download homepage results, sidebar results, or both.\\n\\ncreate-audit-snippets - The following target creates snippets for each set of audit results. Note this may take a while as some videos have large transcripts. Video\\'s over an hour have no transcript removed, however by modifying the target code this can be removed if you are able to wait the additional time.\\n\\nclassify - Run classification on your audit snippets.\\n```\\n\\n## Downloading YouTube Video Data\\nUsing `python run.py seed` will download all seed videos and save it in `data/seed/youtube/videos_{}.csv`.\\n\\nUsing `python run.py download-audit` will download all videos from the audit and save the downloaded videos in `data/audit/youtube/videos_{}.csv`.\\n\\nCalling `python3 src/data/youTubeDownloader.py <video_ids seperated by spaces>` will download any videos you want and save it in `data/external/youtube/videos_{}.csv`. \\n\\n## GPT Prompting and Fine Tuning\\nUnlike traditional classifiers, GPT-3 is not trained on an existing dataset. Instead, predictions are generated through a set of \\'prompts\\', instructing the large language model what the task is.\\n\\nAt the start of this project, GPT-3 was used. GPT-3 can be fine-tuned using a labelled dataset and prompts. For those interested in using a fine-tuned GPT-3 model, we encourage you to use the following <a href=\"https://platform.openai.com/docs/guides/fine-tuning\">resource</a>.\\n\\nStarting March 1st, GPT-3.5 was released. This API powers ChatGPT. Due to significant reduction in cost and increases in accuracy, we migrated to <a href = \"https://platform.openai.com/docs/models/gpt-3\">GPT 3.5.</a>\\n\\nYou are open to modify the prompt for our model however you see fit. The prompt can be found in `gpt.py` in the `create_message()` function.\\n\\nThe following prompt was used for our classification:\\n```\\n{\"role\": \"system\", \"content\" : \\n\\n\"You are a classifier that determines if a YouTube video snippet falls under a label. A snippet is a concatenation of the video title, summarized transcript, and video tags. The labels and additional instructions will be included in the first user message.\"},\\n\\n{\"role\": \"user\", \"content\" : \\n\\n\"\"\"Labels:\\n\\nTraditional: Videos that recommend or educate about stocks, bonds, real estate, commodities, retirement accounts, or other traditional investments or keywords related to them.\\nBlockchain: Videos that recommend or educate about cryptocurrency (BTC, ETH, etc.), NFTs, or other Web3 investments or keywords related to them.\\nMixed: Videos that recommend or educate about both blockchain and traditional investments or keywords related to both.\\nUnrelated: Videos that do not recommend or educate about either blockchain or traditional investments or keywords related to them.\\n\\nInstructions:\\n- The classifier should consider the context and meaning of the keywords used to determine whether the snippet is related to traditional or blockchain investments.\\n- If talks about making money from jobs, side hustles, or other alternative assets (cars, watches, artificial intelligence, trading cards, art, etc), they are Unrelated.\\n- A video that is only downplaying an investment or discussing it negatively should be classified as Unrelated.\\n- Please return predictions in the format\" {Label} : {20 word or shorter rationale}\"\"\"},\\n\\n{\"role\": \"assistant\", \"content\": \\n\\n\"\"\"Understood. I will classify YouTube video snippets based on the provided labels and instructions. Here\\'s how I will format the predictions:\\n\\n    {Label} : {20-word or shorter rationale}\\n\\nPlease provide me with the YouTube video snippet you would like me to classify.\"\"\"}\\n```\\nNote that unlike GPT-3, the ChatCompletions API endpoint expects a message, not a prompt. The system message biases the model towards a specific task, while the user messages provide prompts and instructions. The assistant messages can be used to affirm what has already been stated in the user messages.\\n\\nYou can test out your prompt on your seed videos!\\n\\n`python run.py classify-seed`\\n\\nThis will run classification on the seed videos and return a classification report and confusion matrix. The confusion matrix can be found in `references/figures`.\\n\\nMore information on writing prompts can be found <a href = \"https://github.com/openai/openai-cookbook\">here</a>\\n\\nNote: Occasionally, the output of a prediction will be \\'Label\\'. This is due to GPT not adhering to the format we instruct it to. There is no known fix to this as of now.\\n\\nFor example:\\n```\\n### Expected Output\\nBlockchain: Rationale\\n\\n### Occasional Output\\nLabel: Blockchain: Rationale\\n```\\n\\nBy default, our seed classifier will mark videos with a prediction of \\'Label\\' as unrelated. This may result in accuracy being off by 1-2% when in reality it should be higher. Please inspect your output yourself to verify!\\n\\n## Conducting the Audit\\n\\nThe audit may take a very long time to run. Please make sure you\\'re connected to a VPN before you start. We recommend using some program to keep your computer awake (Caffeine, Amphetamine). \\n\\nIf you are using `python run.py audit`, the audit will prompt you if you want to conduct a single audit or all audits (in our case six audits.)\\n\\nIf you choose one audit, the one audit will be run based on these parameters in `config/Audit.py`\\n```\\n# AUDIT VARIABLES\\nUSER_AGE = \"old\"  #\\'young\\' or \\'old\\'\\nFINANCE_VIDEO_TYPE = \"blockchain\"  #\\'traditional\\', \\'blockchain\\', \\'mixed\\'\\n```\\n\\nIf you choose multiple audits, the code will iterate through the following dictionary in `config/Audit.py`, sleeping ten minutes between each audit.\\n```\\nAUDITS = [\\n    {\"type\": \"traditional\", \"age\": \"young\"},\\n    {\"type\": \"mixed\", \"age\": \"young\"},\\n    {\"type\": \"blockchain\", \"age\": \"young\"},\\n    {\"type\": \"traditional\", \"age\": \"old\"},\\n    {\"type\": \"mixed\", \"age\": \"old\"},\\n    {\"type\": \"blockchain\", \"age\": \"old\"},\\n]\\n```\\n\\nAfter running the audit, please run:\\n\\n```python\\npython run.py download-audit\\n\\npython run.py create-audit-snippets\\n```\\n\\nAudits are saved to `data/audit` with the following sub-folders:\\n```\\nraw - The raw video information. Please only keep the results of one run in this folder, as the pre-processing scripts read the entire directory to load in files. Having multiple audits in the folder will result in overriden and missing files in data cleaning steps.\\n\\nprocessed - The downloaded videos. The sub-folder `snippets` contains the downloaded videos with snippets appended.\\n\\nresults - The results of our predictions.\\n```\\n\\n## Running Inference\\n\\nRunning inference on the audit videos is easy!\\n\\n`python run.py classify`\\n\\nRunning the following command with run classification on every single audit file. Results are stored in `data/audit/results/`.\\n\\n## Analyzing Audit Results\\n\\nGPT models use a parameter called \\'temperature\\' to adjust how much risk their models take. 0 temperature means the model is deterministic, outputting the same response everytime. We\\'ve used a temeprate of 0.25 as we want the model to take a bit of risk. This, however, does resukt in cases where our predictions come back in the wrong format.\\n\\nFor example:\\n```\\n### Expected Output:\\n\\nBlockchain: This video is blockchain\\n\\n### Occassional Output:\\n\\nLabel: Blockchain: This video is blockchain\\n```\\n\\nBecause of this, we can not include a target for analyzing the results. It\\'s much easier to analyze the result on your own through a notebook. We have a reference notebook: `notebooks/insepectPredictions.ipynb` that demonstrate loading in the result data, cleaning it, and running a Chi-Squared test. Please use that for reference. You can also generate any plots you need using `matplotlib` and `seaborn`. \\n\\n## Acknowledgements\\n\\nWe\\'d like to make a special thanks to the Data Science Capstone faculty, HDSI, Professor Stuart Geiger, and all our friends and family who provied us the opportunity to work on this project.\\n',\n",
       "  \"This text is about a data science capstone project called CryptoWho. The project aims to investigate YouTube's recommendation algorithm and its role in promoting investment recommendations, particularly in the cryptocurrency and NFT (non-fungible token) space. The authors collected seed videos and created persona users to evaluate the recommendations across different age groups. They also used GPT-3, a language model, for video sentiment classification. The project includes code for downloading YouTube data, conducting the audit, running inference, and analyzing the results. The authors emphasize the importance of investigating platforms like YouTube to prevent scams and protect young investors.\"],\n",
       " 'https://github.com/gprasad125/dsc180b': ['# Analyzing U.S. Congressional Tweets with OpenAI GPT-3\\n\\n# Repository for the Spring 2023 Quarter (DSC180b)\\n\\nThis project covers Tweet sentiment analysis for Tweets originating from US Congresspeople as it relates to China.\\nThis is an extension to Quarter 1\\'s project found for each researcher below:\\n\\n[Annie\\'s Q1 codebase](https://github.com/AnnieeeeeF/DSC180A_Project1)\\n\\n[Gokul\\'s Q1 codebase](https://github.com/gprasad125/dsc180a_project)\\n\\nThis project continues the work by exploring the same topic through the lens of a Large Language Model (LLM). \\n\\n## Necessary Configurations:\\n\\nYou will *need* an API key from OpenAI to utilize the GPT-3 model.\\nSign up for an account and get a key [here](https://openai.com/api/)\\n\\nYou can then pass your API Key to our scripts in one of two ways:\\n\\n1. Export your key by running the following in your command line:\\n\\n`export OPENAI_API_KEY=...`\\n\\n2. Create a .env file in the root directory and paste in your key like so:\\n\\n`OPENAI_API_KEY=...`\\n\\n## Data Source:\\n\\nRaw data can be found [here](https://drive.google.com/drive/u/1/folders/1VSYdGh12UNVNhfxbSeHRdANvHr5xF8Ea). \\nDownload the file `SentimentLabeled_10112022.csv`, and place it inside the `data/raw` directory. \\n\\nYou can then run the `run.py` file with the following targets:\\n- `test`: runs the file on man-made test data\\n- `data` / `all`: runs the file on Twitter-API sourced data.\\n\\n## Explanation of File Structure:\\n\\n### 📁 Folders:\\n\\n#### config\\nContains JSON configuration for optimized & group-selected models. \\n\\n#### data\\nContains the data for and from the project, divided as such:\\n- raw: the base uncleaned data\\n- out: the output cleaned data used for visualizations and modeling\\n- test: test data used to debug the Python scripts\\n- results: visuals generated from the EDA and modeling, formatted as PNGs\\n\\n#### notebooks\\nContains initial Jupyter Notebooks for EDA / Modeling.\\nNot entirely cleaned up yet. Cleaned versions of this code will be found inside our `src` folder.\\n\\n#### src\\nContains the Python scripts needed to run the project, divided as such:\\n- data: \\n    - `make_dataset.py` cleans and processes the raw data\\n- models: \\n    - `classifier.py`: GPT-3 powered classifier to find \"relevant\" Tweets (i.e, Tweets about Chinese governmental impact on America.)\\n    - `sentiment.py`: GPT-3 powered sentiment scorer to find \"emotion\" of Tweet (i.e, is a Tweet favorable or negative towards China?)\\n- visuals: \\n    - `eda.py`: Generates summary visuals for the two cleaned dataframes going into modeling. Not the full EDA of the dataset. For that, check under `notebooks/EDA.ipynb`\\n- notebooks:\\n    - `nb_functions.py`: All necessary functions for notebook report + visuals. Uses the code from other folders with slight modifications to fit an ipynb environment. \\n\\n### 📜 Files:\\n\\n#### run.py\\nBaseline Python script to run via CLI with targets.\\nCurrent targets include `test` (`all`) and `data`. \\n\\n    - Creates cleaned data file\\n    - Generates exploratory visuals and saves them\\n    - Runs models on data\\n\\n### requirements.txt\\nNecessary Python packages to install via `pip install -r requirements.txt`\\n\\n',\n",
       "  'This project focuses on analyzing the sentiment of tweets from US Congresspeople regarding China. It builds upon the work done in Quarter 1 and utilizes a Large Language Model (LLM) called GPT-3. The necessary configurations include obtaining an API key from OpenAI and downloading the raw data file. The file structure consists of folders for configuration, data, notebooks, src (containing scripts for data processing, models, and visuals), and files such as run.py and requirements.txt.'],\n",
       " 'https://github.com/x6zeng/NLP-Active-Learning-Pipeline': [\"# NLP-Active-Learning-Pipeline\\nThis is the repository for our DSC180B section A12 Group B Quarter 2 Project, which consists of 2 machine learning models, with Active Learning approaches, that can be used to predict the relevance and sentiment toward China of the tweets posted by the members of the U.S. Congress, given the tweet's text content.\\n\\n## Main Content\\n- __data__: folder to store data, including test data and other data. It is also used to store the results data for the author\\n  - test: folder to store the test data\\n  - raw: empty, folder to store the raw data\\n  - result: folder to store the result data\\n- __notebook__: folder to store the pre-development notebooks\\n  - analyses: notebooks containing the active learning results analyses.\\n  - explorations: code explorations for active learning pipeline.\\n  - model_comparison: all the pre-development code for relevance and sentiment model, as well as the comparison between models using different hyperparameters.\\n- __src__: folder to store the files of obtaining the dataset, building the features, and the code for the 2 models\\n  - `utilities.py` - script to preprocess the raw data\\n  - `Relevance_AL_Committee.py` - script to train the relevance model\\n  - `Sentiment_AL_Committee.py` - script to train the sentiment model\\n\\n## Data Source\\nThe data used in this project was provided by the staffs from the China Data Lab at UC San Diego. Click [here](https://drive.google.com/drive/folders/1VSYdGh12UNVNhfxbSeHRdANvHr5xF8Ea?usp=sharing) for data. If running the models with the raw data, please place the `SentimentLabeled_10112022.csv` in the folder `data/raw`.\\n\\n## Important Files\\n- `Dockerfile`: contains the information for building the docker image\\n- `run.py`: the script to run the models. To run the models on test data, use the following command: \\n  - `python3 run.py test`\\n- `submission.json`: contains the submission information\",\n",
       "  'This repository contains the code for a project that focuses on predicting the relevance and sentiment toward China of tweets posted by members of the U.S. Congress. The project includes two machine learning models with active learning approaches. The repository has folders for data storage, pre-development notebooks, and source code files. The data used in the project was provided by the China Data Lab at UC San Diego. Important files include a Dockerfile for building a docker image, a run.py script to run the models, and a submission.json file with submission information.'],\n",
       " 'https://github.com/colts661/Incomplete-Text-Classification': ['# Incomplete Supervision: Text Classification based on a Subset of Labels\\n\\nIn this project, we explore the **I**n**c**omplete **T**ext **C**lassification (IC-TC) setting. We aim to design a text classification model that could suggest class names not belonging to the training corpus to unseen documents, and classify documents into a full set of class names.\\n\\n<div style=\"display:flex;\">\\n    <div style=\"width:100%;float:left\">\\n        Authors: Luning Yang, Yacun Wang<br>Mentor: Jingbo Shang\\n    </div>\\n</div>\\n\\n\\n### Model Pipeline\\n- Find seed words from the supervised set: TF-IDF\\n- Use full corpus to find word embeddings\\n  - Final Model: Pretrained `BERT` contextualized word embeddings using static representations guided by `XClass`, reduced dimensions using PCA\\n  - Baseline Model: Trained `Word2Vec` word embeddings\\n- Find document and class embeddings based on averaged word embeddings, from the documents or seed words\\n- Compute similarity as confidence score, predict argmax if confidence over threshold\\n- For other unconfident documents, run clustering and label generation (LI-TF-IDF or Prompted ChatGPT)\\n<p align=\"center\"><img width=\"60%\" src=\"others/model-pipeline.png\"/></p>\\n\\n\\n### Environment\\n\\n- [**DSMLP Users**]: Since the data for this project is large, please run DSMLP launch script using a larger RAM. The suggested command is `launch.sh -i yaw006/incomplete-tc:final -m 16 -g 1`. Please **DO NOT** use the default, otherwise Python processes will be killed halfway.\\n- Other options:\\n  - Option 1: Run the docker container: `docker run yaw006/incomplete-tc:final`;\\n  - Option 2: Install all required packages in `requirements.txt`.\\n\\n### Data\\n#### Data Information\\n- The datasets used in the experiments can be found on [Google Drive](https://drive.google.com/drive/folders/1kf3AXpKbwbZuQhcVSiaMzCiaSrWTdO7i?usp=sharing).\\n- The datasets used in the experiments are: `DBPedia`, `DBPedia-small`, `nyt-fine`, `Reddit`\\n- **Note**: `DBPedia-small` is the default experiment target dataset, as it contains a subset of documents for the full `DBPedia` dataset, and could be run in a few minutes.\\n\\n#### Get Data\\n- [**DSMLP Users**]: For the 3 datasets provided, convenient Linux commands to download and get the data are provided in the [documentation of raw data](data/raw/). Please run the commands in the **repository root directory**.\\n- Generally, under Linux command line, for any Google Drive zip file, \\n  - Follow the `wget` [tutorial](https://medium.com/@acpanjan/download-google-drive-files-using-wget-3c2c025a8b99)\\n    - Find the Large File section (highlighted code section towards the end)\\n    - Paste the `<FILEID>` from the `zip` file **sharing link** found on Google Drive\\n    - Change the `<FILENAME>` to your data title\\n  - Run `cd <dir>` to change directory into the data directory\\n  - Run `unzip -o <zip name>` to unzip the data\\n  - Run `rm <zip name>` to avoid storing too many objects in the container\\n  - Run `cd <root>` to change directory back to your working directory\\n  - Run `mkdir <data>` to create the processed data directory\\n- Under non-command line, go to the Google Drive link, download the zip directly, place the files according to the requirements in the **Data Format** section, and manually created the directory needed for processed files. See the **File Outline** section for example.\\n\\n#### Data Format\\n- Raw Data: Each dataset must contain a `df.pkl` placed in `data/raw/`. The file should be a compressed Pandas DataFrame using `pickle` containing two columns: `sentence` (for documents) and `label` (for the corresponding label).\\n- Processed Data: \\n  - The corpus will be processed after the first run, and processed files will be placed in `data/processed`.\\n  - The processed file will be directly loaded for subsequent runs.\\n\\n### Commands\\n[**DSMLP Users**]: \\n- The `test` target could be easily run as `python run.py test`.\\n- The `experiment` target could be run as `python run.py exp -d <dataset>`.\\n- When prompted from the prompt, insert values.\\n\\nThe main script is located in the root directory. It supports 3 targets:\\n- `test`: Run the test data. All other flags are ignored.\\n- `experiment` (or `exp`) [default]: Perform one vanilla run.\\n\\nThe full command is:\\n```\\npython run.py [-h] target [-d DATA] [-m MODEL]\\n\\nrequired: target {test,experiment,exp}\\n  run target. Default experiment; if test is selected, run baseline model on testdata.\\n\\noptional arguments:\\n  -h, --help                 show this help message and exit\\n  -d DATA, --data DATA       data path, required for non-testdata\\n  -m MODEL, --model MODEL    model pipeline, {\\'final\\', \\'baseline\\'}. Default \\'final\\'\\n```\\n**Notes**: \\n1. Due to data size constraints to run large BERT embeddings or train embeddings based on the corpus, the `test` target will run the baseline model with pre-trained `glove-twitter-25` word embedding to speed up computation time.\\n2. Due to time constraints and container constraints, the short experiments are chosen to run fast, which means performance is not guaranteed.\\n\\n\\n### Code File Outline\\n```\\nIncomplete-Text-Classification/\\n├── run.py                           <- main run script\\n├── data/                            <- all data files\\n│   ├── raw                          <- raw files (after download)\\n│   │   ├── nyt-fine\\n│   │   |   └── df.pkl               <- required DataFrame pickle file\\n│   |   └── ...\\n│   └── processed/                   <- processed files (after preprocessing)\\n├── src/                             <- source code library\\n│   ├── data.py                      <- data class definition\\n│   ├── word_embedding.py            <- word embedding modules\\n│   ├── similarity.py                <- computing similarities and cutoff\\n│   ├── unsupervised.py              <- dimensionality reduction and clustering\\n│   ├── generation.py                <- label or seed word generation\\n│   ├── evaluation.py                <- evaluation methods\\n│   ├── models.py                    <- model pipelines\\n│   └── util.py                      <- other utility functions\\n└── test/                            <- test target data\\n```\\n\\n---\\n### Citations\\n\\n#### Word2Vec\\n```\\n@article{word2vec,\\n    title={Efficient estimation of word representations in vector space},\\n    author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},\\n    journal={arXiv preprint arXiv:1301.3781},\\n    year={2013}\\n}\\n```\\n\\n#### XClass\\n```\\n@misc{wang2020xclass,\\n      title={X-Class: Text Classification with Extremely Weak Supervision}, \\n      author={Zihan Wang and Dheeraj Mekala and Jingbo Shang},\\n      year={2020},\\n      eprint={2010.12794},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n```\\n\\n#### BERT\\n```\\n@article{devlin2018bert,\\n  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},\\n  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\\n  journal={arXiv preprint arXiv:1810.04805},\\n  year={2018}\\n}\\n```\\n<div style=\"float:right\">\\n    <img width=\"25%\" src=\"others/HDSI.png\" alt=\"Logo\">\\n</div>',\n",
       "  'This project explores the Incomplete Text Classification (IC-TC) setting and aims to design a text classification model that can suggest class names not present in the training corpus to unseen documents. The model pipeline includes steps such as finding seed words, using word embeddings, computing similarity scores, and running clustering and label generation for unconfident documents. The code file outline includes various modules for data processing, word embeddings, similarity computation, unsupervised learning, label generation, evaluation, and model pipelines. The project provides options for running the code on DSMLP or using Docker containers. The datasets used in the experiments can be found on Google Drive.'],\n",
       " 'https://github.com/k6chan/reverse-dictionary-pokemon': ['# pokemon-reverse-dictionary\\n\\n## [Static Website](https://k6chan.github.io/reverse-dictionary-pokemon/)\\n\\nA reverse dictionary for Pokemon as a Python Flask web app.\\n\\n## Setup\\n\\n*With approval from my TA, a `run.py` script is not necessary for this website project.*\\n\\n### With Docker (automatically installs libraries and starts the dev server)\\n\\n**The Flask server hosting is not compatible with Docker on DSMLP. This Docker image can only be run locally, not on DSMLP.**\\n\\n```\\nDockerHub repository:\\n\\nhttps://hub.docker.com/repository/docker/k6chan/reverse-dictionary-pokemon\\n```\\n\\nRun the Docker image using `docker run -it --rm -p 5000:5000 k6chan/reverse-dictionary-pokemon:latest`.\\n\\nThe Flask server should automatically start up. Access the server in your browser, usually `http://127.0.0.1:5000/`.\\n\\n### Without Docker\\n\\nFirst, clone the repository to your device with `git clone`.\\n\\nInstall the Python libraries in `requirements.txt` with `pip install --no-cache-dir -r requirements.txt`.\\n\\nChange directory into `src/models` and use the following command to run the dev server:\\n\\n`python -m flask --app application run`\\n\\nAccess the server in your browser, usually `http://127.0.0.1:5000/`.\\n\\n## Contribution Statement\\n\\nSolo project by Kaitlyn Chan.\\n',\n",
       "  'This is a static website that serves as a reverse dictionary for Pokemon. It is built using Python Flask as a web app. The setup instructions are provided for running the website with or without Docker. The project was solely developed by Kaitlyn Chan.'],\n",
       " 'https://github.com/zaxiang/Spam_Filter': ['# Weakly Supervised Spam-Label Classification\\nDSC180 Quarter 2 Capstone Project \\n\\nUsing a list of categories and words that represent these categories, we classify harmful spam messages into categories such as insurance scams, medical sales, software sales, and more. Doing so, we hope to alleviate the burden on non technical people in todays world as spammers continue to get by detection systems - we want to find and highlight a pattern throughout them all. Leveraging models ranging from simple methods like TFIDF to complex large language models such as ConWea with BERT, we examine the differences between these models and if it is worth using such big, computation costly models.\\n\\nYou can see more details about our project on our [website](https://gbirch11.github.io/SpamLabelClassifier/).\\n\\n# Data\\nThe data is available on [Google Drive](https://drive.google.com/drive/folders/1uTRzRPkom6nUtRB2D4pOi8uOpSpqst7m?usp=share_link)\\\\\\nPlease unzip and place the files into the following locations; \\\\\\nAnnotated Spam Messages -> ```data/raw/spam/Annotated/``` \\\\\\nUnannotated Spam Messages -> ```data/raw/spam/Unannotated/``` \\\\\\nNon-spam (Ham) Messages -> ```data/raw/ham/``` \\n\\n\\nThe dataset should contain the following files:\\n1) Annotated Spam Messages \\\\\\n  ex) ```data/raw/spam/Annotated/medical-sales/xyz.txt```\\n    * Where xyz is any file name that was annotated to be medical sale spam\\n    * Other folders follow same pattern for each category\\n2) Non-spam (ham) Messages \\\\\\n  ex) ```data/raw/ham/xyz.txt```\\n3) Seedwords JSON file \\\\\\n  ex) ```data/out/seedwords.json```\\n\\n## Running the Project\\n**DSMLP Command**\\n``` \\nlaunch.sh -i gbirch11/dsc180b [-m d] [-g 1]\\n```\\nNote: -m is an optional argument to include more RAM on the machine; HIGLHLY RECOMMEND setting $d$ to 16 or 32 for faster processing \\\\\\nAlso highly recommended to run with -g 1, especially if running ConWea model.\\n``` \\nlaunch.sh -i gbirch11/dsc180b -m 32 -g 1\\n```\\n<br> <br>\\nTo run this project, execute the following command;\\n```\\npython run.py [test | data]\\n```\\nNote: If running ```python run.py test``` \\\\\\nVery simple set of test data will be used to produce results. \\\\\\nResult trend not consistent with running on full dataset.\\n\\nIf running ```python run.py data```: \\\\\\nWhole dataset will be used to produce results.\\n\\nExample commands include: \\\\\\n``` python run.py test ``` \\\\\\n``` python run.py data ```\\n\\nNote: The above commands only run on the TF-IDF, Word2Vec, and FastText models. To run our best model, ConWea, see the section below.\\n\\n## Running the ConWea Model\\nSince ConWea is a huge model using BERT, we have separated this model into the following separate commands;\\n1) Navigate to the ConWea model directory using \\\\\\n``` cd src/models/ConWea ``` <br> <br>\\n2) To contextualize the corpus and seed words run \\\\\\na) For testing: ``` python contextualize.py --dataset_path \"../../../test/testdata/\" --temp_dir \"temp/\" --gpu_id 0 ``` \\\\\\nb) For full data: ``` python contextualize.py --dataset_path \"../../../data/raw/spam/Annotated/\" --temp_dir \"temp/\" --gpu_id 0 ```  <br> <br>\\n3) To train model + observe results run \\\\\\na) For testing: ``` python train.py --dataset_path \"../../../test/testdata/\" --gpu_id 0 ``` \\\\\\nb) For full data: ``` python train.py --dataset_path \"../../../data/raw/spam/Annotated/\" --gpu_id 0 ```  <br> <br>\\n\\nNote: Be warned that running ConWea on the full dataset will ~ 3 hours to run. Running ConWea on test data runs in ~ 20 minutes. <br>\\nNote: ConWea trains using multiple layers and tons of epochs, since our test data is small it is safe to interrupt the terminal (CTRL+C) after first iteration has occured. The layers are kept for consistency for full datasets.\\n',\n",
       "  'The project aims to classify harmful spam messages into categories such as insurance scams, medical sales, and software sales. The team explores different models, from simple methods like TFIDF to complex large language models like ConWea with BERT, to determine their effectiveness and computational cost. The data for the project can be found on Google Drive, and the project can be run using the provided commands. Running the ConWea model requires separate commands for contextualizing the corpus and seed words, as well as training the model and observing results.'],\n",
       " 'https://github.com/ym820/foreground_window_forcast': ['# Intel Capstone: Improving App Launch Time with Deep Learning\\nAuthors: Yikai(Mike) Mao, Alan Zhang, Mandy Lee \\\\\\nWebsite: https://ym820.github.io/foreground_window_forcast/\\n\\n## Abstract\\nApplication launch time is a crucial element of the user experience. Long wait times can cause frustration and prompt users to upgrade to more powerful machines, resulting in increased electronic waste (e-waste) at landfills. Improving app launch time is vital in reducing e-waste by extending the average lifespan of computers. While software has become more resource efficient, it is still challenging to prevent large programs from being bloated and slow to run. In this paper, we propose utilizing neural networks to analyze system usage reports and pre-launch applications in the background before the user needs them. This approach can be universally applied to all computers, making it more economically viable than asking all developers to optimize their applications. We developed our data collection software to minimize resource usage and to identify applications that the system can pre-launch using Hidden Markov Model (HMM) and Long Short-Term Memory (LSTM) models.\\n\\n## Prerequisites\\n\\n> __Below we assume the working directory is the repository root.__\\n\\n### Install dependencies\\n- Using docker\\\\\\nYou may pull the docker image from `mikem820/intel_capstone:latest` and then clone this github repo\\n- Using pip3\\n\\n  ```sh\\n  # Install the dependencies\\n  pip3 install -r requirements.txt\\n  ```\\n\\n## Run \"test\" code\\nYou can use the below command to run the \"test\" code with a sample of our collected dataset. You **must** pass two arguments. The first is `test` or `all`, indicating whether to run test code or not. The second argument is to choose the model (must be either `hmm` or `lstm`) which corresond to the task we will explain in the later section. \\n```\\npython3 run.py test hmm/lstm\\n```\\nYou can also run the full pipeline with the entire dataset by the following command, with the default hyperparameters we implemented.\\n```\\npython3 run.py all hmm/lstm\\n```\\nIf you want to explore different sets of hyperparameters, we explained our tasks and specific instructions to run the scripts in the following section.\\n\\n## Task 1: Next-App Prediction with Hidden Markov Model (HMM)\\nIn this task, our goal is to predict the next application the user will use based on the previous usage data.\\n### Run\\n```\\ncd src/model/HMM\\npython3 run.py\\n```\\n### Arguments\\n\\n| Parameter                 | Default       | Description   |\\t\\n| :------------------------ |:-------------:| :-------------|\\n| -ts --test_size \\t       |\\t0.2\\t            |Test set size (percentage of entire dataset)\\n| -t --top  \\t\\t       | 1\\t           | Number of executables to predict for each data point\\n| -ex  --experiment \\t        | 1           | The experiment number\\n\\n### Notes\\nAfter running, there will be a folder created at `outputs` and named as \"HMM_expt_`experiment`\". Then, the parameters, transition matrix, model accuracy, and visualization will be stored in the folder.\\n\\n## Task 2: App Duration Prediction with Long Short-Term Memory (LSTM)\\nAs for the primary objective for our project, we aim to forecast the amount of time (in seconds) an individual will spend on a specific application within a specific hour. \\n### Run\\n```\\ncd src/model/LSTM\\npython3 run.py\\n```\\n### Arguments\\n\\n| Parameter                 | Default       | Description   |\\t\\n| :------------------------ |:-------------:| :-------------|\\n| -exe --exe_name\\t       |\\tfirefox.exe          |The executable name to predict\\n| -lb --lookback          | 5           |Lookback window (hyper-parameter) for dataset processing\\n| -ts --test_size \\t       |\\t0.2\\t            |Test set size (percentage of entire dataset)\\n| -e --epochs \\t       |\\t100\\t            |Number of epochs\\n| -lr --learning_rate  \\t\\t       | 0.001\\t           | Learning rate\\n| -l --loss \\t\\t           | mse             | Loss function\\n| -ex  --experiment \\t        | 1           | The experiment number\\n| -r  --random\\t        | False           | Whether to choose the start index for test set randomly\\n\\n### Notes\\nAfter running, there will be a folder created at `outputs` and named as \"LSTM_expt_`experiment`\". Then, the parameters, trained model, Keras training history, loss plot, and prediction plot will be stored in the folder.\\n\\n## Mentors\\nWe would like to express our gratitude to all the mentors at the Intel DCA & Telemetry team who provided invaluable guidance and support throughout this project. Special thanks to\\n- Bijan Arbab (Intel)\\n- Jamel Tayeb (Intel)\\n- Sruti Sahani (Intel)\\n- Oumaima Makhlouk (Intel)\\n- Teresa Rexin (UCSD)\\n- Praveen Polasam (Intel)\\n- Chansik Im (Intel)\\n',\n",
       "  'This paper discusses the use of deep learning to improve app launch time. The authors propose using neural networks to analyze system usage reports and pre-launch applications in the background before the user needs them. This approach can help reduce electronic waste by extending the lifespan of computers. The paper provides instructions for running test code for two tasks: next-app prediction with Hidden Markov Model (HMM) and app duration prediction with Long Short-Term Memory (LSTM). The authors express gratitude to the mentors who supported their project.'],\n",
       " 'https://github.com/miloncl/System-Usage-Analysis': ['# Intel & UCSD HDSI -- Data Science Capstone Project -- 2022-2023\\n\\n## Introduction\\n- Hello everyone, we are Thy Nguyen, Milon Chakkalakal, and Pranav Thaenraj from UC San Diego\\n- Our advisors are Jamel Tayeb, Bijan Arbab, Scruti Sahani, Oumaima Makhlouk, Praveen Polasam, and Chansik Im from Intel\\n- This is our Github repo including all the source codes and data files to do data collection and analysis for our capstone project _\"Discover User-App Interactions and Solutions to Reducing the Initial User-CPU Latency\"_\\n\\n## Overview\\n- We try to closely follow the template for Data Science projects by <a href=\"https://drivendata.github.io/cookiecutter-data-science/\">Cookie Cutter Data Sciece </a>\\n- Please check out the below template to understand how to navigate our repo\\n```\\nProject\\n├── .gitignore         <- Files to keep out of version control (e.g. data/binaries).\\n├── run.py             <- run.py with calls to functions in src.\\n├── README.md          <- The top-level README for developers using this project.\\n├── data\\n│   ├── temp           <- Intermediate data that has been transformed.\\n│   ├── out            <- The final, canonical data sets for modeling.\\n│   └── raw            <- The original, immutable data dump.\\n├── notebooks          <- Jupyter notebooks (presentation only).\\n|   ├── Process and EDA.ipynb\\n|   ├── HMM.ipynb\\n|   └── LSTM_RNN.ipynb\\n├── references         <- Data dictionaries, explanatory materials.\\n|   ├── data_dictionaries\\n|   |   ├── dataframe_description.txt\\n|   |   └── schema.txt\\n|   ├── weekly_presentation\\n|   |   ├── [DSC 180B] - Quarter 2 Week 2.pdf\\n|   |   ├── [DSC 180B] - Quarter 2 Week 3.pdf\\n|   |   ├── [DSC 180B] - Quarter 2 Week 4.pdf\\n|   |   ├── [DSC 180B] - Quarter 2 Week 5.pdf\\n|   |   └── [DSC 180B] - Quarter 2 Week 6.pdf\\n|   └── poster.pdf\\n├── requirements.txt   <- For reproducing the analysis environment, \\n├── src                <- Source code for use in this project.\\n│   ├── data           <- Scripts to download or generate data.\\n│   │   ├── make_dataset.py\\n│   │   └── foreground\\n|   |       ├── foreground.c\\n|   |       └── foreground.h \\n│   ├── features       <- Scripts to turn raw data into features for modeling.\\n│   │   └── build_features.py\\n│   ├── models         <- Scripts to train models and make predictions.\\n│   │   ├── hmm_model.py\\n│   │   └── lstm_model.py\\n│   └── visualization  <- Scripts to create exploratory and results-oriented viz.\\n│       └── visualize.py\\n├── outputs \\n|   └── HMM           <- HMM model results (LSTM/RNN model results are inside the notebook)\\n│       └── emission_mt_user1.txt\\n|       ├── emission_mt_user2.txt\\n|       ├── transition_mt_user1_top15apps.txt\\n|       ├── transition_mt_user1_top1app.txt\\n|       ├── transition_mt_user2_top15apps.txt\\n|       └── transition_mt_user2_top1app.txt\\n└── config\\n    ├── data-params.json <- Save the inputs for the function calls\\n    └── submission.json <- GitHub repo and Docker image links\\n\\n```\\n\\n## Instruction to Run the code\\n- For the Methodology Staff\\n    \\n    On DSMLP,\\n    1. Cloning our GitHub repository.\\n    2. Launching a container with your Docker image.\\n    3. Running ```python run.py test```.\\n\\n## Specific Links to Presentations and Source Code\\n\\n### Week 1: \\n- Introduction\\n### Week 2:\\n- Presentation: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20-%20Quarter%202%20Week%202.pdf\">Evaluate Data Quality and Conduct EDA</a>\\n- Source Code: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/Process%20and%20EDA.ipynb\">Process_and_EDA.ipynb</a>\\n\\n### Week 3:\\n- Presentation: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20-%20Quarter%202%20Week%203.pdf\">HMM: Transition Matrix, Model Accuracy, and Emission Matrix</a>\\n- Source Code: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/HMM.ipynb\">HMM.ipynb</a>,  <a href=https://github.com/miloncl/System-Usage-Analysis/blob/main/src/models/hmm_model.py>hmm_model.py</a>\\n- Outputs: <a href=\"https://github.com/miloncl/System-Usage-Analysis/tree/main/outputs/HMM\">HMM outfiles</a>\\n\\n### Week 4:\\n- Presentation: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20-%20Quarter%202%20Week%204.pdf\">Study LSTM: Data Prep, Data Viz, Research on RNN/LSTM</a>\\n- Source Code: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/Process%20and%20EDA.ipynb\"> Process_and_EDA.ipynb</a>\\n\\n### Week 5:\\n- Presentation: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20-%20Quarter%202%20Week%205.pdf\">RNN (Vanilla + LSTM)</a>\\n- Source Code: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/LSTM_RNN.ipynb\">LSTM_RNN.ipynb</a>,  <a href=https://github.com/miloncl/System-Usage-Analysis/blob/main/src/models/lstm_model.py>lstm_model.py</a>\\n\\n### Week 6:\\n- Presentation: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/references/weekly_presentation/%5BDSC%20180B%5D%20%20-%20Quarter%202%20Week%206.pdf\">LSTM Experiments</a>\\n- Source Code: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/notebooks/LSTM_RNN.ipynb\">LSTM_RNN.ipynb</a>, <a href=https://github.com/miloncl/System-Usage-Analysis/blob/main/src/models/lstm_model.py>lstm_model.py</a>\\n\\n### Week 7-9:\\n- Project Poster: <a href=\"https://github.com/miloncl/System-Usage-Analysis/blob/main/references/poster.pdf\">Poster</a>\\n- Practice presentation and elevator pitch in class\\n',\n",
       "  \"This is a summary of the Intel & UCSD HDSI Data Science Capstone Project for 2022-2023. The project aims to discover user-app interactions and solutions to reducing the initial user-CPU latency. The project team consists of Thy Nguyen, Milon Chakkalakal, and Pranav Thaenraj from UC San Diego, with advisors from Intel. The project's GitHub repository includes all the source codes and data files for data collection and analysis. The repository follows the template for Data Science projects by Cookie Cutter Data Science. The code can be run by cloning the GitHub repository, launching a container with a Docker image, and running `python run.py test`. The summary also provides specific links to presentations and source code for each week of the project.\"],\n",
       " 'https://github.com/KeaganBenson/DSC180Flock': ['# DSC180Flock\\n\\nThis project\\'s model comprises of 3 sub-models (predicting average, amount, and standard deviation) that are used for an algorithm at the end\\nOn Command Prompt,for the first time, run the following to clone the repo for the first time:\\n```\\ngit clone https://github.com/KeaganBenson/DSC180Flock.git\\n```\\nThen open Anaconda Prompt, for the first time, enter the new folder, and run the following to create a new conda environment with the requirements.txt. \\n```\\ncd dsc180flock\\nconda create --name flock_env --file requirements.txt\\nconda activate flock_env\\npython run.py all\\n```\\nWhile python run.py all is being ran, plots and maps may open up on seperate windows during the execution, and the execution pauses until those windows are closed. After the execution is complete, observe the metrics printed in the console and close the anaconda\\nNow that the repo is cloned locally and the environment is created, anytime you want to run the model again, you open anaconda prompt and run:\\n```\\ncd dsc180flock\\nconda activate flock_env\\npython run.py all\\n```\\n\\n\\nTargets supported:\\n* **data** - performs the ETL that extracts data from online, and fills the empty data/raw folder with the raw data\\n* **features** - performs the data-cleaning and feature engineering for the intermediate data in data/temp folder and final data in data/out folder\\n* **model** - trains the data on the final data made by the features target, and outputs prediction accuracy metrics and confusion matrix from the validation data\\n* **clear** - empties the data folders raw, temp, and out\\n* **all** - complete cycle of ETL, data preparation (cleaning, feature engineering), training, and prediction. Equivalent of \"python run.py clear data features model\". \\n* **test-data**: all subsequent arguments will be using only the test-data folder, not the data folder.\\n* **test** - complete cycle but only on the \"test data\". Equivalent of \"python run.py test-data clear feature model\"\\n\\n\\n\\n',\n",
       "  'The project\\'s model consists of three sub-models used for an algorithm. To get started, clone the repository using the provided command. Then, create a new conda environment and activate it. Run \"python run.py all\" to execute the model. During execution, plots and maps may open in separate windows. After completion, observe the metrics printed in the console and close Anaconda. To run the model again, open Anaconda prompt and use the command \"python run.py all\". There are different targets supported such as data, features, model, clear, all, test-data, and test.'],\n",
       " 'https://github.com/taekunkim/flock-freight': ['\\n\\nData:\\nThis repo uses Orders and Offers data provided by FlockFreight.\\nThe threeDigitZipCode.json file comes from https://github.com/billfienberg/zip3.\\nCanadian postal codes are from postalcodes_ca, a Python library.',\n",
       "  'This summary mentions the data sources used in a repository, including Orders and Offers data from FlockFreight, a zip code file from GitHub, and Canadian postal codes from a Python library.'],\n",
       " 'https://github.com/ESR76/Capstone-Brick-Modeling': ['# Energy Cost and HVAC Optimization in Smart Buildings\\n\\nIn this project, we attempt to make predictions about future energy usage and cost in a building using energy data collected from UC San Diego\\'s EBU-3B (the Computer Science & Engineering) building\\'s HVAC system.\\n\\nWe also have a public-facing [poster](https://www.canva.com/design/DAFZKQlLOLo/2ALw0oHRO8qrPj--Q-8huw/view?utm_content=DAFZKQlLOLo&utm_campaign=designshare&utm_medium=link&utm_source=publishsharelink) from our poster session and [website](https://xenonition.github.io/) associated with this project.\\n\\n## Running the Code\\nTo test our code, please first clone the repository. \\n\\nIf you\\'re concerned that you won\\'t have the proper packages to run something in our code, use the [Docker image](https://hub.docker.com/repository/docker/esr76/capstone-brick-modeling/general) associated with this project - we recommend using the **\"final\"** tag (the \"latest\" tag should also work). It is designed to be run on UCSD\\'s JupyterHub service using the base UCSD notebook as a base, for which instructions to use it can be found [here](https://github.com/ucsd-ets/datahub-example-notebook).\\n\\nAnother option is running \"pip install -r requirements.txt\".\\n\\n- To run the modeling pipeline on the original data and compare to our paper/poster run the line:\\n    - \"python3 run.py all\"\\n    **OR** \\n    - \"python3 run.py data features model optimize visualize\"\\n\\nIf you try to run a later part of the pipeline (ie. model) before running the earlier parts (ie. data, features), this **will** raise an error.\\n\\n- To use a smaller set of test data and test the output for the pipeline run:\\n    - \"python3 run.py test\"\\n\\nResults from running this line will appear in the /test directory, with data in /test/testdata and visualizations in /test/testviz.\\n\\n- To remove any files created by running the script, please run:\\n    - \"python3 run.py clean\"\\n\\nAfter any call of run.py, the script will run through the steps called, creating files/file organization as necessary, and then will print which steps it took in the order it took them in.\\n\\n***NOTE:***\\nOnce you\\'ve run one of the stages in the main training pipeline or run whole the pipeline before, calling it again will SKIP regenerating the files and print what was skipped. Running the \"clean\" keyword is the only way to ensure that the files will regenerate. The test pipeline will rerun each time the \"test\" keyword is called.\\n\\n## Data Notes\\n\\n### Getting the Data\\nIdeally in our pipeline, data would be obtained by pairing sensor data with mappings from our building\\'s [Brick Schema](https://brickschema.org/) in order to query the locations and floors for relevant sensors to perform our calculation, then using UCSD\\'s Brick server.\\n\\nHowever, we were not able to obtain access to the Brick server in the time we had for the project, so we used data from [a data pull from a previous project](https://github.com/HYDesmondLiu/B2RL/tree/master/real_building_buffers). This represents 15 rooms worth of data on floors 2, 3, and 4 of UC San Diego\\'s EBU-3B (Computer Science) building with data from July 2017 to early January 2019. This data should download automatically when the data part of the pipeline runs - the code should also generate several directories for you, including: data and its subdirectories, test and its subdirectories, and visualizations.\\n\\nThe features part of the data pipeline is split into two parts that perform data cleaning steps that are detailed more on our website, but essentially:\\n1. We separate our data into training and testing sets based on dates in the original datasets, where approximately 70% of the data is before August 1, 2018 and the rest is August 1, 2018 and onwards.\\n2. We floor timestamps in the dataset to the nearest hour and use medians to aggregate into buckets of that time, since the energy values range because of the 15 unidentified rooms in the dataset.\\n3. We impute the training dataset with data based on the median for the value at that hour - this was the most stable trend that we found in the original data. We do not impute the testing dataset because we want to ensure that we are not evaluating the model on predictions of false values.\\n\\n### Other Data and Goals\\nAlong with predicting future energy usage using the energy values from the data pull above, we will be using data from UCSD\\'s pricing plan to scale this for energy. We will only be scaling our data by a constant (derived from UCSD\\'s cost of electricity in fiscal years 2017-2018/2018-2019), although we understand that with UCSD using both its own energy and energy from San Diego Gas and Electric, this likely leads to an underestimation.\\n\\nWhile we don\\'t use it in the final version of our model, we also have the EBU 3B Turtle file (ie. the building\\'s representation in Brick) in our raw data to understand relationships between components of the HVAC system in the building. If you\\'d like to take a look at this, here\\'s the [link](https://brickschema.org/ttl/ebu3b_brick.ttl). This will not auto-download for you.\\n\\nWe also initially pulled other temperature and climate information to use in this project from [NOAA (the National Oceanic and Atmospheric Administration)](https://www.noaa.gov/) and the [EIA (U.S. Energy Information Administration)](https://www.eia.gov/), but we did not end up having time to incorporate this data in our final model.\\n\\n## Credits\\n\\n### Authors\\nWe are four undergraduate Data Science students in our final year at UC San Diego.\\n\\nIf you\\'re interested in our work, here\\'s where you can find more about us:\\n| Name | GitHub | LinkedIn |\\n| ---- | ---- | ---- |\\n| **Jonah Bomwell** | [Link](https://github.com/Jbomwell) | [Link](https://www.linkedin.com/in/jonah-bomwell-0756191b7/) | \\n| **Alise Bruevich** | [Link](https://github.com/alisebruevich) | [Link](https://www.linkedin.com/in/alisebruevich/) |\\n| **William Nathan** | [Link](https://github.com/Xenonition) | [Link](https://www.linkedin.com/in/william-nathan-5019661b2/) |\\n| **Esperanza Rozas** | [Link](https://github.com/ESR76) | [Link](https://www.linkedin.com/in/esperanza-r/) |\\n\\n\\n### Acknowledgments\\nThis project was completed as a capstone project for the Data Science major at UC San Diego in Winter of 2023.\\nFor more information on the course, please read about the class [here](https://dsc-capstone.github.io/).\\n\\nWe\\'d also like to thank: \\n- Rajesh Gupta, our mentor and one of the creators of the Brick Schema.\\n- Xiaohan Fu and Hsin-Yu Liu, who provided us with the data we used and additional mentoring.\\n- Keaton Chia and the DERConnect team, who helped us generate ideas for this project, provided us with UCSD\\'s cost model, and discussed the possibilities of using Brick for future work in this area with us.\\n- Suraj Rampure, our instructor for the course.',\n",
       "  \"This project focuses on energy cost and HVAC optimization in smart buildings. The goal is to predict future energy usage and cost using data collected from UC San Diego's EBU-3B building's HVAC system. The project includes a poster session and a website. The code can be run by cloning the repository or using a Docker image. The data used in the project was obtained from a previous project due to limited access to the Brick server. The data pipeline involves cleaning and aggregating the data. The model also incorporates UCSD's pricing plan for scaling the energy data. The authors of the project are undergraduate Data Science students at UC San Diego, and the project was completed as a capstone project for the Data Science major. Acknowledgments are given to mentors, contributors, and instructors involved in the project.\"],\n",
       " 'https://github.com/SamuelBAguirre/DSC180A_Project': ['This is a repo for our DSC180A Q1 project. In this project we explore measuring the change in surface water over time for Lake Oroville.\\nPlease download image data from https://drive.google.com/drive/folders/1b5-gGvu5K4WNVqeRQ1BLvdQ5DwA2KkQg?usp=share_link, make sure to unzip file and place images directory inside of ./data/\\n\\nA quick breakdown of the structure of this repository:\\n\\nconfig: Contains config files for params used in our notebook  \\nnotebooks: Contains our data exploration notebooks  \\nsrc: Contains our source code  \\ntest: Contains some of our testing data  \\nout: Contains the processed images outputted from our source code\\n\\nWebsite: https://samuelbaguirre.github.io/\\n',\n",
       "  \"This is a repository for the DSC180A Q1 project, which focuses on measuring the change in surface water over time for Lake Oroville. The repository includes image data that can be downloaded from a Google Drive link provided. The repository is structured with folders for config files, notebooks, source code, testing data, and processed images. The project's website can be found at https://samuelbaguirre.github.io/.\"],\n",
       " 'https://github.com/alexmak001/SAR-satelite-image-ship-detection': ['# Maritime Ship Detection Using Synthetic Aperture Radar Satellite Imagery\\nSatellites are being launched into space at an exponential rate and are able to produce high quality images in relatively short intervals of time on any part of Earth. The amount of data and types of it are also increasing significantly and in this paper we specifically use Synthetic Aperture Radar (SAR) satellite imagery in order to detect ships traveling through bodies of water. We created a ship counting tool that intakes a start date, end date, and an area of interest and returns the number of ships for each day between the two dates. The images are first classified into offshore or inshore and a separate object detection algorithm counts the number of ships per image. The classifier and object detection networks are trained using the Large-Scale SAR Ship Detection Dataset-v1.0 (LS-SSDD-v1.0) and deployed on Google Earth Engine.\\n\\n## Testing:\\nWhen running on DSMLP, be sure to use use the launch script\\n`launch-scipy-ml.sh -g 1 -i snng/sar_ship_detection` to launch a pod with a GPU. Otherwise the script will fail to run. \\n\\nTo run the test, simply run python run.py test\\n\\n## run.py file\\nUsing the \"data\" target, it downaloads and formats the dataset locally. This also downloads the models as well. \\nThe \"train_ret\" target will start the training of the RetinaNet model, which requires the data to be loaded. Similarly, the \"train_faster\" will begin to train the Faster R-CNN model. The hyperparameters can be configured in the src/models/ folder for each of the models.\\nThe \"predict\" target uses the model to predict on all of the test data and returns the key metrics for both models. The \"viz\" target causes the models to predict the bounding boxes on the tif files saved in the src/visualization folder. It then saves the resulting image with bounding boxes in the same folder as a jpg.\\n\\n## HOW TO RUN THE SHIP COUNTING SCRIPT:\\n[//]: <> (Have to figure out what to do about json key)\\n1. Once you have activated your DSMLP environment using the launch script above, please ensure you have the private json key downloaded. This will be used for initializing and authenticating Google Earth Engine. [website](https://developers.google.com/earth-engine/guides/service_account)\\n2. We will have to get the coordinates for the desired area of interest from the Google Earth Engine [website](https://code.earthengine.google.com/).\\n    - Note: If you do not have a Google Earth Engine account you will have to make one.\\n    - To get the coordinates, navigate on the map to your desired place of interest\\n    - Then draw a bounding box over your area of interest using the shape tool. (The shape tool is button with the gray square underneath the scripts panel. \\n    ![tut1](https://user-images.githubusercontent.com/69220036/221438416-ca8513ea-412e-43c6-8a8e-5b87e30ac128.png)\\n    ![tut2](https://user-images.githubusercontent.com/69220036/221438475-eac5c729-4478-46bd-8691-88648845255a.png)\\n    - Once you get the desired vertices, the middle panel which is usually labeled new script will have a geometry variable and you expand that until you get the list of 5 vertices and those are the place coordinate values to pass into image downloader function.\\n  ![tut3](https://user-images.githubusercontent.com/69220036/221438515-9acf67df-450b-4f66-b4a7-deed39eb1013.png)\\n3. Once you have obtained your coordinates, run this command in terminal to start counting ships.\\n[//]: <> (Might have to change this command depending on how we implement shipcounter.py.)\\n\\n`python -c from shipcounter.py import shipcounter(place_coords, start_date, end_date, del_images)`\\n\\nwhere:\\n- `place_coords` are the coordinates from Google Earth Engine\\n- `start_date` is the desired start date in the format \\'MM/DD/YYYY\\'\\n- `end_date` is the desired end date in the format \\'MM/DD/YYYY\\'\\n- `del_images` set to `True` if you want to delete the images locally afterwards, `False` if not.\\n',\n",
       "  'This paper discusses the use of Synthetic Aperture Radar (SAR) satellite imagery to detect ships in bodies of water. The authors created a ship counting tool that takes a start date, end date, and area of interest as input and returns the number of ships for each day between the two dates. The tool uses a classifier to classify images into offshore or inshore, and an object detection algorithm to count the number of ships per image. The classifier and object detection networks are trained using the Large-Scale SAR Ship Detection Dataset-v1.0 (LS-SSDD-v1.0) and deployed on Google Earth Engine. The paper also provides instructions on how to run the ship counting script using the obtained coordinates from Google Earth Engine.'],\n",
       " 'https://github.com/rtvo20/dsc180_capstone_q2': ['# DSC 180 Capstone Project\\n\\n*Most of this description was from the README/documentation located on our previous repo, with some slight changes. The repo can be found here* [here](https://github.com/rtvo20/dsc180_quarter1_submission).\\n\\nThis project uses data from David Fenning\\'s Solar Energy Innovation Laboratory (SOLEIL) that creates solar cell samples. The data they collect are information from the manufacturing process of these solar cells, along with data they collect when testing the samples. Our project cleans and transforms the data, which are originally in JSON format, before saving them as CSVs that allow it to be imported and graphed in Neo4j, a Graph DBMS. The purpose of this task is to have a pipeline that can organize and transform the data so that it can be graphed in Neo4j and can be queried.\\n\\nInput data are JSON files containing information from the SOLEIL lab in the form of a worklist. Our functions extract data from these worklists, such as step names (e.g. \\'drop\\', \\'spin\\', \\'hotplate\\'), chemical names, and output data (measurements and tests done on the resulting sample). Running run.py on the input data gives us the output data file, which can be used to generate our output, a graph representation of the data.\\n\\nThe output data (CSVs) after they are cleaned and transformed are run through a function that generates a script file with queries in Neo4j\\'s query language, Cypher. The script (a .cypher file) automates the process to graph the data using Neo4j\\'s Cypher shell terminal. To run the script, we use a Docker container running Neo4j, which requires us to copy this \\'output.cypher\\' file into the docker container\\'s root directory before we can run it with a command. All of the instructions to do so are located in the section below.\\n\\nThis current iteration includes test data under test/testdata, which is one sample from some actual data to show how our code works on \"barebones\" test data.\\n\\nMore detailed instructions are below, however an overview of the process to reproduce our results:\\n\\nrun python run.py test to generate a script file with queries to generate a graph\\'s nodes and links.\\nUsing Docker, pull the latest Neo4j Docker image and start a container with this image.\\nCopy the script file from DSMLP to the local setting, then copy it to the docker container\\'s root directory\\nOpen a terminal in the docker container and run the script file and produce the results.\\n\\nMore detailed instructions are below, however an overview of the process to reproduce our results:\\n1. run ```python run.py test``` to generate a script file with queries to generate a graph\\'s nodes and links.\\n2. Using Docker, pull the latest Neo4j Docker image and start a container with this image.\\n   * Copy the script file from DSMLP to the local setting, then copy it to the docker container\\'s root directory\\n   * Open a terminal in the docker container and run the script file and produce the results.\\n\\n## To run the project use run.py and follow the instructions below.\\n\\n* The filepaths to the test data are already coded into ```run.py``` and are under the folder \"test/testdata\".\\n* The available targets for running ```python run.py <target>``` and the order of the targets are:\\n    * ```data```>```features```>```queries```\\n    * Alternatively, running the command ```python run.py test``` is equivalent to running each of the above targets sequentially.\\n* Running ```run.py``` cleans and transforms the data and creates queries in Neo4j\\'s query language (Cypher) that allows for nodes and links to be graphed. Each graph in our implementation currently requires 6 queries to create and link all the nodes, so to help automate the process, the output of ```run.py``` is a Neo4j script-type file (.cypher file) that performs all of these queries in less inputs than doing so manually.\\n  * Our output file is named \"output.cypher\" and will be located in the project\\'s root directory.\\n\\n## To run the script generated by the run.py script above, use Docker\\n\\n* The following docker run command sets up a docker container with all of the necessary flags and config settings. The command is all one line, it should be copied and pasted in its entirety in a local terminal.\\n    * ```docker run -p 7474:7474 -p 7687:7687 -v $PWD/data:/data -v $PWD/plugins:/plugins --name neo4j-apoc -e NEO4J_apoc_export_file_enabled=true -e NEO4J_apoc_import_file_enabled=true -e NEO4J_apoc_import_file_use__neo4j__config=true -e NEO4JLABS_PLUGINS=\\\\[\\\\\"apoc\\\\\"\\\\] neo4j:4.0```\\n    * In short, the flags set up permissions that allow for moving files between the Docker container\\'s storage volume and local storage. It also enables for usage of APOC, a Neo4j package used to help export queried data.\\n    * Wait for the Docker container to initialize and start up, and in an internet browser navigate to `localhost:7474`.\\n    * This opens up Neo4j\\'s browser UI and upon accessing it for the first time, should prompt the user to create a username/password; although it is possible to set it up with no authentication required under the \"Authentication type\" drop-down menu.\\n    * The default username/password is neo4j/neo4j. Once entered, it will then ask for a new password.\\n\\n* The next steps require copying the output of the `run.py` file, \"output.cypher\" and the output CSVs into the Docker container\\'s directory.\\n    * If the output.cypher file is located on DSMLP, it should be downloaded/copied to a local directory first.\\n    * In a local terminal, change directory to the location of the cypher file and CSVs, then run the following command\\n      * ```docker cp output.cypher neo4j-apoc:/var/lib/neo4j/import/output.cypher```\\n    * Then, perform the same process and copy the CSVs into the Docker container.\\n      * ```docker cp b19_sample0_chem.csv neo4j-apoc:/var/lib/neo4j/import/```\\n      * ```docker cp b19_sample0_action.csv neo4j-apoc:/var/lib/neo4j/import/```\\n      * ```docker cp b19_sample0_link.csv neo4j-apoc:/var/lib/neo4j/import/```\\n\\n* With the necessary files copied over to the docker container, go to the docker container and select \"Open in Terminal\", as seen in the image below. \\n\\n![image](https://user-images.githubusercontent.com/59627502/218381794-04ed9f95-5fc9-4102-aa9c-d5c87adcee41.png)\\n\\n  * In this docker terminal, `cd import`\\n  * and again, in the docker terminal, run the following command:\\n    * `cypher-shell -f output.cypher -u neo4j -p test` (replacing your username and password where `neo4j` and `test` are respectively.  \\n  * Back in the browser at `localhost:7474`, `MATCH(n) RETURN n` can be entered in the query field and run to return the nodes and relationships graphed by our output.\\n\\n![image](https://user-images.githubusercontent.com/59627502/218383326-7880d998-0aeb-4b63-8ce6-24aad0ae5f85.png)\\n* This is our result from a graph containing multiple samples, but the test data will contain just 1 sample.\\n\\n## Additional features\\nAt the final stage of our project, we moved from using Neo4j Docker to Neo4j Desktop, as this version has more practical use for the lab team. Our run.py and this README file contain instructions and functionality for Neo4j Docker in order to support reproducibility; however we have included a `preprocessing.ipynb` that contains several features used with Neo4j Desktop that is useful in practice for the lab team. \\nTo briefly describe those features:\\n  * Saving `action`, `chem`, and `link` CSV files along with cypher files to the appropriate folders within the local Neo4j Desktop installation\\n    * CSV files are saved to the `import` folder within the Neo4j Desktop database directory\\n    * Cypher files are saved to the `bin` folder within the Neo4j Desktop database directory \\n  * Version control to keep track of batches of data that have already been processed, to avoid unnecessary redundant processing of batchs.\\n  * Deleting CSV files after they are used to load data into Neo4j to reduce clutter in file storage.\\n',\n",
       "  \"This project uses data from David Fenning's Solar Energy Innovation Laboratory (SOLEIL) to create a pipeline that organizes and transforms the data so that it can be graphed in Neo4j and queried. The input data is in JSON format and contains information about the manufacturing process of solar cells, as well as test data. The project cleans and transforms the data, saves it as CSV files, and generates a script file with queries in Neo4j's query language (Cypher) to graph the data. The script file can be run using Docker to produce the results. The project also includes additional features for using Neo4j Desktop.\"],\n",
       " 'https://github.com/nahmann/DSC180-B16': ['# DSC 180 B16 - Decentralized Location Consensus\\n\\nGroup Members:  \\nNathan Ahmann  \\nMason Chan  \\nAlex Guan  \\nAlan Miyazaki  \\nMentor: Haojian Jin\\n\\n\\nOur Heroku site can be found [here](https://dsc180-decentralized-location.herokuapp.com/)  \\nA static verison of our website can be found [here](https://nahmann.github.io/DSC180-B16/)\\n\\nOur original work from Fall Quarter:  \\n[Video Demo](https://youtu.be/Ixj5MV3JIbA) that shows the server working with GET and POST requests.\\n  \\nOur new video showcasing the final project:  \\n[Video Demo](https://youtu.be/sOopTH0-ghM) that walks through an example scenario.\\n\\n\\n## Content\\n\\nThis repository contains all the files sent to our Heroku server which can be accessed [here](https://dsc180-decentralized-location.herokuapp.com/).  \\n\\nThis main server has the API for the backend and a landing page for the website. In conjunction with B16-2, the app team, an Android device will send get/post requests to this backend which will allow the app to verify the location of users. The verification is done via blacklisting malicious users by having Heroku run `trust_algorithm.py` on a set timer (currently on the hour). This file updates the database for the blacklist on the backend.\\n\\nIf you would like to run this website locally, you can install the necessary packages and run the following code in the terminal. Note that the local development version of the server utilizes sqlite3 as the database since it is easier to locally utilize than Postgres which we use on our Heroku server.\\n\\n`python manage.py makemigrations` - sets up the migrations for the database  \\n`python manage.py migrate` - makes the migration files  \\n`python manage.py runserver` - will run the server which can then be accessed at 127.0.0.1  \\n\\n## Contains parts of code modified from the following tutorials:\\nhttps://docs.djangoproject.com/en/4.1/intro/   \\nhttps://github.com/heroku/python-getting-started  \\nhttps://devcenter.heroku.com/articles/getting-started-with-python  \\nhttps://www.django-rest-framework.org/tutorial/quickstart/   \\nhttps://www.django-rest-framework.org/tutorial/1-serialization/ \\n\\nand our static webpages contain CSS from Bootstrap\\n',\n",
       "  'This is a summary of the project \"Decentralized Location Consensus\" by group members Nathan Ahmann, Mason Chan, Alex Guan, and Alan Miyazaki. The project involves a Heroku site that serves as the backend API and landing page for the website. The site allows an Android app to verify the location of users by sending get/post requests to the backend. The verification process involves blacklisting malicious users using a trust algorithm that updates the database on the backend. The project also includes tutorials and code modifications from various sources.'],\n",
       " 'https://github.com/acanonig/DSC180B-Proxensus-': [\"# DSC 180B Project Code - Proxensus\\nGroup Members:\\nFrans Timothy Juacalla,\\nAndrew Canonigo,\\nMartin Thai,\\nAryaman Sinha,\\n\\nThis code was adapted and modified from the Pre-standard Codebase of the DP3T SDK for Android.\\nhttps://github.com/DP-3T/dp3t-sdk-android/releases/tag/prestandard\\n\\nDP3T SDK\\nhttps://github.com/DP-3T/dp3t-sdk-android\\n\\n### How to run project\\nTo Run the project in Android Studio, Please open the 'calibration-app' instead of the entire project. The project needs at least 3 Android smartphones with working Bluetooth.\\n\\nDemonstration Video: https://youtu.be/stuOTvUJUmk\\n\",\n",
       "  'The Proxensus project code is adapted from the Pre-standard Codebase of the DP3T SDK for Android. It can be found at https://github.com/DP-3T/dp3t-sdk-android/releases/tag/prestandard. The project requires at least 3 Android smartphones with working Bluetooth to run. A demonstration video can be found at https://youtu.be/stuOTvUJUmk.'],\n",
       " 'https://github.com/pnagasam/dsc180a_capstone_project': ['# dsc180a_capstone_project\\n\\nThis program takes care of all data loading and preprocessing, model building and training, and visualizations regarding our DSC 180 Capstone project.\\n\\n## Usage\\nAll functionality is used by running `python3 run.py` followed by some arguments in the root directory. The first argument is `exec_type` which indicates what action you would like to take. Possible values are:\\n```\\ndata     # for data loading\\n\\ntrain    # for model training\\n\\nOT       # for optimal transport\\n\\neval     # for gathering results\\n\\nviz      # for creating visualizations\\n\\nall      # for doing all of the above\\n\\ntest     # for testing all of the above on dummy data\\n         # (except downloading the data)\\n```\\n\\nThe second and final argument is `-c` or `--clean`, which, when included cleans the save directories used by the specified `exec_type`.\\n\\n### All\\nThe `all` function runs the other functions sequentially, so you don\\'t have to run each individual function. It is up to the user to set values in the `config/` files correctly. To run:\\n```bash\\npython3 run.py all\\n```\\n\\nTo remove all models, OT, results, and visualizations generated by the program, run:\\n```bash\\npython3 run.py all -c    # or --clean\\n```\\nNote: running this will NOT remove the 13.1 GB of data downloaded by running the `data` function. This is to prevent headache on behalf of the user. You\\'re welcome.\\n\\n### Data Loading\\nThe dataset is downloaded from the WILDS project using their python package, which is a prerequisite. To install run:\\n```bash\\npip install wilds\\n```\\n\\nRunning the following command in the root project directory will download the dataset to the proper location:\\n```bash\\npython3 run.py data\\n```\\n\\n### Model Training\\nThe model we used for this project was a custom CNN built in pytorch and trained entirely on either urban or rural data from one country. To train a model with settings specified in the `config/train.json`, run:\\n```bash\\npython3 run.py train\\n```\\nTo change which country the model should be trained on, whether an urban/rural model should be trained, the percentile cutoffs the model uses for classification, or other training parameters, please consult the config section below.\\n\\nTo remove all trained models, run:\\n```bash\\npython3 run.py train -c    # or --clean\\n```\\n\\n### Optimal Transport\\nOptimal transport was achieved using the python optimal transport package, which is a prerequisite. To install run:\\n```bash\\npip install ot\\n```\\n\\nIn our implementation, optimal transport is used for domain adaptation. The goal is to adapt the color profiles of one country to another, in the hope that the CNN trained on only one country can more accurately classify images from another country.\\nIn order to fit the optimal transport to transport images from a source country to a target country, first edit the source and target country fields in `config/OT.json`, then run:\\n```bash\\npython3 run.py OT\\n```\\n\\nTo remove all saved OT models, run:\\n```bash\\npython3 run.py OT -c    # or --clean\\n```\\n\\n### Results\\nThe results are gathered and using data, models and OT objects saved from above. This function will error if run without running `data`, `train`, and `OT` first with aligning configuration to generate objects. To run:\\n```bash\\npython3 run.py eval\\n```\\n\\nTo remove all saved results, run:\\n```bash\\npython3 run.py eval -c    # or --clean\\n```\\n\\n### Visulizations\\nSimilarly to results, the `viz` function requires previous data to be present in the directories specified in each `config/` file. Visualizations are generated using matplotlib and pandas, both prerequisites. To run:\\n```bash\\npython3 run viz\\n```\\n\\nTo remove all saved visualizations, run:\\n```bash\\npython3 run.py viz -c    # or --clean\\n```\\n\\n### Test\\nThe `test` function is similar to the `all` function except it uses randomly generated data. A valid `config/` is still required for the program to run entirely. And onjects will still be saved. To run:\\n```bash\\npython3 run.py test -c    # or --clean\\n```\\n\\nRunning `python3 run.py test -c    # or --clean` won\\'t remove anything. To remove all saved objects, run:\\n```bash\\npython3 run.py all -c    # or --clean\\n```\\n\\n## Config\\nBelow is the default `config/` files along with descriptions of each option.\\n\\n### Train\\n`config/train.json`\\n\\n```json\\n{\\n    \"country\": \"nigeria\",  // the country to train the model on (usually same as \"target_country\")\\n    \"train_proportion\": 0.7,  // proportion of data to use to train the model\\n    \"valid_proportion\": 0.2,   // proportion of data to use to validate the model\\n    \"urban\": true,  // whether to train a model on urban data\\n    \"rural\": true,  // whether to train a rural on urban data\\n    \"low_quantile\": 0.3333333,  // the lower percentile used for classification cutoff\\n    \"high_quantile\": 0.6666666,  // the lower percentile used for classification cutoff\\n    \"n_epochs\": 300,   // number of epochs to train the model on (best one will be used for evaluation)\\n    \"batch_size\": 48,  // batch size to use\\n    \"save_path\": \"models/\",  // path to save models at\\n    \"random_seed\": 10  // random seed to use for training\\n}\\n```\\n\\n### OT\\n`config/OT.json`\\n\\n```json\\n{\\n    \"target_country\": \"nigeria\",  // country to transport to\\n    \"source_country\": \"mali\",  // country to transport from\\n    \"n_samples\": 500,  // number of pixel samples to take from each country\\n    \"reg\": 0.1,  // sinkhorn\\'s regularization parameter\\n    \"batch_size\": 10,  // batch size to use for OT\\n    \"save_path\": \"OT/\",  // path to save OT objects to\\n    \"random_seed\": 10  // random seed to use for OT\\n}\\n```\\n\\n### Results\\n`config/eval.json`\\n\\n```json\\n{\\n    \"target_country\": \"nigeria\", // OT \"target_country\" (usually the same as \"country\" in config/train.json)\\n    \"source_country\": \"mali\",  // OT \"source_country\"\\n    \"urban\": true,  // whether to evaluate results on urban model\\n    \"rural\": true,  // whether to evaluate results on rural model\\n    \"batch_size\": 10,  // batch size to use when evaluating\\n    \"save_path\": \"results/\",  // path to save results at\\n    \"random_seed\": 76  // random seed to use for evaluation\\n}\\n```\\n\\n### Visualizations\\n`config/viz.json`\\n\\n```json\\n{\\n    \"asset_index_dist\": true,  // whether to show asset index distribution visualization\\n    \"clf_cutoffs\": true,  // whether to show classification cutoff visualization\\n    \"target_country\": \"nigeria\",  // OT \"target_country\" and train \"country\"\\n    \"source_country\": \"mali\",  // OT \"source_country\"\\n    \"low_quantile\": 0.33333,  // the lower percentile used for classification cutoff\\n    \"high_quantile\": 0.66666,  // the lower percentile used for classification cutoff\\n    \"training_info\": {\\n        \"urban\": true,  // whether to show training info for urban model\\n        \"rural\": true  // whether to show training info for rural model\\n    },\\n    \"source_confusion_matrix\": {\\n        \"urban\": {\\n            \"without_OT\": true,  // whether to show confusion matrix for urban model without OT\\n            \"with_OT\": true  // whether to show confusion matrix for urban model with OT\\n            \\n        },\\n        \"rural\": {\\n            \"without_OT\": true,  // whether to show confusion matrix for rural model without OT\\n            \"with_OT\": true  // whether to show confusion matrix for rural model with OT\\n        }\\n    },\\n    \"show_changed\": true,  // whether to an example of where optimal transport changed the models prediction\\n    \"save_path\": \"viz/\", // path to save visualizations to\\n    \"random_seed\": 53 // random seed to use for visualizations\\n}\\n```\\n',\n",
       "  'This program is designed for the DSC 180 Capstone project. It handles data loading, preprocessing, model building and training, and visualizations. The program can be run with different arguments to perform specific actions such as data loading, model training, optimal transport, result gathering, and visualization creation. The program also provides the option to clean the save directories used by each action. The configuration files specify various options for each action, such as the country to train the model on, the proportion of data to use for training and validation, whether to train on urban or rural data, classification cutoffs, number of epochs, batch size, save paths for models and results, and random seeds.'],\n",
       " 'https://github.com/BillChen24/DSC180B-Project-B319-2': ['# DSC180B-Project-B319-2\\n# Domain Adaptation of CNN in Animal Classification Task\\n\\n### For Test Trails:\\nAfter downloading the githubt repo (including the sample data in data folder), run the following code in terminal\\n```\\npython run.py test\\n```\\nThis code will train a custom CNN model on the sample data and generate a loss curve over epochs.\\n\\nThe loss plot will be saved in the path printed at the end of the execution.\\n\\nThe trained model will be saved in the path printed at the end of the execution.\\n\\nCode will create a \"result/\" folder if such folder doesn\\'t exist in the local repository and store the loss plot and model in this folder\\n\\n#### After Runing\\nTo clear all output files, run the following command in terminal:\\n```\\nrm -r result/\\n```\\n\\n\\n### To Get Whole dataset\\nCreate environment for loading iwildcam dataset \\n\\n0: Open a terminal \\n\\n1: ssh in tothe dsmlp environment:\\nssh <user_name>@dsmlp-login.ucsd.edu\\n\\nExample: ssh zhc023@dsmlp-login.ucsd.edu\\n\\n2: launch-scipy-ml.sh -c 8 -m 50 -i billchen24/dsc180b-project -P Always // request 8 cpu and 50 GB, and create specific environment with my docker image \\n\\n3: Create new terminal \\n\\n4: ssh -N -L 8889:127.0.0.1:16585 zhc023@dsmlp-login.ucsd.edu\\n\\n5: Go to http://localhost:8889/user/zhc023/tree/ \\n\\n### To Run the Full Experiment\\nRun the following command\\n```\\npython run.py main 10\\n```\\n\"10\" specified the maximum number of epoch the model will train, Feel free to update it to any value.\\nSome other hyperparameters can be view and change in \"run.py\"\\n\\nSimilar to the test trail, all output from can be removed by \\n```\\nrm -r result/\\n```\\n',\n",
       "  'This document provides instructions for running test trials and obtaining the whole dataset for the domain adaptation of CNN in an animal classification task. To run a test trial, download the GitHub repository and run the command \"python run.py test\" in the terminal. The code will train a custom CNN model on the sample data and generate a loss curve over epochs. The loss plot and trained model will be saved in a \"result/\" folder. To clear all output files, use the command \"rm -r result/\". To obtain the whole dataset, follow the provided steps to create an environment for loading iwildcam dataset and access it through a web browser. To run the full experiment, use the command \"python run.py main 10\" where \"10\" specifies the maximum number of epochs for training. Output files can be removed using \"rm -r result/\".'],\n",
       " 'https://github.com/TallMessiWu/dota2-drafting-backend': ['Project DOTA 2 drafting.\\n\\n[API Documentation](https://docs.opendota.com/)\\n',\n",
       "  'The given link leads to the API documentation for Project DOTA 2 drafting.'],\n",
       " 'https://github.com/DSC-Capstone/projects-2020-2021/tree/main/projects/project_67': ['# DSC180B_Capstone_Project\\nRyan Cummings,\\nGregory Thein,\\nJustin Kang,\\nProf. Shannon Ellis,\\nCode Artifact Checkpoint\\n\\n#### In this you will find our Checkpoint Code, We are in the B04 Genetics domain and this is our Capstone Project. For our Capstone Project we are looking at Alzheimer\\'s Diseased Patient\\'s Blood miRNA Data. Our Pipeline functions are seen in the all_pipeline.py file. Running the full pipeline takes multiple hours to run and implements the tools in our Genetics Pipeline (FastQC, CutAdapt, Kallisto, DESeq2). Our project implements both python and R to perform successful analysis on our dataset of blood based miRNA in which we find miRNAs with significantly changed expression level.\\n\\n#### Our repo consists of 4 folders, and 3 files (a .gitignore, the README, and the run.py). The 4 folders consist of: config, notebooks, references, src. Inside config is our data-params.json file, eda-params.json file, report-params.json, analyze-params.json, viz-params.json, and test-params.json. These files specifies the data-input and output locations/file paths that is necessary for this Checkpoint\\'s data retrieval. The eda-params file specifies the input and output of the report generated by the `eda` call, while the test-params has the names of the samples that we run the `test` keyword argument on, report-params has the input and output locations of the full report that is generated at the end of the `all` call, analyze-params has the the filepaths for the input/output of the analyze notebook that is ran when `analyze` is passed as a target, viz-params has the locations for the notebook that is generated when the `viz` param is called. Notebooks folder consists of all of our .ipynb files that we used for testing, and as a dev tool (to see what we did along the way). It also contains the notebooks that are converted for each of the targets that can be passed into our program. References has our SRARunTable from the patients we used in our project, and also contains static images that are loaded for some of the notebooks when converting to output report. The data folder is where we created the symlink between our folder and the dataset on DSMLP. The data folder (and test/testdata) will also consist of the data/out information once the `test` keyword is ran, specifically the output from Kallisto is stored here. The contents of our src folder contains our etl.py file, eda.py file, utils.py file, test_pipeline.py, and all_pipeline.py. Our etl.py file is where our file is extracting the dataset from the DSMLP\\'s /teams dataset. Utils.py is where we created a function that turns a notebook into an HTML format, which then outputs that HTML file as a report. test_pipeline and all_pipeline contain the pipeline that is created for our project, varying slightly since test is only ran on a portion while all is ran on the entire dataset!\\n\\n### Project Decisions\\n\\n- Our project focus shifted from looking at gene expression data for Alzheimer\\'s Disease patients, to observing blood sample data of patients diagnosed with Alzheimer\\'s Disease and a control group. This was done in large part because of the lack of access to the databases we initially wanted to retrieve data from\\n- After spending time searching for a viable replacement dataset on Recount2, we set on data from SRA Study SRP022043 and downloaded the data onto DSMLP from the SRA Run Selector Tool \\n- We initially implemented the dockerfile for this project based on the dockerfile used in last quarters replication and had hoped to implement TrimGalore as a new tool into our pipeline. Incompatibility issues, however, led us to drop TrimGalore as tool and stick with running Cutadapt and FastQC separately.\\n- The Kallisto reference file was originally stored in our data file in our Github but the `.gitignore` was hiding that file when we would pull the repo. We need it in order to run Kallisto so we moved it to our teams directory on DSMLP.\\n\\n\\n### Project Targets:\\n#### all\\nRuns entire pipeline on all of the data. Running `all` will run the full pipeline from scratch, this does take hours and sometimes even days to run, it can be ran from scratch but is not needed to be ran from scratch to see our results!\\n```\\n{\\n    \"outdir\": \"data/report\",\\n    \"report_in_path\": \"notebooks/Alzheimers-Biomarker-Analysis.ipynb\",\\n    \"report_out_path\": \"report/Alzheimers-Biomarker-Analysis.html\"\\n}\\n```\\n#### test\\nRuns part of pipeline on a couple fastq files. Implements fastqc and kallisto. Then generates this report!\\n```\\n{\\n  \"test_1\": \"SRR837440.fastq.gz\",\\n  \"test_2\": \"SRR837444.fastq.gz\"\\n}\\n```\\n#### data\\nIn Progress! Gets and outputs the data and generates the report as well!\\n```\\n{\\n  \"file_path\": \"/teams/DSC180A_FA20_A00/b04genetics/group_1/raw_data\"\\n}\\n```\\n#### eda\\nRuns EDA process. Makes report with data and plots figures.\\n```\\n{\\n    \"outdir\": \"data/report\",\\n    \"report_in_path\": \"notebooks/EDA.ipynb\",\\n    \"report_out_path\": \"notebooks/EDA.html\"\\n}\\n```\\n#### viz\\nRuns Visualization process. Simply outputs all the charts and graphs used in the project.\\n```\\n{\\n    \"outdir\": \"data/report\",\\n    \"report_in_path\": \"notebooks/Viz.ipynb\",\\n    \"report_out_path\": \"notebooks/Viz.html\"\\n}\\n```\\n\\n#### analyze\\nRuns the Notebook used for our Analysis portion of the project. Generating the plots that are used to explain our results.\\n```\\n{\\n    \"outdir\": \"data/report\",\\n    \"report_in_path\": \"notebooks/analyze.ipynb\",\\n    \"report_out_path\": \"notebooks/analyze.html\"\\n}\\n```\\n\\n\\n#### Running `python run.py all` will run the full pipeline from scrath, this does take hours and sometimes even days to run, it can be ran from scratch but is not needed to be ran from scratch to see our results! Other keywords that can be passed into the funciton are `test eda data viz analyze`. Running `python run.py test` is actually the most recommended one, this gives you the full pipeline experience on a fraction of the data, running in just a few minutes. Portions of the code can also be ran with `python run.py data` or `python run.py eda` or a combination of these: `python run.py data eda` etc. We also printed steps along the way to notify the user what is currently running in the pipeline. Our code assumes it is ran on the DSMLP Servers! Without running on the DSMLP Servers we would not be able to access the data, which is why it is important to be connected to the server.\\n\\n\\n\\n### Responsibilities\\n\\nRyan: \\nRyan created the Pipeline that we are using for our project so far: FastQC, Cutadapt, FastQC (2), and Kallisto. Along with formatting the Github repo to the Cookiecutter Data Science standard. \\n\\nJustin: \\nJustin worked mainly on getting the report side of the project complete. He, alongside Gregory, spent time researching what MicroRNA and biomarkers are to include as part of our background. Researching additional information about Alzheimer’s Disease was also completed. He also worked on getting the initial structure of the report completed prior to the checkpoint. \\n\\nGregory: \\nGregory, alongside Justin worked on the researching miRNA and biomarkers, and their relation to AD. Furthermore, he helped research various parameters and settings for parts of the pipeline. \\n\\nAll assisted in the implementation of the pipeline alongside editing/reviewing each other’s work. \\n',\n",
       "  \"This is a summary of the DSC180B Capstone Project. The project focuses on analyzing Alzheimer's Diseased Patient's Blood miRNA Data. The pipeline functions include FastQC, CutAdapt, Kallisto, and DESeq2. The project uses both Python and R for analysis. The repository consists of four folders: config, notebooks, references, and src. The project targets include all, test, data, eda, viz, and analyze. Running the full pipeline takes multiple hours to run. The responsibilities of the team members are also mentioned.\"],\n",
       " 'https://github.com/nickthegroot/recipe-recommendation': ['<h1 align=\"center\">\\n   <img src=\"reports/badges/ucsdseal.png\" width=20% />\\n   <img src=\"reports/badges/tigergraph.png\" width=20% />\\n\\nPersonalized Recipe Recommendation Using Heterogeneous Graphs\\n\\n</h1>\\n\\n**Authors**:\\n\\n- Nicholas DeGroot (Halıcıoğlu Data Science Institute, UC San Diego)\\n\\n## Description\\n\\nThis project was created for UCSD\\'s DSC 180: Data Science Capstone. According to the university, the course:\\n\\n> Span(s) the entire lifecycle, including assessing the problem, learning domain knowledge, collecting/cleaning data, creating a model, addressing ethical issues, designing the system, analyzing the output, and presenting the results.\\n>\\n> https://catalog.ucsd.edu/courses/DSC.html#dsc180b\\n\\n## Getting Started\\n\\nThis project is configured with `devcontainer` support. This automatically creates a fully isolated environment with all required dependencies installed.\\n\\nThe easiest way to get started with `devcontainers` is through [GitHub Codespaces](https://github.com/features/codespaces).\\n\\n1. Click [here](https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=571806935) to create a new codespace on this repository.\\n   - Alternatively, this can be done through the `gh` CLI.\\n2. Configure the codespace to your liking. We recommend the 8-core machine.\\n3. Start the codespace and connect. It might take a minute to install all the dependencies. Grab a :coffee:!\\n4. Connect to the codespace through your preferred method (browser / VS Code).\\n\\n## Testing\\n\\nThis project is setup with an array of tests using `pytest` to ensure things are working. With a working environment, run the following command.\\n\\n```\\nmake test\\n```\\n\\n### Testing on DSLMP\\n\\nFor UCSD students & staff, we\\'ve ensured that everything works on the Data Science Machine Learning Platform servers.\\n\\nThe (auto!) published Docker image contains everything you need to test the project. Under the hood, it\\'s running the same container that any `devcontainer` is.\\n\\nIn DSMLP: log in with your credentials, then run the following:\\n\\n```\\nlaunch.sh -s -i ghcr.io/nickthegroot/recipe-recommendation:main\\ncd /app\\nmake test\\n```\\n\\nThis will begin a full run of every test in the project. Currently, this includes a full pipeline test and a smaller data processing test.\\n\\n## Downloading/Preparing the Data\\n\\n1. Download the data by creating an Kaggle account and downloading the [`shuyangli94/food-com-recipes-and-user-interactions`](https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions) dataset.\\n2. Unzip the data into `data/raw`.\\n   - You should see a number of files, including `data/raw/RAW_interactions.csv` and `data/raw/RAW_recipes.csv`\\n3. Run `make data` to clean the data into its cleaned form.\\n\\n## Running\\n\\nAll models can be trained using `python src/cli/train.py`.\\n\\n- Run `python src/cli/train.py --help` for all configuration options\\n- In general, all models can be trained via `python src/cli/train.py --model {model}`\\n  - For example, `LightGCN` is trained with `python src/cli/train.py --model LightGCN`\\n',\n",
       "  \"This project is about personalized recipe recommendation using heterogeneous graphs. It was created for UCSD's DSC 180: Data Science Capstone course. The project provides instructions on how to get started, run tests, download and prepare the data, and train the models.\"],\n",
       " 'https://github.com/mjw49/DSC180B-Quarter-2-Project': [\"# Running the Project\\nFor building, please run the commands below in this order \\n\\n- `launch-scipy-ml.sh -i mjw49/q1project`\\n- `git clone https://github.com/mjw49/DSC180B-Quarter-2-Project.git`\\n- `python run.py test`\\n- `python run.py sampling_city_single (takes around 10 minutes to run)`\\n\\n# Obtaining the Data Locally\\n\\nThe raw data for this project is obtainable from the website for Stanford's Network Analysis Project (SNAP): http://snap.stanford.edu/higher-order/data.html\\n\\nOnce downloaded, extract the compressed zip file and drop the file into the `data/raw` folder.\\n\\n\",\n",
       "  \"To run the project, follow these steps in order:\\n1. Run `launch-scipy-ml.sh -i mjw49/q1project`\\n2. Clone the repository using `git clone https://github.com/mjw49/DSC180B-Quarter-2-Project.git`\\n3. Execute `python run.py test`\\n4. Run `python run.py sampling_city_single` (this step takes approximately 10 minutes)\\n\\nTo obtain the data locally, download it from Stanford's Network Analysis Project (SNAP) website: http://snap.stanford.edu/higher-order/data.html. Extract the compressed zip file and place it in the `data/raw` folder.\"],\n",
       " 'https://github.com/camille-004/Graph-HSCN': ['<h1 align=\"center\">\\nGraphHSCN: Heterogenized Spectral Cluster Network for Long Range Representation Learning</h1>\\n<div align=\"center\">\\n\\n  <a href=\"https://camille-004.github.io/\">Camille Dunning</a>, <a href=\"https://www.linkedin.com/in/zhishang-luo-a51a8120b/\">Zhishang Luo</a>, <a href=\"https://dylantao.github.io/\">Sirui Tao</a>\\n  <p><a href=\"https://datascience.ucsd.edu/\">Halıcıoğlu Data Science Institute</a>, UC San Diego, La Jolla, CA</p>\\n</div>\\n\\n<p align=\"center\">\\n  <a href=\"https://drive.google.com/file/d/1kODg7Qw4hAj1e2Ct91R_tvom8MHdeGln/view\" alt=\"Paper\">\\n        <img src=\"https://img.shields.io/badge/Project-Paper-%238affca?style=plastic\" /></a>\\n        \\n  <a href=\"https://graphhscn.github.io//\" alt=\"Website\">\\n        <img src=\"https://img.shields.io/badge/Project-Website-%238affca?style=plastic\" /></a>\\n        \\n  <a href=\"https://github.com/camille-004/Graph-HSCN/actions/workflows/build-and-push.yml\" alt=\"Build\">\\n        <img src=\"https://github.com/camille-004/Graph-HSCN/actions/workflows/build-and-push.yml/badge.svg\" /></a>\\n\\n</p>\\n<hr/>\\n\\n\\n<!-- [![Paper (First Draft)](https://img.shields.io/badge/Project-Paper-9cf)](https://drive.google.com/file/d/1kODg7Qw4hAj1e2Ct91R_tvom8MHdeGln/view) -->\\n\\n## :rocket: Highlights and Contributions\\n\\nTODO: Flowchart figure\\n\\n>**<p align=\"justify\"> Abstract:** *Graph Neural Networks (GNNs) have gained tremendous popularity for their potential to effectively learn from graph-structured data, commonly encountered in real-world applications. However, most of these models, based on the message-passing paradigm (interactions within a neighborhood of a few nodes), can only handle local interactions within a graph. When we enforce the models to use information from far away nodes, we will encounter two major issues: oversmoothing & oversquashing. Architectures such as the transformer and diffusion models are introduced to solve this; although transformers are powerful, they require significant computational resources for both training and inference, thereby limiting their scalability, particularly for graphs with long-term dependencies. Hence, this paper proposes GraphHSCN—a Heterogenized Spectral Cluster Network, a message-passing-based approach specifically designed for capturing long-range interaction. On our first iteration of ablation studies, we observe reduced time complexities compared to SAN, the most popular graph transformer model, yet comparable performance in graph-level prediction tasks.*\\n\\n### Main Contributions\\n1. **Graph coarsening via spectral clustering**: We propose a scheme to coarsen graph representation via spectral clustering with the relaxed formulation of the MinCUT problem, as presented in the [paper](https://arxiv.org/abs/1907.00481) from Bianchi et. al. We observe the structural patterns uncovered by SC reveal which long-range virtual connections should be made.\\n2. **New connections learned by a heterogeneous network**: We create an intra-cluster connection with a virtual node, and learn the new relationship as a graph indepdenent of the original graph. A heterogeneous convolutional network is trained on these separate relations, further coarsening the representations. On our set of ablation studies, and after hyperparameter tuning, Graph-HSCN out-performs the traditional message-passing architectures by up to 10 percent, achieving metrics similar to those of SAN while reducing the time complexity.\\n\\n## Getting Started\\n\\n### Prerequisites\\nTo set up the environment and install all dependencies, run `make env`. The `logs` and `datasets` directories will be created automatically at the project level.\\n  \\n### `.devcontainers` Support\\nTODO\\n### Running with CLI\\nTODO\\n### Running in Prefect UI\\nTODO\\n\\n<hr/>\\n\\n## Hyperparameter Tuning & Results\\nTODO\\n\\n<hr/>\\n\\n## Contact\\nFeel free to open an issue on this repository or e-mail adunning@ucsd.edu.\\n  \\n## Acknowledgements\\nThe code in this project is heavily adapted and modified from the following repositories:\\n1. [Long Range Graph Benchmark](https://github.com/vijaydwivedi75/lrgb)\\n2. [torch_geometric GraphGym](https://github.com/pyg-team/pytorch_geometric/tree/master/graphgym)\\n3. [Hierarchical Graph Net](https://github.com/rampasek/HGNet)\\n',\n",
       "  'The paper titled \"GraphHSCN: Heterogenized Spectral Cluster Network for Long Range Representation Learning\" proposes a message-passing-based approach called GraphHSCN for capturing long-range interactions in graph-structured data. The authors introduce a scheme to coarsen graph representation using spectral clustering and create new connections using a heterogeneous network. GraphHSCN outperforms traditional message-passing architectures in graph-level prediction tasks while reducing time complexity. The paper provides details on getting started with the code, hyperparameter tuning, and contact information.'],\n",
       " 'https://github.com/bliu8923/dsc180b-project': ['## GNN Performance on Long Range Node Classification and Graph Classification\\n\\nThis repository holds the code to test 4 different neural network architectures\\non 2 different long range datasets. \\n\\nNetwork architectures can be found under src/models, and test data (Cora) can\\nbe found under the test directory.\\n\\nTo test the model\\'s performance on a small dataset, use the docker repo b6liu/dsc180b (cpu or gpu for tag) and run:\\n```azure\\npython run.py --test True --bz (number)\\n```\\nIF ON DSMLP: Run a smaller bz if GAN errors, defaults to 32 for test. Also, we recommend using\\n16+ GB of ram as the networks tend to have large numbers of parameters (especially for GAN/SAN).\\n\\nWe would also recommend a GPU for running these tests/benchmarks, in this case, you should pull\\nthe GPU docker image that has Cuda 11.7 support. (b6liu/dsc180b:gpu)\\n\\nDifferent parameters can be run on the file as well.\\n\\n```--datatype```: Where to extract dataset (LRGB for pascal and peptides, 3D for PSB)\\n\\n```--dataset```: Dataset to run, currently only support all LRGB datasets. Defaults to PascalVOC-SP\\n\\n```--model```: Model to run, currently GNN, GatedGCN, GIN, GAT, SAN\\n\\n```--bz```: Batch size, defaults to 32\\n\\n```--epoch```: Number of epochs to run the model\\n\\n```--criterion```: Loss function, defaults to cross entropy \\n\\n```--optimizer```: Optimizer to use, defaults to adam\\n\\n```--lr```: Learning rate, defaults to 0.0005\\n\\n```--momentum```: Momentum term, defaults to 0.9\\n\\n```--weight-decay```: Weight decay term, defaults to 5e-6\\n\\n```--task```: task for network, defaults to node level\\n\\n```--metric```: Accuracy metric to perform, defaults to macro f1, support for AP\\n\\n```--gamma```: (SAN only) sparcity of attention, 0 indicates sparse attention while 1 indicates no bias\\n\\n```--hidden```: Hidden parameters, made after linearly encoding data\\n\\n```--scheduler```: Enable or disable scheduling on plateau\\n\\nShortcut methods have been added:\\n\\n```--add_edges```: ratio of edges to be created (fake, random connections between nodes)\\n\\n```--encode```: Positional encoding, we support \"lap\" for laplacian encoding or \"walk\" for RWSE\\n\\n```--encode_k```: number of features to be added by encoding\\n\\nAnd for partial (GAT supported, distance weighting):\\n\\n```--partial```: Number of distance weighted layers, should be 1\\n\\n```--space```: Spacial representation of data (2 for 2d, 3 for 3d, etc)\\n\\n```--k```: for KNN in distance weighting\\n\\nThese are the recommended commands to run all datasets on the best models:\\n\\n```python run.py --model san --dataset PascalVOC-SP --metric macrof1  --add_edges 1 --encode lap --encode_k 10```\\n\\n```python run.py --model san --dataset peptides-func --task graph --metric ap  --encode lap --encode_k 10```\\n\\n```python run.py --model san --datatype 3d --dataset psb --metric ap --encode walk --encode_k 10 --add_edges 1```\\n\\nYou can find the results in the results folder, under the model, timestamped.\\n\\n### Citations\\nThank you to Long Range Graph Benchmarks for the SAN implementation and datasets.\\n```\\n@article{dwivedi2022LRGB,\\n  title={Long Range Graph Benchmark}, \\n  author={Dwivedi, Vijay Prakash and Rampášek, Ladislav and Galkin, Mikhail and Parviz, Ali and Wolf, Guy and Luu, Anh Tuan and Beaini, Dominique},\\n  journal={arXiv:2206.08164},\\n  year={2022}\\n}\\n```\\nAnd to GraphGPS, for many loss functions, SAN, and encoders:\\n```\\n@article{rampasek2022GPS,\\n  title={{Recipe for a General, Powerful, Scalable Graph Transformer}}, \\n  author={Ladislav Ramp\\\\\\'{a}\\\\v{s}ek and Mikhail Galkin and Vijay Prakash Dwivedi and Anh Tuan Luu and Guy Wolf and Dominique Beaini},\\n  journal={Advances in Neural Information Processing Systems},\\n  volume={35},\\n  year={2022}\\n}\\n```',\n",
       "  'This repository contains code for testing four different neural network architectures on two long-range datasets. The network architectures can be found in the src/models directory, and the test data (Cora) is located in the test directory. To test the model\\'s performance on a small dataset, you can use the provided docker repository and run the command \"python run.py --test True --bz (number)\". It is recommended to have at least 16GB of RAM and a GPU for running these tests. Different parameters can be specified when running the file, such as dataset type, model, batch size, number of epochs, loss function, optimizer, learning rate, momentum term, weight decay term, task for network, accuracy metric, sparsity of attention (for SAN), hidden parameters, scheduler enable/disable option, and shortcut methods. The recommended commands to run all datasets on the best models are also provided. The results can be found in the results folder under the corresponding model and timestamped. Citations are given to Long Range Graph Benchmarks for the SAN implementation and datasets and to GraphGPS for various loss functions, SAN, and encoders.']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.read_pickle(\"../data/report_readme_dict.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
